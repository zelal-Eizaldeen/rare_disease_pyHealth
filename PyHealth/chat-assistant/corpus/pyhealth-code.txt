The code content for pyhealthHere is the code content for utils.py:
import os
import pickle
import random

import numpy as np
import torch


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)


def create_directory(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)


def load_pickle(filename):
    with open(filename, "rb") as f:
        return pickle.load(f)


def save_pickle(data, filename):
    with open(filename, "wb") as f:
        pickle.dump(data, f)


Here is the code content for __init__.py:
import logging
import os
from pathlib import Path
import sys

__version__ = "1.1.4"

# package-level cache path
BASE_CACHE_PATH = os.path.join(str(Path.home()), ".cache/pyhealth/")
if not os.path.exists(BASE_CACHE_PATH):
    os.makedirs(BASE_CACHE_PATH)

# logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter("%(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)


Here is the code content for tokenizer.py:
from typing import List, Optional, Tuple


class Vocabulary:
    """Vocabulary class for mapping between tokens and indices."""

    def __init__(self, tokens: List[str], special_tokens: Optional[List[str]] = None):
        """Initializes the vocabulary.

        This function initializes the vocabulary by adding the special tokens first
        and then the tokens. The order of the tokens is preserved.

        If <unk> is not provided in the special_tokens, then the tokenizer
        will raise an exception if an unknown token is encountered.

        If padding is performed on the input tokens, padding token <pad> should always
        be added to the special_tokens.

        Args:
            tokens: List[str], list of tokens in the vocabulary.
            special_tokens: Optional[List[str]], list of special tokens to add to
                the vocabulary. (e.g., <pad>, <unk>). Default is empty list.

        Note:
            If vocabulary is used to convert output labels to indices, one should
                be very careful about the special tokens.
        """
        if special_tokens is None:
            special_tokens = []
        all_tokens = special_tokens + tokens
        self.token2idx = {}
        self.idx2token = {}
        self.idx = 0
        for token in all_tokens:
            self.add_token(token)

    def add_token(self, token):
        """Adds a token to the vocabulary."""
        if token not in self.token2idx:
            self.token2idx[token] = self.idx
            self.idx2token[self.idx] = token
            self.idx += 1

    def __call__(self, token):
        """Retrieves the index of the token.

        Note that if the token is not in the vocabulary, this function will try to
        return the index of <unk>. If <unk> is not in the vocabulary,
        an exception will be raised.
        """
        if token not in self.token2idx:
            if "<unk>" in self.token2idx:
                return self.token2idx["<unk>"]
            else:
                raise ValueError("Unknown token: {}".format(token))
        return self.token2idx[token]

    def __len__(self):
        """Returns the size of the vocabulary."""
        return len(self.token2idx)

    def __contains__(self, token):
        return token in self.token2idx


class Tokenizer:
    """Tokenizer class for converting tokens to indices and vice versa.

    This class will build a vocabulary from the provided tokens and provide the
    functionality to convert tokens to indices and vice versa. This class also
    provides the functionality to tokenize a batch of data.
    
    Examples:
            >>> from pyhealth.tokenizer import Tokenizer
            >>> token_space = ['A01A', 'A02A', 'A02B', 'A02X', 'A03A', 'A03B', 'A03C', 'A03D', 'A03E', \
            ...                'A03F', 'A04A', 'A05A', 'A05B', 'A05C', 'A06A', 'A07A', 'A07B', 'A07C', \
            ...                'A07D', 'A07E', 'A07F', 'A07X', 'A08A', 'A09A', 'A10A', 'A10B', 'A10X', \
            ...                'A11A', 'A11B', 'A11C', 'A11D', 'A11E', 'A11G', 'A11H', 'A11J', 'A12A', \
            ...                'A12B', 'A12C', 'A13A', 'A14A', 'A14B', 'A16A']
            >>> tokenizer = Tokenizer(tokens=token_space, special_tokens=["<pad>", "<unk>"])
    """

    def __init__(self, tokens: List[str], special_tokens: Optional[List[str]] = None):
        """Initializes the tokenizer.

        Args:
            tokens: List[str], list of tokens in the vocabulary.
            special_tokens: Optional[List[str]], list of special tokens to add to
                the vocabulary. (e.g., <pad>, <unk>). Default is empty list.
        """
        self.vocabulary = Vocabulary(tokens=tokens, special_tokens=special_tokens)

    def get_padding_index(self):
        """Returns the index of the padding token."""
        return self.vocabulary("<pad>")

    def get_vocabulary_size(self):
        """Returns the size of the vocabulary.

        Examples:
            >>> tokenizer.get_vocabulary_size()
            44
        """
        return len(self.vocabulary)

    def convert_tokens_to_indices(self, tokens: List[str]) -> List[int]:
        """Converts a list of tokens to indices.
        
        Examples:
            >>> tokens = ['A03C', 'A03D', 'A03E', 'A03F', 'A04A', 'A05A', 'A05B', 'B035', 'C129']
            >>> indices = tokenizer.convert_tokens_to_indices(tokens)
            >>> print(indices)
            [8, 9, 10, 11, 12, 13, 14, 1, 1]
        """
        return [self.vocabulary(token) for token in tokens]

    def convert_indices_to_tokens(self, indices: List[int]) -> List[str]:
        """Converts a list of indices to tokens.
        
        Examples:
            >>> indices = [0, 1, 2, 3, 4, 5]
            >>> tokens = tokenizer.convert_indices_to_tokens(indices)
            >>> print(tokens)
            ['<pad>', '<unk>', 'A01A', 'A02A', 'A02B', 'A02X']
        """
        return [self.vocabulary.idx2token[idx] for idx in indices]

    def batch_encode_2d(
        self,
        batch: List[List[str]],
        padding: bool = True,
        truncation: bool = True,
        max_length: int = 512,
    ):
        """Converts a list of lists of tokens (2D) to indices.

        Args:
            batch: List of lists of tokens to convert to indices.
            padding: whether to pad the tokens to the max number of tokens in
                the batch (smart padding).
            truncation: whether to truncate the tokens to max_length.
            max_length: maximum length of the tokens. This argument is ignored
                if truncation is False.
        
        Examples:
            >>> tokens = [
            ...     ['A03C', 'A03D', 'A03E', 'A03F'],
            ...     ['A04A', 'B035', 'C129']
            ... ]

            >>> indices = tokenizer.batch_encode_2d(tokens)
            >>> print ('case 1:', indices)
            case 1: [[8, 9, 10, 11], [12, 1, 1, 0]]

            >>> indices = tokenizer.batch_encode_2d(tokens, padding=False)
            >>> print ('case 2:', indices)
            case 2: [[8, 9, 10, 11], [12, 1, 1]]

            >>> indices = tokenizer.batch_encode_2d(tokens, max_length=3)
            >>> print ('case 3:', indices)
            case 3: [[9, 10, 11], [12, 1, 1]]
        """

        if truncation:
            batch = [tokens[-max_length:] for tokens in batch]
        if padding:
            batch_max_length = max([len(tokens) for tokens in batch])
            batch = [
                tokens + ["<pad>"] * (batch_max_length - len(tokens))
                for tokens in batch
            ]
        return [[self.vocabulary(token) for token in tokens] for tokens in batch]

    def batch_decode_2d(
        self,
        batch: List[List[int]],
        padding: bool = False,
    ):
        """Converts a list of lists of indices (2D) to tokens.

        Args:
            batch: List of lists of indices to convert to tokens.
            padding: whether to keep the padding tokens from the tokens.
        
        Examples:
            >>> indices = [
            ...     [8, 9, 10, 11],
            ...     [12, 1, 1, 0]
            ... ]

            >>> tokens = tokenizer.batch_decode_2d(indices)
            >>> print ('case 1:', tokens)
            case 1: [['A03C', 'A03D', 'A03E', 'A03F'], ['A04A', '<unk>', '<unk>']]

            >>> tokens = tokenizer.batch_decode_2d(indices, padding=True)
            >>> print ('case 2:', tokens)
            case 2: [['A03C', 'A03D', 'A03E', 'A03F'], ['A04A', '<unk>', '<unk>', '<pad>']]
        """

        batch = [[self.vocabulary.idx2token[idx] for idx in tokens] for tokens in batch]
        if not padding:
            return [[token for token in tokens if token != "<pad>"] for tokens in batch]
        return batch

    def batch_encode_3d(
        self,
        batch: List[List[List[str]]],
        padding: Tuple[bool, bool] = (True, True),
        truncation: Tuple[bool, bool] = (True, True),
        max_length: Tuple[int, int] = (10, 512),
    ):
        """Converts a list of lists of lists of tokens (3D) to indices.

        Args:
            batch: List of lists of lists of tokens to convert to indices.
            padding: a tuple of two booleans indicating whether to pad the tokens
                to the max number of tokens and visits (smart padding).
            truncation: a tuple of two booleans indicating whether to truncate the
                tokens to the corresponding element in max_length
            max_length: a tuple of two integers indicating the maximum length of the
                tokens along the first and second dimension. This argument is ignored
                if truncation is False.
        
        Examples:
                >>> tokens = [
                ...     [
                ...         ['A03C', 'A03D', 'A03E', 'A03F'],
                ...         ['A08A', 'A09A'],
                ...     ],
                ...     [
                ...         ['A04A', 'B035', 'C129'],
                ...     ]
                ... ]

                >>> indices = tokenizer.batch_encode_3d(tokens)
                >>> print ('case 1:', indices)
                case 1: [[[8, 9, 10, 11], [24, 25, 0, 0]], [[12, 1, 1, 0], [0, 0, 0, 0]]]

                >>> indices = tokenizer.batch_encode_3d(tokens, padding=(False, True))
                >>> print ('case 2:', indices)
                case 2: [[[8, 9, 10, 11], [24, 25, 0, 0]], [[12, 1, 1, 0]]]

                >>> indices = tokenizer.batch_encode_3d(tokens, padding=(True, False))
                >>> print ('case 3:', indices)
                case 3: [[[8, 9, 10, 11], [24, 25]], [[12, 1, 1], [0]]]

                >>> indices = tokenizer.batch_encode_3d(tokens, padding=(False, False))
                >>> print ('case 4:', indices)
                case 4: [[[8, 9, 10, 11], [24, 25]], [[12, 1, 1]]]

                >>> indices = tokenizer.batch_encode_3d(tokens, max_length=(2,2))
                >>> print ('case 5:', indices)
                case 5: [[[10, 11], [24, 25]], [[1, 1], [0, 0]]]
        """
        if truncation[0]:
            batch = [tokens[-max_length[0] :] for tokens in batch]
        if truncation[1]:
            batch = [
                [tokens[-max_length[1] :] for tokens in visits] for visits in batch
            ]
        if padding[0]:
            batch_max_length = max([len(tokens) for tokens in batch])
            batch = [
                tokens + [["<pad>"]] * (batch_max_length - len(tokens))
                for tokens in batch
            ]
        if padding[1]:
            batch_max_length = max(
                [max([len(tokens) for tokens in visits]) for visits in batch]
            )
            batch = [
                [
                    tokens + ["<pad>"] * (batch_max_length - len(tokens))
                    for tokens in visits
                ]
                for visits in batch
            ]
        return [
            [[self.vocabulary(token) for token in tokens] for tokens in visits]
            for visits in batch
        ]

    def batch_decode_3d(
        self,
        batch: List[List[List[int]]],
        padding: bool = False,
    ):
        """Converts a list of lists of lists of indices (3D) to tokens.

        Args:
            batch: List of lists of lists of indices to convert to tokens.
            padding: whether to keep the padding tokens from the tokens.
        
        Examples:
            >>> indices = [
            ...     [
            ...         [8, 9, 10, 11], 
            ...         [24, 25, 0, 0]
            ...     ], 
            ...     [
            ...         [12, 1, 1, 0], 
            ...         [0, 0, 0, 0]
            ...     ]
            ... ]

            >>> tokens = tokenizer.batch_decode_3d(indices)
            >>> print ('case 1:', tokens)
            case 1: [[['A03C', 'A03D', 'A03E', 'A03F'], ['A08A', 'A09A']], [['A04A', '<unk>', '<unk>']]]

            >>> tokens = tokenizer.batch_decode_3d(indices, padding=True)
            >>> print ('case 2:', tokens)
            case 2: [[['A03C', 'A03D', 'A03E', 'A03F'], ['A08A', 'A09A', '<pad>', '<pad>']], [['A04A', '<unk>', '<unk>', '<pad>'], ['<pad>', '<pad>', '<pad>', '<pad>']]]
        """
        batch = [
            self.batch_decode_2d(batch=visits, padding=padding) for visits in batch
        ]
        if not padding:
            batch = [[visit for visit in visits if visit != []] for visits in batch]
        return batch


if __name__ == "__main__":
    tokens = ["a", "b", "c", "d", "e", "f", "g", "h"]
    tokenizer = Tokenizer(tokens=tokens, special_tokens=["<pad>", "<unk>"])
    print(tokenizer.get_vocabulary_size())

    out = tokenizer.convert_tokens_to_indices(["a", "b", "c", "d", "e", "z"])
    print(out)
    print(tokenizer.convert_indices_to_tokens(out))

    out = tokenizer.batch_encode_2d(
        [["a", "b", "c", "e", "z"], ["a", "b", "c", "d", "e", "z"]],
        padding=True,
        truncation=True,
        max_length=10,
    )
    print(out)
    print(tokenizer.batch_decode_2d(out, padding=False))

    out = tokenizer.batch_encode_3d(
        [
            [["a", "b", "c", "e", "z"], ["a", "b", "c", "d", "e", "z"]],
            [["a", "b", "c", "e", "z"], ["a", "b", "c", "d", "e", "z"], ["c", "f"]],
        ],
        padding=(True, True),
        truncation=(True, True),
        max_length=(10, 10),
    )
    print(out)
    print(tokenizer.batch_decode_3d(out, padding=False))


Here is the code content for trainer.py:
import logging
import os
from datetime import datetime
from typing import Callable, Dict, List, Optional, Type

import numpy as np
import torch
from torch import nn
from torch.optim import Optimizer
from torch.utils.data import DataLoader
from tqdm import tqdm
from tqdm.autonotebook import trange

from pyhealth.metrics import (binary_metrics_fn, multiclass_metrics_fn,
                              multilabel_metrics_fn)
from pyhealth.utils import create_directory

logger = logging.getLogger(__name__)


def is_best(best_score: float, score: float, monitor_criterion: str) -> bool:
    if monitor_criterion == "max":
        return score > best_score
    elif monitor_criterion == "min":
        return score < best_score
    else:
        raise ValueError(f"Monitor criterion {monitor_criterion} is not supported")


def set_logger(log_path: str) -> None:
    create_directory(log_path)
    log_filename = os.path.join(log_path, "log.txt")
    handler = logging.FileHandler(log_filename)
    formatter = logging.Formatter("%(asctime)s %(message)s", "%Y-%m-%d %H:%M:%S")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    return


def get_metrics_fn(mode: str) -> Callable:
    if mode == "binary":
        return binary_metrics_fn
    elif mode == "multiclass":
        return multiclass_metrics_fn
    elif mode == "multilabel":
        return multilabel_metrics_fn
    else:
        raise ValueError(f"Mode {mode} is not supported")


class Trainer:
    """Trainer for PyTorch models.

    Args:
        model: PyTorch model.
        checkpoint_path: Path to the checkpoint. Default is None, which means
            the model will be randomly initialized.
        metrics: List of metric names to be calculated. Default is None, which
            means the default metrics in each metrics_fn will be used.
        device: Device to be used for training. Default is None, which means
            the device will be GPU if available, otherwise CPU.
        enable_logging: Whether to enable logging. Default is True.
        output_path: Path to save the output. Default is "./output".
        exp_name: Name of the experiment. Default is current datetime.
    """

    def __init__(
        self,
        model: nn.Module,
        checkpoint_path: Optional[str] = None,
        metrics: Optional[List[str]] = None,
        device: Optional[str] = None,
        enable_logging: bool = True,
        output_path: Optional[str] = None,
        exp_name: Optional[str] = None,
    ):
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"

        self.model = model
        self.metrics = metrics
        self.device = device

        # set logger
        if enable_logging:
            if output_path is None:
                output_path = os.path.join(os.getcwd(), "output")
            if exp_name is None:
                exp_name = datetime.now().strftime("%Y%m%d-%H%M%S")
            self.exp_path = os.path.join(output_path, exp_name)
            set_logger(self.exp_path)
        else:
            self.exp_path = None

        # set device
        self.model.to(self.device)

        # logging
        logger.info(self.model)
        logger.info(f"Metrics: {self.metrics}")
        logger.info(f"Device: {self.device}")

        # load checkpoint
        if checkpoint_path is not None:
            logger.info(f"Loading checkpoint from {checkpoint_path}")
            self.load_ckpt(checkpoint_path)

        logger.info("")
        return

    def train(
        self,
        train_dataloader: DataLoader,
        val_dataloader: Optional[DataLoader] = None,
        test_dataloader: Optional[DataLoader] = None,
        epochs: int = 5,
        optimizer_class: Type[Optimizer] = torch.optim.Adam,
        optimizer_params: Optional[Dict[str, object]] = None,
        weight_decay: float = 0.0,
        max_grad_norm: float = None,
        monitor: Optional[str] = None,
        monitor_criterion: str = "max",
        load_best_model_at_last: bool = True,
    ):
        """Trains the model.

        Args:
            train_dataloader: Dataloader for training.
            val_dataloader: Dataloader for validation. Default is None.
            test_dataloader: Dataloader for testing. Default is None.
            epochs: Number of epochs. Default is 5.
            optimizer_class: Optimizer class. Default is torch.optim.Adam.
            optimizer_params: Parameters for the optimizer. Default is {"lr": 1e-3}.
            weight_decay: Weight decay. Default is 0.0.
            max_grad_norm: Maximum gradient norm. Default is None.
            monitor: Metric name to monitor. Default is None.
            monitor_criterion: Criterion to monitor. Default is "max".
            load_best_model_at_last: Whether to load the best model at the last.
                Default is True.
        """
        if optimizer_params is None:
            optimizer_params = {"lr": 1e-3}

        # logging
        logger.info("Training:")
        logger.info(f"Batch size: {train_dataloader.batch_size}")
        logger.info(f"Optimizer: {optimizer_class}")
        logger.info(f"Optimizer params: {optimizer_params}")
        logger.info(f"Weight decay: {weight_decay}")
        logger.info(f"Max grad norm: {max_grad_norm}")
        logger.info(f"Val dataloader: {val_dataloader}")
        logger.info(f"Monitor: {monitor}")
        logger.info(f"Monitor criterion: {monitor_criterion}")
        logger.info(f"Epochs: {epochs}")

        # set optimizer
        param = list(self.model.named_parameters())
        no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in param if not any(nd in n for nd in no_decay)],
                "weight_decay": weight_decay,
            },
            {
                "params": [p for n, p in param if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]
        optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)

        # initialize
        data_iterator = iter(train_dataloader)
        best_score = -1 * float("inf") if monitor_criterion == "max" else float("inf")
        steps_per_epoch = len(train_dataloader)
        global_step = 0

        # epoch training loop
        for epoch in range(epochs):
            training_loss = []
            self.model.zero_grad()
            self.model.train()
            # batch training loop
            logger.info("")
            for _ in trange(
                steps_per_epoch,
                desc=f"Epoch {epoch} / {epochs}",
                smoothing=0.05,
            ):
                try:
                    data = next(data_iterator)
                except StopIteration:
                    data_iterator = iter(train_dataloader)
                    data = next(data_iterator)
                # forward
                output = self.model(**data)
                loss = output["loss"]
                # backward
                loss.backward()
                if max_grad_norm is not None:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), max_grad_norm
                    )
                # update
                optimizer.step()
                optimizer.zero_grad()
                training_loss.append(loss.item())
                global_step += 1
            # log and save
            logger.info(f"--- Train epoch-{epoch}, step-{global_step} ---")
            logger.info(f"loss: {sum(training_loss) / len(training_loss):.4f}")
            if self.exp_path is not None:
                self.save_ckpt(os.path.join(self.exp_path, "last.ckpt"))

            # validation
            if val_dataloader is not None:
                scores = self.evaluate(val_dataloader)
                logger.info(f"--- Eval epoch-{epoch}, step-{global_step} ---")
                for key in scores.keys():
                    logger.info("{}: {:.4f}".format(key, scores[key]))
                # save best model
                if monitor is not None:
                    score = scores[monitor]
                    if is_best(best_score, score, monitor_criterion):
                        logger.info(
                            f"New best {monitor} score ({score:.4f}) "
                            f"at epoch-{epoch}, step-{global_step}"
                        )
                        best_score = score
                        if self.exp_path is not None:
                            self.save_ckpt(os.path.join(self.exp_path, "best.ckpt"))

        # load best model
        if load_best_model_at_last and self.exp_path is not None and os.path.isfile(os.path.join(self.exp_path, "best.ckpt")):
            logger.info("Loaded best model")
            self.load_ckpt(os.path.join(self.exp_path, "best.ckpt"))

        # test
        if test_dataloader is not None:
            scores = self.evaluate(test_dataloader)
            logger.info(f"--- Test ---")
            for key in scores.keys():
                logger.info("{}: {:.4f}".format(key, scores[key]))

        return

    def inference(self, dataloader, additional_outputs=None, return_patient_ids=False) -> Dict[str, float]:
        """Model inference.

        Args:
            dataloader: Dataloader for evaluation.
            additional_outputs: List of additional output to collect.
                Defaults to None ([]).

        Returns:
            y_true_all: List of true labels.
            y_prob_all: List of predicted probabilities.
            loss_mean: Mean loss over batches.
            additional_outputs (only if requested): Dict of additional results.
            patient_ids (only if requested): List of patient ids in the same order as y_true_all/y_prob_all.
        """
        loss_all = []
        y_true_all = []
        y_prob_all = []
        patient_ids = []
        if additional_outputs is not None:
            additional_outputs = {k: [] for k in additional_outputs}
        for data in tqdm(dataloader, desc="Evaluation"):
            self.model.eval()
            with torch.no_grad():
                output = self.model(**data)
                loss = output["loss"]
                y_true = output["y_true"].cpu().numpy()
                y_prob = output["y_prob"].cpu().numpy()
                loss_all.append(loss.item())
                y_true_all.append(y_true)
                y_prob_all.append(y_prob)
                if additional_outputs is not None:
                    for key in additional_outputs.keys():
                        additional_outputs[key].append(output[key].cpu().numpy())
            if return_patient_ids:
                patient_ids.extend(data["patient_id"])
        loss_mean = sum(loss_all) / len(loss_all)
        y_true_all = np.concatenate(y_true_all, axis=0)
        y_prob_all = np.concatenate(y_prob_all, axis=0)
        outputs = [y_true_all, y_prob_all, loss_mean]
        if additional_outputs is not None:
            additional_outputs = {key: np.concatenate(val)
                                  for key, val in additional_outputs.items()}
            outputs.append(additional_outputs)
        if return_patient_ids:
            outputs.append(patient_ids)
        return outputs

    def evaluate(self, dataloader) -> Dict[str, float]:
        """Evaluates the model.

        Args:
            dataloader: Dataloader for evaluation.

        Returns:
            scores: a dictionary of scores.
        """
        y_true_all, y_prob_all, loss_mean = self.inference(dataloader)

        mode = self.model.mode
        metrics_fn = get_metrics_fn(mode)
        scores = metrics_fn(y_true_all, y_prob_all, metrics=self.metrics)
        scores["loss"] = loss_mean
        return scores

    def save_ckpt(self, ckpt_path: str) -> None:
        """Saves the model checkpoint."""
        state_dict = self.model.state_dict()
        torch.save(state_dict, ckpt_path)
        return

    def load_ckpt(self, ckpt_path: str) -> None:
        """Saves the model checkpoint."""
        state_dict = torch.load(ckpt_path, map_location=self.device)
        self.model.load_state_dict(state_dict)
        return


if __name__ == "__main__":
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, Dataset
    from torchvision import datasets, transforms

    from pyhealth.datasets.utils import collate_fn_dict

    class MNISTDataset(Dataset):
        def __init__(self, train=True):
            transform = transforms.Compose(
                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]
            )
            self.dataset = datasets.MNIST(
                "../data", train=train, download=True, transform=transform
            )

        def __getitem__(self, index):
            x, y = self.dataset[index]
            return {"x": x, "y": y}

        def __len__(self):
            return len(self.dataset)

    class Model(nn.Module):
        def __init__(self):
            super(Model, self).__init__()
            self.mode = "multiclass"
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.conv1 = nn.Conv2d(1, 32, 3, 1)
            self.conv2 = nn.Conv2d(32, 64, 3, 1)
            self.dropout1 = nn.Dropout(0.25)
            self.dropout2 = nn.Dropout(0.5)
            self.fc1 = nn.Linear(9216, 128)
            self.fc2 = nn.Linear(128, 10)
            self.loss = nn.CrossEntropyLoss()

        def forward(self, x, y, **kwargs):
            x = torch.stack(x, dim=0).to(self.device)
            y = torch.tensor(y).to(self.device)
            x = self.conv1(x)
            x = torch.relu(x)
            x = self.conv2(x)
            x = torch.relu(x)
            x = torch.max_pool2d(x, 2)
            x = self.dropout1(x)
            x = torch.flatten(x, 1)
            x = self.fc1(x)
            x = torch.relu(x)
            x = self.dropout2(x)
            x = self.fc2(x)
            loss = self.loss(x, y)
            y_prob = torch.softmax(x, dim=1)
            return {"loss": loss, "y_prob": y_prob, "y_true": y}

    train_dataset = MNISTDataset(train=True)
    val_dataset = MNISTDataset(train=False)

    train_dataloader = DataLoader(
        train_dataset, collate_fn=collate_fn_dict, batch_size=64, shuffle=True
    )
    val_dataloader = DataLoader(
        val_dataset, collate_fn=collate_fn_dict, batch_size=64, shuffle=False
    )

    model = Model()

    trainer = Trainer(model, device="cuda" if torch.cuda.is_available() else "cpu")
    trainer.train(
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        monitor="accuracy",
        epochs=5,
        test_dataloader=val_dataloader,
    )


Here is the code content for __init__.py:
from .data import (
    Event,
    Visit,
    Patient,
)


Here is the code content for data.py:
from collections import OrderedDict
from datetime import datetime
from typing import Optional, List


class Event:
    """Contains information about a single event.

    An event can be anything from a diagnosis to a prescription or a lab test
    that happened in a visit of a patient at a specific time.

    Args:
        code: code of the event. E.g., "428.0" for congestive heart failure.
        table: name of the table where the event is recorded. This corresponds
            to the raw csv file name in the dataset. E.g., "DIAGNOSES_ICD".
        vocabulary: vocabulary of the code. E.g., "ICD9CM" for ICD-9 diagnosis codes.
        visit_id: unique identifier of the visit.
        patient_id: unique identifier of the patient.
        timestamp: timestamp of the event. Default is None.
        **attr: optional attributes to add to the event as key=value pairs.

    Attributes:
        attr_dict: Dict, dictionary of visit attributes. Each key is an attribute
            name and each value is the attribute's value.

    Examples:
        >>> from pyhealth.data import Event
        >>> event = Event(
        ...     code="00069153041",
        ...     table="PRESCRIPTIONS",
        ...     vocabulary="NDC",
        ...     visit_id="v001",
        ...     patient_id="p001",
        ...     dosage="250mg",
        ... )
        >>> event
        Event with NDC code 00069153041 from table PRESCRIPTIONS
        >>> event.attr_dict
        {'dosage': '250mg'}
    """

    def __init__(
        self,
        code: str = None,
        table: str = None,
        vocabulary: str = None,
        visit_id: str = None,
        patient_id: str = None,
        timestamp: Optional[datetime] = None,
        **attr,
    ):
        assert timestamp is None or isinstance(
            timestamp, datetime
        ), "timestamp must be a datetime object"
        self.code = code
        self.table = table
        self.vocabulary = vocabulary
        self.visit_id = visit_id
        self.patient_id = patient_id
        self.timestamp = timestamp
        self.attr_dict = dict()
        self.attr_dict.update(attr)

    def __repr__(self):
        return f"Event with {self.vocabulary} code {self.code} from table {self.table}"

    def __str__(self):
        lines = list()
        lines.append(f"Event from patient {self.patient_id} visit {self.visit_id}:")
        lines.append(f"\t- Code: {self.code}")
        lines.append(f"\t- Table: {self.table}")
        lines.append(f"\t- Vocabulary: {self.vocabulary}")
        lines.append(f"\t- Timestamp: {self.timestamp}")
        for k, v in self.attr_dict.items():
            lines.append(f"\t- {k}: {v}")
        return "\n".join(lines)


class Visit:
    """Contains information about a single visit.

    A visit is a period of time in which a patient is admitted to a hospital or
    a specific department. Each visit is associated with a patient and contains
    a list of different events.

    Args:
        visit_id: unique identifier of the visit.
        patient_id: unique identifier of the patient.
        encounter_time: timestamp of visit's encounter. Default is None.
        discharge_time: timestamp of visit's discharge. Default is None.
        discharge_status: patient's status upon discharge. Default is None.
        **attr: optional attributes to add to the visit as key=value pairs.

    Attributes:
        attr_dict: Dict, dictionary of visit attributes. Each key is an attribute
            name and each value is the attribute's value.
        event_list_dict: Dict[str, List[Event]], dictionary of event lists.
            Each key is a table name and each value is a list of events from that
            table ordered by timestamp.

    Examples:
        >>> from pyhealth.data import Event, Visit
        >>> event = Event(
        ...     code="00069153041",
        ...     table="PRESCRIPTIONS",
        ...     vocabulary="NDC",
        ...     visit_id="v001",
        ...     patient_id="p001",
        ...     dosage="250mg",
        ... )
        >>> visit = Visit(
        ...     visit_id="v001",
        ...     patient_id="p001",
        ... )
        >>> visit.add_event(event)
        >>> visit
        Visit v001 from patient p001 with 1 events from tables ['PRESCRIPTIONS']
        >>> visit.available_tables
        ['PRESCRIPTIONS']
        >>> visit.num_events
        1
        >>> visit.get_event_list('PRESCRIPTIONS')
        [Event with NDC code 00069153041 from table PRESCRIPTIONS]
        >>> visit.get_code_list('PRESCRIPTIONS')
        ['00069153041']
        >>> patient.available_tables
        ['PRESCRIPTIONS']
        >>> patient.get_visit_by_index(0)
        Visit v001 from patient p001 with 1 events from tables ['PRESCRIPTIONS']
        >>> patient.get_visit_by_index(0).get_code_list(table="PRESCRIPTIONS")
        ['00069153041']
    """

    def __init__(
        self,
        visit_id: str,
        patient_id: str,
        encounter_time: Optional[datetime] = None,
        discharge_time: Optional[datetime] = None,
        discharge_status=None,
        **attr,
    ):
        assert encounter_time is None or isinstance(
            encounter_time, datetime
        ), "encounter_time must be a datetime object"
        assert discharge_time is None or isinstance(
            discharge_time, datetime
        ), "discharge_time must be a datetime object"
        self.visit_id = visit_id
        self.patient_id = patient_id
        self.encounter_time = encounter_time
        self.discharge_time = discharge_time
        self.discharge_status = discharge_status
        self.attr_dict = dict()
        self.attr_dict.update(attr)
        self.event_list_dict = dict()

    def add_event(self, event: Event) -> None:
        """Adds an event to the visit.

        If the event's table is not in the visit's event list dictionary, it is
        added as a new key. The event is then added to the list of events of
        that table.

        Args:
            event: event to add.

        Note:
            As for now, there is no check on the order of the events. The new event
                is simply appended to end of the list.
        """
        assert event.visit_id == self.visit_id, "visit_id unmatched"
        assert event.patient_id == self.patient_id, "patient_id unmatched"
        table = event.table
        if table not in self.event_list_dict:
            self.event_list_dict[table] = list()
        self.event_list_dict[table].append(event)

    def get_event_list(self, table: str) -> List[Event]:
        """Returns a list of events from a specific table.

        If the table is not in the visit's event list dictionary, an empty list
        is returned.

        Args:
            table: name of the table.

        Returns:
           List of events from the specified table.

        Note:
            As for now, there is no check on the order of the events. The list of
                events is simply returned as is.
        """
        if table in self.event_list_dict:
            return self.event_list_dict[table]
        else:
            return list()

    def get_code_list(
        self, table: str, remove_duplicate: Optional[bool] = True
    ) -> List[str]:
        """Returns a list of codes from a specific table.

        If the table is not in the visit's event list dictionary, an empty list
        is returned.

        Args:
            table: name of the table.
            remove_duplicate: whether to remove duplicate codes
                (but keep the relative order). Default is True.

        Returns:
            List of codes from the specified table.

        Note:
            As for now, there is no check on the order of the codes. The list of
                codes is simply returned as is.
        """
        event_list = self.get_event_list(table)
        code_list = [event.code for event in event_list]
        if remove_duplicate:
            # remove duplicate codes but keep the order
            code_list = list(dict.fromkeys(code_list))
        return code_list

    def set_event_list(self, table: str, event_list: List[Event]) -> None:
        """Sets the list of events from a specific table.

        This function will overwrite any existing list of events from
        the specified table.

        Args:
            table: name of the table.
            event_list: list of events to set.

        Note:
            As for now, there is no check on the order of the events. The list of
                events is simply set as is.
        """
        self.event_list_dict[table] = event_list

    @property
    def available_tables(self) -> List[str]:
        """Returns a list of available tables for the visit.

        Returns:
            List of available tables.
        """
        return list(self.event_list_dict.keys())

    @property
    def num_events(self) -> int:
        """Returns the total number of events in the visit.

        Returns:
            Total number of events.
        """
        return sum([len(event_list) for event_list in self.event_list_dict.values()])

    def __repr__(self):
        return (
            f"Visit {self.visit_id} "
            f"from patient {self.patient_id} "
            f"with {self.num_events} events "
            f"from tables {self.available_tables}"
        )

    def __str__(self):
        lines = list()
        lines.append(
            f"Visit {self.visit_id} from patient {self.patient_id} "
            f"with {self.num_events} events:"
        )
        lines.append(f"\t- Encounter time: {self.encounter_time}")
        lines.append(f"\t- Discharge time: {self.discharge_time}")
        lines.append(f"\t- Discharge status: {self.discharge_status}")
        lines.append(f"\t- Available tables: {self.available_tables}")
        for k, v in self.attr_dict.items():
            lines.append(f"\t- {k}: {v}")
        for table, event_list in self.event_list_dict.items():
            for event in event_list:
                event_str = str(event).replace("\n", "\n\t")
                lines.append(f"\t- {event_str}")
        return "\n".join(lines)


class Patient:
    """Contains information about a single patient.

    A patient is a person who is admitted at least once to a hospital or
    a specific department. Each patient is associated with a list of visits.

    Args:
        patient_id: unique identifier of the patient.
        birth_datetime: timestamp of patient's birth. Default is None.
        death_datetime: timestamp of patient's death. Default is None.
        gender: gender of the patient. Default is None.
        ethnicity: ethnicity of the patient. Default is None.
        **attr: optional attributes to add to the patient as key=value pairs.

    Attributes:
        attr_dict: Dict, dictionary of patient attributes. Each key is an attribute
            name and each value is the attribute's value.
        visits: OrderedDict[str, Visit], an ordered dictionary of visits. Each key
            is a visit_id and each value is a visit.
        index_to_visit_id: Dict[int, str], dictionary that maps the index of a visit
            in the visits list to the corresponding visit_id.

    Examples:
            >>> from pyhealth.data import Event, Visit, Patient
            >>> event = Event(
            ...     code="00069153041",
            ...     table="PRESCRIPTIONS",
            ...     vocabulary="NDC",
            ...     visit_id="v001",
            ...     patient_id="p001",
            ...     dosage="250mg",
            ... )
            >>> visit = Visit(
            ...     visit_id="v001",
            ...     patient_id="p001",
            ... )
            >>> visit.add_event(event)
            >>> patient = Patient(
            ...     patient_id="p001",
            ... )
            >>> patient.add_visit(visit)
            >>> patient
            Patient p001 with 1 visits
    """

    def __init__(
        self,
        patient_id: str,
        birth_datetime: Optional[datetime] = None,
        death_datetime: Optional[datetime] = None,
        gender=None,
        ethnicity=None,
        **attr,
    ):
        self.patient_id = patient_id
        self.birth_datetime = birth_datetime
        self.death_datetime = death_datetime
        self.gender = gender
        self.ethnicity = ethnicity
        self.attr_dict = dict()
        self.attr_dict.update(attr)
        self.visits = OrderedDict()
        self.index_to_visit_id = dict()

    def add_visit(self, visit: Visit) -> None:
        """Adds a visit to the patient.

        If the visit's visit_id is already in the patient's visits dictionary,
        it will be overwritten by the new visit.

        Args:
            visit: visit to add.

        Note:
            As for now, there is no check on the order of the visits. The new visit
                is simply added to the end of the ordered dictionary of visits.
        """
        assert visit.patient_id == self.patient_id, "patient_id unmatched"
        self.visits[visit.visit_id] = visit
        # incrementing index
        self.index_to_visit_id[len(self.visits) - 1] = visit.visit_id

    def add_event(self, event: Event) -> None:
        """Adds an event to the patient.

        If the event's visit_id is not in the patient's visits dictionary, this
        function will raise KeyError.

        Args:
            event: event to add.

        Note:
            As for now, there is no check on the order of the events. The new event
                is simply appended to the end of the list of events of the
                corresponding visit.
        """
        assert event.patient_id == self.patient_id, "patient_id unmatched"
        visit_id = event.visit_id
        if visit_id not in self.visits:
            raise KeyError(
                f"Visit with id {visit_id} not found in patient {self.patient_id}"
            )
        self.get_visit_by_id(visit_id).add_event(event)

    def get_visit_by_id(self, visit_id: str) -> Visit:
        """Returns a visit by visit_id.

        Args:
            visit_id: unique identifier of the visit.

        Returns:
            Visit with the given visit_id.
        """
        return self.visits[visit_id]

    def get_visit_by_index(self, index: int) -> Visit:
        """Returns a visit by its index.

        Args:
            index: int, index of the visit to return.

        Returns:
            Visit with the given index.
        """
        if index not in self.index_to_visit_id:
            raise IndexError(
                f"Visit with  index {index} not found in patient {self.patient_id}"
            )
        visit_id = self.index_to_visit_id[index]
        return self.get_visit_by_id(visit_id)

    @property
    def available_tables(self) -> List[str]:
        """Returns a list of available tables for the patient.

        Returns:
            List of available tables.
        """
        tables = []
        for visit in self:
            tables.extend(visit.available_tables)
        return list(set(tables))

    def __len__(self):
        """Returns the number of visits in the patient."""
        return len(self.visits)

    def __getitem__(self, index) -> Visit:
        """Returns a visit by its index."""
        return self.get_visit_by_index(index)

    def __repr__(self):
        return f"Patient {self.patient_id} with {len(self)} visits"

    def __str__(self):
        lines = list()
        # patient info
        lines.append(f"Patient {self.patient_id} with {len(self)} visits:")
        lines.append(f"\t- Birth datetime: {self.birth_datetime}")
        lines.append(f"\t- Death datetime: {self.death_datetime}")
        lines.append(f"\t- Gender: {self.gender}")
        lines.append(f"\t- Ethnicity: {self.ethnicity}")
        for k, v in self.attr_dict.items():
            lines.append(f"\t- {k}: {v}")
        # visit info
        for visit in self:
            visit_str = str(visit).replace("\n", "\n\t")
            lines.append(f"\t- {visit_str}")
        return "\n".join(lines)


Here is the code content for TUAB.py:
import os

import numpy as np

from pyhealth.datasets import BaseSignalDataset


class TUABDataset(BaseSignalDataset):
    """Base EEG dataset for the TUH Abnormal EEG Corpus

    Dataset is available at https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml

    The TUAB dataset (or Temple University Hospital EEG Abnormal Corpus) is a collection of EEG data acquired at the Temple University Hospital. 
    
    The dataset contains both normal and abnormal EEG readings.

    Files are named in the form aaaaamye_s001_t000.edf. This includes the subject identifier ("aaaaamye"), the session number ("s001") and a token number ("t000"). EEGs are split into a series of files starting with *t000.edf, *t001.edf, ...

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data. *You can choose to use the path to Cassette portion or the Telemetry portion.*
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.

    Attributes:
        task: Optional[str], name of the task (e.g., "EEG_abnormal").
            Default is None.
        samples: Optional[List[Dict]], a list of samples, each sample is a dict with
            patient_id, record_id, and other task-specific attributes as key.
            Default is None.
        patient_to_index: Optional[Dict[str, List[int]]], a dict mapping patient_id to
            a list of sample indices. Default is None.
        visit_to_index: Optional[Dict[str, List[int]]], a dict mapping visit_id to a
            list of sample indices. Default is None.

    Examples:
        >>> from pyhealth.datasets import TUABDataset
        >>> dataset = TUABDataset(
        ...         root="/srv/local/data/TUH/tuh_eeg_abnormal/v3.0.0/edf/",
        ...     )
        >>> dataset.stat()
        >>> dataset.info()
    """

    def process_EEG_data(self):
        # create a map for data sets for latter mapping patients
        data_map = {
            "train/abnormal": "0",
            "train/normal": "1",
            "eval/abnormal": "2",
            "eval/normal": "3",
        }

        data_map_reverse = {
            "0": "train/abnormal",
            "1": "train/normal",
            "2": "eval/abnormal",
            "3": "eval/normal",
        }

        # get all file names
        all_files = {}

        train_abnormal_files = os.listdir(os.path.join(self.root, "train/abnormal/01_tcp_ar"))
        all_files["train/abnormal"] = train_abnormal_files

        train_normal_files = os.listdir(os.path.join(self.root, "train/normal/01_tcp_ar"))
        all_files["train/normal"] = train_normal_files

        eval_abnormal_files = os.listdir(os.path.join(self.root, "eval/abnormal/01_tcp_ar"))
        all_files["eval/abnormal"] = eval_abnormal_files

        eval_normal_files = os.listdir(os.path.join(self.root, "eval/normal/01_tcp_ar"))
        all_files["eval/normal"] = eval_normal_files


        # get all patient ids
        patient_ids = []
        for field, sub_data in all_files.items():
            patient_ids.extend(["{}_{}".format(data_map[field], data.split("_")[0]) for data in sub_data])

        patient_ids = list(set(patient_ids))

        if self.dev:
            patient_ids = patient_ids[:5]

        # get patient to record maps
        #    - key: pid:
        #    - value: [{"load_from_path": None, "patient_id": None, "signal_file": None, "label_file": None, "save_to_path": None}, ...]
        patients = {
            pid: []
            for pid in patient_ids
        }
           
        for pid in patient_ids:
            data_field = data_map_reverse[pid.split("_")[0]]
            patient_visits = [file for file in all_files[data_field] if file.split("_")[0] == pid.split("_")[1]]
            
            for visit in patient_visits:
                patients[pid].append({
                    "load_from_path": os.path.join(self.root, data_field, "01_tcp_ar"),
                    "patient_id": pid,
                    "visit_id": visit.strip(".edf").strip(pid.split("_")[1])[1:],
                    "signal_file": visit,
                    "label_file": visit,
                    "save_to_path": self.filepath,
                })
        
        return patients


if __name__ == "__main__":
    dataset = TUABDataset(
        root="/srv/local/data/TUH/tuh_eeg_abnormal/v3.0.0/edf/",
        dev=True,
        refresh_cache=True,
    )
    dataset.stat()
    dataset.info()
    print(list(dataset.patients.items())[0])


Here is the code content for TUEV.py:
import os

import numpy as np

from pyhealth.datasets import BaseSignalDataset


class TUEVDataset(BaseSignalDataset):
    """Base EEG dataset for the TUH EEG Events Corpus

    Dataset is available at https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml

    This corpus is a subset of TUEG that contains annotations of EEG segments as one of six classes: (1) spike and sharp wave (SPSW), (2) generalized periodic epileptiform discharges (GPED), (3) periodic lateralized epileptiform discharges (PLED), (4) eye movement (EYEM), (5) artifact (ARTF) and (6) background (BCKG).

    Files are named in the form of bckg_032_a_.edf in the eval partition:
        bckg: this file contains background annotations.
		032: a reference to the eval index	
		a_.edf: EEG files are split into a series of files starting with a_.edf, a_1.ef, ... These represent pruned EEGs, so the  original EEG is split into these segments, and uninteresting parts of the original recording were deleted.
    or in the form of 00002275_00000001.edf in the train partition:
        00002275: a reference to the train index. 
		0000001: indicating that this is the first file inssociated with this patient. 

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data. *You can choose to use the path to Cassette portion or the Telemetry portion.*
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.

    Attributes:
        task: Optional[str], name of the task (e.g., "EEG_events").
            Default is None.
        samples: Optional[List[Dict]], a list of samples, each sample is a dict with
            patient_id, record_id, and other task-specific attributes as key.
            Default is None.
        patient_to_index: Optional[Dict[str, List[int]]], a dict mapping patient_id to
            a list of sample indices. Default is None.
        visit_to_index: Optional[Dict[str, List[int]]], a dict mapping visit_id to a
            list of sample indices. Default is None.

    Examples:
        >>> from pyhealth.datasets import TUEVDataset
        >>> dataset = TUEVDataset(
        ...         root="/srv/local/data/TUH/tuh_eeg_events/v2.0.0/edf/",
        ...     )
        >>> dataset.stat()
        >>> dataset.info()
    """

    def process_EEG_data(self):
        # get all file names
        all_files = {}

        train_files = os.listdir(os.path.join(self.root, "train/"))
        for id in train_files:
            if id != ".DS_Store":
                all_files["0_{}".format(id)] = [name for name in os.listdir(os.path.join(self.root, "train/", id)) if name.endswith(".edf")]

        eval_files = os.listdir(os.path.join(self.root, "eval/"))
        for id in eval_files:
            if id != ".DS_Store":
                all_files["1_{}".format(id)] = [name for name in os.listdir(os.path.join(self.root, "eval/", id)) if name.endswith(".edf")]

        # get all patient ids
        patient_ids = list(set(list(all_files.keys())))

        if self.dev:
            patient_ids = patient_ids[:30]

        # get patient to record maps
        #    - key: pid:
        #    - value: [{"load_from_path": None, "patient_id": None, "signal_file": None, "label_file": None, "save_to_path": None}, ...]
        patients = {
            pid: []
            for pid in patient_ids
        }
           
        for pid in patient_ids:
            split = "train" if pid.split("_")[0] == "0" else "eval"
            id = pid.split("_")[1]

            patient_visits = all_files[pid]
            
            for visit in patient_visits:
                if split == "train":
                    visit_id = visit.strip(".edf").split("_")[1]
                else:
                    visit_id = visit.strip(".edf")
                    
                patients[pid].append({
                    "load_from_path": os.path.join(self.root, split, id),
                    "patient_id": pid,
                    "visit_id": visit_id,
                    "signal_file": visit,
                    "label_file": visit,
                    "save_to_path": self.filepath,
                })
        
        return patients


if __name__ == "__main__":
    dataset = TUEVDataset(
        root="/srv/local/data/TUH/tuh_eeg_events/v2.0.0/edf",
        dev=True,
        refresh_cache=True,
    )
    dataset.stat()
    dataset.info()
    print(list(dataset.patients.items())[0])


Here is the code content for __init__.py:
from .base_ehr_dataset import BaseEHRDataset
from .base_signal_dataset import BaseSignalDataset
from .cardiology import CardiologyDataset
from .eicu import eICUDataset
from .mimic3 import MIMIC3Dataset
from .mimic4 import MIMIC4Dataset
from .mimicextract import MIMICExtractDataset
from .omop import OMOPDataset
from .sleepedf import SleepEDFDataset
from .isruc import ISRUCDataset
from .shhs import SHHSDataset
from .tuab import TUABDataset
from .tuev import TUEVDataset
from .sample_dataset import SampleBaseDataset, SampleSignalDataset, SampleEHRDataset
from .splitter import split_by_patient, split_by_visit
from .utils import collate_fn_dict, get_dataloader, strptime


Here is the code content for base_ehr_dataset.py:
import logging
import time
import os
from abc import ABC
from collections import Counter
from copy import deepcopy
from typing import Dict, Callable, Tuple, Union, List, Optional

import pandas as pd
from tqdm import tqdm
from pandarallel import pandarallel

from pyhealth.data import Patient, Event
from pyhealth.datasets.sample_dataset import SampleEHRDataset
from pyhealth.datasets.utils import MODULE_CACHE_PATH, DATASET_BASIC_TABLES
from pyhealth.datasets.utils import hash_str
from pyhealth.medcode import CrossMap
from pyhealth.utils import load_pickle, save_pickle

logger = logging.getLogger(__name__)

INFO_MSG = """
dataset.patients: patient_id -> <Patient>

<Patient>
    - visits: visit_id -> <Visit> 
    - other patient-level info
    
    <Visit>
        - event_list_dict: table_name -> List[Event]
        - other visit-level info
    
        <Event>
            - code: str
            - other event-level info
"""


# TODO: parse_tables is too slow


class BaseEHRDataset(ABC):
    """Abstract base dataset class.

    This abstract class defines a uniform interface for all EHR datasets
    (e.g., MIMIC-III, MIMIC-IV, eICU, OMOP).

    Each specific dataset will be a subclass of this abstract class, which can then
    be converted to samples dataset for different tasks by calling `self.set_task()`.

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data (should contain many csv files).
        tables: list of tables to be loaded (e.g., ["DIAGNOSES_ICD", "PROCEDURES_ICD"]). Basic tables will be loaded by default.
        code_mapping: a dictionary containing the code mapping information.
            The key is a str of the source code vocabulary and the value is of
            two formats:
                - a str of the target code vocabulary. E.g., {"NDC", "ATC"}.
                - a tuple with two elements. The first element is a str of the
                    target code vocabulary and the second element is a dict with
                    keys "source_kwargs" or "target_kwargs" and values of the
                    corresponding kwargs for the `CrossMap.map()` method. E.g.,
                    {"NDC", ("ATC", {"target_kwargs": {"level": 3}})}.
            Default is empty dict, which means the original code will be used.
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.
    """

    def __init__(
        self,
        root: str,
        tables: List[str],
        dataset_name: Optional[str] = None,
        code_mapping: Optional[Dict[str, Union[str, Tuple[str, Dict]]]] = None,
        dev: bool = False,
        refresh_cache: bool = False,
    ):
        """Loads tables into a dict of patients and saves it to cache."""

        if code_mapping is None:
            code_mapping = {}

        # base attributes
        self.dataset_name = (
            self.__class__.__name__ if dataset_name is None else dataset_name
        )
        self.root = root

        self.code_mapping = code_mapping
        self.dev = dev

        # if we are using a premade dataset, no basic tables need to be provided.
        if self.dataset_name in DATASET_BASIC_TABLES and [
            table
            for table in tables
            if table in DATASET_BASIC_TABLES[self.dataset_name]
        ]:
            raise AttributeError(
                f"Basic tables are parsed by default and do not need to be explicitly selected. Basic tables for {self.dataset_name}: {DATASET_BASIC_TABLES[self.dataset_name]}"
            )

        self.tables = tables

        # load medcode for code mapping
        self.code_mapping_tools = self._load_code_mapping_tools()

        # hash filename for cache
        args_to_hash = (
            [self.dataset_name, root]
            + sorted(tables)
            + sorted(code_mapping.items())
            + ["dev" if dev else "prod"]
        )
        filename = hash_str("+".join([str(arg) for arg in args_to_hash])) + ".pkl"
        self.filepath = os.path.join(MODULE_CACHE_PATH, filename)
        
        # check if cache exists or refresh_cache is True
        if os.path.exists(self.filepath) and (not refresh_cache):
            # load from cache
            logger.debug(
                f"Loaded {self.dataset_name} base dataset from {self.filepath}"
            )
            self.patients = load_pickle(self.filepath)
        else:
            # load from raw data
            logger.debug(f"Processing {self.dataset_name} base dataset...")
            # parse tables
            patients = self.parse_tables()
            # convert codes
            patients = self._convert_code_in_patient_dict(patients)
            self.patients = patients
            # save to cache
            logger.debug(f"Saved {self.dataset_name} base dataset to {self.filepath}")
            save_pickle(self.patients, self.filepath)

    def _load_code_mapping_tools(self) -> Dict[str, CrossMap]:
        """Helper function which loads code mapping tools CrossMap for code mapping.

        Will be called in `self.__init__()`.

        Returns:
            A dict whose key is the source and target code vocabulary and
                value is the `CrossMap` object.
        """
        code_mapping_tools = {}
        for s_vocab, target in self.code_mapping.items():
            if isinstance(target, tuple):
                assert len(target) == 2
                assert type(target[0]) == str
                assert type(target[1]) == dict
                assert target[1].keys() <= {"source_kwargs", "target_kwargs"}
                t_vocab = target[0]
            else:
                t_vocab = target
            # load code mapping from source to target
            code_mapping_tools[f"{s_vocab}_{t_vocab}"] = CrossMap(s_vocab, t_vocab)
        return code_mapping_tools

    def parse_tables(self) -> Dict[str, Patient]:
        """Parses the tables in `self.tables` and return a dict of patients.

        Will be called in `self.__init__()` if cache file does not exist or
            refresh_cache is True.

        This function will first call `self.parse_basic_info()` to parse the
        basic patient information, and then call `self.parse_[table_name]()` to
        parse the table with name `table_name`. Both `self.parse_basic_info()` and
        `self.parse_[table_name]()` should be implemented in the subclass.

        Returns:
           A dict mapping patient_id to `Patient` object.
        """
        pandarallel.initialize(progress_bar=False)

        # patients is a dict of Patient objects indexed by patient_id
        patients: Dict[str, Patient] = dict()
        # process basic information (e.g., patients and visits)
        tic = time.time()
        patients = self.parse_basic_info(patients)
        print(
            "finish basic patient information parsing : {}s".format(time.time() - tic)
        )
        # process clinical tables
        for table in self.tables:
            try:
                # use lower case for function name
                tic = time.time()
                patients = getattr(self, f"parse_{table.lower()}")(patients)
                print(f"finish parsing {table} : {time.time() - tic}s")
            except AttributeError:
                raise NotImplementedError(
                    f"Parser for table {table} is not implemented yet."
                )
        return patients

    def _add_events_to_patient_dict(
        self,
        patient_dict: Dict[str, Patient],
        group_df: pd.DataFrame,
    ) -> Dict[str, Patient]:
        """Helper function which adds the events column of a df.groupby object to the patient dict.

        Will be called at the end of each `self.parse_[table_name]()` function.

        Args:
            patient_dict: a dict mapping patient_id to `Patient` object.
            group_df: a df.groupby object, having two columns: patient_id and events.
                - the patient_id column is the index of the patient
                - the events column is a list of <Event> objects

        Returns:
            The updated patient dict.
        """
        for _, events in group_df.items():
            for event in events:
                patient_dict = self._add_event_to_patient_dict(patient_dict, event)
        return patient_dict

    @staticmethod
    def _add_event_to_patient_dict(
        patient_dict: Dict[str, Patient],
        event: Event,
    ) -> Dict[str, Patient]:
        """Helper function which adds an event to the patient dict.

        Will be called in `self._add_events_to_patient_dict`.

        Note that if the patient of the event is not in the patient dict, or the
        visit of the event is not in the patient, this function will do nothing.

        Args:
            patient_dict: a dict mapping patient_id to `Patient` object.
            event: an event to be added to the patient dict.

        Returns:
            The updated patient dict.
        """
        patient_id = event.patient_id
        try:
            patient_dict[patient_id].add_event(event)
        except KeyError:
            pass
        return patient_dict

    def _convert_code_in_patient_dict(
        self,
        patients: Dict[str, Patient],
    ) -> Dict[str, Patient]:
        """Helper function which converts the codes for all patients.

        The codes to be converted are specified in `self.code_mapping`.

        Will be called in `self.__init__()` after `self.parse_tables()`.

        Args:
            patients: a dict mapping patient_id to `Patient` object.

        Returns:
            The updated patient dict.
        """
        for p_id, patient in tqdm(patients.items(), desc="Mapping codes"):
            patients[p_id] = self._convert_code_in_patient(patient)
        return patients

    def _convert_code_in_patient(self, patient: Patient) -> Patient:
        """Helper function which converts the codes for a single patient.

        Will be called in `self._convert_code_in_patient_dict()`.

        Args:
            patient:a `Patient` object.

        Returns:
            The updated `Patient` object.
        """
        for visit in patient:
            for table in visit.available_tables:
                all_mapped_events = []
                for event in visit.get_event_list(table):
                    # an event may be mapped to multiple events after code conversion
                    mapped_events: List[Event]
                    mapped_events = self._convert_code_in_event(event)
                    all_mapped_events.extend(mapped_events)
                visit.set_event_list(table, all_mapped_events)
        return patient

    def _convert_code_in_event(self, event: Event) -> List[Event]:
        """Helper function which converts the code for a single event.

        Note that an event may be mapped to multiple events after code conversion.

        Will be called in `self._convert_code_in_patient()`.

        Args:
            event: an `Event` object.

        Returns:
            A list of `Event` objects after code conversion.
        """
        src_vocab = event.vocabulary
        if src_vocab in self.code_mapping:
            target = self.code_mapping[src_vocab]
            if isinstance(target, tuple):
                tgt_vocab, kwargs = target
                source_kwargs = kwargs.get("source_kwargs", {})
                target_kwargs = kwargs.get("target_kwargs", {})
            else:
                tgt_vocab = self.code_mapping[src_vocab]
                source_kwargs = {}
                target_kwargs = {}
            code_mapping_tool = self.code_mapping_tools[f"{src_vocab}_{tgt_vocab}"]
            mapped_code_list = code_mapping_tool.map(
                event.code, source_kwargs=source_kwargs, target_kwargs=target_kwargs
            )
            mapped_event_list = [deepcopy(event) for _ in range(len(mapped_code_list))]
            for i, mapped_event in enumerate(mapped_event_list):
                mapped_event.code = mapped_code_list[i]
                mapped_event.vocabulary = tgt_vocab
            return mapped_event_list
        # TODO: should normalize the code here
        return [event]

    @property
    def available_tables(self) -> List[str]:
        """Returns a list of available tables for the dataset.

        Returns:
            List of available tables.
        """
        tables = []
        for patient in self.patients.values():
            tables.extend(patient.available_tables)
        return list(set(tables))

    def __str__(self):
        """Prints some information of the dataset."""
        return f"Base dataset {self.dataset_name}"

    def stat(self) -> str:
        """Returns some statistics of the base dataset."""
        lines = list()
        lines.append("")
        lines.append(f"Statistics of base dataset (dev={self.dev}):")
        lines.append(f"\t- Dataset: {self.dataset_name}")
        lines.append(f"\t- Number of patients: {len(self.patients)}")
        num_visits = [len(p) for p in self.patients.values()]
        lines.append(f"\t- Number of visits: {sum(num_visits)}")
        lines.append(
            f"\t- Number of visits per patient: {sum(num_visits) / len(num_visits):.4f}"
        )
        for table in self.tables:
            num_events = [
                len(v.get_event_list(table)) for p in self.patients.values() for v in p
            ]
            lines.append(
                f"\t- Number of events per visit in {table}: "
                f"{sum(num_events) / len(num_events):.4f}"
            )
        lines.append("")
        print("\n".join(lines))
        return "\n".join(lines)

    @staticmethod
    def info():
        """Prints the output format."""
        print(INFO_MSG)

    def set_task(
        self,
        task_fn: Callable,
        task_name: Optional[str] = None,
    ) -> SampleEHRDataset:
        """Processes the base dataset to generate the task-specific sample dataset.

        This function should be called by the user after the base dataset is
        initialized. It will iterate through all patients in the base dataset
        and call `task_fn` which should be implemented by the specific task.

        Args:
            task_fn: a function that takes a single patient and returns a
                list of samples (each sample is a dict with patient_id, visit_id,
                and other task-specific attributes as key). The samples will be
                concatenated to form the sample dataset.
            task_name: the name of the task. If None, the name of the task
                function will be used.

        Returns:
            sample_dataset: the task-specific sample dataset.

        Note:
            In `task_fn`, a patient may be converted to multiple samples, e.g.,
                a patient with three visits may be converted to three samples
                ([visit 1], [visit 1, visit 2], [visit 1, visit 2, visit 3]).
                Patients can also be excluded from the task dataset by returning
                an empty list.
        """
        if task_name is None:
            task_name = task_fn.__name__
        samples = []
        for patient_id, patient in tqdm(
            self.patients.items(), desc=f"Generating samples for {task_name}"
        ):
            samples.extend(task_fn(patient))
        sample_dataset = SampleEHRDataset(
            samples,
            dataset_name=self.dataset_name,
            task_name=task_name,
        )
        return sample_dataset


Here is the code content for base_signal_dataset.py:
from typing import Dict, Optional, Tuple, Union, Callable
import os
import logging

from abc import ABC
import pandas as pd
from pandarallel import pandarallel

from pyhealth.datasets.utils import hash_str, MODULE_CACHE_PATH
from pyhealth.datasets.sample_dataset import SampleSignalDataset
from pyhealth.utils import load_pickle, save_pickle

logger = logging.getLogger(__name__)

INFO_MSG = """
dataset.patients:
    - key: patient id
    - value: recoding file paths
"""


class BaseSignalDataset(ABC):
    """Abstract base Signal dataset class.

    This abstract class defines a uniform interface for all EEG datasets
    (e.g., SleepEDF, SHHS).

    Each specific dataset will be a subclass of this abstract class, which can then
    be converted to samples dataset for different tasks by calling `self.set_task()`.

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data (should contain many csv files).
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.
    """

    def __init__(
        self,
        root: str,
        dataset_name: Optional[str] = None,
        dev: bool = False,
        refresh_cache: bool = False,
        **kwargs: Optional[Dict],
    ):

        # base attributes
        self.dataset_name = (
            self.__class__.__name__ if dataset_name is None else dataset_name
        )
        self.root = root
        self.dev = dev
        self.refresh_cache = refresh_cache

        # hash filename for cache
        args_to_hash = [self.dataset_name, root] + ["dev" if dev else "prod"]
        filename = hash_str("+".join([str(arg) for arg in args_to_hash]))
        self.filepath = os.path.join(MODULE_CACHE_PATH, filename)

        # for task-specific attributes
        self.kwargs = kwargs

        self.patients = self.process_EEG_data()

    def __str__(self):
        """Prints some information of the dataset."""
        return f"Base dataset {self.dataset_name}"

    def stat(self) -> str:
        """Returns some statistics of the base dataset."""
        lines = list()
        lines.append("")
        lines.append(f"Statistics of base dataset (dev={self.dev}):")
        lines.append(f"\t- Dataset: {self.dataset_name}")
        lines.append(f"\t- Number of patients: {len(self.patients)}")
        num_records = [len(p) for p in self.patients.values()]
        lines.append(f"\t- Number of recodings: {sum(num_records)}")
        lines.append("")
        print("\n".join(lines))
        return "\n".join(lines)

    @staticmethod
    def info():
        """Prints the output format."""
        print(INFO_MSG)

    def set_task(
        self,
        task_fn: Callable,
        task_name: Optional[str] = None,
    ) -> SampleSignalDataset:
        """Processes the base dataset to generate the task-specific sample dataset.

        This function should be called by the user after the base dataset is
        initialized. It will iterate through all patients in the base dataset
        and call `task_fn` which should be implemented by the specific task.

        Args:
            task_fn: a function that takes a single patient and returns a
                list of samples (each sample is a dict with patient_id, visit_id,
                and other task-specific attributes as key). The samples will be
                concatenated to form the sample dataset.
            task_name: the name of the task. If None, the name of the task
                function will be used.

        Returns:
            sample_dataset: the task-specific sample (Base) dataset.

        Note:
            In `task_fn`, a patient may be converted to multiple samples, e.g.,
                a patient with three visits may be converted to three samples
                ([visit 1], [visit 1, visit 2], [visit 1, visit 2, visit 3]).
                Patients can also be excluded from the task dataset by returning
                an empty list.
        """
        if task_name is None:
            task_name = task_fn.__name__

        # check if cache exists or refresh_cache is True
        if os.path.exists(self.filepath) and (not self.refresh_cache):
            """
            It obtains the signal samples (with path only) to ```self.filepath.pkl``` file
            """
            # load from cache
            logger.debug(
                f"Loaded {self.dataset_name} base dataset from {self.filepath}"
            )
            samples = load_pickle(self.filepath + ".pkl")
        else:
            """
            It stores the actual data and label to ```self.filepath/``` folder
            It also stores the signal samples (with path only) to ```self.filepath.pkl``` file
            """
            # load from raw data
            logger.debug(f"Processing {self.dataset_name} base dataset...")

            pandarallel.initialize(progress_bar=False)

            # transform dict to pandas dataframe
            if not os.path.exists(self.filepath):
                os.makedirs(self.filepath)
            patients = pd.DataFrame(self.patients.items(), columns=["pid", "records"])
            patients.records = patients.records.parallel_apply(lambda x: task_fn(x))

            samples = []
            for _, records in patients.values:
                samples.extend(records)

            # save to cache
            logger.debug(f"Saved {self.dataset_name} base dataset to {self.filepath}")
            save_pickle(samples, self.filepath + ".pkl")

        sample_dataset = SampleSignalDataset(
            samples,
            dataset_name=self.dataset_name,
            task_name=task_name,
        )
        return sample_dataset


Here is the code content for cardiology.py:
import os

import numpy as np

from typing import Optional, List
from itertools import islice

from pyhealth.datasets import BaseSignalDataset


class CardiologyDataset(BaseSignalDataset):
    """Base ECG dataset for Cardiology

    Dataset is available at https://physionet.org/content/challenge-2020/1.0.2/

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data.
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.
        chosen_dataset: a list of (0,1) of length 6 indicting which datasets will be used. Default: [1, 1, 1, 1, 1, 1]
            The datasets contain "cpsc_2018", "cpsc_2018_extra", "georgia", "ptb", "ptb-xl", "st_petersburg_incart".
            eg. [0,1,1,1,1,1] indicates that "cpsc_2018_extra", "georgia", "ptb", "ptb-xl" and "st_petersburg_incart" will be used.

    Attributes:
        task: Optional[str], name of the task (e.g., "sleep staging").
            Default is None.
        samples: Optional[List[Dict]], a list of samples, each sample is a dict with
            patient_id, record_id, and other task-specific attributes as key.
            Default is None.
        patient_to_index: Optional[Dict[str, List[int]]], a dict mapping patient_id to
            a list of sample indices. Default is None.
        visit_to_index: Optional[Dict[str, List[int]]], a dict mapping visit_id to a
            list of sample indices. Default is None.

    Examples:
        >>> from pyhealth.datasets import CardiologyDataset
        >>> dataset = CardiologyDataset(
        ...         root="/srv/local/data/physionet.org/files/challenge-2020/1.0.2/training",
        ...     )
        >>> dataset.stat()
        >>> dataset.info()
    """

    def __init__(self, root: str, chosen_dataset: List[int] = [1,1,1,1,1,1], dataset_name: Optional[str] = None, dev: bool = False, refresh_cache: bool = False):
        self.chosen_dataset = chosen_dataset

        super().__init__(dataset_name=dataset_name, root=root, dev=dev, refresh_cache=refresh_cache) 
        self.root = root
        self.dev = dev
        self.refresh_cache = refresh_cache

    def process_EEG_data(self):
        
        # get all file names depending on user-defined dataset
        dataset_lists = ["cpsc_2018", "cpsc_2018_extra", "georgia", "ptb", "ptb-xl", "st_petersburg_incart"]

        all_files = []
        for idx in range(6):
            if self.chosen_dataset[idx] == 0:
                all_files.append([])
            else:
                dataset_root = os.path.join(self.root, dataset_lists[idx])
                dataset_samples = []
                for patient in range(len(os.listdir(dataset_root)) - 1): #exclude RECORDS 
                    patient_id = "g" + str(patient+1)
                    patient_root = os.path.join(dataset_root, patient_id)
                    dataset_samples.append([i.split(".")[0] for i in os.listdir(patient_root) if i != "RECORDS" and i != "index.html"])
                all_files.append(dataset_samples)  #[dataset:[patient:[sample1, sample2...]...]...]
        
        #print(all_files)
        # get all patient ids
        patient_ids = []
        for dataset_idx in range(len(all_files)):
            if all_files[dataset_idx] != []:
                for patient_idx in range(len(all_files[dataset_idx])):
                    cur_id = "{}_{}".format(dataset_idx, patient_idx)
                    patient_ids.append(cur_id)

        #print(patient_ids)
        
        if self.dev:
            patient_ids = patient_ids[:5]

        # get patient to record maps
        #    - key: pid:
        #    - value: [{"load_from_path": None, "signal_file": None, "label_file": None, "save_to_path": None}, ...]
        patients = {
            pid: []
            for pid in patient_ids
        }
        
        for dataset_idx in range(len(all_files)):
            if all_files[dataset_idx] != []:
                for patient_idx in range(len(all_files[dataset_idx])):
                    pid = "{}_{}".format(dataset_idx, patient_idx)
                    if pid in patient_ids:
                        for sample in all_files[dataset_idx][patient_idx]: 
                            patients[pid].append({
                                "load_from_path": os.path.join(self.root, dataset_lists[dataset_idx], "g{}".format(patient_idx+1)),
                                "patient_id": pid,
                                "signal_file": sample + ".mat",
                                "label_file": sample + ".hea",
                                "save_to_path": self.filepath,
                            })

                    
        return patients


if __name__ == "__main__":
    dataset = CardiologyDataset(
        root="/srv/local/data/physionet.org/files/challenge-2020/1.0.2/training",
        dev=True,
        refresh_cache=True,
    )
    dataset.stat()
    dataset.info()
    # the number of records for the first patient
    keys = list(dataset.patients.keys())
    print(len(dataset.patients[keys[0]]))


Here is the code content for eicu.py:
import os
from typing import Optional, List, Dict, Tuple, Union

import pandas as pd
from tqdm import tqdm
from datetime import datetime

from pyhealth.data import Event, Visit, Patient
from pyhealth.datasets import BaseEHRDataset
from pyhealth.datasets.utils import strptime, padyear

# TODO: add other tables


class eICUDataset(BaseEHRDataset):
    """Base dataset for eICU dataset.

    The eICU dataset is a large dataset of de-identified health records of ICU
    patients. The dataset is available at https://eicu-crd.mit.edu/.

    The basic information is stored in the following tables:
        - patient: defines a patient (uniquepid), a hospital admission
            (patienthealthsystemstayid), and a ICU stay (patientunitstayid)
            in the database.
        - hospital: contains information about a hospital (e.g., region).

    Note that in eICU, a patient can have multiple hospital admissions and each
    hospital admission can have multiple ICU stays. The data in eICU is centered
    around the ICU stay and all timestamps are relative to the ICU admission time.
    Thus, we only know the order of ICU stays within a hospital admission, but not
    the order of hospital admissions within a patient. As a result, we use `Patient`
    object to represent a hospital admission of a patient, and use `Visit` object to
    store the ICU stays within that hospital admission.

    We further support the following tables:
        - diagnosis: contains ICD diagnoses (ICD9CM and ICD10CM code)
            and diagnosis information (under attr_dict) for patients
        - treatment: contains treatment information (eICU_TREATMENTSTRING code)
            for patients.
        - medication: contains medication related order entries (eICU_DRUGNAME
            code) for patients.
        - lab: contains laboratory measurements (eICU_LABNAME code)
            for patients
        - physicalExam: contains all physical exam (eICU_PHYSICALEXAMPATH)
            conducted for patients.
        - admissionDx:  table contains the primary diagnosis for admission to
            the ICU per the APACHE scoring criteria. (eICU_ADMITDXPATH)

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data (should contain many csv files).
        tables: list of tables to be loaded (e.g., ["DIAGNOSES_ICD", "PROCEDURES_ICD"]).
        code_mapping: a dictionary containing the code mapping information.
            The key is a str of the source code vocabulary and the value is of
            two formats:
                (1) a str of the target code vocabulary;
                (2) a tuple with two elements. The first element is a str of the
                    target code vocabulary and the second element is a dict with
                    keys "source_kwargs" or "target_kwargs" and values of the
                    corresponding kwargs for the `CrossMap.map()` method.
            Default is empty dict, which means the original code will be used.
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.

    Attributes:
        task: Optional[str], name of the task (e.g., "mortality prediction").
            Default is None.
        samples: Optional[List[Dict]], a list of samples, each sample is a dict with
            patient_id, visit_id, and other task-specific attributes as key.
            Default is None.
        patient_to_index: Optional[Dict[str, List[int]]], a dict mapping patient_id to
            a list of sample indices. Default is None.
        visit_to_index: Optional[Dict[str, List[int]]], a dict mapping visit_id to a
            list of sample indices. Default is None.

    Examples:
        >>> from pyhealth.datasets import eICUDataset
        >>> dataset = eICUDataset(
        ...         root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        ...         tables=["diagnosis", "medication", "lab", "treatment", "physicalExam", "admissionDx"],
        ...     )
        >>> dataset.stat()
        >>> dataset.info()
    """

    def __init__(self, **kwargs):
        # store a mapping from visit_id to patient_id
        # will be used to parse clinical tables as they only contain visit_id
        self.visit_id_to_patient_id: Dict[str, str] = {}
        self.visit_id_to_encounter_time: Dict[str, datetime] = {}
        super(eICUDataset, self).__init__(**kwargs)

    def parse_basic_info(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper functions which parses patient and hospital tables.

        Will be called in `self.parse_tables()`.

        Docs:
            - patient: https://eicu-crd.mit.edu/eicutables/patient/
            - hospital: https://eicu-crd.mit.edu/eicutables/hospital/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.

        Note:
            We use `Patient` object to represent a hospital admission of a patient,
            and use `Visit` object to store the ICU stays within that hospital
            admission.
        """
        # read patient table
        patient_df = pd.read_csv(
            os.path.join(self.root, "patient.csv"),
            dtype={
                "uniquepid": str,
                "patienthealthsystemstayid": str,
                "patientunitstayid": str,
            },
            nrows=5000 if self.dev else None,
        )
        # read hospital table
        hospital_df = pd.read_csv(os.path.join(self.root, "hospital.csv"))
        hospital_df.region = hospital_df.region.fillna("Unknown").astype(str)
        # merge patient and hospital tables
        df = pd.merge(patient_df, hospital_df, on="hospitalid", how="left")
        # sort by ICU admission and discharge time
        df["neg_hospitaladmitoffset"] = -df["hospitaladmitoffset"]
        df = df.sort_values(
            [
                "uniquepid",
                "patienthealthsystemstayid",
                "neg_hospitaladmitoffset",
                "unitdischargeoffset",
            ],
            ascending=True,
        )
        # group by patient and hospital admission
        df_group = df.groupby(["uniquepid", "patienthealthsystemstayid"])
        # load patients
        for (p_id, ha_id), p_info in tqdm(df_group, desc="Parsing patients"):
            # each Patient object is a single hospital admission of a patient
            patient_id = f"{p_id}+{ha_id}"

            # hospital admission time (Jan 1 of hospitaldischargeyear, 00:00:00)
            ha_datetime = strptime(padyear(str(p_info["hospitaldischargeyear"].values[0])))

            # no exact birth datetime in eICU
            # use hospital admission time and age to approximate birth datetime
            age = p_info["age"].values[0]
            if pd.isna(age):
                birth_datetime = None
            elif age == "> 89":
                birth_datetime = ha_datetime - pd.DateOffset(years=89)
            else:
                birth_datetime = ha_datetime - pd.DateOffset(years=int(age))

            # no exact death datetime in eICU
            # use hospital discharge time to approximate death datetime
            death_datetime = None
            if p_info["hospitaldischargestatus"].values[0] == "Expired":
                ha_los_min = (
                    p_info["hospitaldischargeoffset"].values[0]
                    - p_info["hospitaladmitoffset"].values[0]
                )
                death_datetime = ha_datetime + pd.Timedelta(minutes=ha_los_min)

            patient = Patient(
                patient_id=patient_id,
                birth_datetime=birth_datetime,
                death_datetime=death_datetime,
                gender=p_info["gender"].values[0],
                ethnicity=p_info["ethnicity"].values[0],
            )

            # load visits
            for v_id, v_info in p_info.groupby("patientunitstayid"):
                # each Visit object is a single ICU stay within a hospital admission

                # base time is the hospital admission time
                unit_admit = v_info["neg_hospitaladmitoffset"].values[0]
                unit_discharge = unit_admit + v_info["unitdischargeoffset"].values[0]
                encounter_time = ha_datetime + pd.Timedelta(minutes=unit_admit)
                discharge_time = ha_datetime + pd.Timedelta(minutes=unit_discharge)

                visit = Visit(
                    visit_id=v_id,
                    patient_id=patient_id,
                    encounter_time=encounter_time,
                    discharge_time=discharge_time,
                    discharge_status=v_info["unitdischargestatus"].values[0],
                    hospital_id=v_info["hospitalid"].values[0],
                    region=v_info["region"].values[0],
                )

                # add visit
                patient.add_visit(visit)
                # add visit id to patient id mapping
                self.visit_id_to_patient_id[v_id] = patient_id
                # add visit id to encounter time mapping
                self.visit_id_to_encounter_time[v_id] = encounter_time
            # add patient
            patients[patient_id] = patient
        return patients

    def parse_diagnosis(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses diagnosis table.

        Will be called in `self.parse_tables()`.

        Docs:
            - diagnosis: https://eicu-crd.mit.edu/eicutables/diagnosis/

        Args:
            patients: a dict of Patient objects indexed by patient_id.

        Returns:
            The updated patients dict.

        Note:
            This table contains both ICD9CM and ICD10CM codes in one single
                cell. We need to use medcode to distinguish them.
        """

        # load ICD9CM and ICD10CM coding systems
        from pyhealth.medcode import ICD9CM, ICD10CM

        icd9cm = ICD9CM()
        icd10cm = ICD10CM()

        def icd9cm_or_icd10cm(code):
            if code in icd9cm:
                return "ICD9CM"
            elif code in icd10cm:
                return "ICD10CM"
            else:
                return "Unknown"

        table = "diagnosis"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"patientunitstayid": str, "icd9code": str, "diagnosisstring": str},
        )
        # drop rows with missing values
        df = df.dropna(subset=["patientunitstayid", "icd9code", "diagnosisstring"])
        # sort by diagnosisoffset
        df = df.sort_values(["patientunitstayid", "diagnosisoffset"], ascending=True)
        # add the patient id info
        df["patient_id"] = df["patientunitstayid"].apply(
            lambda x: self.visit_id_to_patient_id.get(x, None)
        )
        # add the visit encounter time info
        df["v_encounter_time"] = df["patientunitstayid"].apply(
            lambda x: self.visit_id_to_encounter_time.get(x, None)
        )
        # group by visit
        group_df = df.groupby("patientunitstayid")

        # parallel unit of diagnosis (per visit)
        def diagnosis_unit(v_info):
            v_id = v_info["patientunitstayid"].values[0]
            patient_id = v_info["patient_id"].values[0]
            v_encounter_time = v_info["v_encounter_time"].values[0]
            if patient_id is None:
                return []

            events = []
            for offset, codes, dxstr in zip(v_info["diagnosisoffset"], v_info["icd9code"],
                                            v_info["diagnosisstring"]):
                # compute the absolute timestamp
                timestamp = v_encounter_time + pd.Timedelta(minutes=offset)
                codes = [c.strip() for c in codes.split(",")]
                # for each code in a single cell (mixed ICD9CM and ICD10CM)
                for code in codes:
                    vocab = icd9cm_or_icd10cm(code)
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary=vocab,
                        visit_id=v_id,
                        patient_id=patient_id,
                        timestamp=timestamp,
                        diagnosisString=dxstr
                    )
                    # update patients
                    events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(lambda x: diagnosis_unit(x))

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients

    def parse_treatment(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses treatment table.

        Will be called in `self.parse_tables()`.

        Docs:
            - treatment: https://eicu-crd.mit.edu/eicutables/treatment/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "treatment"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"patientunitstayid": str, "treatmentstring": str},
        )
        # drop rows with missing values
        df = df.dropna(subset=["patientunitstayid", "treatmentstring"])
        # sort by treatmentoffset
        df = df.sort_values(["patientunitstayid", "treatmentoffset"], ascending=True)
        # add the patient id info
        df["patient_id"] = df["patientunitstayid"].apply(
            lambda x: self.visit_id_to_patient_id.get(x, None)
        )
        # add the visit encounter time info
        df["v_encounter_time"] = df["patientunitstayid"].apply(
            lambda x: self.visit_id_to_encounter_time.get(x, None)
        )
        # group by visit
        group_df = df.groupby("patientunitstayid")

        # parallel unit of treatment (per visit)
        def treatment_unit(v_info):
            v_id = v_info["patientunitstayid"].values[0]
            patient_id = v_info["patient_id"].values[0]
            v_encounter_time = v_info["v_encounter_time"].values[0]
            if patient_id is None:
                return []

            events = []
            for offset, code in zip(
                v_info["treatmentoffset"], v_info["treatmentstring"]
            ):
                # compute the absolute timestamp
                timestamp = v_encounter_time + pd.Timedelta(minutes=offset)
                event = Event(
                    code=code,
                    table=table,
                    vocabulary="eICU_TREATMENTSTRING",
                    visit_id=v_id,
                    patient_id=patient_id,
                    timestamp=timestamp,
                )
                # update patients
                events.append(event)

            return events

        # parallel apply
        group_df = group_df.parallel_apply(lambda x: treatment_unit(x))

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients

    def parse_medication(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses medication table.

        Will be called in `self.parse_tables()`.

        Docs:
            - medication: https://eicu-crd.mit.edu/eicutables/medication/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "medication"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            low_memory=False,
            dtype={"patientunitstayid": str, "drugname": str},
        )
        # drop rows with missing values
        df = df.dropna(subset=["patientunitstayid", "drugname"])
        # sort by drugstartoffset
        df = df.sort_values(["patientunitstayid", "drugstartoffset"], ascending=True)
        # add the patient id info
        df["patient_id"] = df["patientunitstayid"].apply(
            lambda x: self.visit_id_to_patient_id.get(x, None)
        )
        # add the visit encounter time info
        df["v_encounter_time"] = df["patientunitstayid"].apply(
            lambda x: self.visit_id_to_encounter_time.get(x, None)
        )
        # group by visit
        group_df = df.groupby("patientunitstayid")

        # parallel unit of medication (per visit)
        def medication_unit(v_info):
            v_id = v_info["patientunitstayid"].values[0]
            patient_id = v_info["patient_id"].values[0]
            v_encounter_time = v_info["v_encounter_time"].values[0]
            if patient_id is None:
                return []

            events = []
            for offset, code in zip(v_info["drugstartoffset"], v_info["drugname"]):
                # compute the absolute timestamp
                timestamp = v_encounter_time + pd.Timedelta(minutes=offset)
                event = Event(
                    code=code,
                    table=table,
                    vocabulary="eICU_DRUGNAME",
                    visit_id=v_id,
                    patient_id=patient_id,
                    timestamp=timestamp,
                )
                # update patients
                events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(lambda x: medication_unit(x))

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients

    def parse_lab(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses lab table.

        Will be called in `self.parse_tables()`.

        Docs:
            - lab: https://eicu-crd.mit.edu/eicutables/lab/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "lab"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"patientunitstayid": str, "labname": str},
        )
        # drop rows with missing values
        df = df.dropna(subset=["patientunitstayid", "labname"])
        # sort by labresultoffset
        df = df.sort_values(["patientunitstayid", "labresultoffset"], ascending=True)
        # add the patient id info
        df["patient_id"] = df["patientunitstayid"].apply(
            lambda x: self.visit_id_to_patient_id.get(x, None)
        )
        # add the visit encounter time info
        df["v_encounter_time"] = df["patientunitstayid"].apply(
            lambda x: self.visit_id_to_encounter_time.get(x, None)
        )
        # group by visit
        group_df = df.groupby("patientunitstayid")

        # parallel unit of lab (per visit)
        def lab_unit(v_info):
            v_id = v_info["patientunitstayid"].values[0]
            patient_id = v_info["patient_id"].values[0]
            v_encounter_time = v_info["v_encounter_time"].values[0]
            if patient_id is None:
                return []

            events = []
            for offset, code in zip(v_info["labresultoffset"], v_info["labname"]):
                # compute the absolute timestamp
                timestamp = v_encounter_time + pd.Timedelta(minutes=offset)
                event = Event(
                    code=code,
                    table=table,
                    vocabulary="eICU_LABNAME",
                    visit_id=v_id,
                    patient_id=patient_id,
                    timestamp=timestamp,
                )
                # update patients
                events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(lambda x: lab_unit(x))

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients

    def parse_physicalexam(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses physicalExam table.

        Will be called in `self.parse_tables()`.

        Docs:
            - physicalExam: https://eicu-crd.mit.edu/eicutables/physicalexam/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "physicalExam"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"patientunitstayid": str, "physicalexampath": str},
        )
        # drop rows with missing values
        df = df.dropna(subset=["patientunitstayid", "physicalexampath"])
        # sort by treatmentoffset
        df = df.sort_values(["patientunitstayid", "physicalexamoffset"], ascending=True)
        # add the patient id info
        df["patient_id"] = df["patientunitstayid"].apply(
            lambda x: self.visit_id_to_patient_id.get(x, None)
        )
        # add the visit encounter time info
        df["v_encounter_time"] = df["patientunitstayid"].apply(
            lambda x: self.visit_id_to_encounter_time.get(x, None)
        )
        # group by visit
        group_df = df.groupby("patientunitstayid")

        # parallel unit of physicalExam (per visit)
        def physicalExam_unit(v_info):
            v_id = v_info["patientunitstayid"].values[0]
            patient_id = v_info["patient_id"].values[0]
            v_encounter_time = v_info["v_encounter_time"].values[0]
            if patient_id is None:
                return []

            events = []
            for offset, code in zip(
                v_info["physicalexamoffset"], v_info["physicalexampath"]
            ):
                # compute the absolute timestamp
                timestamp = v_encounter_time + pd.Timedelta(minutes=offset)
                event = Event(
                    code=code,
                    table=table,
                    vocabulary="eICU_PHYSICALEXAMPATH",
                    visit_id=v_id,
                    patient_id=patient_id,
                    timestamp=timestamp,
                )
                # update patients
                events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(lambda x: physicalExam_unit(x))

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients

    def parse_admissiondx(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses admissionDx (admission diagnosis) table.

        Will be called in `self.parse_tables()`.

        Docs:
            - admissionDx: https://eicu-crd.mit.edu/eicutables/admissiondx/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "admissionDx"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"patientunitstayid": str, "admitdxpath": str},
        )
        # drop rows with missing values
        df = df.dropna(subset=["patientunitstayid", "admitdxpath"])
        # sort by admitDxEnteredOffset
        df = df.sort_values(["patientunitstayid", "admitdxenteredoffset"], ascending=True)
        # add the patient id info
        df["patient_id"] = df["patientunitstayid"].apply(
            lambda x: self.visit_id_to_patient_id.get(x, None)
        )
        # add the visit encounter time info
        df["v_encounter_time"] = df["patientunitstayid"].apply(
            lambda x: self.visit_id_to_encounter_time.get(x, None)
        )
        # group by visit
        group_df = df.groupby("patientunitstayid")

        # parallel unit of admissionDx (per visit)
        def admissionDx_unit(v_info):
            v_id = v_info["patientunitstayid"].values[0]
            patient_id = v_info["patient_id"].values[0]
            v_encounter_time = v_info["v_encounter_time"].values[0]
            if patient_id is None:
                return []

            events = []
            for offset, code in zip(
                v_info["admitdxenteredoffset"], v_info["admitdxpath"]
            ):
                # compute the absolute timestamp
                timestamp = v_encounter_time + pd.Timedelta(minutes=offset)
                event = Event(
                    code=code,
                    table=table,
                    vocabulary="eICU_ADMITDXPATH",
                    visit_id=v_id,
                    patient_id=patient_id,
                    timestamp=timestamp,
                )
                # update patients
                events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(lambda x: admissionDx_unit(x))

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients


if __name__ == "__main__":
    dataset = eICUDataset(
        root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        tables=["diagnosis", "medication", "lab", "treatment", "physicalExam", "admissionDx"],
        refresh_cache=True,
    )
    dataset.stat()
    dataset.info()


Here is the code content for isruc.py:
import subprocess
import shutil
import os
from typing import List
from urllib.request import urlretrieve

from pyhealth.datasets import BaseSignalDataset

DEV_NUM_PAT = 5
FULL_NUM_PAT = 100


def _download_file(online_filepath, local_filepath, refresh_cache=False):
    if (not os.path.exists(local_filepath)) or refresh_cache:
        urlretrieve(online_filepath, local_filepath)
    return local_filepath


def _unrar_function(rar_path, dst_path):
    if os.name == "nt":
        try:
            import patoolib
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "Please install patool to download ISRUC data. \
            You might need to have 7z/rar/unrar installed as well."
            )

        patoolib.extract_archive(rar_path, outdir=dst_path)
    else:
        # Linux, we use 7zzs, which can be downloaded from https://www.7-zip.org/download.html
        path_7z = shutil.which("7zzs")
        assert (
            path_7z is not None
        ), "Please download 7z for linux, reference: https://www.7-zip.org/download.html"
        subprocess.call([path_7z, "x", rar_path, f"-o{dst_path}"])


def _download_ISRUC_group1(data_dir: str, dev: bool, exclude_subjects: List[int]):
    """Download all group 1 data for ISRUC.

    Args:
        data_dir (str):
            path to download the data.
        exclude_subjects (List[int]):
            List of subjects to exclude.
    Returns:
        raw_dir: directory the dataset is extracted to (in data_dir).
    """
    rar_dir = os.path.join(data_dir, "rar_files")
    raw_dir = os.path.join(data_dir, "raw")
    for _ in [rar_dir, raw_dir]:
        if not os.path.isdir(_):
            os.makedirs(_)
    exclude_subjects = set(exclude_subjects)

    NUM_PAT = DEV_NUM_PAT if dev else FULL_NUM_PAT
    print(f"Downloading ISRUC Group 1 data, the first {NUM_PAT} patients")

    for subject_id in range(1, NUM_PAT + 1):
        if subject_id in exclude_subjects:
            continue
        if os.path.isfile(os.path.join(raw_dir, f"{subject_id}/{subject_id}.edf")):
            continue
        rar_url = f"http://dataset.isr.uc.pt/ISRUC_Sleep/subgroupI/{subject_id}.rar"
        rar_dst = os.path.join(rar_dir, f"{subject_id}.rar")
        _download_file(rar_url, rar_dst)
        _unrar_function(rar_dst, raw_dir)
        os.rename(
            os.path.join(raw_dir, f"{subject_id}/{subject_id}.rec"),
            os.path.join(raw_dir, f"{subject_id}/{subject_id}.edf"),
        )
    return raw_dir


class ISRUCDataset(BaseSignalDataset):
    """Base EEG dataset for ISRUC Group I.

    Dataset is available at https://sleeptight.isr.uc.pt/

        - The EEG signals are sampled at 200 Hz.
        - There are 100 subjects in the orignal dataset.
        - Each subject's data is about a night's sleep.


    Args:
        dataset_name: name of the dataset.
            Default is 'ISRUCDataset'.
        root: root directory of the raw data.
            We expect `root/raw` to contain all extracted files (.txt, .rec, ...)
            You can also download the data to a new directory by using download=True.
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: Whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.
        download: Whether to download the data automatically.
            Default is False.


    Examples:
        >>> from pyhealth.datasets import ISRUCDataset
        >>> dataset = ISRUCDataset(
        ...         root="/srv/local/data/data/ISRUC-I",
        ...         download=True,
        ...     )
        >>> dataset.stat()
        >>> dataset.info()
    """

    _EXCLUDE_SUBJECTS = [8]  # This subject has missing channels

    def process_EEG_data(self):
        # download the data or check if the data exists
        if ("download" in self.kwargs) and self.kwargs["download"]:
            _download_ISRUC_group1(
                self.root, self.dev, exclude_subjects=self._EXCLUDE_SUBJECTS
            )
        else:
            assert os.path.exists(
                os.path.join(self.root, "raw")
            ), "raw data {root}/raw does not exist, please \
                download the data by enabling 'download=True' first."

        raw_dir = os.path.join(self.root, "raw")
        subject_ids = os.listdir(raw_dir)
        if self.dev:
            subject_ids = subject_ids[:DEV_NUM_PAT]
        subjects = {
            subject_id: [
                {
                    "load_from_path": raw_dir,
                    "signal_file": f"{subject_id}/{subject_id}.edf",
                    "label1_file": f"{subject_id}/{subject_id}_1.txt",
                    "label2_file": f"{subject_id}/{subject_id}_2.txt",
                    "save_to_path": self.filepath,
                    "subject_id": subject_id,
                }
            ]
            for subject_id in subject_ids
        }
        return subjects


if __name__ == "__main__":
    dataset = ISRUCDataset(
        root="/srv/local/data/trash/",
        dev=True,
        refresh_cache=True,
        download=True,
    )

    dataset.stat()
    dataset.info()
    print(list(dataset.patients.items())[0])


Here is the code content for mimic3.py:
import os
from typing import Optional, List, Dict, Tuple, Union

import pandas as pd

from pyhealth.data import Event, Visit, Patient
from pyhealth.datasets import BaseEHRDataset
from pyhealth.datasets.utils import strptime

# TODO: add other tables


class MIMIC3Dataset(BaseEHRDataset):
    """Base dataset for MIMIC-III dataset.

    The MIMIC-III dataset is a large dataset of de-identified health records of ICU
    patients. The dataset is available at https://mimic.physionet.org/.

    The basic information is stored in the following tables:
        - PATIENTS: defines a patient in the database, SUBJECT_ID.
        - ADMISSIONS: defines a patient's hospital admission, HADM_ID.

    We further support the following tables:
        - DIAGNOSES_ICD: contains ICD-9 diagnoses (ICD9CM code) for patients.
        - PROCEDURES_ICD: contains ICD-9 procedures (ICD9PROC code) for patients.
        - PRESCRIPTIONS: contains medication related order entries (NDC code)
            for patients.
        - LABEVENTS: contains laboratory measurements (MIMIC3_ITEMID code)
            for patients

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data (should contain many csv files).
        tables: list of tables to be loaded (e.g., ["DIAGNOSES_ICD", "PROCEDURES_ICD"]).
        code_mapping: a dictionary containing the code mapping information.
            The key is a str of the source code vocabulary and the value is of
            two formats:
                (1) a str of the target code vocabulary;
                (2) a tuple with two elements. The first element is a str of the
                    target code vocabulary and the second element is a dict with
                    keys "source_kwargs" or "target_kwargs" and values of the
                    corresponding kwargs for the `CrossMap.map()` method.
            Default is empty dict, which means the original code will be used.
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.

    Attributes:
        task: Optional[str], name of the task (e.g., "mortality prediction").
            Default is None.
        samples: Optional[List[Dict]], a list of samples, each sample is a dict with
            patient_id, visit_id, and other task-specific attributes as key.
            Default is None.
        patient_to_index: Optional[Dict[str, List[int]]], a dict mapping patient_id to
            a list of sample indices. Default is None.
        visit_to_index: Optional[Dict[str, List[int]]], a dict mapping visit_id to a
            list of sample indices. Default is None.

    Examples:
        >>> from pyhealth.datasets import MIMIC3Dataset
        >>> dataset = MIMIC3Dataset(
        ...         root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        ...         tables=["DIAGNOSES_ICD", "PRESCRIPTIONS"],
        ...         code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
        ...     )
        >>> dataset.stat()
        >>> dataset.info()
    """

    def parse_basic_info(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses PATIENTS and ADMISSIONS tables.

        Will be called in `self.parse_tables()`

        Docs:
            - PATIENTS: https://mimic.mit.edu/docs/iii/tables/patients/
            - ADMISSIONS: https://mimic.mit.edu/docs/iii/tables/admissions/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id which is updated with the mimic-3 table result.

        Returns:
            The updated patients dict.
        """
        # read patients table
        patients_df = pd.read_csv(
            os.path.join(self.root, "PATIENTS.csv"),
            dtype={"SUBJECT_ID": str},
            nrows=1000 if self.dev else None,
        )
        # read admissions table
        admissions_df = pd.read_csv(
            os.path.join(self.root, "ADMISSIONS.csv"),
            dtype={"SUBJECT_ID": str, "HADM_ID": str},
        )
        # merge patient and admission tables
        df = pd.merge(patients_df, admissions_df, on="SUBJECT_ID", how="inner")
        # sort by admission and discharge time
        df = df.sort_values(["SUBJECT_ID", "ADMITTIME", "DISCHTIME"], ascending=True)
        # group by patient
        df_group = df.groupby("SUBJECT_ID")

        # parallel unit of basic information (per patient)
        def basic_unit(p_id, p_info):
            patient = Patient(
                patient_id=p_id,
                birth_datetime=strptime(p_info["DOB"].values[0]),
                death_datetime=strptime(p_info["DOD_HOSP"].values[0]),
                gender=p_info["GENDER"].values[0],
                ethnicity=p_info["ETHNICITY"].values[0],
            )
            # load visits
            for v_id, v_info in p_info.groupby("HADM_ID"):
                visit = Visit(
                    visit_id=v_id,
                    patient_id=p_id,
                    encounter_time=strptime(v_info["ADMITTIME"].values[0]),
                    discharge_time=strptime(v_info["DISCHTIME"].values[0]),
                    discharge_status=v_info["HOSPITAL_EXPIRE_FLAG"].values[0],
                )
                # add visit
                patient.add_visit(visit)
            return patient

        # parallel apply
        df_group = df_group.parallel_apply(
            lambda x: basic_unit(x.SUBJECT_ID.unique()[0], x)
        )
        # summarize the results
        for pat_id, pat in df_group.items():
            patients[pat_id] = pat

        return patients

    def parse_diagnoses_icd(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses DIAGNOSES_ICD table.

        Will be called in `self.parse_tables()`

        Docs:
            - DIAGNOSES_ICD: https://mimic.mit.edu/docs/iii/tables/diagnoses_icd/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.

        Note:
            MIMIC-III does not provide specific timestamps in DIAGNOSES_ICD
                table, so we set it to None.
        """
        table = "DIAGNOSES_ICD"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"SUBJECT_ID": str, "HADM_ID": str, "ICD9_CODE": str},
        )
        # drop records of the other patients
        df = df[df["SUBJECT_ID"].isin(patients.keys())]
        # drop rows with missing values
        df = df.dropna(subset=["SUBJECT_ID", "HADM_ID", "ICD9_CODE"])
        # sort by sequence number (i.e., priority)
        df = df.sort_values(["SUBJECT_ID", "HADM_ID", "SEQ_NUM"], ascending=True)
        # group by patient and visit
        group_df = df.groupby("SUBJECT_ID")

        # parallel unit of diagnosis (per patient)
        def diagnosis_unit(p_id, p_info):
            events = []
            for v_id, v_info in p_info.groupby("HADM_ID"):
                for code in v_info["ICD9_CODE"]:
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary="ICD9CM",
                        visit_id=v_id,
                        patient_id=p_id,
                    )
                    events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(
            lambda x: diagnosis_unit(x.SUBJECT_ID.unique()[0], x)
        )

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients

    def parse_procedures_icd(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses PROCEDURES_ICD table.

        Will be called in `self.parse_tables()`

        Docs:
            - PROCEDURES_ICD: https://mimic.mit.edu/docs/iii/tables/procedures_icd/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.

        Note:
            MIMIC-III does not provide specific timestamps in PROCEDURES_ICD
                table, so we set it to None.
        """
        table = "PROCEDURES_ICD"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"SUBJECT_ID": str, "HADM_ID": str, "ICD9_CODE": str},
        )
        # drop records of the other patients
        df = df[df["SUBJECT_ID"].isin(patients.keys())]
        # drop rows with missing values
        df = df.dropna(subset=["SUBJECT_ID", "HADM_ID", "SEQ_NUM", "ICD9_CODE"])
        # sort by sequence number (i.e., priority)
        df = df.sort_values(["SUBJECT_ID", "HADM_ID", "SEQ_NUM"], ascending=True)
        # group by patient and visit
        group_df = df.groupby("SUBJECT_ID")

        # parallel unit of procedure (per patient)
        def procedure_unit(p_id, p_info):
            events = []
            for v_id, v_info in p_info.groupby("HADM_ID"):
                for code in v_info["ICD9_CODE"]:
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary="ICD9PROC",
                        visit_id=v_id,
                        patient_id=p_id,
                    )
                    events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(
            lambda x: procedure_unit(x.SUBJECT_ID.unique()[0], x)
        )

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients

    def parse_prescriptions(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses PRESCRIPTIONS table.

        Will be called in `self.parse_tables()`

        Docs:
            - PRESCRIPTIONS: https://mimic.mit.edu/docs/iii/tables/prescriptions/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "PRESCRIPTIONS"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            low_memory=False,
            dtype={"SUBJECT_ID": str, "HADM_ID": str, "NDC": str},
        )
        # drop records of the other patients
        df = df[df["SUBJECT_ID"].isin(patients.keys())]
        # drop rows with missing values
        df = df.dropna(subset=["SUBJECT_ID", "HADM_ID", "NDC"])
        # sort by start date and end date
        df = df.sort_values(
            ["SUBJECT_ID", "HADM_ID", "STARTDATE", "ENDDATE"], ascending=True
        )
        # group by patient and visit
        group_df = df.groupby("SUBJECT_ID")

        # parallel unit for prescription (per patient)
        def prescription_unit(p_id, p_info):
            events = []
            for v_id, v_info in p_info.groupby("HADM_ID"):
                for timestamp, code in zip(v_info["STARTDATE"], v_info["NDC"]):
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary="NDC",
                        visit_id=v_id,
                        patient_id=p_id,
                        timestamp=strptime(timestamp),
                    )
                    events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(
            lambda x: prescription_unit(x.SUBJECT_ID.unique()[0], x)
        )

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients

    def parse_labevents(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses LABEVENTS table.

        Will be called in `self.parse_tables()`

        Docs:
            - LABEVENTS: https://mimic.mit.edu/docs/iii/tables/labevents/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "LABEVENTS"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"SUBJECT_ID": str, "HADM_ID": str, "ITEMID": str},
        )
        # drop records of the other patients
        df = df[df["SUBJECT_ID"].isin(patients.keys())]
        # drop rows with missing values
        df = df.dropna(subset=["SUBJECT_ID", "HADM_ID", "ITEMID"])
        # sort by charttime
        df = df.sort_values(["SUBJECT_ID", "HADM_ID", "CHARTTIME"], ascending=True)
        # group by patient and visit
        group_df = df.groupby("SUBJECT_ID")

        # parallel unit for lab (per patient)
        def lab_unit(p_id, p_info):
            events = []
            for v_id, v_info in p_info.groupby("HADM_ID"):
                for timestamp, code in zip(v_info["CHARTTIME"], v_info["ITEMID"]):
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary="MIMIC3_ITEMID",
                        visit_id=v_id,
                        patient_id=p_id,
                        timestamp=strptime(timestamp),
                    )
                    events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(
            lambda x: lab_unit(x.SUBJECT_ID.unique()[0], x)
        )

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients


if __name__ == "__main__":
    dataset = MIMIC3Dataset(
        root="https://storage.googleapis.com/pyhealth/mimiciii-demo/1.4/",
        tables=[
            "DIAGNOSES_ICD",
            "PROCEDURES_ICD",
            "PRESCRIPTIONS",
            "LABEVENTS",
        ],
        code_mapping={"NDC": "ATC"},
        dev=True,
        refresh_cache=True,
    )
    dataset.stat()
    dataset.info()

    # dataset = MIMIC3Dataset(
    #     root="/srv/local/data/physionet.org/files/mimiciii/1.4",
    #     tables=["DIAGNOSES_ICD", "PRESCRIPTIONS"],
    #     dev=True,
    #     code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
    #     refresh_cache=False,
    # )
    # print(dataset.stat())
    # print(dataset.available_tables)
    # print(list(dataset.patients.values())[4])


Here is the code content for mimic4.py:
import os
from typing import Optional, List, Dict, Union, Tuple

import pandas as pd

from pyhealth.data import Event, Visit, Patient
from pyhealth.datasets import BaseEHRDataset
from pyhealth.datasets.utils import strptime

# TODO: add other tables


class MIMIC4Dataset(BaseEHRDataset):
    """Base dataset for MIMIC-IV dataset.

    The MIMIC-IV dataset is a large dataset of de-identified health records of ICU
    patients. The dataset is available at https://mimic.physionet.org/.

    The basic information is stored in the following tables:
        - patients: defines a patient in the database, subject_id.
        - admission: define a patient's hospital admission, hadm_id.

    We further support the following tables:
        - diagnoses_icd: contains ICD diagnoses (ICD9CM and ICD10CM code)
            for patients.
        - procedures_icd: contains ICD procedures (ICD9PROC and ICD10PROC
            code) for patients.
        - prescriptions: contains medication related order entries (NDC code)
            for patients.
        - labevents: contains laboratory measurements (MIMIC4_ITEMID code)
            for patients

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data (should contain many csv files).
        tables: list of tables to be loaded (e.g., ["DIAGNOSES_ICD", "PROCEDURES_ICD"]).
        code_mapping: a dictionary containing the code mapping information.
            The key is a str of the source code vocabulary and the value is of
            two formats:
                (1) a str of the target code vocabulary;
                (2) a tuple with two elements. The first element is a str of the
                    target code vocabulary and the second element is a dict with
                    keys "source_kwargs" or "target_kwargs" and values of the
                    corresponding kwargs for the `CrossMap.map()` method.
            Default is empty dict, which means the original code will be used.
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.

    Attributes:
        task: Optional[str], name of the task (e.g., "mortality prediction").
            Default is None.
        samples: Optional[List[Dict]], a list of samples, each sample is a dict with
            patient_id, visit_id, and other task-specific attributes as key.
            Default is None.
        patient_to_index: Optional[Dict[str, List[int]]], a dict mapping patient_id to
            a list of sample indices. Default is None.
        visit_to_index: Optional[Dict[str, List[int]]], a dict mapping visit_id to a
            list of sample indices. Default is None.

    Examples:
        >>> from pyhealth.datasets import MIMIC4Dataset
        >>> dataset = MIMIC4Dataset(
        ...         root="/srv/local/data/physionet.org/files/mimiciv/2.0/hosp",
        ...         tables=["diagnoses_icd", "procedures_icd", "prescriptions", "labevents"],
        ...         code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
        ...     )
        >>> dataset.stat()
        >>> dataset.info()
    """

    def parse_basic_info(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper functions which parses patients and admissions tables.

        Will be called in `self.parse_tables()`

        Docs:
            - patients:https://mimic.mit.edu/docs/iv/modules/hosp/patients/
            - admissions: https://mimic.mit.edu/docs/iv/modules/hosp/admissions/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        # read patients table
        patients_df = pd.read_csv(
            os.path.join(self.root, "patients.csv"),
            dtype={"subject_id": str},
            nrows=1000 if self.dev else None,
        )
        # read admissions table
        admissions_df = pd.read_csv(
            os.path.join(self.root, "admissions.csv"),
            dtype={"subject_id": str, "hadm_id": str},
        )
        # merge patients and admissions tables
        df = pd.merge(patients_df, admissions_df, on="subject_id", how="inner")
        # sort by admission and discharge time
        df = df.sort_values(["subject_id", "admittime", "dischtime"], ascending=True)
        # group by patient
        df_group = df.groupby("subject_id")

        # parallel unit of basic information (per patient)
        def basic_unit(p_id, p_info):
            # no exact birth datetime in MIMIC-IV
            # use anchor_year and anchor_age to approximate birth datetime
            anchor_year = int(p_info["anchor_year"].values[0])
            anchor_age = int(p_info["anchor_age"].values[0])
            birth_year = anchor_year - anchor_age
            patient = Patient(
                patient_id=p_id,
                # no exact month, day, and time, use Jan 1st, 00:00:00
                birth_datetime=strptime(str(birth_year)),
                # no exact time, use 00:00:00
                death_datetime=strptime(p_info["dod"].values[0]),
                gender=p_info["gender"].values[0],
                ethnicity=p_info["race"].values[0],
                anchor_year_group=p_info["anchor_year_group"].values[0],
            )
            # load visits
            for v_id, v_info in p_info.groupby("hadm_id"):
                visit = Visit(
                    visit_id=v_id,
                    patient_id=p_id,
                    encounter_time=strptime(v_info["admittime"].values[0]),
                    discharge_time=strptime(v_info["dischtime"].values[0]),
                    discharge_status=v_info["hospital_expire_flag"].values[0],
                )
                # add visit
                patient.add_visit(visit)
            return patient

        # parallel apply
        df_group = df_group.parallel_apply(
            lambda x: basic_unit(x.subject_id.unique()[0], x)
        )
        # summarize the results
        for pat_id, pat in df_group.items():
            patients[pat_id] = pat

        return patients

    def parse_diagnoses_icd(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses diagnosis_icd table.

        Will be called in `self.parse_tables()`

        Docs:
            - diagnosis_icd: https://mimic.mit.edu/docs/iv/modules/hosp/diagnoses_icd/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.

        Note:
            MIMIC-IV does not provide specific timestamps in diagnoses_icd
                table, so we set it to None.
        """
        table = "diagnoses_icd"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"subject_id": str, "hadm_id": str, "icd_code": str},
        )
        # drop rows with missing values
        df = df.dropna(subset=["subject_id", "hadm_id", "icd_code", "icd_version"])
        # sort by sequence number (i.e., priority)
        df = df.sort_values(["subject_id", "hadm_id", "seq_num"], ascending=True)
        # group by patient and visit
        group_df = df.groupby("subject_id")

        # parallel unit of diagnosis (per patient)
        def diagnosis_unit(p_id, p_info):
            events = []
            # iterate over each patient and visit
            for v_id, v_info in p_info.groupby("hadm_id"):
                for code, version in zip(v_info["icd_code"], v_info["icd_version"]):
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary=f"ICD{version}CM",
                        visit_id=v_id,
                        patient_id=p_id,
                    )
                    events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(
            lambda x: diagnosis_unit(x.subject_id.unique()[0], x)
        )

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients

    def parse_procedures_icd(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses procedures_icd table.

        Will be called in `self.parse_tables()`

        Docs:
            - procedures_icd: https://mimic.mit.edu/docs/iv/modules/hosp/procedures_icd/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.

        Note:
            MIMIC-IV does not provide specific timestamps in procedures_icd
                table, so we set it to None.
        """
        table = "procedures_icd"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"subject_id": str, "hadm_id": str, "icd_code": str},
        )
        # drop rows with missing values
        df = df.dropna(subset=["subject_id", "hadm_id", "icd_code", "icd_version"])
        # sort by sequence number (i.e., priority)
        df = df.sort_values(["subject_id", "hadm_id", "seq_num"], ascending=True)
        # group by patient and visit
        group_df = df.groupby("subject_id")

        # parallel unit of procedure (per patient)
        def procedure_unit(p_id, p_info):
            events = []
            for v_id, v_info in p_info.groupby("hadm_id"):
                for code, version in zip(v_info["icd_code"], v_info["icd_version"]):
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary=f"ICD{version}PROC",
                        visit_id=v_id,
                        patient_id=p_id,
                    )
                    # update patients
                    events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(
            lambda x: procedure_unit(x.subject_id.unique()[0], x)
        )

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)

        return patients

    def parse_prescriptions(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses prescriptions table.

        Will be called in `self.parse_tables()`

        Docs:
            - prescriptions: https://mimic.mit.edu/docs/iv/modules/hosp/prescriptions/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "prescriptions"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            low_memory=False,
            dtype={"subject_id": str, "hadm_id": str, "ndc": str},
        )
        # drop rows with missing values
        df = df.dropna(subset=["subject_id", "hadm_id", "ndc"])
        # sort by start date and end date
        df = df.sort_values(
            ["subject_id", "hadm_id", "starttime", "stoptime"], ascending=True
        )
        # group by patient and visit
        group_df = df.groupby("subject_id")

        # parallel unit of prescription (per patient)
        def prescription_unit(p_id, p_info):
            events = []
            for v_id, v_info in p_info.groupby("hadm_id"):
                for timestamp, code in zip(v_info["starttime"], v_info["ndc"]):
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary="NDC",
                        visit_id=v_id,
                        patient_id=p_id,
                        timestamp=strptime(timestamp),
                    )
                    # update patients
                    events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(
            lambda x: prescription_unit(x.subject_id.unique()[0], x)
        )

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)

        return patients

    def parse_labevents(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses labevents table.

        Will be called in `self.parse_tables()`

        Docs:
            - labevents: https://mimic.mit.edu/docs/iv/modules/hosp/labevents/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "labevents"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"subject_id": str, "hadm_id": str, "itemid": str},
        )
        # drop rows with missing values
        df = df.dropna(subset=["subject_id", "hadm_id", "itemid"])
        # sort by charttime
        df = df.sort_values(["subject_id", "hadm_id", "charttime"], ascending=True)
        # group by patient and visit
        group_df = df.groupby("subject_id")

        # parallel unit of labevent (per patient)
        def lab_unit(p_id, p_info):
            events = []
            for v_id, v_info in p_info.groupby("hadm_id"):
                for timestamp, code in zip(v_info["charttime"], v_info["itemid"]):
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary="MIMIC4_ITEMID",
                        visit_id=v_id,
                        patient_id=p_id,
                        timestamp=strptime(timestamp),
                    )
                    events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(
            lambda x: lab_unit(x.subject_id.unique()[0], x)
        )

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients

    def parse_hcpcsevents(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses hcpcsevents table.

        Will be called in `self.parse_tables()`

        Docs:
            - hcpcsevents: https://mimic.mit.edu/docs/iv/modules/hosp/hcpcsevents/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.

        Note:
            MIMIC-IV does not provide specific timestamps in hcpcsevents
                table, so we set it to None.
        """
        table = "hcpcsevents"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"subject_id": str, "hadm_id": str, "hcpcs_cd": str},
        )
        # drop rows with missing values
        df = df.dropna(subset=["subject_id", "hadm_id", "hcpcs_cd"])
        # sort by sequence number (i.e., priority)
        df = df.sort_values(["subject_id", "hadm_id", "seq_num"], ascending=True)
        # group by patient and visit
        group_df = df.groupby("subject_id")

        # parallel unit of hcpcsevents (per patient)
        def hcpcsevents_unit(p_id, p_info):
            events = []
            for v_id, v_info in p_info.groupby("hadm_id"):
                for code in v_info["hcpcs_cd"]:
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary="MIMIC4_HCPCS_CD",
                        visit_id=v_id,
                        patient_id=p_id,
                    )
                    # update patients
                    events.append(event)
            return events
            
        # parallel apply
        group_df = group_df.parallel_apply(
            lambda x: hcpcsevents_unit(x.subject_id.unique()[0], x)
        )

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        
        return patients
        
if __name__ == "__main__":
    dataset = MIMIC4Dataset(
        root="/srv/local/data/physionet.org/files/mimiciv/2.0/hosp",
        tables=["diagnoses_icd", "procedures_icd", "prescriptions", "labevents", "hcpcsevents"],
        code_mapping={"NDC": "ATC"},
        refresh_cache=False,
    )
    dataset.stat()
    dataset.info()


Here is the code content for mimicextract.py:
import os
from typing import Optional, List, Dict, Tuple, Union

import pandas as pd

from pyhealth.data import Event, Visit, Patient
from pyhealth.datasets import BaseEHRDataset
from pyhealth.datasets.utils import strptime

# TODO: add other tables

class MIMICExtractDataset(BaseEHRDataset):
    """Base dataset for MIMIC-Extract dataset.

    Reads the HDF5 data produced by 
    [MIMIC-Extract](https://github.com/MLforHealth/MIMIC_Extract#step-4-set-cohort-selection-and-extraction-criteria).
    Works with files created with or without LEVEL2 grouping and with restricted cohort population
    sizes, other optional parameter values, and should work with many customized versions of the pipeline.

    You can create or obtain a MIMIC-Extract dataset in several ways:

    * The default chort dataset is [available on GCP](https://console.cloud.google.com/storage/browser/mimic_extract)
      (requires PhysioNet access provisioned in GCP).
    * Follow the [step-by-step instructions](https://github.com/MLforHealth/MIMIC_Extract#step-by-step-instructions)
      on the MIMIC_Extract github site, which includes setting up a PostgreSQL database and loading
      the MIMIC-III data files.
    * Use the instructions at [MIMICExtractEasy](https://github.com/SphtKr/MIMICExtractEasy) which uses DuckDB
      instead and should be a good bit simpler.
    
    Any of these methods will provide you with a set of HDF5 files containing a cleaned subset of the MIMIC-III dataset.
    This class can be used to read that dataset (mainly the `all_hourly_data.h5` file). Consult the MIMIC-Extract
    documentation for all the options available for dataset generation (cohort selection, aggregation level, etc.).

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data (should contain one or more HDF5 files).
        tables: list of tables to be loaded (e.g., ["vitals_labs", "interventions"]). 
        code_mapping: a dictionary containing the code mapping information.
            The key is a str of the source code vocabulary and the value is of
            two formats:
                (1) a str of the target code vocabulary;
                (2) a tuple with two elements. The first element is a str of the
                    target code vocabulary and the second element is a dict with
                    keys "source_kwargs" or "target_kwargs" and values of the
                    corresponding kwargs for the `CrossMap.map()` method.
            Default is empty dict, which means the original code will be used.
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.
        pop_size: If your MIMIC-Extract dataset was created with a pop_size parameter,
            include it here. This is used to find the correct filenames.
        itemid_to_variable_map: Path to the CSV file used for aggregation mapping during
            your dataset's creation. Probably the one located in the MIMIC-Extract
            repo at `resources/itemid_to_variable_map.csv`, or your own version if you
            have customized it.

    Attributes:
        task: Optional[str], name of the task (e.g., "mortality prediction").
            Default is None.
        samples: Optional[List[Dict]], a list of samples, each sample is a dict with
            patient_id, visit_id, and other task-specific attributes as key.
            Default is None.
        patient_to_index: Optional[Dict[str, List[int]]], a dict mapping patient_id to
            a list of sample indices. Default is None.
        visit_to_index: Optional[Dict[str, List[int]]], a dict mapping visit_id to a
            list of sample indices. Default is None.

    Examples:
        >>> from pyhealth.datasets import MIMICExtractDataset
        >>> dataset = MIMICExtractDataset(
        ...         root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        ...         tables=["DIAGNOSES_ICD", "NOTES"], TODO: What here?
        ...         code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
        ...     )
        >>> dataset.stat()
        >>> dataset.info()
    """

    def __init__(
        self,
        root: str,
        tables: List[str],
        dataset_name: Optional[str] = None,
        code_mapping: Optional[Dict[str, Union[str, Tuple[str, Dict]]]] = None,
        dev: bool = False,
        refresh_cache: bool = False,
        pop_size: Optional[int] = None,
        itemid_to_variable_map: Optional[str] = None,
        #is_icustay_visit: Optional[bool] = False #TODO: implement fully
    ):
        if pop_size is not None:
            self._fname_suffix = f"_{pop_size}"
        else:
            self._fname_suffix = ""
        self._ahd_filename = os.path.join(root, f"all_hourly_data{self._fname_suffix}.h5")
        self._c_filename = os.path.join(root, f"C{self._fname_suffix}.h5")
        self._notes_filename = os.path.join(root, f"all_hourly_data{self._fname_suffix}.hdf")
        self._v_id_column = 'hadm_id' #'icustay_id' if is_icustay_visit else 'hadm_id'

        # This could be implemented with MedCode.CrossMap, however part of the idea behind
        # MIMIC-Extract is that the user can customize this mapping--therefore we will
        # make a map specific to this dataset instance based on a possibly-customized CSV.
        self._vocab_map = { "chartevents": {}, "labevents": {}, "vitals_labs": {} }
        if itemid_to_variable_map is not None:
            # We are just going to read some metadata here...
            df_ahd = pd.read_hdf(self._ahd_filename, 'vitals_labs')
            grptype = "LEVEL1" if "LEVEL1" in df_ahd.columns.names else "LEVEL2"

            itemid_map = pd.read_csv(itemid_to_variable_map)
            for linksto, dict in self._vocab_map.items():
                df = itemid_map
                if linksto != 'vitals_labs':
                    df = df[df["LINKSTO"] == linksto]
                # Pick the most common ITEMID to use for our vocabulary...
                df = df.sort_values(by="COUNT", ascending=False).groupby(grptype).head(1)
                df = df[[grptype,"ITEMID"]].set_index([grptype])
                #TODO: Probably a better way than iterrows? At least this is a small df.
                #self._vocab_map[linksto] = df[["ITEMID"]].to_dict(orient="index")
                for r in df.iterrows():
                    self._vocab_map[linksto][r[0].lower()] = r[1]["ITEMID"]

        # reverse engineered from mimic-code concepts SQL and MIMIC-Extractt SQL...
        self._vocab_map['interventions'] = { 
            'vent': 467,
            'adenosine': 4649,
            'dobutamine': 30042,
            'dopamine': 30043,
            'epinephrine': 30044,
            'isuprel': 30046,
            'milrinone': 30125,
            'norepinephrine': 30047,
            'phenylephrine': 30127,
            'vasopressin': 30051,
            'colloid_bolus': 46729, # "Dextran" Arbitrary! No general itemid!
            'crystalloid_bolus': 41491, # "fluid bolus"
            'nivdurations': 468
        }

                
        super().__init__(root=root, tables=tables,
            dataset_name=dataset_name, code_mapping=code_mapping,
            dev=dev, refresh_cache=refresh_cache)
        

    def parse_basic_info(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses `patients` dataset (within `all_hourly_data.h5`)

        Will be called in `self.parse_tables()`

        Docs:
            - PATIENTS: https://mimic.mit.edu/docs/iii/tables/patients/
            - ADMISSIONS: https://mimic.mit.edu/docs/iii/tables/admissions/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id which is updated with the mimic-3 table result.

        Returns:
            The updated patients dict.
        """
        # read patients table
        patients_df = pd.read_hdf(self._ahd_filename, 'patients')
        # sort by admission and discharge time
        df = patients_df.reset_index().sort_values(["subject_id", "admittime", "dischtime"], ascending=True)
        # group by patient
        #TODO: This can probably be simplified--MIMIC-Extract includes only the first ICU
        # visit for each patient (see paper)... it is unclear whether it might be easily
        # modified to include multiple visits however, so this may have value for customised
        # versions of the pipeline.
        df_group = df.groupby("subject_id")

        # parallel unit of basic information (per patient)
        def basic_unit(p_id, p_info):
            #FIXME: This is insanity.
            #tdelta = pd.Timedelta(days=365.2425*p_info["age"].values[0]) 
            # pd.Timedelta cannot handle 300-year deltas!
            tdeltahalf = pd.Timedelta(days=0.5*365.2425*p_info["age"].values[0]) 
            patient = Patient(
                patient_id=p_id,
                birth_datetime=pd.to_datetime(p_info["admittime"].values[0]-tdeltahalf-tdeltahalf), #see?
                death_datetime=p_info["deathtime"].values[0],
                gender=p_info["gender"].values[0],
                ethnicity=p_info["ethnicity"].values[0],
            )
            # load visits
            for v_id, v_info in p_info.groupby("hadm_id"):
                visit = Visit(
                    visit_id=v_id,
                    patient_id=p_id,
                    encounter_time=pd.to_datetime(v_info["admittime"].values[0]),
                    discharge_time=pd.to_datetime(v_info["dischtime"].values[0]),
                    discharge_status=v_info["hospital_expire_flag"].values[0],
                )
                # add visit
                patient.add_visit(visit)
            return patient

        # parallel apply
        df_group = df_group.parallel_apply(
            lambda x: basic_unit(x.subject_id.unique()[0], x)
        )
        # summarize the results
        for pat_id, pat in df_group.items():
            patients[pat_id] = pat

        return patients

    def parse_diagnoses_icd(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses the `C` (ICD9 diagnosis codes) dataset (within `C.h5`) in
          a way compatible with MIMIC3Dataset.

        Will be called in `self.parse_tables()`

        Docs:
            - DIAGNOSES_ICD: https://mimic.mit.edu/docs/iii/tables/diagnoses_icd/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.

        Note:
            MIMIC-III does not provide specific timestamps in DIAGNOSES_ICD
                table, so we set it to None.
        """
        return self._parse_c(patients, table='DIAGNOSES_ICD')

    def parse_c(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses the `C` (ICD9 diagnosis codes) dataset (within `C.h5`).

        Will be called in `self.parse_tables()`

        Docs:
            - DIAGNOSES_ICD: https://mimic.mit.edu/docs/iii/tables/diagnoses_icd/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.

        Note:
            MIMIC-III does not provide specific timestamps in DIAGNOSES_ICD
                table, so we set it to None.
        """
        return self._parse_c(patients, table='C')

    def _parse_c(self, patients: Dict[str, Patient], table: str = 'C') -> Dict[str, Patient]:
        # read table
        df = pd.read_hdf(self._c_filename, 'C')
        # drop records of the other patients
        df = df.loc[(list(patients.keys()),slice(None),slice(None)),:]
        # drop rows with missing values
        #df = df.dropna(subset=["subject_id", "hadm_id", "icd9_codes"])
        dfgroup = df.reset_index().groupby("subject_id")

        #display(df)
        #df = df.reset_index(['icustay_id']) #drops this one only.. interesting
        #display(df)
        captured_v_id_column = self._v_id_column
        def diagnosis_unit(p_id, p_info):
            events = []
            for v_id, v_info in p_info.groupby(captured_v_id_column):
                codes = set(v_info['icd9_codes'].sum())
                for code in codes:
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary="ICD9CM",
                        visit_id=v_id,
                        patient_id=p_id,
                    )
                    events.append(event)
            return events

        # parallel apply
        dfgroup = dfgroup.parallel_apply(
        #dfgroup = dfgroup.apply(
            lambda x: diagnosis_unit(x.subject_id.unique()[0], x)
        )
        # summarize the results
        patients = self._add_events_to_patient_dict(patients, dfgroup)
        return patients

    def parse_labevents(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses the `vitals_labs` dataset (within `all_hourly_data.h5`)
        in a way compatible with MIMIC3Dataset.

        Features in `vitals_labs` are corellated with MIMIC-III ITEM_ID values, and those ITEM_IDs
        that correspond to LABEVENTS table items in raw MIMIC-III will be
        added as events. This corellation depends on the contents of the provided `itemid_to_variable_map.csv`
        file. Note that this will likely *not* match the raw MIMIC-III data because of the
        harmonization/aggregation done by MIMIC-Extract.

        See also `self.parse_vitals_labs()` 

        Will be called in `self.parse_tables()`

        Docs:
            - LABEVENTS: https://mimic.mit.edu/docs/iii/tables/labevents/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "LABEVENTS"
        return self._parse_vitals_labs(patients=patients, table=table)

    def parse_chartevents(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses the `vitals_labs` dataset (within `all_hourly_data.h5`)
        in a way compatible with MIMIC3Dataset.

        Features in `vitals_labs` are corellated with MIMIC-III ITEM_ID values, and those ITEM_IDs
        that correspond to CHARTEVENTS table items in raw MIMIC-III will be
        added as events. This corellation depends on the contents of the provided `itemid_to_variable_map.csv`
        file. Note that this will likely *not* match the raw MIMIC-III data because of the
        harmonization/aggregation done in MIMIC-Extract. 

        Will be called in `self.parse_tables()`

        Docs:
            - CHARTEVENTS: https://mimic.mit.edu/docs/iii/tables/chartevents/

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "CHARTEVENTS"
        return self._parse_vitals_labs(patients=patients, table=table)

    def parse_vitals_labs(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses the `vitals_labs` dataset (within `all_hourly_data.h5`). 

        Events are added using the `MIMIC3_ITEMID` vocabulary, and the mapping is determined by the
        CSV file passed to the constructor in `itemid_to_variable_map`. Since MIMIC-Extract aggregates
        like events, only a single MIMIC-III ITEMID will be used to represent all like items in the
        MIMIC-Extract dataset--so the data here will likely *not* match raw MIMIC-III data. Which ITEMIDs are
        used depends on the aggregation level in your dataset (i.e. whether you used `--no_group_by_level2`).

        Will be called in `self.parse_tables()`

        See also `self.parse_chartevents()` and `self.parse_labevents()`

        Docs:
            - https://github.com/MLforHealth/MIMIC_Extract#step-4-set-cohort-selection-and-extraction-criteria

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "vitals_labs"
        return self._parse_vitals_labs(patients=patients, table=table)

    def _parse_vitals_labs(self, patients: Dict[str, Patient], table: str = 'vitals_labs') -> Dict[str, Patient]:
        linksto = table.lower()
        # read table
        df = pd.read_hdf(self._ahd_filename, 'vitals_labs')
        # drop records of the other patients
        df = df.loc[(list(patients.keys()),slice(None),slice(None)),:]

        # parallel unit for lab (per patient)
        captured_v_id_column = self._v_id_column
        captured_vocab_map = self._vocab_map[linksto]
        def vl_unit(p_id, p_info):
            events = []
            for v_id, v_info in p_info.groupby(captured_v_id_column):
                for e_id, e_info in v_info.iterrows():
                    #print(e_id)
                    #print(f"{e_info['variable']} -> {self._vocab_map[linksto][e_info['variable']]=}")
                    event = Event(
                        code=captured_vocab_map[e_info['variable']],
                        table=table,
                        vocabulary="MIMIC3_ITEMID",
                        visit_id=v_id,
                        patient_id=p_id,
                        timestamp=pd.Timestamp.to_pydatetime(e_info['timestamp']),
                        hours_in=int(e_info['hours_in']),
                        #level_n=e_info['variable'], #this can be reverse-looked up, so save some mem here?
                        mean=e_info['mean'], # Value is not stored in MIMIC3Dataset... why? Unit uncertainty?
                        #TODO: Units, somewhere?
                        count=e_info['count'],
                        std=e_info['std']
                    )
                    events.append(event)
            return events

        ahd_index = ["subject_id","hadm_id","icustay_id","hours_in"]

        df.columns = df.columns.values # Collapse MultiIndex to tuples!
        is_level1 = True if(len(df.columns[0]) == 5) else False

        # drop columns not applicable to the wanted table...
        if is_level1:
            df = df.drop(columns=[col for col in df.columns if col[2] not in self._vocab_map[linksto]])
        else:
            df = df.drop(columns=[col for col in df.columns if col[0] not in self._vocab_map[linksto]])

        # "melt" down to a per-event representation...
        df = df.reset_index().melt(id_vars=ahd_index).dropna()
        if is_level1:
            _,_,df['variable'],_,df['Aggregation Function'] = zip(*df['variable'])
        else:
            df['variable'],df['Aggregation Function'] = zip(*df['variable'])

        # Discard count == 0.0 rows
        df = df.loc[(df['Aggregation Function'] != 'count') | (df['value'] != 0.0)]
        df = df.drop_duplicates()

        # Manual/brute force "pivot", as I can't get pivot functions to work right with the MultiIndex columns...
        df = df.reset_index().sort_values(ahd_index+['variable']).set_index(ahd_index+['variable'])
        df_mean = df.loc[df['Aggregation Function'] == 'mean'].rename(columns={"value":"mean"})['mean']
        df_count = df.loc[df['Aggregation Function'] == 'count'].rename(columns={"value":"count"})['count']
        df_std = df.loc[df['Aggregation Function'] == 'std'].rename(columns={"value":"std"})['std']
        if is_level1:
            #FIXME: Duplicates appear in the LEVEL1 representation... this is puzzling.
            # These should all be almost equal, or there is a significant problem.
            # For now, take some mean, and the highest count and std... though they 
            # may not match. LEVEL1 representation is usually not preferred anyway.
            # In theory, these should probably be aggregated??
            df_mean = df_mean[~df_mean.index.duplicated(keep='first')]
            df_count = df_count.sort_values(ascending=False)
            df_count = df_count[~df_count.index.duplicated(keep='first')]
            df_std = df_std.sort_values(ascending=False)
            df_std = df_std[~df_std.index.duplicated(keep='first')]
        df = pd.concat([df_mean, df_count, df_std], axis=1)
        df = df.reset_index().sort_values(ahd_index+['variable'])

        # reconstruct nominal timestamps for hours_in values...
        df_p = pd.read_hdf(self._ahd_filename, 'patients')
        df_p = df_p.loc[(list(patients.keys()),slice(None),slice(None)),:][['intime']]
        df = df.merge(df_p, on=['subject_id','hadm_id','icustay_id'], how="left")
        df['timestamp'] = df['intime'].dt.ceil('H')+pd.to_timedelta(df['hours_in'], unit="H")

        group_df = df.groupby("subject_id")

        # parallel apply
        group_df = group_df.parallel_apply(
            lambda x: vl_unit(x.subject_id.unique()[0], x)
        )
        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients

    def parse_interventions(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses the `interventions` dataset (within `all_hourly_data.h5`). 
        Events are added using the `MIMIC3_ITEMID` vocabulary, using a manually derived mapping corresponding to
        general items descriptive of the intervention. Since the raw MIMIC-III data had multiple codes, and 
        MIMIC-Extract aggregates like items, these will not match raw MIMIC-III data.
        
        In particular, note
        that ITEMID 41491 ("fluid bolus") is used for `crystalloid_bolus` and ITEMID 46729 ("Dextran") is used 
        for `colloid_bolus` because there is no existing general ITEMID for colloid boluses.

        Will be called in `self.parse_tables()`

        Docs:
            - https://github.com/MLforHealth/MIMIC_Extract#step-4-set-cohort-selection-and-extraction-criteria

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = 'interventions'
        linksto = table.lower() # we might put these in CHARTEVENTS also?

        # read table
        df = pd.read_hdf(self._ahd_filename, 'interventions')
        # drop records of the other patients
        df = df.loc[(list(patients.keys()),slice(None),slice(None)),:]

        # parallel unit for interventions (per patient)
        captured_v_id_column = self._v_id_column
        captured_vocab_map = self._vocab_map[linksto] 
        def interv_unit(p_id, p_info):
            events = []
            for v_id, v_info in p_info.groupby(captured_v_id_column):
                for e_id, e_info in v_info.iterrows():
                    event = Event(
                        code=captured_vocab_map[e_info['variable']],
                        table=table,
                        vocabulary="MIMIC3_ITEMID",
                        visit_id=v_id,
                        patient_id=p_id,
                        timestamp=pd.Timestamp.to_pydatetime(e_info['timestamp']),
                        hours_in=int(e_info['hours_in']),
                        intervention=e_info['variable']
                    )
                    events.append(event)
            return events

        ahd_index = ["subject_id","hadm_id","icustay_id","hours_in"]

        #if 'LEVEL1' in df.columns.names:
        #    df.columns = df.columns.get_level_values(2)
        #else:
        #    df.columns = df.columns.get_level_values(0)

        # reconstruct nominal timestamps for hours_in values...
        df_p = pd.read_hdf(self._ahd_filename, 'patients')
        df_p = df_p.loc[(list(patients.keys()),slice(None),slice(None)),:][['intime']]
        df = df.merge(df_p, left_on=['subject_id','hadm_id','icustay_id'], right_index=True, how="left")
        df['timestamp'] = df['intime'].dt.ceil('H')+pd.to_timedelta(df.index.get_level_values(3), unit="H")

        df = df.drop(columns=[col for col in df.columns if col not in self._vocab_map[linksto] and col not in ['timestamp']])
        df = df.reset_index()
        df = df.melt(id_vars=ahd_index+['timestamp'])
        df = df[df['value'] > 0]
        df = df.sort_values(ahd_index)
        group_df = df.groupby("subject_id")

        # parallel apply
        group_df = group_df.parallel_apply(
            lambda x: interv_unit(x.subject_id.unique()[0], x)
        )

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients


if __name__ == "__main__":
    dataset = MIMICExtractDataset(
        root="../mimic3demo/grouping",
        tables=[
            #"DIAGNOSES_ICD",
            "C",
            #"LABEVENTS",
            #"CHARTEVENTS",
            "vitals_labs",
            "interventions",
        ],
        dev=True,
        refresh_cache=True,
        itemid_to_variable_map='../MIMIC_Extract/resources/itemid_to_variable_map.csv'
    )
    dataset.stat()
    dataset.info()


Here is the code content for omop.py:
import os
from typing import Optional, List, Dict, Tuple, Union

import pandas as pd

from pyhealth.data import Event, Visit, Patient
from pyhealth.datasets import BaseEHRDataset
from pyhealth.datasets.utils import strptime


# TODO: add other tables


class OMOPDataset(BaseEHRDataset):
    """Base dataset for OMOP dataset.

    The Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM)
    is an open community data standard, designed to standardize the structure
    and content of observational data and to enable efficient analyses that
    can produce reliable evidence.

    See: https://www.ohdsi.org/data-standardization/the-common-data-model/.

    The basic information is stored in the following tables:
        - person: contains records that uniquely identify each person or patient,
            and some demographic information.
        - visit_occurrence: contains info for how a patient engages with the
            healthcare system for a duration of time.
        - death: contains info for how and when a patient dies.

    We further support the following tables:
        - condition_occurrence.csv: contains the condition information
            (CONDITION_CONCEPT_ID code) of patients' visits.
        - procedure_occurrence.csv: contains the procedure information
            (PROCEDURE_CONCEPT_ID code) of patients' visits.
        - drug_exposure.csv: contains the drug information (DRUG_CONCEPT_ID code)
            of patients' visits.
        - measurement.csv: contains all laboratory measurements
            (MEASUREMENT_CONCEPT_ID code) of patients' visits.

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data (should contain many csv files).
        tables: list of tables to be loaded (e.g., ["DIAGNOSES_ICD", "PROCEDURES_ICD"]).
        code_mapping: a dictionary containing the code mapping information.
            The key is a str of the source code vocabulary and the value is of
            two formats:
                (1) a str of the target code vocabulary;
                (2) a tuple with two elements. The first element is a str of the
                    target code vocabulary and the second element is a dict with
                    keys "source_kwargs" or "target_kwargs" and values of the
                    corresponding kwargs for the `CrossMap.map()` method.
            Default is empty dict, which means the original code will be used.
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.

    Attributes:
        task: Optional[str], name of the task (e.g., "mortality prediction").
            Default is None.
        samples: Optional[List[Dict]], a list of samples, each sample is a dict with
            patient_id, visit_id, and other task-specific attributes as key.
            Default is None.
        patient_to_index: Optional[Dict[str, List[int]]], a dict mapping patient_id to
            a list of sample indices. Default is None.
        visit_to_index: Optional[Dict[str, List[int]]], a dict mapping visit_id to a
            list of sample indices. Default is None.

    Examples:
        >>> from pyhealth.datasets import OMOPDataset
        >>> dataset = OMOPDataset(
        ...         root="/srv/local/data/zw12/pyhealth/raw_data/synpuf1k_omop_cdm_5.2.2",
        ...         tables=["condition_occurrence", "procedure_occurrence", "drug_exposure", "measurement",],
        ...     )
        >>> dataset.stat()
        >>> dataset.info()

    """

    def parse_basic_info(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper functions which parses person, visit_occurrence, and death tables.

        Will be called in `self.parse_tables()`

        Docs:
            - person: http://ohdsi.github.io/CommonDataModel/cdm53.html#PERSON
            - visit_occurrence: http://ohdsi.github.io/CommonDataModel/cdm53.html#VISIT_OCCURRENCE
            - death: http://ohdsi.github.io/CommonDataModel/cdm53.html#DEATH

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        # read person table
        person_df = pd.read_csv(
            os.path.join(self.root, "person.csv"),
            dtype={"person_id": str},
            nrows=1000 if self.dev else None,
            sep="\t",
        )
        # read visit_occurrence table
        visit_occurrence_df = pd.read_csv(
            os.path.join(self.root, "visit_occurrence.csv"),
            dtype={"person_id": str, "visit_occurrence_id": str},
            sep="\t",
        )
        # read death table
        death_df = pd.read_csv(
            os.path.join(self.root, "death.csv"),
            sep="\t",
            dtype={"person_id": str},
        )
        # merge
        df = pd.merge(person_df, visit_occurrence_df, on="person_id", how="left")
        df = pd.merge(df, death_df, on="person_id", how="left")
        # sort by admission time
        df = df.sort_values(
            ["person_id", "visit_occurrence_id", "visit_start_datetime"], ascending=True
        )
        # group by patient
        df_group = df.groupby("person_id")

        # parallel unit of basic informatin (per patient)
        def basic_unit(p_info):
            p_id = p_info["person_id"].values[0]
            birth_y = p_info["year_of_birth"].values[0]
            birth_m = p_info["month_of_birth"].values[0]
            birth_d = p_info["day_of_birth"].values[0]
            birth_date = f"{birth_y}-{birth_m}-{birth_d}"
            patient = Patient(
                patient_id=p_id,
                # no exact time, use 00:00:00
                birth_datetime=strptime(birth_date),
                death_datetime=strptime(p_info["death_date"].values[0]),
                gender=p_info["gender_concept_id"].values[0],
                ethnicity=p_info["race_concept_id"].values[0],
            )
            # load visits
            for v_id, v_info in p_info.groupby("visit_occurrence_id"):
                death_date = v_info["death_date"].values[0]
                visit_start_date = v_info["visit_start_date"].values[0]
                visit_end_date = v_info["visit_end_date"].values[0]
                if pd.isna(death_date):
                    discharge_status = 0
                elif death_date > visit_end_date:
                    discharge_status = 0
                else:
                    discharge_status = 1
                visit = Visit(
                    visit_id=v_id,
                    patient_id=p_id,
                    encounter_time=strptime(visit_start_date),
                    discharge_time=strptime(visit_end_date),
                    discharge_status=discharge_status,
                )
                # add visit
                patient.add_visit(visit)
            return patient

        # parallel apply
        df_group = df_group.parallel_apply(lambda x: basic_unit(x))
        # summarize the results
        for pat_id, pat in df_group.items():
            patients[pat_id] = pat

        return patients

    def parse_condition_occurrence(
        self, patients: Dict[str, Patient]
    ) -> Dict[str, Patient]:
        """Helper function which parses condition_occurrence table.

        Will be called in `self.parse_tables()`

        Docs:
            - condition_occurrence: http://ohdsi.github.io/CommonDataModel/cdm53.html#CONDITION_OCCURRENCE

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "condition_occurrence"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={
                "person_id": str,
                "visit_occurrence_id": str,
                "condition_concept_id": str,
            },
            sep="\t",
        )
        # drop rows with missing values
        df = df.dropna(
            subset=["person_id", "visit_occurrence_id", "condition_concept_id"]
        )
        # sort by condition_start_datetime
        df = df.sort_values(
            ["person_id", "visit_occurrence_id", "condition_start_datetime"],
            ascending=True,
        )
        # group by patient and visit
        group_df = df.groupby("person_id")

        # parallel unit of condition occurrence (per patient)
        def condition_unit(p_info):
            p_id = p_info["person_id"].values[0]

            events = []
            for v_id, v_info in p_info.groupby("visit_occurrence_id"):
                for timestamp, code in zip(
                    v_info["condition_start_datetime"], v_info["condition_concept_id"]
                ):
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary="CONDITION_CONCEPT_ID",
                        visit_id=v_id,
                        patient_id=p_id,
                        timestamp=strptime(timestamp),
                    )
                    # update patients
                    events.append(event)

        # parallel apply
        group_df = group_df.parallel_apply(lambda x: condition_unit(x))

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)

        return patients

    def parse_procedure_occurrence(
        self, patients: Dict[str, Patient]
    ) -> Dict[str, Patient]:
        """Helper function which parses procedure_occurrence table.

        Will be called in `self.parse_tables()`

        Docs:
            - procedure_occurrence: http://ohdsi.github.io/CommonDataModel/cdm53.html#PROCEDURE_OCCURRENCE

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "procedure_occurrence"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={
                "person_id": str,
                "visit_occurrence_id": str,
                "procedure_concept_id": str,
            },
            sep="\t",
        )
        # drop rows with missing values
        df = df.dropna(
            subset=["person_id", "visit_occurrence_id", "procedure_concept_id"]
        )
        # sort by procedure_datetime
        df = df.sort_values(
            ["person_id", "visit_occurrence_id", "procedure_datetime"], ascending=True
        )
        # group by patient and visit
        group_df = df.groupby("person_id")

        # parallel unit of procedure occurrence (per patient)
        def procedure_unit(p_info):
            p_id = p_info["person_id"].values[0]

            events = []
            for v_id, v_info in p_info.groupby("visit_occurrence_id"):
                for timestamp, code in zip(
                    v_info["procedure_datetime"], v_info["procedure_concept_id"]
                ):
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary="PROCEDURE_CONCEPT_ID",
                        visit_id=v_id,
                        patient_id=p_id,
                        timestamp=strptime(timestamp),
                    )
                    events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(lambda x: procedure_unit(x))

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)
        return patients

    def parse_drug_exposure(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses drug_exposure table.

        Will be called in `self.parse_tables()`

        Docs:
            - procedure_occurrence: http://ohdsi.github.io/CommonDataModel/cdm53.html#DRUG_EXPOSURE

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "drug_exposure"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={
                "person_id": str,
                "visit_occurrence_id": str,
                "drug_concept_id": str,
            },
            sep="\t",
        )
        # drop rows with missing values
        df = df.dropna(subset=["person_id", "visit_occurrence_id", "drug_concept_id"])
        # sort by drug_exposure_start_datetime
        df = df.sort_values(
            ["person_id", "visit_occurrence_id", "drug_exposure_start_datetime"],
            ascending=True,
        )
        # group by patient and visit
        group_df = df.groupby("person_id")

        # parallel unit of drug exposure (per patient)
        def drug_unit(p_info):
            p_id = p_info["person_id"].values[0]

            events = []
            for v_id, v_info in p_info.groupby("visit_occurrence_id"):
                for timestamp, code in zip(
                    v_info["drug_exposure_start_datetime"], v_info["drug_concept_id"]
                ):
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary="DRUG_CONCEPT_ID",
                        visit_id=v_id,
                        patient_id=p_id,
                        timestamp=strptime(timestamp),
                    )
                    events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(lambda x: drug_unit(x))

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)

        return patients

    def parse_measurement(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:
        """Helper function which parses measurement table.

        Will be called in `self.parse_tables()`

        Docs:
            - measurement: http://ohdsi.github.io/CommonDataModel/cdm53.html#MEASUREMENT

        Args:
            patients: a dict of `Patient` objects indexed by patient_id.

        Returns:
            The updated patients dict.
        """
        table = "measurement"
        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={
                "person_id": str,
                "visit_occurrence_id": str,
                "measurement_concept_id": str,
            },
            sep="\t",
        )
        # drop rows with missing values
        df = df.dropna(
            subset=["person_id", "visit_occurrence_id", "measurement_concept_id"]
        )
        # sort by measurement_datetime
        df = df.sort_values(
            ["person_id", "visit_occurrence_id", "measurement_datetime"], ascending=True
        )
        # group by patient and visit
        group_df = df.groupby("person_id")

        # parallel unit of measurement (per patient)
        def measurement_unit(p_info):
            p_id = p_info["person_id"].values[0]

            events = []
            for v_id, v_info in p_info.groupby("visit_occurrence_id"):
                for timestamp, code in zip(
                    v_info["measurement_datetime"], v_info["measurement_concept_id"]
                ):
                    event = Event(
                        code=code,
                        table=table,
                        vocabulary="MEASUREMENT_CONCEPT_ID",
                        visit_id=v_id,
                        patient_id=p_id,
                        timestamp=strptime(timestamp),
                    )
                    events.append(event)
            return events

        # parallel apply
        group_df = group_df.parallel_apply(lambda x: measurement_unit(x))

        # summarize the results
        patients = self._add_events_to_patient_dict(patients, group_df)

        return patients


if __name__ == "__main__":
    dataset = OMOPDataset(
        root="/srv/local/data/zw12/pyhealth/raw_data/synpuf1k_omop_cdm_5.2.2",
        tables=[
            "condition_occurrence",
            "procedure_occurrence",
            "drug_exposure",
            "measurement",
        ],
        dev=False,
        refresh_cache=True,
    )
    dataset.stat()
    dataset.info()


Here is the code content for sample_dataset.py:
from collections import Counter
from typing import Dict, List
import pickle

from torch.utils.data import Dataset

from pyhealth.datasets.utils import list_nested_levels, flatten_list


class SampleBaseDataset(Dataset):
    """Sample base dataset class.

    This class the takes a list of samples as input (either from
    `BaseDataset.set_task()` or user-provided input), and provides
    a uniform interface for accessing the samples.

    Args:
        samples: a list of samples, each sample is a dict with
            patient_id, visit_id, and other task-specific attributes as key.
        dataset_name: the name of the dataset. Default is None.
        task_name: the name of the task. Default is None.
    """

    def __init__(self, samples: List[Dict], dataset_name="", task_name=""):
        self.samples = samples
        self.dataset_name: str = dataset_name
        self.task_name: str = task_name
        self.type_ = "base"

    def __getitem__(self, index) -> Dict:
        """Returns a sample by index.

        Returns:
             Dict, a dict with patient_id, visit_id/record_id, and other task-specific
                attributes as key. Conversion to index/tensor will be done
                in the model.
        """
        return self.samples[index]

    def __str__(self):
        """Prints some information of the dataset."""
        return f"Sample dataset {self.dataset_name} {self.task_name}"

    def __len__(self):
        """Returns the number of samples in the dataset."""
        return len(self.samples)

    def get_all_tokens(
        self, key: str, remove_duplicates: bool = True, sort: bool = True
    ) -> List[str]:
        """Gets all tokens with a specific key in the samples.

        Args:
            key: the key of the tokens in the samples.
            remove_duplicates: whether to remove duplicates. Default is True.
            sort: whether to sort the tokens by alphabet order. Default is True.

        Returns:
            tokens: a list of tokens.
        """
        input_type = self.input_info[key]["type"]
        input_dim = self.input_info[key]["dim"]
        if input_type in [float, int]:
            assert input_dim == 0, f"Cannot get tokens for vector with key {key}"

        tokens = []
        for sample in self.samples:
            if input_dim == 0:
                # a single value
                tokens.append(sample[key])
            elif input_dim == 2:
                # a list of codes
                tokens.extend(sample[key])
            elif input_dim == 3:
                # a list of list of codes
                tokens.extend(flatten_list(sample[key]))
            else:
                raise NotImplementedError
        if remove_duplicates:
            tokens = list(set(tokens))
        if sort:
            tokens.sort()
        return tokens


class SampleSignalDataset(SampleBaseDataset):
    """Sample signal dataset class.

    This class the takes a list of samples as input (either from
    `BaseDataset.set_task()` or user-provided input), and provides
    a uniform interface for accessing the samples.

    Args:
        samples: a list of samples, each sample is a dict with
            patient_id, record_id, and other task-specific attributes as key.
        classes: a list of classes, e.g., ["W", "1", "2", "3", "R"].
        dataset_name: the name of the dataset. Default is None.
        task_name: the name of the task. Default is None.
    """

    def __init__(self, samples: List[Dict], dataset_name="", task_name=""):
        super().__init__(samples, dataset_name, task_name)
        self.patient_to_index: Dict[str, List[int]] = self._index_patient()
        self.record_to_index: Dict[str, List[int]] = self._index_record()
        self.input_info: Dict = self._validate()
        self.type_ = "signal"

    def _index_patient(self) -> Dict[str, List[int]]:
        """Helper function which indexes the samples by patient_id.

        Will be called in `self.__init__()`.
        Returns:
            patient_to_index: Dict[str, int], a dict mapping patient_id to a list
                of sample indices.
        """
        patient_to_index = {}
        for idx, sample in enumerate(self.samples):
            patient_to_index.setdefault(sample["patient_id"], []).append(idx)
        return patient_to_index

    def _index_record(self) -> Dict[str, List[int]]:
        """Helper function which indexes the samples by record_id.

        Will be called in `self.__init__()`.

        Returns:
            visit_to_index: Dict[str, int], a dict mapping record_id to a list
                of sample indices.
        """
        record_to_index = {}
        for idx, sample in enumerate(self.samples):
            record_to_index.setdefault(sample["record_id"], []).append(idx)
        return record_to_index

    def _validate(self) -> Dict:
        """Helper function which gets the input information of each attribute.

        Will be called in `self.__init__()`.

        Returns:
            input_info: Dict, a dict whose keys are the same as the keys in the
                samples, and values are the corresponding input information:
                - "length": the length of the input.
                - "n_channels": the number of channels of the input.

        """
        input_info = {}
        # get signal info
        sample_path_0 = self.samples[0]["epoch_path"]
        sample = pickle.load(open(sample_path_0, "rb"))
        n_channels, length = sample["signal"].shape
        input_info["signal"] = {"length": length, "n_channels": n_channels}
        # get label signal info
        input_info["label"] = {"type": str, "dim": 0}
        return input_info

    def __getitem__(self, index) -> Dict:
        """Returns a sample by index.

        Returns:
             Dict, a dict with patient_id, visit_id/record_id, and other task-specific
                attributes as key. Conversion to index/tensor will be done
                in the model.
        """
        sample = self.samples[index]
        loaded_sample = pickle.load(open(sample["epoch_path"], "rb"))
        cur_sample = sample.copy()
        cur_sample.update(loaded_sample)
        cur_sample.pop("epoch_path", None)
        return cur_sample

    def stat(self) -> str:
        """Returns some statistics of the task-specific dataset."""
        lines = list()
        lines.append(f"Statistics of sample dataset:")
        lines.append(f"\t- Dataset: {self.dataset_name}")
        lines.append(f"\t- Task: {self.task_name}")
        lines.append(f"\t- Number of samples: {len(self)}")
        num_patients = len(set([sample["patient_id"] for sample in self.samples]))
        lines.append(f"\t- Number of patients: {num_patients}")
        num_records = len(set([sample["record_id"] for sample in self.samples]))
        lines.append(f"\t- Number of visits: {num_records}")
        lines.append(
            f"\t- Number of samples per patient: {len(self) / num_patients:.4f}"
        )
        print("\n".join(lines))
        return "\n".join(lines)


class SampleEHRDataset(SampleBaseDataset):
    """Sample EHR dataset class.

    This class inherits from `SampleBaseDataset` and is specifically designed
        for EHR datasets.

    Args:
        samples: a list of samples, each sample is a dict with
            patient_id, visit_id, and other task-specific attributes as key.
        dataset_name: the name of the dataset. Default is None.
        task_name: the name of the task. Default is None.

    Currently, the following types of attributes are supported:
        - a single value. Type: int/float/str. Dim: 0.
        - a single vector. Type: int/float. Dim: 1.
        - a list of codes. Type: str. Dim: 2.
        - a list of vectors. Type: int/float. Dim: 2.
        - a list of list of codes. Type: str. Dim: 3.
        - a list of list of vectors. Type: int/float. Dim: 3.

    Attributes:
        input_info: Dict, a dict whose keys are the same as the keys in the
            samples, and values are the corresponding input information:
            - "type": the element type of each key attribute, one of float, int, str.
            - "dim": the list dimension of each key attribute, one of 0, 1, 2, 3.
            - "len": the length of the vector, only valid for vector-based attributes.
        patient_to_index: Dict[str, List[int]], a dict mapping patient_id to
            a list of sample indices.
        visit_to_index: Dict[str, List[int]], a dict mapping visit_id to a list
            of sample indices.

    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "single_vector": [1, 2, 3],
        ...             "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
        ...             "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        ...             "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
        ...             "list_list_vectors": [
        ...                 [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
        ...                 [[7.7, 8.5, 9.4]],
        ...             ],
        ...             "label": 1,
        ...         },
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-1",
        ...             "single_vector": [1, 5, 8],
        ...             "list_codes": [
        ...                 "55154191800",
        ...                 "551541928",
        ...                 "55154192800",
        ...                 "705182798",
        ...                 "70518279800",
        ...             ],
        ...             "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7]],
        ...             "list_list_codes": [["A04A", "B035", "C129"], ["A07B", "A07C"]],
        ...             "list_list_vectors": [
        ...                 [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6]],
        ...                 [[7.7, 8.4, 1.3]],
        ...             ],
        ...             "label": 0,
        ...         },
        ...     ]
        >>> dataset = SampleEHRDataset(samples=samples)
        >>> dataset.input_info
        {'patient_id': {'type': <class 'str'>, 'dim': 0}, 'visit_id': {'type': <class 'str'>, 'dim': 0}, 'single_vector': {'type': <class 'int'>, 'dim': 1, 'len': 3}, 'list_codes': {'type': <class 'str'>, 'dim': 2}, 'list_vectors': {'type': <class 'float'>, 'dim': 2, 'len': 3}, 'list_list_codes': {'type': <class 'str'>, 'dim': 3}, 'list_list_vectors': {'type': <class 'float'>, 'dim': 3, 'len': 3}, 'label': {'type': <class 'int'>, 'dim': 0}}
        >>> dataset.patient_to_index
        {'patient-0': [0, 1]}
        >>> dataset.visit_to_index
        {'visit-0': [0], 'visit-1': [1]}
    """

    def __init__(self, samples: List[Dict], dataset_name="", task_name=""):
        super().__init__(samples, dataset_name, task_name)
        self.input_info: Dict = self._validate()
        self.patient_to_index: Dict[str, List[int]] = self._index_patient()
        self.visit_to_index: Dict[str, List[int]] = self._index_visit()
        self.type_ = "ehr"

    def _validate(self) -> Dict:
        """Helper function which validates the samples.

        Will be called in `self.__init__()`.

        Returns:
            input_info: Dict, a dict whose keys are the same as the keys in the
                samples, and values are the corresponding input information:
                - "type": the element type of each key attribute, one of float,
                    int, str.
                - "dim": the list dimension of each key attribute, one of 0, 1, 2, 3.
                - "len": the length of the vector, only valid for vector-based
                    attributes.
        """
        """ 1. Check if all samples are of type dict. """
        assert all(
            [isinstance(s, dict) for s in self.samples],
        ), "Each sample should be a dict"
        keys = self.samples[0].keys()

        """ 2. Check if all samples have the same keys. """
        assert all(
            [set(s.keys()) == set(keys) for s in self.samples]
        ), "All samples should have the same keys"

        """ 3. Check if "patient_id" and "visit_id" are in the keys."""
        assert "patient_id" in keys, "patient_id should be in the keys"
        assert "visit_id" in keys, "visit_id should be in the keys"

        """
        4. For each key, check if it is either:
            - a single value
            - a single vector
            - a list of codes
            - a list of vectors
            - a list of list of codes
            - a list of list of vectors
        Note that a value is either float, int, or str; a vector is a list of float 
        or int; and a code is str.
        """
        # record input information for each key
        input_info = {}
        for key in keys:
            """
            4.1. Check nested list level: all samples should either all be
            - a single value (level=0)
            - a single vector (level=1)
            - a list of codes (level=1)
            - a list of vectors (level=2)
            - a list of list of codes (level=2)
            - a list of list of vectors (level=3)
            """
            levels = set([list_nested_levels(s[key]) for s in self.samples])
            assert (
                len(levels) == 1 and len(list(levels)[0]) == 1
            ), f"Key {key} has mixed nested list levels across samples"
            level = levels.pop()[0]
            assert level in [
                0,
                1,
                2,
                3,
            ], f"Key {key} has unsupported nested list level across samples"

            # flatten the list
            if level == 0:
                flattened_values = [s[key] for s in self.samples]
            elif level == 1:
                flattened_values = [i for s in self.samples for i in s[key]]
            elif level == 2:
                flattened_values = [j for s in self.samples for i in s[key] for j in i]
            else:
                flattened_values = [
                    k for s in self.samples for i in s[key] for j in i for k in j
                ]

            """
            4.2. Check type: the basic type of each element should be float, 
            int, or str.
            """
            types = set([type(v) for v in flattened_values])
            assert (
                types == set([str]) or len(types.difference(set([int, float]))) == 0
            ), f"Key {key} has mixed or unsupported types ({types}) across samples"
            type_ = types.pop()
            """
            4.3. Combined level and type check.
            """
            if level == 0:
                # a single value
                input_info[key] = {"type": type_, "dim": 0}
            elif level == 1:
                # a single vector or a list of codes
                if type_ in [float, int]:
                    # a single vector
                    lens = set([len(s[key]) for s in self.samples])
                    assert len(lens) == 1, f"Key {key} has vectors of different lengths"
                    input_info[key] = {"type": type_, "dim": 1, "len": lens.pop()}
                else:
                    # a list of codes
                    # note that dim is different from level here
                    input_info[key] = {"type": type_, "dim": 2}
            elif level == 2:
                # a list of vectors or a list of list of codes
                if type_ in [float, int]:
                    lens = set([len(i) for s in self.samples for i in s[key]])
                    assert len(lens) == 1, f"Key {key} has vectors of different lengths"
                    input_info[key] = {"type": type_, "dim": 2, "len": lens.pop()}
                else:
                    # a list of list of codes
                    # note that dim is different from level here
                    input_info[key] = {"type": type_, "dim": 3}
            else:
                # a list of list of vectors
                assert type_ in [
                    float,
                    int,
                ], f"Key {key} has unsupported type across samples"
                lens = set([len(j) for s in self.samples for i in s[key] for j in i])
                assert len(lens) == 1, f"Key {key} has vectors of different lengths"
                input_info[key] = {"type": type_, "dim": 3, "len": lens.pop()}

        return input_info

    def _index_patient(self) -> Dict[str, List[int]]:
        """Helper function which indexes the samples by patient_id.

        Will be called in `self.__init__()`.
        Returns:
            patient_to_index: Dict[str, int], a dict mapping patient_id to a list
                of sample indices.
        """
        patient_to_index = {}
        for idx, sample in enumerate(self.samples):
            patient_to_index.setdefault(sample["patient_id"], []).append(idx)
        return patient_to_index

    def _index_visit(self) -> Dict[str, List[int]]:
        """Helper function which indexes the samples by visit_id.

        Will be called in `self.__init__()`.

        Returns:
            visit_to_index: Dict[str, int], a dict mapping visit_id to a list
                of sample indices.
        """
        visit_to_index = {}
        for idx, sample in enumerate(self.samples):
            visit_to_index.setdefault(sample["visit_id"], []).append(idx)
        return visit_to_index

    @property
    def available_keys(self) -> List[str]:
        """Returns a list of available keys for the dataset.

        Returns:
            List of available keys.
        """
        keys = self.samples[0].keys()
        return list(keys)

    def get_distribution_tokens(self, key: str) -> Dict[str, int]:
        """Gets the distribution of tokens with a specific key in the samples.

        Args:
            key: the key of the tokens in the samples.

        Returns:
            distribution: a dict mapping token to count.
        """

        tokens = self.get_all_tokens(key, remove_duplicates=False, sort=False)
        counter = Counter(tokens)
        return counter

    def stat(self) -> str:
        """Returns some statistics of the task-specific dataset."""
        lines = list()
        lines.append(f"Statistics of sample dataset:")
        lines.append(f"\t- Dataset: {self.dataset_name}")
        lines.append(f"\t- Task: {self.task_name}")
        lines.append(f"\t- Number of samples: {len(self)}")
        num_patients = len(set([sample["patient_id"] for sample in self.samples]))
        lines.append(f"\t- Number of patients: {num_patients}")
        num_visits = len(set([sample["visit_id"] for sample in self.samples]))
        lines.append(f"\t- Number of visits: {num_visits}")
        lines.append(
            f"\t- Number of visits per patient: {len(self) / num_patients:.4f}"
        )
        for key in self.samples[0]:
            if key in ["patient_id", "visit_id"]:
                continue
            input_type = self.input_info[key]["type"]
            input_dim = self.input_info[key]["dim"]

            if input_dim <= 1:
                # a single value or vector
                num_events = [1 for sample in self.samples]
            elif input_dim == 2:
                # a list
                num_events = [len(sample[key]) for sample in self.samples]
            elif input_dim == 3:
                # a list of list
                num_events = [len(flatten_list(sample[key])) for sample in self.samples]
            else:
                raise NotImplementedError
            lines.append(f"\t- {key}:")
            lines.append(
                f"\t\t- Number of {key} per sample: "
                f"{sum(num_events) / len(num_events):.4f}"
            )
            if input_type == str or input_dim == 0:
                # single value or code-based
                lines.append(
                    f"\t\t- Number of unique {key}: {len(self.get_all_tokens(key))}"
                )
                distribution = self.get_distribution_tokens(key)
                top10 = sorted(distribution.items(), key=lambda x: x[1], reverse=True)[
                    :10
                ]
                lines.append(f"\t\t- Distribution of {key} (Top-10): {top10}")
            else:
                # vector-based
                vector = self.samples[0][key]
                lines.append(f"\t\t- Length of {key}: {self.input_info[key]['len']}")
        print("\n".join(lines))
        return "\n".join(lines)


if __name__ == "__main__":
    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            "single_vector": [1, 2, 3],
            "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
            "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
            "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
            "list_list_vectors": [
                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
                [[7.7, 8.5, 9.4]],
            ],
            "label": 1,
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-1",
            "single_vector": [1, 5, 8],
            "list_codes": [
                "55154191800",
                "551541928",
                "55154192800",
                "705182798",
                "70518279800",
            ],
            "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7]],
            "list_list_codes": [["A04A", "B035", "C129"], ["A07B", "A07C"]],
            "list_list_vectors": [
                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6]],
                [[7.7, 8.4, 1.3]],
            ],
            "label": 0,
        },
    ]

    dataset = SampleEHRDataset(samples=samples)

    dataset.stat()
    data = iter(dataset)
    print(next(data))


Here is the code content for shhs.py:
import os

import numpy as np

from pyhealth.datasets import BaseSignalDataset


class SHHSDataset(BaseSignalDataset):
    """Base EEG dataset for Sleep Heart Health Study (SHHS)

    Dataset is available at https://sleepdata.org/datasets/shhs

    The Sleep Heart Health Study (SHHS) is a multi-center cohort study implemented by the National Heart Lung & Blood Institute to determine the cardiovascular and other consequences of sleep-disordered breathing. It tests whether sleep-related breathing is associated with an increased risk of coronary heart disease, stroke, all cause mortality, and hypertension.  In all, 6,441 men and women aged 40 years and older were enrolled between November 1, 1995 and January 31, 1998 to take part in SHHS Visit 1. During exam cycle 3 (January 2001- June 2003), a second polysomnogram (SHHS Visit 2) was obtained in 3,295 of the participants. CVD Outcomes data were monitored and adjudicated by parent cohorts between baseline and 2011. More than 130 manuscripts have been published investigating predictors and outcomes of sleep disorders.

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data (should contain many csv files).
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.

    Attributes:
        task: Optional[str], name of the task (e.g., "sleep staging").
            Default is None.
        samples: Optional[List[Dict]], a list of samples, each sample is a dict with
            patient_id, record_id, and other task-specific attributes as key.
            Default is None.
        patient_to_index: Optional[Dict[str, List[int]]], a dict mapping patient_id to
            a list of sample indices. Default is None.
        visit_to_index: Optional[Dict[str, List[int]]], a dict mapping visit_id to a
            list of sample indices. Default is None.

    Examples:
        >>> from pyhealth.datasets import SHHSDataset
        >>> dataset = SHHSDataset(
        ...         root="/srv/local/data/SHHS/",
        ...     )
        >>> dataset.stat()
        >>> dataset.info()
    """

    def parse_patient_id(self, file_name):
        """
        Args:
            file_name: the file name of the shhs datasets. e.g., shhs1-200001.edf
        Returns:
            patient_id: the patient id of the shhs datasets. e.g., 200001
        """
        return file_name.split("-")[1].split(".")[0]

    def process_EEG_data(self):

        # get shhs1
        shhs1 = []
        if os.path.exists(os.path.join(self.root, "edfs/shhs1")):
            print("shhs1 exists and load shhs1")
            shhs1 = os.listdir(os.path.join(self.root, "edfs/shhs1"))
        else:
            print("shhs1 does not exist")

        # get shhs2
        shhs2 = []
        if os.path.exists(os.path.join(self.root, "edfs/shhs2")):
            print("shhs2 exists and load shhs2")
            shhs2 = os.listdir(os.path.join(self.root, "edfs/shhs2"))
        else:
            print("shhs2 does not exist")

        # get all patient ids
        patient_ids = np.unique([self.parse_patient_id(file) for file in shhs1 + shhs2])
        if self.dev:
            patient_ids = patient_ids[:5]
        # get patient to record maps
        #    - key: pid:
        #    - value: [{"load_from_path": None, "file": None, "save_to_path": None}, ...]
        patients = {pid: [] for pid in patient_ids}

        # parse shhs1
        for file in shhs1:
            pid = self.parse_patient_id(file)
            if pid in patient_ids:
                patients[pid].append(
                    {
                        "load_from_path": self.root,
                        "signal_file": os.path.join("edfs/shhs1", file),
                        "label_file": os.path.join("annotations-events-profusion/shhs1", f"shhs1-{pid}-profusion.xml"),
                        "save_to_path": os.path.join(self.filepath),
                    }
                )

        # parse shhs2
        for file in shhs2:
            pid = self.parse_patient_id(file)
            if pid in patient_ids:
                patients[pid].append(
                    {
                        "load_from_path": self.root,
                        "signal_file": os.path.join("edfs/shhs2", file),
                        "label_file": os.path.join("annotations-events-profusion/label", f"shhs2-{pid}-profusion.xml"),
                        "save_to_path": os.path.join(self.filepath),
                    }
                )
        return patients


if __name__ == "__main__":
    dataset = SHHSDataset(
        root="/srv/local/data/SHHS/polysomnography",
        dev=True,
        refresh_cache=True,
    )
    dataset.stat()
    dataset.info()
    print(list(dataset.patients.items())[0])


Here is the code content for sleepedf.py:
import os

import numpy as np

from pyhealth.datasets import BaseSignalDataset


class SleepEDFDataset(BaseSignalDataset):
    """Base EEG dataset for SleepEDF

    Dataset is available at https://www.physionet.org/content/sleep-edfx/1.0.0/

    For the Sleep Cassette Study portion:
        - The 153 SC* files (SC = Sleep Cassette) were obtained in a 1987-1991 study of age effects on sleep in healthy Caucasians aged 25-101, without any sleep-related medication [2]. Two PSGs of about 20 hours each were recorded during two subsequent day-night periods at the subjects homes. Subjects continued their normal activities but wore a modified Walkman-like cassette-tape recorder described in chapter VI.4 (page 92) of Bob's 1987 thesis [7].

        - Files are named in the form SC4ssNEO-PSG.edf where ss is the subject number, and N is the night. The first nights of subjects 36 and 52, and the second night of subject 13, were lost due to a failing cassette or laserdisk.

        - The EOG and EEG signals were each sampled at 100 Hz. The submental-EMG signal was electronically highpass filtered, rectified and low-pass filtered after which the resulting EMG envelope expressed in uV rms (root-mean-square) was sampled at 1Hz. Oro-nasal airflow, rectal body temperature and the event marker were also sampled at 1Hz.

        - Subjects and recordings are further described in the file headers, the descriptive spreadsheet SC-subjects.xls, and in [2].

    For the Sleep Telemetry portoin:
        - The 44 ST* files (ST = Sleep Telemetry) were obtained in a 1994 study of temazepam effects on sleep in 22 Caucasian males and females without other medication. Subjects had mild difficulty falling asleep but were otherwise healthy. The PSGs of about 9 hours were recorded in the hospital during two nights, one of which was after temazepam intake, and the other of which was after placebo intake. Subjects wore a miniature telemetry system with very good signal quality described in [8].

        - Files are named in the form ST7ssNJ0-PSG.edf where ss is the subject number, and N is the night.

        - EOG, EMG and EEG signals were sampled at 100 Hz, and the event marker at 1 Hz. The physical marker dimension ID+M-E relates to the fact that pressing the marker (M) button generated two-second deflections from a baseline value that either identifies the telemetry unit (ID = 1 or 2 if positive) or marks an error (E) in the telemetry link if negative. Subjects and recordings are further described in the file headers, the descriptive spreadsheet ST-subjects.xls, and in [1].

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data. *You can choose to use the path to Cassette portion or the Telemetry portion.*
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.

    Attributes:
        task: Optional[str], name of the task (e.g., "sleep staging").
            Default is None.
        samples: Optional[List[Dict]], a list of samples, each sample is a dict with
            patient_id, record_id, and other task-specific attributes as key.
            Default is None.
        patient_to_index: Optional[Dict[str, List[int]]], a dict mapping patient_id to
            a list of sample indices. Default is None.
        visit_to_index: Optional[Dict[str, List[int]]], a dict mapping visit_id to a
            list of sample indices. Default is None.

    Examples:
        >>> from pyhealth.datasets import SleepEDFDataset
        >>> dataset = SleepEDFDataset(
        ...         root="/srv/local/data/SLEEPEDF/sleep-edf-database-expanded-1.0.0/sleep-cassette",
        ...     )
        >>> dataset.stat()
        >>> dataset.info()
    """

    def process_EEG_data(self):

        # get all file names
        all_files = os.listdir(self.root)
        # get all patient ids
        patient_ids = np.unique([file[:6] for file in all_files])
        if self.dev:
            patient_ids = patient_ids[:5]
        # get patient to record maps
        #    - key: pid:
        #    - value: [{"load_from_path": None, "signal_file": None, "label_file": None, "save_to_path": None}, ...]
        patients = {
            pid: [
                {
                    "load_from_path": self.root,
                    "signal_file": None,
                    "label_file": None,
                    "save_to_path": self.filepath,
                }
            ]
            for pid in patient_ids
        }
        for record in all_files:
            pid = record[:6]
            if pid in patient_ids:
                if "PSG" in record:
                    patients[pid][0]["signal_file"] = record
                elif "Hypnogram" in record:
                    patients[pid][0]["label_file"] = record
                else:
                    raise ValueError(f"Unknown record: {record}")
        return patients


if __name__ == "__main__":
    dataset = SleepEDFDataset(
        root="/srv/local/data/SLEEPEDF/sleep-edf-database-expanded-1.0.0/sleep-telemetry",
        dev=True,
        refresh_cache=True,
    )
    dataset.stat()
    dataset.info()
    print(list(dataset.patients.items())[0])


Here is the code content for splitter.py:
from itertools import chain
from typing import Optional, Tuple, Union, List

import numpy as np
import torch

from pyhealth.datasets import SampleBaseDataset


# TODO: train_dataset.dataset still access the whole dataset which may leak information
# TODO: add more splitting methods


def split_by_visit(
    dataset: SampleBaseDataset,
    ratios: Union[Tuple[float, float, float], List[float]],
    seed: Optional[int] = None,
):
    """Splits the dataset by visit (i.e., samples).

    Args:
        dataset: a `SampleBaseDataset` object
        ratios: a list/tuple of ratios for train / val / test
        seed: random seed for shuffling the dataset

    Returns:
        train_dataset, val_dataset, test_dataset: three subsets of the dataset of
            type `torch.utils.data.Subset`.

    Note:
        The original dataset can be accessed by `train_dataset.dataset`,
            `val_dataset.dataset`, and `test_dataset.dataset`.
    """
    if seed is not None:
        np.random.seed(seed)
    assert sum(ratios) == 1.0, "ratios must sum to 1.0"
    index = np.arange(len(dataset))
    np.random.shuffle(index)
    train_index = index[: int(len(dataset) * ratios[0])]
    val_index = index[
        int(len(dataset) * ratios[0]) : int(len(dataset) * (ratios[0] + ratios[1]))
    ]
    test_index = index[int(len(dataset) * (ratios[0] + ratios[1])) :]
    train_dataset = torch.utils.data.Subset(dataset, train_index)
    val_dataset = torch.utils.data.Subset(dataset, val_index)
    test_dataset = torch.utils.data.Subset(dataset, test_index)
    return train_dataset, val_dataset, test_dataset


def split_by_patient(
    dataset: SampleBaseDataset,
    ratios: Union[Tuple[float, float, float], List[float]],
    seed: Optional[int] = None,
):
    """Splits the dataset by patient.

    Args:
        dataset: a `SampleBaseDataset` object
        ratios: a list/tuple of ratios for train / val / test
        seed: random seed for shuffling the dataset

    Returns:
        train_dataset, val_dataset, test_dataset: three subsets of the dataset of
            type `torch.utils.data.Subset`.

    Note:
        The original dataset can be accessed by `train_dataset.dataset`,
            `val_dataset.dataset`, and `test_dataset.dataset`.
    """
    if seed is not None:
        np.random.seed(seed)
    assert sum(ratios) == 1.0, "ratios must sum to 1.0"
    patient_indx = list(dataset.patient_to_index.keys())
    num_patients = len(patient_indx)
    np.random.shuffle(patient_indx)
    train_patient_indx = patient_indx[: int(num_patients * ratios[0])]
    val_patient_indx = patient_indx[
        int(num_patients * ratios[0]) : int(num_patients * (ratios[0] + ratios[1]))
    ]
    test_patient_indx = patient_indx[int(num_patients * (ratios[0] + ratios[1])) :]
    train_index = list(
        chain(*[dataset.patient_to_index[i] for i in train_patient_indx])
    )
    val_index = list(chain(*[dataset.patient_to_index[i] for i in val_patient_indx]))
    test_index = list(chain(*[dataset.patient_to_index[i] for i in test_patient_indx]))
    train_dataset = torch.utils.data.Subset(dataset, train_index)
    val_dataset = torch.utils.data.Subset(dataset, val_index)
    test_dataset = torch.utils.data.Subset(dataset, test_index)
    return train_dataset, val_dataset, test_dataset


Here is the code content for tuab.py:
import os

import numpy as np

from pyhealth.datasets import BaseSignalDataset


class TUABDataset(BaseSignalDataset):
    """Base EEG dataset for the TUH Abnormal EEG Corpus

    Dataset is available at https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml

    The TUAB dataset (or Temple University Hospital EEG Abnormal Corpus) is a collection of EEG data acquired at the Temple University Hospital. 
    
    The dataset contains both normal and abnormal EEG readings.

    Files are named in the form aaaaamye_s001_t000.edf. This includes the subject identifier ("aaaaamye"), the session number ("s001") and a token number ("t000"). EEGs are split into a series of files starting with *t000.edf, *t001.edf, ...

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data. *You can choose to use the path to Cassette portion or the Telemetry portion.*
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.

    Attributes:
        task: Optional[str], name of the task (e.g., "EEG_abnormal").
            Default is None.
        samples: Optional[List[Dict]], a list of samples, each sample is a dict with
            patient_id, record_id, and other task-specific attributes as key.
            Default is None.
        patient_to_index: Optional[Dict[str, List[int]]], a dict mapping patient_id to
            a list of sample indices. Default is None.
        visit_to_index: Optional[Dict[str, List[int]]], a dict mapping visit_id to a
            list of sample indices. Default is None.

    Examples:
        >>> from pyhealth.datasets import TUABDataset
        >>> dataset = TUABDataset(
        ...         root="/srv/local/data/TUH/tuh_eeg_abnormal/v3.0.0/edf/",
        ...     )
        >>> dataset.stat()
        >>> dataset.info()
    """

    def process_EEG_data(self):
        # create a map for data sets for latter mapping patients
        data_map = {
            "train/abnormal": "0",
            "train/normal": "1",
            "eval/abnormal": "2",
            "eval/normal": "3",
        }

        data_map_reverse = {
            "0": "train/abnormal",
            "1": "train/normal",
            "2": "eval/abnormal",
            "3": "eval/normal",
        }

        # get all file names
        all_files = {}

        train_abnormal_files = os.listdir(os.path.join(self.root, "train/abnormal/01_tcp_ar"))
        all_files["train/abnormal"] = train_abnormal_files

        train_normal_files = os.listdir(os.path.join(self.root, "train/normal/01_tcp_ar"))
        all_files["train/normal"] = train_normal_files

        eval_abnormal_files = os.listdir(os.path.join(self.root, "eval/abnormal/01_tcp_ar"))
        all_files["eval/abnormal"] = eval_abnormal_files

        eval_normal_files = os.listdir(os.path.join(self.root, "eval/normal/01_tcp_ar"))
        all_files["eval/normal"] = eval_normal_files


        # get all patient ids
        patient_ids = []
        for field, sub_data in all_files.items():
            patient_ids.extend(["{}_{}".format(data_map[field], data.split("_")[0]) for data in sub_data])

        patient_ids = list(set(patient_ids))

        if self.dev:
            patient_ids = patient_ids[:20]

        # get patient to record maps
        #    - key: pid:
        #    - value: [{"load_from_path": None, "patient_id": None, "signal_file": None, "label_file": None, "save_to_path": None}, ...]
        patients = {
            pid: []
            for pid in patient_ids
        }
           
        for pid in patient_ids:
            data_field = data_map_reverse[pid.split("_")[0]]
            patient_visits = [file for file in all_files[data_field] if file.split("_")[0] == pid.split("_")[1]]
            
            for visit in patient_visits:
                patients[pid].append({
                    "load_from_path": os.path.join(self.root, data_field, "01_tcp_ar"),
                    "patient_id": pid,
                    "visit_id": visit.strip(".edf").strip(pid.split("_")[1])[1:],
                    "signal_file": visit,
                    "label_file": visit,
                    "save_to_path": self.filepath,
                })
        
        return patients


if __name__ == "__main__":
    dataset = TUABDataset(
        root="/srv/local/data/TUH/tuh_eeg_abnormal/v3.0.0/edf/",
        dev=True,
        refresh_cache=True,
    )
    dataset.stat()
    dataset.info()
    print(list(dataset.patients.items())[0])


Here is the code content for tuev.py:
import os

import numpy as np

from pyhealth.datasets import BaseSignalDataset


class TUEVDataset(BaseSignalDataset):
    """Base EEG dataset for the TUH EEG Events Corpus

    Dataset is available at https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml

    This corpus is a subset of TUEG that contains annotations of EEG segments as one of six classes: (1) spike and sharp wave (SPSW), (2) generalized periodic epileptiform discharges (GPED), (3) periodic lateralized epileptiform discharges (PLED), (4) eye movement (EYEM), (5) artifact (ARTF) and (6) background (BCKG).

    Files are named in the form of bckg_032_a_.edf in the eval partition:
        bckg: this file contains background annotations.
		032: a reference to the eval index	
		a_.edf: EEG files are split into a series of files starting with a_.edf, a_1.ef, ... These represent pruned EEGs, so the  original EEG is split into these segments, and uninteresting parts of the original recording were deleted.
    or in the form of 00002275_00000001.edf in the train partition:
        00002275: a reference to the train index. 
		0000001: indicating that this is the first file inssociated with this patient. 

    Args:
        dataset_name: name of the dataset.
        root: root directory of the raw data. *You can choose to use the path to Cassette portion or the Telemetry portion.*
        dev: whether to enable dev mode (only use a small subset of the data).
            Default is False.
        refresh_cache: whether to refresh the cache; if true, the dataset will
            be processed from scratch and the cache will be updated. Default is False.

    Attributes:
        task: Optional[str], name of the task (e.g., "EEG_events").
            Default is None.
        samples: Optional[List[Dict]], a list of samples, each sample is a dict with
            patient_id, record_id, and other task-specific attributes as key.
            Default is None.
        patient_to_index: Optional[Dict[str, List[int]]], a dict mapping patient_id to
            a list of sample indices. Default is None.
        visit_to_index: Optional[Dict[str, List[int]]], a dict mapping visit_id to a
            list of sample indices. Default is None.

    Examples:
        >>> from pyhealth.datasets import TUEVDataset
        >>> dataset = TUEVDataset(
        ...         root="/srv/local/data/TUH/tuh_eeg_events/v2.0.0/edf/",
        ...     )
        >>> dataset.stat()
        >>> dataset.info()
    """

    def process_EEG_data(self):
        # get all file names
        all_files = {}

        train_files = os.listdir(os.path.join(self.root, "train/"))
        for id in train_files:
            if id != ".DS_Store":
                all_files["0_{}".format(id)] = [name for name in os.listdir(os.path.join(self.root, "train/", id)) if name.endswith(".edf")]

        eval_files = os.listdir(os.path.join(self.root, "eval/"))
        for id in eval_files:
            if id != ".DS_Store":
                all_files["1_{}".format(id)] = [name for name in os.listdir(os.path.join(self.root, "eval/", id)) if name.endswith(".edf")]

        # get all patient ids
        patient_ids = list(set(list(all_files.keys())))

        if self.dev:
            patient_ids = patient_ids[:20]
            # print(patient_ids)

        # get patient to record maps
        #    - key: pid:
        #    - value: [{"load_from_path": None, "patient_id": None, "signal_file": None, "label_file": None, "save_to_path": None}, ...]
        patients = {
            pid: []
            for pid in patient_ids
        }
           
        for pid in patient_ids:
            split = "train" if pid.split("_")[0] == "0" else "eval"
            id = pid.split("_")[1]

            patient_visits = all_files[pid]
            
            for visit in patient_visits:
                if split == "train":
                    visit_id = visit.strip(".edf").split("_")[1]
                else:
                    visit_id = visit.strip(".edf")
                    
                patients[pid].append({
                    "load_from_path": os.path.join(self.root, split, id),
                    "patient_id": pid,
                    "visit_id": visit_id,
                    "signal_file": visit,
                    "label_file": visit,
                    "save_to_path": self.filepath,
                })
        
        return patients


if __name__ == "__main__":
    dataset = TUEVDataset(
        root="/srv/local/data/TUH/tuh_eeg_events/v2.0.0/edf",
        dev=True,
        refresh_cache=True,
    )
    dataset.stat()
    dataset.info()
    print(list(dataset.patients.items())[0])


Here is the code content for utils.py:
import hashlib
import os
from datetime import datetime
from typing import List, Tuple, Optional
import pickle

from dateutil.parser import parse as dateutil_parse
from torch.utils.data import DataLoader

from pyhealth import BASE_CACHE_PATH
from pyhealth.utils import create_directory

MODULE_CACHE_PATH = os.path.join(BASE_CACHE_PATH, "datasets")
create_directory(MODULE_CACHE_PATH)


# basic tables which are a part of the defined datasets
DATASET_BASIC_TABLES = {
    "MIMIC3Dataset": {"PATIENTS", "ADMISSIONS"},
    "MIMIC4Dataset": {"patients", "admission"},
}


def hash_str(s):
    return hashlib.md5(s.encode()).hexdigest()


def strptime(s: str) -> Optional[datetime]:
    """Helper function which parses a string to datetime object.

    Args:
        s: str, string to be parsed.

    Returns:
        Optional[datetime], parsed datetime object. If s is nan, return None.
    """
    # return None if s is nan
    if s != s:
        return None
    return dateutil_parse(s)

def padyear(year: str, month='1', day='1') -> str:
    """Pad a date time year of format 'YYYY' to format 'YYYY-MM-DD'
    
    Args: 
        year: str, year to be padded. Must be non-zero value.
        month: str, month string to be used as padding. Must be in [1, 12]
        day: str, day string to be used as padding. Must be in [1, 31]
        
    Returns:
        padded_date: str, padded year.
    
    """
    return f"{year}-{month}-{day}"

def flatten_list(l: List) -> List:
    """Flattens a list of list.

    Args:
        l: List, the list of list to be flattened.

    Returns:
        List, the flattened list.

    Examples:
        >>> flatten_list([[1], [2, 3], [4]])
        [1, 2, 3, 4]R
        >>> flatten_list([[1], [[2], 3], [4]])
        [1, [2], 3, 4]
    """
    assert isinstance(l, list), "l must be a list."
    return sum(l, [])


def list_nested_levels(l: List) -> Tuple[int]:
    """Gets all the different nested levels of a list.

    Args:
        l: the list to be checked.

    Returns:
        All the different nested levels of the list.

    Examples:
        >>> list_nested_levels([])
        (1,)
        >>> list_nested_levels([1, 2, 3])
        (1,)
        >>> list_nested_levels([[]])
        (2,)
        >>> list_nested_levels([[1, 2, 3], [4, 5, 6]])
        (2,)
        >>> list_nested_levels([1, [2, 3], 4])
        (1, 2)
        >>> list_nested_levels([[1, [2, 3], 4]])
        (2, 3)
    """
    if not isinstance(l, list):
        return tuple([0])
    if not l:
        return tuple([1])
    levels = []
    for i in l:
        levels.extend(list_nested_levels(i))
    levels = [i + 1 for i in levels]
    return tuple(set(levels))


def is_homo_list(l: List) -> bool:
    """Checks if a list is homogeneous.

    Args:
        l: the list to be checked.

    Returns:
        bool, True if the list is homogeneous, False otherwise.

    Examples:
        >>> is_homo_list([1, 2, 3])
        True
        >>> is_homo_list([])
        True
        >>> is_homo_list([1, 2, "3"])
        False
        >>> is_homo_list([1, 2, 3, [4, 5, 6]])
        False
    """
    if not l:
        return True

    # if the value vector is a mix of float and int, convert all to float
    l = [float(i) if type(i) == int else i for i in l]
    return all(isinstance(i, type(l[0])) for i in l)


def collate_fn_dict(batch):
    return {key: [d[key] for d in batch] for key in batch[0]}


def get_dataloader(dataset, batch_size, shuffle=False):

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        collate_fn=collate_fn_dict,
    )

    return dataloader


if __name__ == "__main__":
    print(list_nested_levels([1, 2, 3]))
    print(list_nested_levels([1, [2], 3]))
    print(list_nested_levels([[1, [2], [[3]]]]))
    print(is_homo_list([1, 2, 3]))
    print(is_homo_list([1, 2, [3]]))
    print(is_homo_list([1, 2.0]))


Here is the code content for __init__.py:
from .codes.atc import ATC
from .codes.ccscm import CCSCM
from .codes.ccsproc import CCSPROC
from .codes.icd10cm import ICD10CM
from .codes.icd10proc import ICD10PROC
from .codes.icd9cm import ICD9CM
from .codes.icd9proc import ICD9PROC
from .codes.ndc import NDC
from .codes.rxnorm import RxNorm
from .cross_map import CrossMap
from .inner_map import InnerMap


Here is the code content for cross_map.py:
import logging
import os
from collections import defaultdict
from typing import List, Optional, Dict
from urllib.error import HTTPError

import pyhealth.medcode as medcode
from pyhealth.medcode.utils import MODULE_CACHE_PATH, download_and_read_csv
from pyhealth.utils import load_pickle, save_pickle

logger = logging.getLogger(__name__)


class CrossMap:
    """Contains mapping between two medical code systems.

    `CrossMap` is a base class for all possible mappings. It will be
    initialized with two specific medical code systems with
    `CrossMap.load(source_vocabulary, target_vocabulary)`.
    """

    def __init__(
        self,
        source_vocabulary: str,
        target_vocabulary: str,
        refresh_cache: bool = False,
    ):
        self.s_vocab = source_vocabulary
        self.t_vocab = target_vocabulary

        # load mapping
        pickle_filename = f"{self.s_vocab}_to_{self.t_vocab}.pkl"
        pickle_filepath = os.path.join(MODULE_CACHE_PATH, pickle_filename)
        if os.path.exists(pickle_filepath) and (not refresh_cache):
            logger.debug(
                f"Loaded {self.s_vocab}->{self.t_vocab} mapping "
                f"from {pickle_filepath}"
            )
            self.mapping = load_pickle(pickle_filepath)
        else:
            logger.debug(f"Processing {self.s_vocab}->{self.t_vocab} mapping...")
            try:
                local_filename = f"{self.s_vocab}_to_{self.t_vocab}.csv"
                df = download_and_read_csv(local_filename, refresh_cache)
            except HTTPError:
                local_filename = f"{self.t_vocab}_to_{self.s_vocab}.csv"
                df = download_and_read_csv(local_filename, refresh_cache)
            self.mapping = defaultdict(list)
            for _, row in df.iterrows():
                self.mapping[row[self.s_vocab]].append(row[self.t_vocab])
            logger.debug(
                f"Saved {self.s_vocab}->{self.t_vocab} mapping " f"to {pickle_filepath}"
            )
            save_pickle(self.mapping, pickle_filepath)

        # load source and target vocabulary classes
        self.s_class = getattr(medcode, source_vocabulary)()
        self.t_class = getattr(medcode, target_vocabulary)()
        return

    def __repr__(self):
        return f"CrossMap(source_vocabulary={self.s_vocab}, source_class={self.s_class} target_vocabulary={self.t_vocab}, target_class={self.t_class})"

    @classmethod
    def load(
        cls,
        source_vocabulary: str,
        target_vocabulary: str,
        refresh_cache: bool = False,
    ):
        """Initializes the mapping between two medical code systems.

        Args:
            source_vocabulary: source medical code system.
            target_vocabulary: target medical code system.
            refresh_cache: whether to refresh the cache. Default is False.

        Examples:
            >>> from pyhealth.medcode import CrossMap
            >>> mapping = CrossMap("ICD9CM", "CCSCM")
            >>> mapping.map("428.0")
            ['108']

            >>> mapping = CrossMap.load("NDC", "ATC")
            >>> mapping.map("00527051210", target_kwargs={"level": 3})
            ['A11C']
        """
        return cls(source_vocabulary, target_vocabulary, refresh_cache)

    def map(
        self,
        source_code: str,
        source_kwargs: Optional[Dict] = None,
        target_kwargs: Optional[Dict] = None,
    ) -> List[str]:
        """Maps a source code to a list of target codes.

        Args:
            source_code: source code.
            **source_kwargs: additional arguments for the source code. Will be
                passed to `self.s_class.convert()`. Default is empty dict.
            **target_kwargs: additional arguments for the target code. Will be
                passed to `self.t_class.convert()`. Default is empty dict.

        Returns:
            A list of target codes.
        """
        if source_kwargs is None:
            source_kwargs = {}
        if target_kwargs is None:
            target_kwargs = {}
        source_code = self.s_class.standardize(source_code)
        source_code = self.s_class.convert(source_code, **source_kwargs)
        target_codes = self.mapping[source_code]
        target_codes = [self.t_class.convert(c, **target_kwargs) for c in target_codes]
        return target_codes

Here is the code content for inner_map.py:
import logging
import os
from abc import ABC, abstractmethod
from typing import List

import networkx as nx
import pandas as pd

import pyhealth.medcode as medcode
from pyhealth.medcode.utils import MODULE_CACHE_PATH, download_and_read_csv
from pyhealth.utils import load_pickle, save_pickle

logger = logging.getLogger(__name__)


# TODO: add this callable method: InnerMap(vocab)
class InnerMap(ABC):
    """Contains information for a specific medical code system.

    `InnerMap` is a base abstract class for all medical code systems.
    It will be instantiated as a specific medical code system with
    `InnerMap.load(vocabulary).`

    Note:
        This class cannot be instantiated using `__init__()` (throws an error).
    """

    @abstractmethod
    def __init__(
        self,
        vocabulary: str,
        refresh_cache: bool = False,
    ):
        # abstractmethod prevents initialization of this class
        self.vocabulary = vocabulary

        pickle_filepath = os.path.join(MODULE_CACHE_PATH, self.vocabulary + ".pkl")
        csv_filename = self.vocabulary + ".csv"
        if os.path.exists(pickle_filepath) and (not refresh_cache):
            logger.debug(f"Loaded {vocabulary} code from {pickle_filepath}")
            self.graph = load_pickle(pickle_filepath)
        else:
            logger.debug(f"Processing {vocabulary} code...")
            df = download_and_read_csv(csv_filename, refresh_cache)
            # create graph
            df = df.set_index("code")
            self.graph = nx.DiGraph()
            # add nodes
            for code, row in df.iterrows():
                row_dict = row.to_dict()
                row_dict.pop("parent_code", None)
                self.graph.add_node(code, **row_dict)
            # add edges
            for code, row in df.iterrows():
                if "parent_code" in row:
                    if not pd.isna(row["parent_code"]):
                        self.graph.add_edge(row["parent_code"], code)
            logger.debug(f"Saved {vocabulary} code to {pickle_filepath}")
            save_pickle(self.graph, pickle_filepath)
        return

    def __repr__(self):
        return f"InnerMap(vocabulary={self.vocabulary}, graph={self.graph})"

    @classmethod
    def load(_, vocabulary: str, refresh_cache: bool = False):
        """Initializes a specific medical code system inheriting from `InnerMap`.

        Args:
            vocabulary: vocabulary name. E.g., "ICD9CM", "ICD9PROC".
            refresh_cache: whether to refresh the cache. Default is False.

        Examples:
            >>> from pyhealth.medcode import InnerMap
            >>> icd9cm = InnerMap.load("ICD9CM")
            >>> icd9cm.lookup("428.0")
            'Congestive heart failure, unspecified'
            >>> icd9cm.get_ancestors("428.0")
            ['428', '420-429.99', '390-459.99', '001-999.99']
        """
        cls = getattr(medcode, vocabulary)
        return cls(refresh_cache=refresh_cache)

    @property
    def available_attributes(self) -> List[str]:
        """Returns a list of available attributes.

        Returns:
            List of available attributes.
        """
        return list(list(self.graph.nodes.values())[0].keys())

    def stat(self):
        """Prints statistics of the code system."""
        print()
        print(f"Statistics for {self.vocabulary}:")
        print(f"\t- Number of nodes: {len(self.graph.nodes)}")
        print(f"\t- Number of edges: {len(self.graph.edges)}")
        print(f"\t- Available attributes: {self.available_attributes}")
        print()

    @staticmethod
    def standardize(code: str) -> str:
        """Standardizes a given code.

        Subclass will override this method based on different
        medical code systems.
        """
        return code

    @staticmethod
    def convert(code: str, **kwargs) -> str:
        """Converts a given code.

        Subclass will override this method based on different
        medical code systems.
        """
        return code

    def lookup(self, code: str, attribute: str = "name"):
        """Looks up the code.

        Args:
            code: code to look up.
            attribute: attribute to look up. One of `self.available_attributes`.
                Default is "name".

        Returns:
            The attribute value of the code.
        """
        code = self.standardize(code)
        return self.graph.nodes[code][attribute]

    def __contains__(self, code: str) -> bool:
        """Checks if the code is in the code system."""
        code = self.standardize(code)
        return code in self.graph.nodes

    def get_ancestors(self, code: str) -> List[str]:
        """Gets the ancestors of the code.

        Args:
            code: code to look up.

        Returns:
            List of ancestors ordered from the closest to the farthest.
        """
        code = self.standardize(code)
        # ordered ancestors
        ancestors = nx.ancestors(self.graph, code)
        ancestors = list(ancestors)
        ancestors = sorted(
            ancestors, key=lambda x: (nx.shortest_path_length(self.graph, x, code), x)
        )
        return ancestors

    def get_descendants(self, code: str) -> List[str]:
        """Gets the descendants of the code.

        Args:
            code: code to look up.

        Returns:
            List of ancestors ordered from the closest to the farthest.
        """
        code = self.standardize(code)
        # ordered descendants
        descendants = nx.descendants(self.graph, code)
        descendants = list(descendants)
        descendants = sorted(
            descendants, key=lambda x: (nx.shortest_path_length(self.graph, code, x), x)
        )
        return descendants


if __name__ == "__main__":
    icd9cm = InnerMap.load("ICD9CM")
    print(icd9cm.stat())
    print("428.0" in icd9cm)
    print(icd9cm.lookup("4280"))
    print(icd9cm.get_ancestors("428.0"))
    print(icd9cm.get_descendants("428.0"))

Here is the code content for utils.py:
import logging
import os
from urllib.parse import urljoin
from urllib.request import urlretrieve

import pandas as pd

from pyhealth import BASE_CACHE_PATH
from pyhealth.utils import create_directory

BASE_URL = "https://storage.googleapis.com/pyhealth/resource/"
MODULE_CACHE_PATH = os.path.join(BASE_CACHE_PATH, "medcode")
create_directory(MODULE_CACHE_PATH)

logger = logging.getLogger(__name__)


def download_and_read_csv(filename: str, refresh_cache: bool = False) -> pd.DataFrame:
    """Reads a csv file from the pyhealth resource folder.

    This function will read the csv file from `MODULE_CACHE_PATH` if it exists.
    Otherwise, it will download the csv file from `BASE_URL` and save it to
    `MODULE_CACHE_PATH`.

    Args:
        filename: The name of the csv file.
        refresh_cache: Whether to refresh the cache. Default is False.

    Returns:
        A pandas DataFrame.
    """
    local_filepath = os.path.join(MODULE_CACHE_PATH, filename)
    online_filepath = urljoin(BASE_URL, filename)
    if (not os.path.exists(local_filepath)) or refresh_cache:
        logger.debug(f"downloading {online_filepath} to {local_filepath}")
        urlretrieve(online_filepath, local_filepath)
    return pd.read_csv(local_filepath, dtype=str)


Here is the code content for __init__.py:


Here is the code content for ccscm.py:
from pyhealth.medcode.inner_map import InnerMap


class CCSCM(InnerMap):
    """Classification of Diseases, Clinical Modification."""

    def __init__(self, **kwargs):
        super(CCSCM, self).__init__(vocabulary="CCSCM", **kwargs)


if __name__ == "__main__":
    code_sys = CCSCM(refresh_cache=True)
    code_sys.stat()
    print("20" in code_sys)
    print(code_sys.lookup("10"))
    print(code_sys.get_ancestors("10"))
    print(code_sys.get_descendants("10"))


Here is the code content for ccsproc.py:
from pyhealth.medcode.inner_map import InnerMap


class CCSPROC(InnerMap):
    """Classification of Diseases, Procedure."""

    def __init__(self, **kwargs):
        super(CCSPROC, self).__init__(vocabulary="CCSPROC", **kwargs)


if __name__ == "__main__":
    code_sys = CCSPROC(refresh_cache=True)
    code_sys.stat()
    print("1" in code_sys)
    print(code_sys.lookup("20"))
    print(code_sys.get_ancestors("20"))
    print(code_sys.get_descendants("20"))


Here is the code content for rxnorm.py:
from pyhealth.medcode.inner_map import InnerMap


class RxNorm(InnerMap):
    """RxNorm."""

    def __init__(self, **kwargs):
        super(RxNorm, self).__init__(vocabulary="RxNorm", **kwargs)


if __name__ == "__main__":
    code_sys = RxNorm(refresh_cache=True)
    code_sys.stat()
    print("21914" in code_sys)
    print(code_sys.graph.nodes["21914"])
    print(code_sys.get_ancestors("21914"))
    print(code_sys.get_descendants("21914"))


Here is the code content for atc.py:
from typing import List

from pyhealth.medcode.inner_map import InnerMap
from pyhealth.medcode.utils import download_and_read_csv


class ATC(InnerMap):
    """Anatomical Therapeutic Chemical."""

    def __init__(self, **kwargs):
        super(ATC, self).__init__(vocabulary="ATC", **kwargs)
        self.ddi = dict()

    @staticmethod
    def convert(code: str, level=5):
        """Convert ATC code to a specific level."""
        if type(level) is str:
            level = int(level)
        assert level in [1, 2, 3, 4, 5]
        if level == 1:
            return code[:1]
        elif level == 2:
            return code[:3]
        elif level == 3:
            return code[:4]
        elif level == 4:
            return code[:5]
        else:
            return code

    def get_ddi(
        self, gamenet_ddi: bool = False, refresh_cache: bool = False
    ) -> List[str]:
        """Gets the drug-drug interactions (DDI).

        Args:
            gamenet_ddi: Whether to use the DDI from the GAMENet paper,
                which is a subset of the DDI from the ATC.
            refresh_cache: Whether to refresh the cache. Default is False.
        """
        filename = "DDI_GAMENet.csv" if gamenet_ddi else "DDI.csv"
        if filename not in self.ddi or refresh_cache:
            df = download_and_read_csv(filename, refresh_cache)
            ddi = []
            for idx, row in df.iterrows():
                ddi.append([row["ATC i"], row["ATC j"]])
            self.ddi[filename] = ddi
        return self.ddi[filename]


if __name__ == "__main__":
    code_sys = ATC(refresh_cache=True)
    code_sys.stat()
    print(code_sys.lookup("N01AB07"))
    print(code_sys.lookup("N01AB07", attribute="level"))
    print(code_sys.lookup("N01AB07", attribute="description"))
    print(code_sys.lookup("N01AB07", attribute="indication"))
    print(code_sys.lookup("N01AB07", attribute="smiles"))
    print(code_sys.lookup("N01AB07", attribute="drugbank_id"))
    print(code_sys.get_ancestors("N01AB07"))
    print(code_sys.get_descendants("N01AB"))
    print(len(code_sys.get_ddi(gamenet_ddi=True)))
    print(len(code_sys.get_ddi(gamenet_ddi=False)))


Here is the code content for icd10cm.py:
from pyhealth.medcode.inner_map import InnerMap


# TODO: add convert


class ICD10CM(InnerMap):
    """10-th International Classification of Diseases, Clinical Modification."""

    def __init__(self, **kwargs):
        super(ICD10CM, self).__init__(vocabulary="ICD10CM", **kwargs)

    @staticmethod
    def standardize(code: str):
        """Standardizes ICD10CM code."""
        if "." in code:
            return code
        if len(code) <= 3:
            return code
        return code[:3] + "." + code[3:]


if __name__ == "__main__":
    code_sys = ICD10CM(refresh_cache=True)
    code_sys.stat()
    print("A00.0" in code_sys)
    print(code_sys.lookup("D50.0"))
    print(code_sys.get_ancestors("D50.0"))
    print(code_sys.get_descendants("D50"))


Here is the code content for icd10proc.py:
from pyhealth.medcode.inner_map import InnerMap


# TODO: add convert


class ICD10PROC(InnerMap):
    """10-th International Classification of Diseases, Procedure."""

    def __init__(self, **kwargs):
        super(ICD10PROC, self).__init__(vocabulary="ICD10PROC", **kwargs)


if __name__ == "__main__":
    code_sys = ICD10PROC(refresh_cache=True)
    code_sys.stat()
    print("0LBG0ZZ" in code_sys)
    print(code_sys.lookup("0LBG0ZZ"))
    print(code_sys.get_ancestors("0LBG0ZZ"))
    print(code_sys.get_descendants("0LBG0"))


Here is the code content for icd9cm.py:
from pyhealth.medcode.inner_map import InnerMap


# TODO: add convert


class ICD9CM(InnerMap):
    """9-th International Classification of Diseases, Clinical Modification."""

    def __init__(self, **kwargs):
        super(ICD9CM, self).__init__(vocabulary="ICD9CM", **kwargs)

    @staticmethod
    def standardize(code: str):
        """Standardizes ICD9CM code."""
        if "." in code:
            return code
        if code.startswith("E"):
            if len(code) <= 4:
                return code
            return code[:4] + "." + code[4:]
        else:
            if len(code) <= 3:
                return code
            return code[:3] + "." + code[3:]


if __name__ == "__main__":
    code_sys = ICD9CM(refresh_cache=True)
    code_sys.stat()
    print("821.01" in code_sys)
    print(code_sys.lookup("82101"))
    print(code_sys.get_ancestors("821.01"))
    print(code_sys.get_descendants("821"))


Here is the code content for icd9proc.py:
from pyhealth.medcode.inner_map import InnerMap


# TODO: add convert


class ICD9PROC(InnerMap):
    """9-th International Classification of Diseases, Procedure."""

    def __init__(self, **kwargs):
        super(ICD9PROC, self).__init__(vocabulary="ICD9PROC", **kwargs)

    @staticmethod
    def standardize(code: str):
        """Standardizes ICD9PROC code."""
        if "." in code:
            return code
        if len(code) <= 2:
            return code
        return code[:2] + "." + code[2:]


if __name__ == "__main__":
    code_sys = ICD9PROC(refresh_cache=True)
    code_sys.stat()
    print("81.01" in code_sys)
    print(code_sys.lookup("01.31"))
    print(code_sys.get_ancestors("01.31"))
    print(code_sys.get_descendants("01"))


Here is the code content for ndc.py:
from pyhealth.medcode.inner_map import InnerMap


# TODO: add standardize for different formats of NDC codes


class NDC(InnerMap):
    """National Drug Code."""

    def __init__(self, **kwargs):
        super(NDC, self).__init__(vocabulary="NDC", **kwargs)


if __name__ == "__main__":
    code_sys = NDC(refresh_cache=True)
    code_sys.stat()
    print("00527051210" in code_sys)
    print(code_sys.lookup("00527051210"))
    print(code_sys.get_ancestors("00527051210"))
    print(code_sys.get_descendants("00527051210"))


Here is the code content for drug_recommendation.py:
from typing import List

import numpy as np


# TODO: this metric is very ad-hoc, need to be improved


def ddi_rate_score(medications: List[np.ndarray], ddi_matrix: np.ndarray) -> float:
    """DDI rate score.

    Args:
        medications: list of medications for each patient, where each medication
            is represented by the corresponding index in the ddi matrix.
        ddi_matrix: array-like of shape (n_classes, n_classes).

    Returns:
        result: DDI rate score.
    """
    all_cnt = 0
    ddi_cnt = 0
    for sample in medications:
        for i, med_i in enumerate(sample):
            for j, med_j in enumerate(sample):
                if j <= i:
                    continue
                all_cnt += 1
                if ddi_matrix[med_i, med_j] == 1 or ddi_matrix[med_j, med_i] == 1:
                    ddi_cnt += 1
    if all_cnt == 0:
        return 0
    return ddi_cnt / all_cnt


Here is the code content for __init__.py:
from .binary import binary_metrics_fn
from .drug_recommendation import ddi_rate_score
from .multiclass import multiclass_metrics_fn
from .multilabel import multilabel_metrics_fn
from .fairness import fairness_metrics_fn


Here is the code content for binary.py:
from typing import Dict, List, Optional

import numpy as np
import sklearn.metrics as sklearn_metrics

import pyhealth.metrics.calibration as calib


def binary_metrics_fn(
    y_true: np.ndarray,
    y_prob: np.ndarray,
    metrics: Optional[List[str]] = None,
    threshold: float = 0.5,
) -> Dict[str, float]:
    """Computes metrics for binary classification.

    User can specify which metrics to compute by passing a list of metric names.
    The accepted metric names are:
        - pr_auc: area under the precision-recall curve
        - roc_auc: area under the receiver operating characteristic curve
        - accuracy: accuracy score
        - balanced_accuracy: balanced accuracy score (usually used for imbalanced
          datasets)
        - f1: f1 score
        - precision: precision score
        - recall: recall score
        - cohen_kappa: Cohen's kappa score
        - jaccard: Jaccard similarity coefficient score
        - ECE: Expected Calibration Error (with 20 equal-width bins). Check :func:`pyhealth.metrics.calibration.ece_confidence_binary`.
        - ECE_adapt: adaptive ECE (with 20 equal-size bins). Check :func:`pyhealth.metrics.calibration.ece_confidence_binary`.
    If no metrics are specified, pr_auc, roc_auc and f1 are computed by default.

    This function calls sklearn.metrics functions to compute the metrics. For
    more information on the metrics, please refer to the documentation of the
    corresponding sklearn.metrics functions.

    Args:
        y_true: True target values of shape (n_samples,).
        y_prob: Predicted probabilities of shape (n_samples,).
        metrics: List of metrics to compute. Default is ["pr_auc", "roc_auc", "f1"].
        threshold: Threshold for binary classification. Default is 0.5.

    Returns:
        Dictionary of metrics whose keys are the metric names and values are
            the metric values.

    Examples:
        >>> from pyhealth.metrics import binary_metrics_fn
        >>> y_true = np.array([0, 0, 1, 1])
        >>> y_prob = np.array([0.1, 0.4, 0.35, 0.8])
        >>> binary_metrics_fn(y_true, y_prob, metrics=["accuracy"])
        {'accuracy': 0.75}
    """
    if metrics is None:
        metrics = ["pr_auc", "roc_auc", "f1"]

    y_pred = y_prob.copy()
    y_pred[y_pred >= threshold] = 1
    y_pred[y_pred < threshold] = 0

    output = {}
    for metric in metrics:
        if metric == "pr_auc":
            pr_auc = sklearn_metrics.average_precision_score(y_true, y_prob)
            output["pr_auc"] = pr_auc
        elif metric == "roc_auc":
            roc_auc = sklearn_metrics.roc_auc_score(y_true, y_prob)
            output["roc_auc"] = roc_auc
        elif metric == "accuracy":
            accuracy = sklearn_metrics.accuracy_score(y_true, y_pred)
            output["accuracy"] = accuracy
        elif metric == "balanced_accuracy":
            balanced_accuracy = sklearn_metrics.balanced_accuracy_score(y_true, y_pred)
            output["balanced_accuracy"] = balanced_accuracy
        elif metric == "f1":
            f1 = sklearn_metrics.f1_score(y_true, y_pred)
            output["f1"] = f1
        elif metric == "precision":
            precision = sklearn_metrics.precision_score(y_true, y_pred)
            output["precision"] = precision
        elif metric == "recall":
            recall = sklearn_metrics.recall_score(y_true, y_pred)
            output["recall"] = recall
        elif metric == "cohen_kappa":
            cohen_kappa = sklearn_metrics.cohen_kappa_score(y_true, y_pred)
            output["cohen_kappa"] = cohen_kappa
        elif metric == "jaccard":
            jaccard = sklearn_metrics.jaccard_score(y_true, y_pred)
            output["jaccard"] = jaccard
        elif metric in {"ECE", "ECE_adapt"}:
            output[metric] = calib.ece_confidence_binary(
                y_prob, y_true, bins=20, adaptive=metric.endswith("_adapt")
            )
        else:
            raise ValueError(f"Unknown metric for binary classification: {metric}")
    return output


if __name__ == "__main__":
    all_metrics = [
        "pr_auc",
        "roc_auc",
        "accuracy",
        "balanced_accuracy",
        "f1",
        "precision",
        "recall",
        "cohen_kappa",
        "jaccard",
    ]
    y_true = np.random.randint(2, size=100000)
    y_prob = np.random.random(size=100000)
    print(binary_metrics_fn(y_true, y_prob, metrics=all_metrics))


Here is the code content for calibration.py:
"""Metrics that meature model calibration.

Reference Papers:

    [1] Lin, Zhen, Shubhendu Trivedi, and Jimeng Sun.
    "Taking a Step Back with KCal: Multi-Class Kernel-Based Calibration
    for Deep Neural Networks."
    ICLR 2023.

    [2] Nixon, Jeremy, Michael W. Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran.
    "Measuring Calibration in Deep Learning."
    In CVPR workshops, vol. 2, no. 7. 2019.

    [3] Patel, Kanil, William Beluch, Bin Yang, Michael Pfeiffer, and Dan Zhang.
    "Multi-class uncertainty calibration via mutual information maximization-based binning."
    ICLR 2021.

    [4] Guo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger.
    "On calibration of modern neural networks."
    ICML 2017.

    [5] Kull, Meelis, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, and Peter Flach.
    "Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration."
    Advances in neural information processing systems 32 (2019).

    [6] Brier, Glenn W.
    "Verification of forecasts expressed in terms of probability."
    Monthly weather review 78, no. 1 (1950): 1-3.

"""
import bisect

import numpy as np
import pandas as pd


def _get_bins(bins):
    if isinstance(bins, int):
        bins = list(np.arange(bins+1) / bins)
    return bins

def assign_bin(sorted_ser: pd.Series, bins:int, adaptive:bool=False):
    ret = pd.DataFrame(sorted_ser)
    if adaptive:
        assert isinstance(bins, int)
        step = len(sorted_ser) // bins
        nvals = [step for _ in range(bins)]
        for _ in range(len(sorted_ser) % bins):
            nvals[-_-1] += 1
        ret['bin'] = [ith for ith, val in enumerate(nvals) for _ in range(val)]
        nvals = list(np.asarray(nvals).cumsum())
        bins = [ret.iloc[0]['conf']]
        for iloc in nvals:
            bins.append(ret.iloc[iloc-1]['conf'])
            if iloc != nvals[-1]:
                bins[-1] = 0.5 * bins[-1] + 0.5 *ret.iloc[iloc]['conf']
    else:
        bins = _get_bins(bins)
        bin_assign = pd.Series(0, index=sorted_ser.index)
        locs = [bisect.bisect(sorted_ser.values, b) for b in bins]
        locs[0], locs[-1] = 0, len(ret)
        for i, loc in enumerate(locs[:-1]):
            bin_assign.iloc[loc:locs[i+1]] = i
        ret['bin'] = bin_assign
    return ret['bin'], bins

def _ECE_loss(summ):
    w = summ['cnt'] / summ['cnt'].sum()
    loss = np.average((summ['conf'] - summ['acc']).abs(), weights=w)
    return loss

def _ECE_confidence(df, bins=20, adaptive=False):
    # df should have columns: conf, acc
    df = df.sort_values(['conf']).reset_index().drop('index', axis=1)
    df['bin'], _ = assign_bin(df['conf'], bins, adaptive=adaptive)
    summ = pd.DataFrame(df.groupby('bin')[['acc', 'conf']].mean())#.fillna(0.)
    summ['cnt'] = df.groupby('bin').size()
    summ = summ.reset_index()
    return summ, _ECE_loss(summ)

def _ECE_classwise(prob:np.ndarray, label_onehot:np.ndarray, bins=20, threshold=0., adaptive=False):
    summs = []
    class_losses = {}
    for k in range(prob.shape[1]):
        msk = prob[:, k] >= threshold
        if msk.sum() == 0:
            continue
        df = pd.DataFrame({"conf": prob[msk, k], 'acc': label_onehot[msk, k]})
        df = df.sort_values(['conf']).reset_index()
        df['bin'], _ = assign_bin(df['conf'], bins, adaptive=adaptive)
        summ = pd.DataFrame(df.groupby('bin')[['acc', 'conf']].mean())
        summ['cnt'] = df.groupby('bin').size()
        summ['k'] = k
        summs.append(summ.reset_index())
        class_losses[k] = _ECE_loss(summs[-1])
    class_losses = pd.Series(class_losses)
    class_losses['avg'], class_losses['sum'] = class_losses.mean(), class_losses.sum()
    summs = pd.concat(summs, ignore_index=True)
    return summs, class_losses

def ece_confidence_multiclass(prob:np.ndarray, label:np.ndarray, bins=20, adaptive=False):
    """Expected Calibration Error (ECE).

    We group samples into 'bins' basing on the top-class prediction.
    Then, we compute the absolute difference between the average top-class prediction and
    the frequency of top-class being correct (i.e. accuracy) for each bin.
    ECE is the average (weighed by number of points in each bin) of these absolute differences.
    It could be expressed by the following formula, with :math:`B_m` denoting the m-th bin:

    .. math::
        ECE = \\sum_{m=1}^M \\frac{|B_m|}{N} |acc(B_m) - conf(B_m)|

    Example:
        >>> pred = np.asarray([[0.2, 0.2, 0.6], [0.2, 0.31, 0.49], [0.1, 0.1, 0.8]])
        >>> label = np.asarray([2,1,2])
        >>> ECE_confidence_multiclass(pred, label, bins=2)
        0.36333333333333334

    Explanation of the example: The bins are [0, 0.5] and (0.5, 1].
    In the first bin, we have one sample with top-class prediction of 0.49, and its
    accuracy is 0. In the second bin, we have average confidence of 0.7 and average
    accuracy of 1. Thus, the ECE is :math:`\\frac{1}{3} \cdot 0.49 + \\frac{2}{3}\cdot 0.3=0.3633`.

    Args:
        prob (np.ndarray): (N, C)
        label (np.ndarray): (N,)
        bins (int, optional): Number of bins. Defaults to 20.
        adaptive (bool, optional): If False, bins are equal width ([0, 0.05, 0.1, ..., 1])
            If True, bin widths are adaptive such that each bin contains the same number
            of points. Defaults to False.
    """
    df = pd.DataFrame({'acc': label == np.argmax(prob, 1), 'conf': prob.max(1)})
    return _ECE_confidence(df, bins, adaptive)[1]

def ece_confidence_binary(prob:np.ndarray, label:np.ndarray, bins=20, adaptive=False):
    """Expected Calibration Error (ECE) for binary classification.

    Similar to :func:`ece_confidence_multiclass`, but on class 1 instead of the top-prediction.


    Args:
        prob (np.ndarray): (N, C)
        label (np.ndarray): (N,)
        bins (int, optional): Number of bins. Defaults to 20.
        adaptive (bool, optional): If False, bins are equal width ([0, 0.05, 0.1, ..., 1])
            If True, bin widths are adaptive such that each bin contains the same number
            of points. Defaults to False.
    """

    df = pd.DataFrame({'acc': label[:,0], 'conf': prob[:,0]})
    return _ECE_confidence(df, bins, adaptive)[1]

def ece_classwise(prob, label, bins=20, threshold=0., adaptive=False):
    """Classwise Expected Calibration Error (ECE).

    This is equivalent to applying :func:`ece_confidence_binary` to each class and take the average.

    Args:
        prob (np.ndarray): (N, C)
        label (np.ndarray): (N,)
        bins (int, optional): Number of bins. Defaults to 20.
        threshold (float): threshold to filter out samples.
            If the number of classes C is very large, many classes receive close to 0
            prediction. Any prediction below threshold is considered noise and ignored.
            In recent papers, this is typically set to a small number (such as 1/C).
        adaptive (bool, optional): If False, bins are equal width ([0, 0.05, 0.1, ..., 1])
            If True, bin widths are adaptive such that each bin contains the same number
            of points. Defaults to False.
    """

    K = prob.shape[1]
    if len(label.shape) == 1:
        # make it one-hot
        label, _ = np.zeros((len(label),K)), label
        label[np.arange(len(label)), _] = 1
    return _ECE_classwise(prob, label, bins, threshold, adaptive)[1]['avg']

def brier_top1(prob:np.ndarray, label:np.ndarray):
    """Brier score (i.e. mean squared error between prediction and 0-1 label) of the top prediction.
    """
    conf = prob.max(1)
    acc = (label == np.argmax(prob, 1)).astype(int)
    return np.mean(np.square(conf - acc))


Here is the code content for fairness.py:
from typing import Dict, List, Optional

import numpy as np

from pyhealth.metrics.fairness_utils import disparate_impact, statistical_parity_difference

def fairness_metrics_fn(
    y_true: np.ndarray,
    y_prob: np.ndarray,
    sensitive_attributes: np.ndarray,
    favorable_outcome: int = 1,
    metrics: Optional[List[str]] = None,
    threshold: float = 0.5,
) -> Dict[str, float]:
    """Computes metrics for binary classification.

    User can specify which metrics to compute by passing a list of metric names.
    The accepted metric names are:
        - disparate_impact:
        - statistical_parity_difference:

    If no metrics are disparate_impact, and statistical_parity_difference are computed by default.

    Args:
        y_true: True target values of shape (n_samples,).
        y_prob: Predicted probabilities of shape (n_samples,).
        sensitive_attributes: Sensitive attributes of shape (n_samples,) where 1 is the protected group and 0 is the unprotected group.
        favorable_outcome: Label value which is considered favorable (i.e. "positive").
        metrics: List of metrics to compute. Default is ["disparate_impact", "statistical_parity_difference"].
        threshold: Threshold for binary classification. Default is 0.5.

    Returns:
        Dictionary of metrics whose keys are the metric names and values are
            the metric values.
    """
    if metrics is None:
        metrics = ["disparate_impact", "statistical_parity_difference"]

    y_pred = y_prob.copy()
    y_pred[y_pred >= threshold] = 1
    y_pred[y_pred < threshold] = 0

    output = {}
    for metric in metrics:
        if metric == "disparate_impact":
            output[metric] = disparate_impact(sensitive_attributes, y_pred, favorable_outcome)
        elif metric == "statistical_parity_difference":
            output[metric] = statistical_parity_difference(sensitive_attributes, y_pred, favorable_outcome)
        else:
            raise ValueError(f"Unknown metric for fairness: {metric}")
    return output



Here is the code content for multiclass.py:
from typing import Dict, List, Optional

import numpy as np
import pandas as pd
import sklearn.metrics as sklearn_metrics

import pyhealth.metrics.calibration as calib
import pyhealth.metrics.prediction_set as pset


def multiclass_metrics_fn(
    y_true: np.ndarray,
    y_prob: np.ndarray,
    metrics: Optional[List[str]] = None,
    y_predset: Optional[np.ndarray] = None,
) -> Dict[str, float]:
    """Computes metrics for multiclass classification.

    User can specify which metrics to compute by passing a list of metric names.
    The accepted metric names are:
        - roc_auc_macro_ovo: area under the receiver operating characteristic curve,
            macro averaged over one-vs-one multiclass classification
        - roc_auc_macro_ovr: area under the receiver operating characteristic curve,
            macro averaged over one-vs-rest multiclass classification
        - roc_auc_weighted_ovo: area under the receiver operating characteristic curve,
            weighted averaged over one-vs-one multiclass classification
        - roc_auc_weighted_ovr: area under the receiver operating characteristic curve,
            weighted averaged over one-vs-rest multiclass classification
        - accuracy: accuracy score
        - balanced_accuracy: balanced accuracy score (usually used for imbalanced
            datasets)
        - f1_micro: f1 score, micro averaged
        - f1_macro: f1 score, macro averaged
        - f1_weighted: f1 score, weighted averaged
        - jaccard_micro: Jaccard similarity coefficient score, micro averaged
        - jaccard_macro: Jaccard similarity coefficient score, macro averaged
        - jaccard_weighted: Jaccard similarity coefficient score, weighted averaged
        - cohen_kappa: Cohen's kappa score
        - brier_top1: brier score between the top prediction and the true label
        - ECE: Expected Calibration Error (with 20 equal-width bins). Check :func:`pyhealth.metrics.calibration.ece_confidence_multiclass`.
        - ECE_adapt: adaptive ECE (with 20 equal-size bins). Check :func:`pyhealth.metrics.calibration.ece_confidence_multiclass`.
        - cwECEt: classwise ECE with threshold=min(0.01,1/K). Check :func:`pyhealth.metrics.calibration.ece_classwise`.
        - cwECEt_adapt: classwise adaptive ECE with threshold=min(0.01,1/K). Check :func:`pyhealth.metrics.calibration.ece_classwise`.

    The following metrics related to the prediction sets are accepted as well, but will be ignored if y_predset is None:
        - rejection_rate: Frequency of rejection, where rejection happens when the prediction set has cardinality other than 1. Check :func:`pyhealth.metrics.prediction_set.rejection_rate`.
        - set_size: Average size of the prediction sets. Check :func:`pyhealth.metrics.prediction_set.size`.
        - miscoverage_ps:  Prob(k not in prediction set). Check :func:`pyhealth.metrics.prediction_set.miscoverage_ps`.
        - miscoverage_mean_ps: The average (across different classes k) of miscoverage_ps.
        - miscoverage_overall_ps: Prob(Y not in prediction set). Check :func:`pyhealth.metrics.prediction_set.miscoverage_overall_ps`.
        - error_ps: Same as miscoverage_ps, but retricted to un-rejected samples. Check :func:`pyhealth.metrics.prediction_set.error_ps`.
        - error_mean_ps: The average (across different classes k) of error_ps.
        - error_overall_ps: Same as miscoverage_overall_ps, but restricted to un-rejected samples. Check :func:`pyhealth.metrics.prediction_set.error_overall_ps`.

    If no metrics are specified, accuracy, f1_macro, and f1_micro are computed
    by default.

    This function calls sklearn.metrics functions to compute the metrics. For
    more information on the metrics, please refer to the documentation of the
    corresponding sklearn.metrics functions.

    Args:
        y_true: True target values of shape (n_samples,).
        y_prob: Predicted probabilities of shape (n_samples, n_classes).
        metrics: List of metrics to compute. Default is ["accuracy", "f1_macro",
            "f1_micro"].

    Returns:
        Dictionary of metrics whose keys are the metric names and values are
            the metric values.

    Examples:
        >>> from pyhealth.metrics import multiclass_metrics_fn
        >>> y_true = np.array([0, 1, 2, 2])
        >>> y_prob = np.array([[0.9,  0.05, 0.05],
        ...                    [0.05, 0.9,  0.05],
        ...                    [0.05, 0.05, 0.9],
        ...                    [0.6,  0.2,  0.2]])
        >>> multiclass_metrics_fn(y_true, y_prob, metrics=["accuracy"])
        {'accuracy': 0.75}
    """
    if metrics is None:
        metrics = ["accuracy", "f1_macro", "f1_micro"]
    prediction_set_metrics = [
        "rejection_rate",
        "set_size",
        "miscoverage_mean_ps",
        "miscoverage_ps",
        "miscoverage_overall_ps",
        "error_mean_ps",
        "error_ps",
        "error_overall_ps",
    ]
    y_pred = np.argmax(y_prob, axis=-1)

    output = {}
    for metric in metrics:
        if metric == "roc_auc_macro_ovo":
            roc_auc_macro_ovo = sklearn_metrics.roc_auc_score(
                y_true, y_prob, average="macro", multi_class="ovo"
            )
            output["roc_auc_macro_ovo"] = roc_auc_macro_ovo
        elif metric == "roc_auc_macro_ovr":
            roc_auc_macro_ovr = sklearn_metrics.roc_auc_score(
                y_true, y_prob, average="macro", multi_class="ovr"
            )
            output["roc_auc_macro_ovr"] = roc_auc_macro_ovr
        elif metric == "roc_auc_weighted_ovo":
            roc_auc_weighted_ovo = sklearn_metrics.roc_auc_score(
                y_true, y_prob, average="weighted", multi_class="ovo"
            )
            output["roc_auc_weighted_ovo"] = roc_auc_weighted_ovo
        elif metric == "roc_auc_weighted_ovr":
            roc_auc_weighted_ovr = sklearn_metrics.roc_auc_score(
                y_true, y_prob, average="weighted", multi_class="ovr"
            )
            output["roc_auc_weighted_ovr"] = roc_auc_weighted_ovr
        elif metric == "accuracy":
            accuracy = sklearn_metrics.accuracy_score(y_true, y_pred)
            output["accuracy"] = accuracy
        elif metric == "balanced_accuracy":
            balanced_accuracy = sklearn_metrics.balanced_accuracy_score(y_true, y_pred)
            output["balanced_accuracy"] = balanced_accuracy
        elif metric == "f1_micro":
            f1_micro = sklearn_metrics.f1_score(y_true, y_pred, average="micro")
            output["f1_micro"] = f1_micro
        elif metric == "f1_macro":
            f1_macro = sklearn_metrics.f1_score(y_true, y_pred, average="macro")
            output["f1_macro"] = f1_macro
        elif metric == "f1_weighted":
            f1_weighted = sklearn_metrics.f1_score(y_true, y_pred, average="weighted")
            output["f1_weighted"] = f1_weighted
        elif metric == "jaccard_micro":
            jacard_micro = sklearn_metrics.jaccard_score(
                y_true, y_pred, average="micro"
            )
            output["jaccard_micro"] = jacard_micro
        elif metric == "jaccard_macro":
            jacard_macro = sklearn_metrics.jaccard_score(
                y_true, y_pred, average="macro"
            )
            output["jaccard_macro"] = jacard_macro
        elif metric == "jaccard_weighted":
            jacard_weighted = sklearn_metrics.jaccard_score(
                y_true, y_pred, average="weighted"
            )
            output["jaccard_weighted"] = jacard_weighted
        elif metric == "cohen_kappa":
            cohen_kappa = sklearn_metrics.cohen_kappa_score(y_true, y_pred)
            output["cohen_kappa"] = cohen_kappa
        elif metric == "brier_top1":
            output[metric] = calib.brier_top1(y_prob, y_true)
        elif metric in {"ECE", "ECE_adapt"}:
            output[metric] = calib.ece_confidence_multiclass(
                y_prob, y_true, bins=20, adaptive=metric.endswith("_adapt")
            )
        elif metric in {"cwECEt", "cwECEt_adapt"}:
            thres = min(0.01, 1.0 / y_prob.shape[1])
            output[metric] = calib.ece_classwise(
                y_prob,
                y_true,
                bins=20,
                adaptive=metric.endswith("_adapt"),
                threshold=thres,
            )
        elif metric in prediction_set_metrics:
            if y_predset is None:
                continue
            if metric == "rejection_rate":
                output[metric] = pset.rejection_rate(y_predset)
            elif metric == "set_size":
                output[metric] = pset.size(y_predset)
            elif metric == "miscoverage_mean_ps":
                output[metric] = pset.miscoverage_ps(y_predset, y_true).mean()
            elif metric == "miscoverage_ps":
                output[metric] = pset.miscoverage_ps(y_predset, y_true)
            elif metric == "miscoverage_overall_ps":
                output[metric] = pset.miscoverage_overall_ps(y_predset, y_true)
            elif metric == "error_mean_ps":
                output[metric] = pset.error_ps(y_predset, y_true).mean()
            elif metric == "error_ps":
                output[metric] = pset.error_ps(y_predset, y_true)
            elif metric == "error_overall_ps":
                output[metric] = pset.error_overall_ps(y_predset, y_true)
        else:
            raise ValueError(f"Unknown metric for multiclass classification: {metric}")

    return output


if __name__ == "__main__":
    all_metrics = [
        "roc_auc_macro_ovo",
        "roc_auc_macro_ovr",
        "roc_auc_weighted_ovo",
        "roc_auc_weighted_ovr",
        "accuracy",
        "balanced_accuracy",
        "f1_micro",
        "f1_macro",
        "f1_weighted",
        "jaccard_micro",
        "jaccard_macro",
        "jaccard_weighted",
        "cohen_kappa",
    ]
    all_metrics += ["brier_top1", "ECE", "ECE_adapt", "cwECEt", "cwECEt_adapt"]
    y_true = np.random.randint(4, size=100000)
    y_prob = np.random.randn(100000, 4)
    y_prob = np.exp(y_prob) / np.sum(np.exp(y_prob), axis=-1, keepdims=True)
    print(multiclass_metrics_fn(y_true, y_prob, metrics=all_metrics))


Here is the code content for multilabel.py:
from typing import Dict, List, Optional

import numpy as np
import sklearn.metrics as sklearn_metrics

import pyhealth.metrics.calibration as calib

def multilabel_metrics_fn(
    y_true: np.ndarray,
    y_prob: np.ndarray,
    metrics: Optional[List[str]] = None,
    threshold: float = 0.5,
    y_predset: Optional[np.ndarray] = None,
) -> Dict[str, float]:
    """Computes metrics for multilabel classification.

    User can specify which metrics to compute by passing a list of metric names.
    The accepted metric names are:
        - roc_auc_micro: area under the receiver operating characteristic curve,
          micro averaged
        - roc_auc_macro: area under the receiver operating characteristic curve,
          macro averaged
        - roc_auc_weighted: area under the receiver operating characteristic curve,
          weighted averaged
        - roc_auc_samples: area under the receiver operating characteristic curve,
          samples averaged
        - pr_auc_micro: area under the precision recall curve, micro averaged
        - pr_auc_macro: area under the precision recall curve, macro averaged
        - pr_auc_weighted: area under the precision recall curve, weighted averaged
        - pr_auc_samples: area under the precision recall curve, samples averaged
        - accuracy: accuracy score
        - f1_micro: f1 score, micro averaged
        - f1_macro: f1 score, macro averaged
        - f1_weighted: f1 score, weighted averaged
        - f1_samples: f1 score, samples averaged
        - precision_micro: precision score, micro averaged
        - precision_macro: precision score, macro averaged
        - precision_weighted: precision score, weighted averaged
        - precision_samples: precision score, samples averaged
        - recall_micro: recall score, micro averaged
        - recall_macro: recall score, macro averaged
        - recall_weighted: recall score, weighted averaged
        - recall_samples: recall score, samples averaged
        - jaccard_micro: Jaccard similarity coefficient score, micro averaged
        - jaccard_macro: Jaccard similarity coefficient score, macro averaged
        - jaccard_weighted: Jaccard similarity coefficient score, weighted averaged
        - jaccard_samples: Jaccard similarity coefficient score, samples averaged
        - hamming_loss: Hamming loss
        - cwECE: classwise ECE (with 20 equal-width bins). Check :func:`pyhealth.metrics.calibration.ece_classwise`.
        - cwECE_adapt: classwise adaptive ECE (with 20 equal-size bins). Check :func:`pyhealth.metrics.calibration.ece_classwise`.

    The following metrics related to the prediction sets are accepted as well, but will be ignored if y_predset is None:
        - fp: Number of false positives.
        - tp: Number of true positives.


    If no metrics are specified, pr_auc_samples is computed by default.

    This function calls sklearn.metrics functions to compute the metrics. For
    more information on the metrics, please refer to the documentation of the
    corresponding sklearn.metrics functions.

    Args:
        y_true: True target values of shape (n_samples, n_labels).
        y_prob: Predicted probabilities of shape (n_samples, n_labels).
        metrics: List of metrics to compute. Default is ["pr_auc_samples"].
        threshold: Threshold to binarize the predicted probabilities. Default is 0.5.

    Returns:
        Dictionary of metrics whose keys are the metric names and values are
            the metric values.

    Examples:
        >>> from pyhealth.metrics import multilabel_metrics_fn
        >>> y_true = np.array([[0, 1, 1], [1, 0, 1]])
        >>> y_prob = np.array([[0.1, 0.9, 0.8], [0.05, 0.95, 0.6]])
        >>> multilabel_metrics_fn(y_true, y_prob, metrics=["accuracy"])
        {'accuracy': 0.5}
    """
    if metrics is None:
        metrics = ["pr_auc_samples"]
    prediction_set_metrics = ['tp', 'fp']

    y_pred = y_prob.copy()
    y_pred[y_pred >= threshold] = 1
    y_pred[y_pred < threshold] = 0

    output = {}
    for metric in metrics:
        if metric == "roc_auc_micro":
            roc_auc_micro = sklearn_metrics.roc_auc_score(
                y_true, y_prob, average="micro"
            )
            output["roc_auc_micro"] = roc_auc_micro
        elif metric == "roc_auc_macro":
            roc_auc_macro = sklearn_metrics.roc_auc_score(
                y_true, y_prob, average="macro"
            )
            output["roc_auc_macro"] = roc_auc_macro
        elif metric == "roc_auc_weighted":
            roc_auc_weighted = sklearn_metrics.roc_auc_score(
                y_true, y_prob, average="weighted"
            )
            output["roc_auc_weighted"] = roc_auc_weighted
        elif metric == "roc_auc_samples":
            roc_auc_samples = sklearn_metrics.roc_auc_score(
                y_true, y_prob, average="samples"
            )
            output["roc_auc_samples"] = roc_auc_samples
        elif metric == "pr_auc_micro":
            pr_auc_micro = sklearn_metrics.average_precision_score(
                y_true, y_prob, average="micro"
            )
            output["pr_auc_micro"] = pr_auc_micro
        elif metric == "pr_auc_macro":
            pr_auc_macro = sklearn_metrics.average_precision_score(
                y_true, y_prob, average="macro"
            )
            output["pr_auc_macro"] = pr_auc_macro
        elif metric == "pr_auc_weighted":
            pr_auc_weighted = sklearn_metrics.average_precision_score(
                y_true, y_prob, average="weighted"
            )
            output["pr_auc_weighted"] = pr_auc_weighted
        elif metric == "pr_auc_samples":
            pr_auc_samples = sklearn_metrics.average_precision_score(
                y_true, y_prob, average="samples"
            )
            output["pr_auc_samples"] = pr_auc_samples
        elif metric == "accuracy":
            accuracy = sklearn_metrics.accuracy_score(y_true.flatten(), y_pred.flatten())
            output["accuracy"] = accuracy
        elif metric == "f1_micro":
            f1_micro = sklearn_metrics.f1_score(y_true, y_pred, average="micro")
            output["f1_micro"] = f1_micro
        elif metric == "f1_macro":
            f1_macro = sklearn_metrics.f1_score(y_true, y_pred, average="macro")
            output["f1_macro"] = f1_macro
        elif metric == "f1_weighted":
            f1_weighted = sklearn_metrics.f1_score(y_true, y_pred, average="weighted")
            output["f1_weighted"] = f1_weighted
        elif metric == "f1_samples":
            f1_samples = sklearn_metrics.f1_score(y_true, y_pred, average="samples")
            output["f1_samples"] = f1_samples
        elif metric == "precision_micro":
            precision_micro = sklearn_metrics.precision_score(
                y_true, y_pred, average="micro"
            )
            output["precision_micro"] = precision_micro
        elif metric == "precision_macro":
            precision_macro = sklearn_metrics.precision_score(
                y_true, y_pred, average="macro"
            )
            output["precision_macro"] = precision_macro
        elif metric == "precision_weighted":
            precision_weighted = sklearn_metrics.precision_score(
                y_true, y_pred, average="weighted"
            )
            output["precision_weighted"] = precision_weighted
        elif metric == "precision_samples":
            precision_samples = sklearn_metrics.precision_score(
                y_true, y_pred, average="samples"
            )
            output["precision_samples"] = precision_samples
        elif metric == "recall_micro":
            recall_micro = sklearn_metrics.recall_score(y_true, y_pred, average="micro")
            output["recall_micro"] = recall_micro
        elif metric == "recall_macro":
            recall_macro = sklearn_metrics.recall_score(y_true, y_pred, average="macro")
            output["recall_macro"] = recall_macro
        elif metric == "recall_weighted":
            recall_weighted = sklearn_metrics.recall_score(
                y_true, y_pred, average="weighted"
            )
            output["recall_weighted"] = recall_weighted
        elif metric == "recall_samples":
            recall_samples = sklearn_metrics.recall_score(
                y_true, y_pred, average="samples"
            )
            output["recall_samples"] = recall_samples
        elif metric == "jaccard_micro":
            jaccard_micro = sklearn_metrics.jaccard_score(
                y_true, y_pred, average="micro"
            )
            output["jaccard_micro"] = jaccard_micro
        elif metric == "jaccard_macro":
            jaccard_macro = sklearn_metrics.jaccard_score(
                y_true, y_pred, average="macro"
            )
            output["jaccard_macro"] = jaccard_macro
        elif metric == "jaccard_weighted":
            jaccard_weighted = sklearn_metrics.jaccard_score(
                y_true, y_pred, average="weighted"
            )
            output["jaccard_weighted"] = jaccard_weighted
        elif metric == "jaccard_samples":
            jaccard_samples = sklearn_metrics.jaccard_score(
                y_true, y_pred, average="samples"
            )
            output["jaccard_samples"] = jaccard_samples
        elif metric == "hamming_loss":
            hamming_loss = sklearn_metrics.hamming_loss(y_true, y_pred)
            output["hamming_loss"] = hamming_loss
        elif metric in {"cwECE", "cwECE_adapt"}:
            output[metric] = calib.ece_classwise(
                y_prob,
                y_true,
                bins=20,
                adaptive=metric.endswith("_adapt"),
                threshold=0.0,
            )
        elif metric in prediction_set_metrics:
            if y_predset is None:
                continue
            if metric == 'tp':
                output[metric] = (y_true * y_predset).sum(1).mean()
            elif metric == 'fp':
                output[metric] = ((1-y_true) * y_predset).sum(1).mean()
        else:
            raise ValueError(f"Unknown metric for multilabel classification: {metric}")

    return output


if __name__ == "__main__":
    all_metrics = [
        "roc_auc_micro",
        "roc_auc_macro",
        "roc_auc_weighted",
        "roc_auc_samples",
        "pr_auc_micro",
        "pr_auc_macro",
        "pr_auc_weighted",
        "pr_auc_samples",
        "accuracy",
        "f1_micro",
        "f1_macro",
        "f1_weighted",
        "f1_samples",
        "precision_micro",
        "precision_macro",
        "precision_weighted",
        "precision_samples",
        "recall_micro",
        "recall_macro",
        "recall_weighted",
        "recall_samples",
        "jaccard_micro",
        "jaccard_macro",
        "jaccard_weighted",
        "jaccard_samples",
        "hamming_loss",
    ]
    y_true = np.random.randint(2, size=(100000, 100))
    y_prob = np.random.random(size=(100000, 100))
    print(multilabel_metrics_fn(y_true, y_prob, metrics=all_metrics))


Here is the code content for prediction_set.py:
import numpy as np


def size(y_pred:np.ndarray):
    """Average size of the prediction set.
    """
    return np.mean(y_pred.sum(1))

def rejection_rate(y_pred:np.ndarray):
    """Rejection rate, defined as the proportion of samples with prediction set size != 1
    """
    return np.mean(y_pred.sum(1) != 1)

def _missrate(y_pred:np.ndarray, y_true:np.ndarray, ignore_rejected=False):
    """Computes the class-wise mis-coverage rate (or risk).

    Args:
        y_pred (np.ndarray): prediction scores.
        y_true (np.ndarray): true labels.
        ignore_rejected (bool, optional): If True, we compute the miscoverage rate
            without rejection  (that is, condition on the unrejected samples). Defaults to False.

    Returns:
        np.ndarray: miss-coverage rates for each class.
    """
    # currently handles multilabel and multiclass
    K = y_pred.shape[1]
    if len(y_true.shape) == 1:
        y_true, _ = np.zeros((len(y_true),K), dtype=bool), y_true
        y_true[np.arange(len(y_true)), _] = 1
    y_true = y_true.astype(bool)

    keep_msk = (y_pred.sum(1) == 1) if ignore_rejected else np.ones(len(y_true), dtype=bool)
    missed = []
    for k in range(K):
        missed.append(1-np.mean(y_pred[keep_msk & y_true[:, k], k]))

    return np.asarray(missed)



def miscoverage_ps(y_pred:np.ndarray, y_true:np.ndarray):
    """Miscoverage rates for all samples (similar to recall).

    Example:
        >>> y_pred = np.asarray([[1,0,0],[1,0,0],[1,1,0],[0, 1, 0]])
        >>> y_true = np.asarray([1,0,1,2])
        >>> error_ps(y_pred, y_true)
        array([0. , 0.5, 1. ])


    Explanation:
    For class 0, the 1-th prediction set ({0}) contains the label, so the miss-coverage is 0/1=0.
    For class 1, the 0-th prediction set ({0}) does not contain the label, the 2-th prediction
    set ({0,1}) contains the label. Thus, the miss-coverage is 1/2=0.5.
    For class 2, the last prediction set is {1} and the label is 2, so the miss-coverage is 1/1=1.
    """
    return _missrate(y_pred, y_true, False)

def error_ps(y_pred:np.ndarray, y_true:np.ndarray):
    """Miscoverage rates for unrejected samples, where rejection is defined to be sets with size !=1).

    Example:
        >>> y_pred = np.asarray([[1,0,0],[1,0,0],[1,1,0],[0, 1, 0]])
        >>> y_true = np.asarray([1,0,1,2])
        >>> error_ps(y_pred, y_true)
        array([0., 1., 1.])

    Explanation:
    For class 0, the 1-th sample is correct and not rejected, so the error is 0/1=0.
    For class 1, the 0-th sample is incorrerct and not rejected, the 2-th is rejected.
    Thus, the error is 1/1=1.
    For class 2, the last sample is not-rejected but the prediction set is {1}, so the error
    is 1/1=1.
    """
    return _missrate(y_pred, y_true, True)

def miscoverage_overall_ps(y_pred:np.ndarray, y_true:np.ndarray):
    """Miscoverage rate for the true label. Only for multiclass.

    Example:
        >>> y_pred = np.asarray([[1,0,0],[1,0,0],[1,1,0]])
        >>> y_true = np.asarray([1,0,1])
        >>> miscoverage_overall_ps(y_pred, y_true)
        0.333333

    Explanation:
    The 0-th prediction set is {0} and the label is 1 (not covered).
    The 1-th prediction set is {0} and the label is 0 (covered).
    The 2-th prediction set is {0,1} and the label is 1 (covered).
    Thus the miscoverage rate is 1/3.
    """
    assert len(y_true.shape) == 1
    truth_pred = y_pred[np.arange(len(y_true)), y_true]

    return 1 - np.mean(truth_pred)

def error_overall_ps(y_pred:np.ndarray, y_true:np.ndarray):
    """Overall error rate for the un-rejected samples.

    Example:
        >>> y_pred = np.asarray([[1,0,0],[1,0,0],[1,1,0]])
        >>> y_true = np.asarray([1,0,1])
        >>> error_overall_ps(y_pred, y_true)
        0.5

    Explanation:
    The 0-th prediction set is {0} and the label is 1, so it is an error (no rejection
    as its prediction set has only one class).
    The 1-th sample is not rejected and incurs on error.
    The 2-th sample is rejected, thus excluded from the computation.
    """
    assert len(y_true.shape) == 1
    truth_pred = y_pred[np.arange(len(y_true)), y_true]
    truth_pred = truth_pred[y_pred.sum(1) == 1]
    return 1 - np.mean(truth_pred)


Here is the code content for __init__.py:
from .group import disparate_impact, statistical_parity_difference
from .utils import sensitive_attributes_from_patient_ids

Here is the code content for group.py:
import numpy as np

"""
Notation:
    - Protected group: P
    - Unprotected group: U
"""

def disparate_impact(sensitive_attributes: np.ndarray, y_pred: np.ndarray, favorable_outcome: int = 1, allow_zero_division = False, epsilon: float = 1e-8) -> float:
    """
    Computes the disparate impact between the the protected and unprotected group.

    disparate_impact = P(y_pred = favorable_outcome | P) / P(y_pred = favorable_outcome | U)
    
    Args:
        sensitive_attributes: Sensitive attributes of shape (n_samples,) where 1 is the protected group and 0 is the unprotected group.
        y_pred: Predicted target values of shape (n_samples,).
        favorable_outcome: Label value which is considered favorable (i.e. "positive").
        allow_zero_division: If True, use epsilon instead of 0 in the denominator if the denominator is 0. Otherwise, raise a ValueError.
    
    Returns:
        The disparate impact between the protected and unprotected group.
    """
    
    p_fav_unpr = np.sum(y_pred[sensitive_attributes == 0] == favorable_outcome) / len(y_pred[sensitive_attributes == 0])
    p_fav_prot = np.sum(y_pred[sensitive_attributes == 1] == favorable_outcome) / len(y_pred[sensitive_attributes == 1])

    if p_fav_unpr == 0:
        if allow_zero_division:
            p_fav_unpr = epsilon
        else:
            raise ValueError("Unprotected group has no instances with a favorable outcome. Disparate impact is undefined.")

    disparate_impact_value = p_fav_prot / p_fav_unpr

    return disparate_impact_value

def statistical_parity_difference(sensitive_attributes: np.ndarray, y_pred: np.ndarray, favorable_outcome: int = 1) -> float:
    """
    Computes the statistical parity difference between the the protected and unprotected group.

    statistical_parity_difference = P(y_pred = favorable_outcome | P) - P(y_pred = favorable_outcome | U)
    Args:
        sensitive_attributes: Sensitive attributes of shape (n_samples,) where 1 is the protected group and 0 is the unprotected group.
        y_pred: Predicted target values of shape (n_samples,).
        favorable_outcome: Label value which is considered favorable (i.e. "positive").
    Returns:
        The statistical parity difference between the protected and unprotected group.
    """

    p_fav_unpr = np.sum(y_pred[sensitive_attributes == 0] == favorable_outcome) / len(y_pred[sensitive_attributes == 0])
    p_fav_prot = np.sum(y_pred[sensitive_attributes == 1] == favorable_outcome) / len(y_pred[sensitive_attributes == 1])
    
    statistical_parity_difference_value = p_fav_prot - p_fav_unpr

    return statistical_parity_difference_value



    

Here is the code content for utils.py:

from typing import List
import numpy as np

from pyhealth.datasets import BaseEHRDataset

def sensitive_attributes_from_patient_ids(dataset: BaseEHRDataset,
                                          patient_ids: List[str],
                                          sensitive_attribute: str,
                                          protected_group: str) -> np.ndarray:
    """
    Returns the desired sensitive attribute array from patient_ids.

    Args:
        dataset: Dataset object.
        patient_ids: List of patient IDs.
        sensitive_attribute: Sensitive attribute to extract.
        protected_group: Value of the protected group.
    
    Returns:
        Sensitive attribute array of shape (n_samples,).
    """

    sensitive_attribute_array = np.zeros(len(patient_ids))
    for idx, patient_id in enumerate(patient_ids):
        sensitive_attribute_value = getattr(dataset.patients[patient_id], sensitive_attribute)
        if sensitive_attribute_value == protected_group:
            sensitive_attribute_array[idx] = 1
    return sensitive_attribute_array

    

Here is the code content for tcn.py:
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.utils.rnn as rnn_utils
from torch.nn.utils import weight_norm

from pyhealth.datasets import SampleEHRDataset
from pyhealth.models import BaseModel
from pyhealth.models.utils import get_last_visit

# VALID_OPERATION_LEVEL = ["visit", "event"]


# From TCN original paper https://github.com/locuslab/TCN
class Chomp1d(nn.Module):
    def __init__(self, chomp_size):
        super(Chomp1d, self).__init__()
        self.chomp_size = chomp_size

    def forward(self, x):
        return x[:, :, : -self.chomp_size].contiguous()


class TemporalBlock(nn.Module):
    def __init__(
        self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2
    ):
        super(TemporalBlock, self).__init__()
        self.conv1 = weight_norm(
            nn.Conv1d(
                n_inputs,
                n_outputs,
                kernel_size,
                stride=stride,
                padding=padding,
                dilation=dilation,
            ),
            dim=None,
        )
        self.chomp1 = Chomp1d(padding)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        self.conv2 = weight_norm(
            nn.Conv1d(
                n_outputs,
                n_outputs,
                kernel_size,
                stride=stride,
                padding=padding,
                dilation=dilation,
            ),
            dim=None,
        )
        self.chomp2 = Chomp1d(padding)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        self.net = nn.Sequential(
            self.conv1,
            self.chomp1,
            self.relu1,
            self.dropout1,
            self.conv2,
            self.chomp2,
            self.relu2,
            self.dropout2,
        )
        self.downsample = (
            nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        )
        self.relu = nn.ReLU()
        self.init_weights()

    def init_weights(self):
        self.conv1.weight.data.normal_(0, 0.01)
        self.conv2.weight.data.normal_(0, 0.01)
        if self.downsample is not None:
            self.downsample.weight.data.normal_(0, 0.01)

    def forward(self, x):
        out = self.net(x)
        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)


class TCNLayer(nn.Module):
    """Temporal Convolutional Networks layer.

    Shaojie Bai et al. An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.

    This layer wraps the PyTorch TCN layer with masking and dropout support. It is
    used in the TCN model. But it can also be used as a standalone layer.

    Args:
        input_dim: input feature size.
        num_channels: int or list of ints. If int, the depth will be automatically decided by the max_seq_length. If list, number of channels in each layer.
        max_seq_length: max sequence length. Used to compute the depth of the TCN.
        kernel_size: kernel size of the TCN.
        dropout: dropout rate. If non-zero, introduces a Dropout layer before each
            TCN blocks. Default is 0.5.

    Examples:
        >>> from pyhealth.models import TCNLayer
        >>> input = torch.randn(3, 128, 5)  # [batch size, sequence len, input_size]
        >>> layer = TCNLayer(5, 64)
        >>> outputs, last_outputs = layer(input)
        >>> outputs.shape
        torch.Size([3, 128, 64])
        >>> last_outputs.shape
        torch.Size([3, 64])
    """

    def __init__(
        self,
        input_dim: int,
        num_channels: int = 128,
        max_seq_length: int = 20,
        kernel_size: int = 2,
        dropout: float = 0.5,
    ):
        super(TCNLayer, self).__init__()
        self.num_channels = num_channels

        layers = []

        # We compute automatically the depth based on the desired seq_length.
        if isinstance(num_channels, int) and max_seq_length:
            num_channels = [num_channels] * int(
                np.ceil(np.log(max_seq_length / 2) / np.log(kernel_size))
            )
        elif isinstance(num_channels, int) and not max_seq_length:
            raise Exception(
                "a maximum sequence length needs to be provided if num_channels is int"
            )
        else:
            pass

        num_levels = len(num_channels)
        for i in range(num_levels):
            dilation_size = 2**i
            in_channels = input_dim if i == 0 else num_channels[i - 1]
            out_channels = num_channels[i]
            layers += [
                TemporalBlock(
                    in_channels,
                    out_channels,
                    kernel_size,
                    stride=1,
                    dilation=dilation_size,
                    padding=(kernel_size - 1) * dilation_size,
                    dropout=dropout,
                )
            ]

        self.network = nn.Sequential(*layers)

    def forward(
        self,
        x: torch.tensor,
        mask: Optional[torch.tensor] = None,
    ) -> Tuple[torch.tensor, torch.tensor]:
        """Forward propagation.

        Args:
            x: a tensor of shape [batch size, sequence len, input size].
            mask: an optional tensor of shape [batch size, sequence len], where
                1 indicates valid and 0 indicates invalid.

        Returns:
            last_out: a tensor of shape [batch size, hidden size], containing
                the output features for the last time step.
            out: a tensor of shape [batch size, sequence len, hidden size],
                containing the output features for each time step.
        """
        out = self.network(x.permute(0, 2, 1)).permute(0, 2, 1)
        last_out = get_last_visit(out, mask)
        return last_out, out


class TCN(BaseModel):
    """Temporal Convolutional Networks model.

    This model applies a separate TCN layer for each feature, and then concatenates
    the final hidden states of each TCN layer. The concatenated hidden states are
    then fed into a fully connected layer to make predictions.

    Note:
        We use separate TCN layers for different feature_keys.
        Currently, we automatically support different input formats:
            - code based input (need to use the embedding table later)
            - float/int based value input
        We follow the current convention for the TCN model:
            - case 1. [code1, code2, code3, ...]
                - we will assume the code follows the order; our model will encode
                each code into a vector and apply TCN on the code level
            - case 2. [[code1, code2]] or [[code1, code2], [code3, code4, code5], ...]
                - we will assume the inner bracket follows the order; our model first
                use the embedding table to encode each code into a vector and then use
                average/mean pooling to get one vector for one inner bracket; then use
                TCN one the braket level
            - case 3. [[1.5, 2.0, 0.0]] or [[1.5, 2.0, 0.0], [8, 1.2, 4.5], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run TCN directly
                on the inner bracket level, similar to case 1 after embedding table
            - case 4. [[[1.5, 2.0, 0.0]]] or [[[1.5, 2.0, 0.0], [8, 1.2, 4.5]], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run TCN directly
                on the inner bracket level, similar to case 2 after embedding table

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        embedding_dim: the embedding dimension. Default is 128.
        num_channels: the number of channels in the TCN layer. Default is 128.
        **kwargs: other parameters for the TCN layer.

    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
        ...             "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        ...             "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
        ...             "list_list_vectors": [
        ...                 [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
        ...                 [[7.7, 8.5, 9.4]],
        ...             ],
        ...             "label": 1,
        ...         },
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-1",
        ...             "list_codes": [
        ...                 "55154191800",
        ...                 "551541928",
        ...                 "55154192800",
        ...                 "705182798",
        ...                 "70518279800",
        ...             ],
        ...             "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
        ...             "list_list_codes": [["A04A", "B035", "C129"]],
        ...             "list_list_vectors": [
        ...                 [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
        ...             ],
        ...             "label": 0,
        ...         },
        ...     ]
        >>> dataset = SampleEHRDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import TCN
        >>> model = TCN(
        ...         dataset=dataset,
        ...         feature_keys=[
        ...             "list_codes",
        ...             "list_vectors",
        ...             "list_list_codes",
        ...             "list_list_vectors",
        ...         ],
        ...         label_key="label",
        ...         mode="binary",
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(1.1641, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),
            'y_prob': tensor([[0.6837],
                            [0.3081]], grad_fn=<SigmoidBackward0>),
            'y_true': tensor([[0.],
                            [1.]]),
            'logit': tensor([[ 0.7706],
                            [-0.8091]], grad_fn=<AddmmBackward0>)
        }
        >>>


    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        embedding_dim: int = 128,
        num_channels: int = 128,
        **kwargs
    ):
        super(TCN, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim
        self.num_channels = num_channels

        if "input_dim" in kwargs:
            raise ValueError("input_dim is determined by embedding_dim")

        # the key of self.feat_tokenizers only contains the code based inputs
        self.feat_tokenizers = {}
        self.label_tokenizer = self.get_label_tokenizer()
        # the key of self.embeddings only contains the code based inputs
        self.embeddings = nn.ModuleDict()
        # the key of self.linear_layers only contains the float/int based inputs
        self.linear_layers = nn.ModuleDict()

        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            # sanity check
            if input_info["type"] not in [str, float, int]:
                raise ValueError(
                    "TCN only supports str code, float and int as input types"
                )
            elif (input_info["type"] == str) and (input_info["dim"] not in [2, 3]):
                raise ValueError(
                    "TCN only supports 2-dim or 3-dim str code as input types"
                )
            elif (input_info["type"] in [float, int]) and (
                input_info["dim"] not in [2, 3]
            ):
                raise ValueError(
                    "TCN only supports 2-dim or 3-dim float and int as input types"
                )
            else:
                pass
            # for code based input, we need Type
            # for float/int based input, we need Type, input_dim
            self.add_feature_transform_layer(feature_key, input_info)

        self.tcn = nn.ModuleDict()
        for feature_key in feature_keys:
            self.tcn[feature_key] = TCNLayer(
                input_dim=embedding_dim, num_channels=num_channels, **kwargs
            )
        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(len(self.feature_keys) * self.num_channels, output_size)

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        The label `kwargs[self.label_key]` is a list of labels for each patient.

        Args:
            **kwargs: keyword arguments for the model. The keys must contain
                all the feature keys and the label key.

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the loss.
                y_prob: a tensor representing the predicted probabilities.
                y_true: a tensor representing the true labels.
        """
        patient_emb = []
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            dim_, type_ = input_info["dim"], input_info["type"]

            # for case 1: [code1, code2, code3, ...]
            if (dim_ == 2) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_2d(
                    kwargs[feature_key]
                )
                # (patient, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, event)
                mask = torch.any(x !=0, dim=2)

            # for case 2: [[code1, code2], [code3, ...], ...]
            elif (dim_ == 3) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_3d(
                    kwargs[feature_key]
                )
                # (patient, visit, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, visit, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                # (patient, visit)
                mask = torch.any(x !=0, dim=2)

            # for case 3: [[1.5, 2.0, 0.0], ...]
            elif (dim_ == 2) and (type_ in [float, int]):
                x, mask = self.padding2d(kwargs[feature_key])
                # (patient, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, event, embedding_dim)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask.bool().to(self.device)

            # for case 4: [[[1.5, 2.0, 0.0], [1.8, 2.4, 6.0]], ...]
            elif (dim_ == 3) and (type_ in [float, int]):
                x, mask = self.padding3d(kwargs[feature_key])
                # (patient, visit, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask[:, :, 0]
                mask = mask.bool().to(self.device)

            else:
                raise NotImplementedError

            x, _ = self.tcn[feature_key](x, mask)
            patient_emb.append(x)

        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)
        y_prob = self.prepare_y_prob(logits)
        results = {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": y_true,
            "logit": logits,
        }
        if kwargs.get("embed", False):
            results["embed"] = patient_emb
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleEHRDataset

    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            # "single_vector": [1, 2, 3],
            "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
            "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
            "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
            "list_list_vectors": [
                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
                [[7.7, 8.5, 9.4]],
            ],
            "label": 1,
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-1",
            # "single_vector": [1, 5, 8],
            "list_codes": [
                "55154191800",
                "551541928",
                "55154192800",
                "705182798",
                "70518279800",
            ],
            "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
            "list_list_codes": [["A04A", "B035", "C129"]],
            "list_list_vectors": [
                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
            ],
            "label": 0,
        },
    ]

    # dataset
    dataset = SampleEHRDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = TCN(
        dataset=dataset,
        feature_keys=[
            "list_codes",
            "list_vectors",
            "list_list_codes",
            "list_list_vectors",
        ],
        label_key="label",
        mode="binary",
    )

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for utils.py:
from typing import List

import torch


def batch_to_multihot(label: List[List[int]], num_labels: int) -> torch.tensor:
    """Converts label to multihot format.

    Args:
        label: [batch size, *]
        num_labels: total number of labels

    Returns:
        multihot: [batch size, num_labels]
    """
    multihot = torch.zeros((len(label), num_labels))
    for i, l in enumerate(label):
        multihot[i, l] = 1
    return multihot


def get_last_visit(hidden_states, mask):
    """Gets the last visit from the sequence model.

    Args:
        hidden_states: [batch size, seq len, hidden_size]
        mask: [batch size, seq len]

    Returns:
        last_visit: [batch size, hidden_size]
    """
    if mask is None:
        return hidden_states[:, -1, :]
    else:
        mask = mask.long()
        last_visit = torch.sum(mask, 1) - 1
        last_visit = last_visit.unsqueeze(-1)
        last_visit = last_visit.expand(-1, hidden_states.shape[1] * hidden_states.shape[2])
        last_visit = torch.reshape(last_visit, hidden_states.shape)
        last_hidden_states = torch.gather(hidden_states, 1, last_visit)
        last_hidden_state = last_hidden_states[:, 0, :]
        return last_hidden_state


Here is the code content for adacare.py:
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.utils.rnn as rnn_utils

from pyhealth.datasets import SampleEHRDataset
from pyhealth.models import BaseModel
from pyhealth.models.utils import get_last_visit


class Sparsemax(nn.Module):
    """Sparsemax function."""

    def __init__(self, dim=None):
        super(Sparsemax, self).__init__()

        self.dim = -1 if dim is None else dim

    def forward(self, input):
        original_size = input.size()
        input = input.view(-1, input.size(self.dim))

        dim = 1
        number_of_logits = input.size(dim)

        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)

        zs = torch.sort(input=input, dim=dim, descending=True)[0]
        range = torch.arange(
            start=1, end=number_of_logits + 1, dtype=torch.float32
        ).view(1, -1)
        range = range.expand_as(zs)

        bound = 1 + range * zs
        cumulative_sum_zs = torch.cumsum(zs, dim)
        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())
        k = torch.max(is_gt * range, dim, keepdim=True)[0]

        zs_sparse = is_gt * zs
        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k
        taus = taus.expand_as(input)

        self.output = torch.max(torch.zeros_like(input), input - taus)

        output = self.output.view(original_size)

        return output

    def backward(self, grad_output):
        dim = 1

        nonzeros = torch.ne(self.output, 0)
        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)
        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))

        return self.grad_input


class CausalConv1d(torch.nn.Conv1d):
    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size,
        stride=1,
        dilation=1,
        groups=1,
        bias=True,
    ):
        self.__padding = (kernel_size - 1) * dilation

        super(CausalConv1d, self).__init__(
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=self.__padding,
            dilation=dilation,
            groups=groups,
            bias=bias,
        )

    def forward(self, input):
        result = super(CausalConv1d, self).forward(input)
        if self.__padding != 0:
            return result[:, :, : -self.__padding]
        return result


class Recalibration(nn.Module):
    def __init__(
        self, channel, reduction=9, use_h=True, use_c=True, activation="sigmoid"
    ):
        super(Recalibration, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.use_h = use_h
        self.use_c = use_c
        scale_dim = 0
        self.activation = activation

        self.nn_c = nn.Linear(channel, channel // reduction)
        scale_dim += channel // reduction

        self.nn_rescale = nn.Linear(scale_dim, channel)
        self.sparsemax = Sparsemax(dim=1)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        b, c, t = x.size()

        y_origin = x.permute(0, 2, 1).reshape(b * t, c).contiguous()
        se_c = self.nn_c(y_origin)
        se_c = torch.relu(se_c)
        y = se_c

        y = self.nn_rescale(y).view(b, t, c).permute(0, 2, 1).contiguous()
        if self.activation == "sigmoid":
            y = torch.sigmoid(y)
        elif self.activation == "sparsemax":
            y = self.sparsemax(y)
        else:
            y = self.softmax(y)
        return x * y.expand_as(x), y.permute(0, 2, 1)


class AdaCareLayer(nn.Module):
    """AdaCare layer.

    Paper: Liantao Ma et al. Adacare: Explainable clinical health status representation learning
    via scale-adaptive feature extraction and recalibration. AAAI 2020.

    This layer is used in the AdaCare model. But it can also be used as a
    standalone layer.

    Args:
        input_dim: the input feature size.
        hidden_dim: the hidden dimension of the GRU layer. Default is 128.
        kernel_size: the kernel size of the causal convolution layer. Default is 2.
        kernel_num: the kernel number of the causal convolution layer. Default is 64.
        r_v: the number of the reduction rate for the original feature calibration. Default is 4.
        r_c: the number of the reduction rate for the convolutional feature recalibration. Default is 4.
        activation: the activation function for the recalibration layer (sigmoid, sparsemax, softmax). Default is "sigmoid".
        dropout: dropout rate. Default is 0.5.

    Examples:
        >>> from pyhealth.models import AdaCareLayer
        >>> input = torch.randn(3, 128, 64)  # [batch size, sequence len, feature_size]
        >>> layer = AdaCareLayer(64)
        >>> c, _, inputatt, convatt = layer(input)
        >>> c.shape
        torch.Size([3, 64])
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = 128,
        kernel_size: int = 2,
        kernel_num: int = 64,
        r_v: int = 4,
        r_c: int = 4,
        activation: str = "sigmoid",
        rnn_type: str = "gru",
        dropout: float = 0.5,
    ):
        super(AdaCareLayer, self).__init__()

        if activation not in ["sigmoid", "softmax", "sparsemax"]:
            raise ValueError(
                "Only sigmoid, softmax and sparsemax are supported for activation."
            )
        if rnn_type not in ["gru", "lstm"]:
            raise ValueError("Only gru and lstm are supported for rnn_type.")

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.kernel_size = kernel_size
        self.kernel_num = kernel_num
        self.r_v = r_v
        self.r_c = r_c
        self.dropout = dropout

        self.nn_conv1 = CausalConv1d(input_dim, kernel_num, kernel_size, 1, 1)
        self.nn_conv3 = CausalConv1d(input_dim, kernel_num, kernel_size, 1, 3)
        self.nn_conv5 = CausalConv1d(input_dim, kernel_num, kernel_size, 1, 5)
        torch.nn.init.xavier_uniform_(self.nn_conv1.weight)
        torch.nn.init.xavier_uniform_(self.nn_conv3.weight)
        torch.nn.init.xavier_uniform_(self.nn_conv5.weight)

        self.nn_convse = Recalibration(
            3 * kernel_num, r_c, use_h=False, use_c=True, activation="sigmoid"
        )
        self.nn_inputse = Recalibration(
            input_dim, r_v, use_h=False, use_c=True, activation=activation
        )

        if rnn_type == "gru":
            self.rnn = nn.GRU(input_dim + 3 * kernel_num, hidden_dim)
        else:
            self.rnn = nn.LSTM(input_dim + 3 * kernel_num, hidden_dim)
        # self.nn_output = nn.Linear(hidden_dim, output_dim)
        self.nn_dropout = nn.Dropout(dropout)

        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        self.tanh = nn.Tanh()

    def forward(
        self,
        x: torch.tensor,
        mask: Optional[torch.tensor] = None,
    ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:
        """Forward propagation.

        Args:
            x: a tensor of shape [batch size, sequence len, input_dim].
            mask: an optional tensor of shape [batch size, sequence len], where
                1 indicates valid and 0 indicates invalid.

        Returns:
            last_output: a tensor of shape [batch size, input_dim] representing the
                patient embedding.
            output: a tensor of shape [batch size, sequence_len, input_dim] representing the patient embedding at each time step.
            inputatt: a tensor of shape [batch size, sequence_len, input_dim] representing the feature importance of the input.
            convatt: a tensor of shape [batch size, sequence_len, 3 * kernel_num] representing the feature importance of the convolutional features.
        """
        # batch_size = x.size(0)
        # time_step = x.size(1)
        # feature_dim = x.size(2)

        conv_input = x.permute(0, 2, 1)
        conv_res1 = self.nn_conv1(conv_input)
        conv_res3 = self.nn_conv3(conv_input)
        conv_res5 = self.nn_conv5(conv_input)

        conv_res = torch.cat((conv_res1, conv_res3, conv_res5), dim=1)
        conv_res = self.relu(conv_res)

        convse_res, convatt = self.nn_convse(conv_res)
        inputse_res, inputatt = self.nn_inputse(x.permute(0, 2, 1))
        concat_input = torch.cat((convse_res, inputse_res), dim=1).permute(0, 2, 1)
        output, _ = self.rnn(concat_input)
        last_output = get_last_visit(output, mask)
        if self.dropout > 0.0:
            last_output = self.nn_dropout(last_output)
        return last_output, output, inputatt, convatt


class AdaCare(BaseModel):
    """AdaCare model.

    Paper: Liantao Ma et al. Adacare: Explainable clinical health status representation learning
    via scale-adaptive feature extraction and recalibration. AAAI 2020.

    Note:
        We use separate AdaCare layers for different feature_keys.
        Currently, we automatically support different input formats:
            - code based input (need to use the embedding table later)
            - float/int based value input
        Since the AdaCare model calibrate the original features to provide interpretability, we do not recommend use embeddings for the input features.
        We follow the current convention for the AdaCare model:
            - case 1. [code1, code2, code3, ...]
                - we will assume the code follows the order; our model will encode
                each code into a vector and apply AdaCare on the code level
            - case 2. [[code1, code2]] or [[code1, code2], [code3, code4, code5], ...]
                - we will assume the inner bracket follows the order; our model first
                use the embedding table to encode each code into a vector and then use
                average/mean pooling to get one vector for one inner bracket; then use
                AdaCare one the braket level
            - case 3. [[1.5, 2.0, 0.0]] or [[1.5, 2.0, 0.0], [8, 1.2, 4.5], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run AdaCare directly
                on the inner bracket level, similar to case 1 after embedding table
            - case 4. [[[1.5, 2.0, 0.0]]] or [[[1.5, 2.0, 0.0], [8, 1.2, 4.5]], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run AdaCare directly
                on the inner bracket level, similar to case 2 after embedding table

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        use_embedding: list of bools indicating whether to use embedding for each feature type,
            e.g. [True, False].
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension. Default is 128.
        **kwargs: other parameters for the AdaCare layer.


    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
        ...             "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        ...             "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
        ...             "list_list_vectors": [
        ...                 [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
        ...                 [[7.7, 8.5, 9.4]],
        ...             ],
        ...             "label": 1,
        ...         },
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-1",
        ...             "list_codes": [
        ...                 "55154191800",
        ...                 "551541928",
        ...                 "55154192800",
        ...                 "705182798",
        ...                 "70518279800",
        ...             ],
        ...             "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
        ...             "list_list_codes": [["A04A", "B035", "C129"]],
        ...             "list_list_vectors": [
        ...                 [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
        ...             ],
        ...             "label": 0,
        ...         },
        ...     ]
        >>> dataset = SampleEHRDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import AdaCare
        >>> model = AdaCare(
        ...         dataset=dataset,
        ...         feature_keys=[
        ...             "list_codes",
        ...             "list_vectors",
        ...             "list_list_codes",
        ...             "list_list_vectors",
        ...         ],
        ...         label_key="label",
        ...         use_embedding=[True, False, True, False],
        ...         mode="binary",
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(0.7167, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),
            'y_prob': tensor([[0.5009], [0.4779]], grad_fn=<SigmoidBackward0>),
            'y_true': tensor([[0.], [1.]]),
            'logit': tensor([[ 0.0036], [-0.0886]], grad_fn=<AddmmBackward0>)
        }
    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        use_embedding: List[bool],
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        **kwargs,
    ):
        super(AdaCare, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim
        self.use_embedding = use_embedding
        self.hidden_dim = hidden_dim

        if "input_dim" in kwargs:
            raise ValueError("input_dim is automatically determined")

        # the key of self.feat_tokenizers only contains the code based inputs
        self.feat_tokenizers = {}
        self.label_tokenizer = self.get_label_tokenizer()
        # the key of self.embeddings only contains the code based inputs
        self.embeddings = nn.ModuleDict()
        # the key of self.linear_layers only contains the float/int based inputs
        self.linear_layers = nn.ModuleDict()
        self.adacare = nn.ModuleDict()

        for idx, feature_key in enumerate(self.feature_keys):
            input_info = self.dataset.input_info[feature_key]
            # sanity check
            if input_info["type"] not in [str, float, int]:
                raise ValueError(
                    "AdaCare only supports str code, float and int as input types"
                )
            elif (input_info["type"] == str) and (input_info["dim"] not in [2, 3]):
                raise ValueError(
                    "AdaCare only supports 2-dim or 3-dim str code as input types"
                )
            elif (input_info["type"] == str) and (use_embedding[idx] == False):
                raise ValueError(
                    "AdaCare only supports embedding for str code as input types"
                )
            elif (input_info["type"] in [float, int]) and (
                input_info["dim"] not in [2, 3]
            ):
                raise ValueError(
                    "AdaCare only supports 2-dim or 3-dim float and int as input types"
                )

            # for code based input, we need Type
            # for float/int based input, we need Type, input_dim
            if use_embedding[idx]:
                self.add_feature_transform_layer(feature_key, input_info)
                self.adacare[feature_key] = AdaCareLayer(
                    input_dim=embedding_dim, hidden_dim=self.hidden_dim, **kwargs
                )
            else:
                self.adacare[feature_key] = AdaCareLayer(
                    input_dim=input_info["len"], hidden_dim=self.hidden_dim, **kwargs
                )

        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(len(self.feature_keys) * self.hidden_dim, output_size)

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        The label `kwargs[self.label_key]` is a list of labels for each patient.

        Args:
            **kwargs: keyword arguments for the model. The keys must contain
                all the feature keys and the label key.

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the loss.
                feature_importance: a list of tensors with shape (feature_type, batch_size, time_step, features)
                                    representing the feature importance.
                conv_feature_importance: a list of tensors with shape (feature_type, batch_size, time_step, 3*kernal_size)
                                        representing the convolutional feature importance.
                y_prob: a tensor representing the predicted probabilities.
                y_true: a tensor representing the true labels.
        """
        patient_emb = []
        feature_importance = []
        conv_feature_importance = []
        for idx, feature_key in enumerate(self.feature_keys):
            input_info = self.dataset.input_info[feature_key]
            dim_, type_ = input_info["dim"], input_info["type"]

            # for case 1: [code1, code2, code3, ...]
            if (dim_ == 2) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_2d(
                    kwargs[feature_key]
                )
                # (patient, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, event)
                mask = torch.any(x !=0, dim=2)

            # for case 2: [[code1, code2], [code3, ...], ...]
            elif (dim_ == 3) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_3d(
                    kwargs[feature_key]
                )
                # (patient, visit, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, visit, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                # (patient, visit)
                mask = torch.any(x !=0, dim=2)

            # for case 3: [[1.5, 2.0, 0.0], ...]
            elif (dim_ == 2) and (type_ in [float, int]):
                x, mask = self.padding2d(kwargs[feature_key])
                # (patient, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, event, embedding_dim)
                if self.use_embedding[idx]:
                    x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask.bool().to(self.device)

            # for case 4: [[[1.5, 2.0, 0.0], [1.8, 2.4, 6.0]], ...]
            elif (dim_ == 3) and (type_ in [float, int]):
                x, mask = self.padding3d(kwargs[feature_key])
                # (patient, visit, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)

                if self.use_embedding[idx]:
                    x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask[:, :, 0]
                mask = mask.bool().to(self.device)

            else:
                raise NotImplementedError

            x, _, inputatt, convatt = self.adacare[feature_key](x, mask)
            feature_importance.append(inputatt)
            conv_feature_importance.append(convatt)
            patient_emb.append(x)

        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)
        y_prob = self.prepare_y_prob(logits)
        results = {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": y_true,
            "logit": logits,
        }
        if kwargs.get("embed", False):
            results["embed"] = patient_emb
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleEHRDataset

    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            # "single_vector": [1, 2, 3],
            "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
            "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
            "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
            "list_list_vectors": [
                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
                [[7.7, 8.5, 9.4]],
            ],
            "label": 1,
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-1",
            # "single_vector": [1, 5, 8],
            "list_codes": [
                "55154191800",
                "551541928",
                "55154192800",
                "705182798",
                "70518279800",
            ],
            "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
            "list_list_codes": [["A04A", "B035", "C129"]],
            "list_list_vectors": [
                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
            ],
            "label": 0,
        },
    ]

    # dataset
    dataset = SampleEHRDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = AdaCare(
        dataset=dataset,
        feature_keys=[
            "list_codes",
            "list_vectors",
            "list_list_codes",
            "list_list_vectors",
        ],
        label_key="label",
        mode="binary",
        use_embedding=[True, False, True, False],
        hidden_dim=64,
    )

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for agent.py:
from typing import List, Tuple, Dict, Optional

import torch
import torch.nn as nn
import torch.nn.utils.rnn as rnn_utils

from pyhealth.datasets import SampleEHRDataset
from pyhealth.models import BaseModel
from pyhealth.models.utils import get_last_visit
import torch.nn.functional as F


class AgentLayer(nn.Module):
    """Dr. Agent layer.

    Paper: Junyi Gao et al. Dr. Agent: Clinical predictive model via mimicked second opinions. JAMIA.

    This layer is used in the Dr. Agent model. But it can also be used as a
    standalone layer.

    Args:
        input_dim: dynamic feature size.
        static_dim: static feature size, if 0, then no static feature is used.
        cell: rnn cell type. Default is "gru".
        use_baseline: whether to use baseline for the RL agent. Default is True.
        n_actions: number of historical visits to choose. Default is 10.
        n_units: number of hidden units in each agent. Default is 64.
        fusion_dim: number of hidden units in the final representation. Default is 128.
        n_hidden: number of hidden units in the rnn. Default is 128.
        dropout: dropout rate. Default is 0.5.
        lamda: weight for the agent selected hidden state and the current hidden state. Default is 0.5.

    Examples:
        >>> from pyhealth.models import AgentLayer
        >>> input = torch.randn(3, 128, 64)  # [batch size, sequence len, feature_size]
        >>> layer = AgentLayer(64)
        >>> c, _ = layer(input)
        >>> c.shape
        torch.Size([3, 128])
    """

    def __init__(
        self,
        input_dim: int,
        static_dim: int = 0,
        cell: str = "gru",
        use_baseline: bool = True,
        n_actions: int = 10,
        n_units: int = 64,
        n_hidden: int = 128,
        dropout: int = 0.5,
        lamda: int = 0.5,
    ):
        super(AgentLayer, self).__init__()

        if cell not in ["gru", "lstm"]:
            raise ValueError("Only gru and lstm are supported for cell.")

        self.cell = cell
        self.use_baseline = use_baseline
        self.n_actions = n_actions
        self.n_units = n_units
        self.input_dim = input_dim
        self.n_hidden = n_hidden
        # self.n_output = n_output
        self.dropout = dropout
        self.lamda = lamda
        self.fusion_dim = n_hidden
        self.static_dim = static_dim

        self.agent1_action = []
        self.agent1_prob = []
        self.agent1_entropy = []
        self.agent1_baseline = []
        self.agent2_action = []
        self.agent2_prob = []
        self.agent2_entropy = []
        self.agent2_baseline = []

        self.agent1_fc1 = nn.Linear(self.n_hidden + self.static_dim, self.n_units)
        self.agent2_fc1 = nn.Linear(self.input_dim + self.static_dim, self.n_units)
        self.agent1_fc2 = nn.Linear(self.n_units, self.n_actions)
        self.agent2_fc2 = nn.Linear(self.n_units, self.n_actions)
        if use_baseline == True:
            self.agent1_value = nn.Linear(self.n_units, 1)
            self.agent2_value = nn.Linear(self.n_units, 1)

        if self.cell == "lstm":
            self.rnn = nn.LSTMCell(self.input_dim, self.n_hidden)
        else:
            self.rnn = nn.GRUCell(self.input_dim, self.n_hidden)

        for name, param in self.rnn.named_parameters():
            if "bias" in name:
                nn.init.constant_(param, 0.0)
            elif "weight" in name:
                nn.init.orthogonal_(param)

        if dropout > 0.0:
            self.nn_dropout = nn.Dropout(p=dropout)
        if self.static_dim > 0:
            self.init_h = nn.Linear(self.static_dim, self.n_hidden)
            self.init_c = nn.Linear(self.static_dim, self.n_hidden)
            self.fusion = nn.Linear(self.n_hidden + self.static_dim, self.fusion_dim)
        # self.output = nn.Linear(self.fusion_dim, self.n_output)

        self.sigmoid = nn.Sigmoid()
        self.softmax = nn.Softmax(dim=1)
        self.tanh = nn.Tanh()
        self.relu = nn.ReLU()

    def choose_action(self, observation, agent=1):
        observation = observation.detach()

        if agent == 1:
            result_fc1 = self.agent1_fc1(observation)
            result_fc1 = self.tanh(result_fc1)
            result_fc2 = self.agent1_fc2(result_fc1)
            if self.use_baseline == True:
                result_value = self.agent1_value(result_fc1)
                self.agent1_baseline.append(result_value)
        else:
            result_fc1 = self.agent2_fc1(observation)
            result_fc1 = self.tanh(result_fc1)
            result_fc2 = self.agent2_fc2(result_fc1)
            if self.use_baseline == True:
                result_value = self.agent2_value(result_fc1)
                self.agent2_baseline.append(result_value)

        probs = self.softmax(result_fc2)
        m = torch.distributions.Categorical(probs)
        actions = m.sample()

        if agent == 1:
            self.agent1_entropy.append(m.entropy())
            self.agent1_action.append(actions.unsqueeze(-1))
            self.agent1_prob.append(m.log_prob(actions))
        else:
            self.agent2_entropy.append(m.entropy())
            self.agent2_action.append(actions.unsqueeze(-1))
            self.agent2_prob.append(m.log_prob(actions))

        return actions.unsqueeze(-1)

    def forward(
        self,
        x: torch.tensor,
        static: Optional[torch.tensor] = None,
        mask: Optional[torch.tensor] = None,
    ) -> Tuple[torch.tensor]:
        """Forward propagation.

        Args:
            x: a tensor of shape [batch size, sequence len, input_dim].
            static: a tensor of shape [batch size, static_dim].
            mask: an optional tensor of shape [batch size, sequence len], where
                1 indicates valid and 0 indicates invalid.

        Returns:
            last_output: a tensor of shape [batch size, n_hidden] representing the
                patient embedding.
            output: a tensor of shape [batch size, sequence len, n_hidden] representing the patient embedding at each time step.
        """
        # rnn will only apply dropout between layers

        batch_size = x.size(0)
        time_step = x.size(1)
        feature_dim = x.size(2)

        self.agent1_action = []
        self.agent1_prob = []
        self.agent1_entropy = []
        self.agent1_baseline = []
        self.agent2_action = []
        self.agent2_prob = []
        self.agent2_entropy = []
        self.agent2_baseline = []

        if self.static_dim > 0:
            cur_h = self.init_h(static)
            if self.cell == "lstm":
                cur_c = self.init_c(static)
        else:
            cur_h = torch.zeros(
                batch_size, self.n_hidden, dtype=torch.float32, device=x.device
            )
            if self.cell == "lstm":
                cur_c = torch.zeros(
                    batch_size, self.n_hidden, dtype=torch.float32, device=x.device
                )

        h = []
        for cur_time in range(time_step):
            cur_input = x[:, cur_time, :]

            if cur_time == 0:
                obs_1 = cur_h
                obs_2 = cur_input

                if self.static_dim > 0:
                    obs_1 = torch.cat((obs_1, static), dim=1)
                    obs_2 = torch.cat((obs_2, static), dim=1)

                self.choose_action(obs_1, 1).long()
                self.choose_action(obs_2, 2).long()

                observed_h = (
                    torch.zeros_like(cur_h, dtype=torch.float32)
                    .view(-1)
                    .repeat(self.n_actions)
                    .view(self.n_actions, batch_size, self.n_hidden)
                )
                action_h = cur_h
                if self.cell == "lstm":
                    observed_c = (
                        torch.zeros_like(cur_c, dtype=torch.float32)
                        .view(-1)
                        .repeat(self.n_actions)
                        .view(self.n_actions, batch_size, self.n_hidden)
                    )
                    action_c = cur_c

            else:
                observed_h = torch.cat((observed_h[1:], cur_h.unsqueeze(0)), 0)

                obs_1 = observed_h.mean(dim=0)
                obs_2 = cur_input

                if self.static_dim > 0:
                    obs_1 = torch.cat((obs_1, static), dim=1)
                    obs_2 = torch.cat((obs_2, static), dim=1)

                act_idx1 = self.choose_action(obs_1, 1).long()
                act_idx2 = self.choose_action(obs_2, 2).long()
                batch_idx = torch.arange(batch_size, dtype=torch.long).unsqueeze(-1)
                action_h1 = observed_h[act_idx1, batch_idx, :].squeeze(1)
                action_h2 = observed_h[act_idx2, batch_idx, :].squeeze(1)
                action_h = (action_h1 + action_h2) / 2
                if self.cell == "lstm":
                    observed_c = torch.cat((observed_c[1:], cur_c.unsqueeze(0)), 0)
                    action_c1 = observed_c[act_idx1, batch_idx, :].squeeze(1)
                    action_c2 = observed_c[act_idx2, batch_idx, :].squeeze(1)
                    action_c = (action_c1 + action_c2) / 2

            if self.cell == "lstm":
                weighted_h = self.lamda * action_h + (1 - self.lamda) * cur_h
                weighted_c = self.lamda * action_c + (1 - self.lamda) * cur_c
                rnn_state = (weighted_h, weighted_c)
                cur_h, cur_c = self.rnn(cur_input, rnn_state)
            else:
                weighted_h = self.lamda * action_h + (1 - self.lamda) * cur_h
                cur_h = self.rnn(cur_input, weighted_h)
            h.append(cur_h)

        h = torch.stack(h, dim=1)

        if self.static_dim > 0:
            static = static.unsqueeze(1).repeat(1, time_step, 1)
            h = torch.cat((h, static), dim=2)
            h = self.fusion(h)

        last_out = get_last_visit(h, mask)

        if self.dropout > 0.0:
            last_out = self.nn_dropout(last_out)
        return last_out, h


class Agent(BaseModel):
    """Dr. Agent model.

    Paper: Junyi Gao et al. Dr. Agent: Clinical predictive model via mimicked second opinions. JAMIA.

    Note:
        We use separate Dr. Agent layers for different feature_keys.
        Currently, we automatically support different input formats:
            - code based input (need to use the embedding table later)
            - float/int based value input
        We follow the current convention for the Dr. Agent model:
            - case 1. [code1, code2, code3, ...]
                - we will assume the code follows the order; our model will encode
                each code into a vector and apply Dr. Agent on the code level
            - case 2. [[code1, code2]] or [[code1, code2], [code3, code4, code5], ...]
                - we will assume the inner bracket follows the order; our model first
                use the embedding table to encode each code into a vector and then use
                average/mean pooling to get one vector for one inner bracket; then use
                Dr. Agent one the braket level
            - case 3. [[1.5, 2.0, 0.0]] or [[1.5, 2.0, 0.0], [8, 1.2, 4.5], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run Dr. Agent directly
                on the inner bracket level, similar to case 1 after embedding table
            - case 4. [[[1.5, 2.0, 0.0]]] or [[[1.5, 2.0, 0.0], [8, 1.2, 4.5]], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run Dr. Agent directly
                on the inner bracket level, similar to case 2 after embedding table

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        static_keys: the key in samples to use as static features, e.g. "demographics". Default is None.
                     we only support numerical static features.
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension of the RNN in the Dr. Agent layer. Default is 128.
        use_baseline: whether to use the baseline value to calculate the RL loss. Default is True.
        **kwargs: other parameters for the Dr. Agent layer.


    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
        ...             "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        ...             "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
        ...             "list_list_vectors": [
        ...                 [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
        ...                 [[7.7, 8.5, 9.4]],
        ...             ],
        ...             "demographic": [0.0, 2.0, 1.5],
        ...             "label": 1,
        ...         },
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-1",
        ...             "list_codes": [
        ...                 "55154191800",
        ...                 "551541928",
        ...                 "55154192800",
        ...                 "705182798",
        ...                 "70518279800",
        ...             ],
        ...             "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
        ...             "list_list_codes": [["A04A", "B035", "C129"]],
        ...             "list_list_vectors": [
        ...                 [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
        ...             ],
        ...             "demographic": [0.0, 2.0, 1.5],
        ...             "label": 0,
        ...         },
        ...     ]
        >>> dataset = SampleEHRDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import Agent
        >>> model = Agent(
        ...         dataset=dataset,
        ...         feature_keys=[
        ...             "list_codes",
        ...             "list_vectors",
        ...             "list_list_codes",
        ...             "list_list_vectors",
        ...         ],
        ...         label_key="label",
        ...         static_key="demographic",
        ...         mode="binary"
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(1.4059, grad_fn=<AddBackward0>),
            'y_prob': tensor([[0.4861], [0.5348]], grad_fn=<SigmoidBackward0>),
            'y_true': tensor([[0.], [1.]]),
            'logit': tensor([[-0.0556], [0.1392]], grad_fn=<AddmmBackward0>)
        }
        >>>

    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        static_key: Optional[str] = None,
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        use_baseline: bool = True,
        **kwargs,
    ):
        super(Agent, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

        # validate kwargs for Dr. Agent layer
        if "feature_size" in kwargs:
            raise ValueError("feature_size is determined by embedding_dim")

        # the key of self.feat_tokenizers only contains the code based inputs
        self.feat_tokenizers = {}
        self.static_key = static_key
        self.use_baseline = use_baseline
        self.label_tokenizer = self.get_label_tokenizer()
        # the key of self.embeddings only contains the code based inputs
        self.embeddings = nn.ModuleDict()
        # the key of self.linear_layers only contains the float/int based inputs
        self.linear_layers = nn.ModuleDict()

        self.static_dim = 0
        if self.static_key is not None:
            self.static_dim = self.dataset.input_info[self.static_key]["len"]

        self.agent = nn.ModuleDict()
        # add feature Dr. Agent layers
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            # sanity check
            if input_info["type"] not in [str, float, int]:
                raise ValueError(
                    "Dr. Agent only supports str code, float and int as input types"
                )
            elif (input_info["type"] == str) and (input_info["dim"] not in [2, 3]):
                raise ValueError(
                    "Dr. Agent only supports 2-dim or 3-dim str code as input types"
                )
            elif (input_info["type"] in [float, int]) and (
                input_info["dim"] not in [2, 3]
            ):
                raise ValueError(
                    "Dr. Agent only supports 2-dim or 3-dim float and int as input types"
                )

            # for code based input, we need Type
            # for float/int based input, we need Type, input_dim
            self.add_feature_transform_layer(feature_key, input_info)
            self.agent[feature_key] = AgentLayer(
                input_dim=embedding_dim,
                static_dim=self.static_dim,
                n_hidden=hidden_dim,
                **kwargs,
            )

        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(len(self.feature_keys) * self.hidden_dim, output_size)

    def get_loss(self, model, pred, true, mask, gamma=0.9, entropy_term=0.01):

        if self.mode == "binary":
            pred = torch.sigmoid(pred)
            rewards = ((pred - 0.5) * 2 * true).squeeze()
        elif self.mode == "multiclass":
            pred = torch.softmax(pred, dim=-1)
            y_onehot = torch.zeros_like(pred).scatter(1, true.unsqueeze(1), 1)
            rewards = (pred * y_onehot).sum(-1).squeeze()
        elif self.mode == "multilabel":
            pred = torch.sigmoid(pred)
            rewards = (
                ((pred - 0.5) * 2 * true).sum(dim=-1) / (true.sum(dim=-1) + 1e-7)
            ).squeeze()
        elif self.mode == "regression":
            rewards = (1 / torch.abs(pred - true)).squeeze()  # b*t
            rewards = torch.clamp(rewards, min=0, max=5)
        else:
            raise ValueError(
                "mode should be binary, multiclass, multilabel or regression"
            )

        act_prob1 = model.agent1_prob
        act_prob1 = torch.stack(act_prob1).permute(1, 0).to(self.device)
        act_prob1 = act_prob1 * mask.view(act_prob1.size(0), act_prob1.size(1))
        act_entropy1 = model.agent1_entropy
        act_entropy1 = torch.stack(act_entropy1).permute(1, 0).to(self.device)
        act_entropy1 = act_entropy1 * mask.view(
            act_entropy1.size(0), act_entropy1.size(1)
        )
        if self.use_baseline == True:
            act_baseline1 = model.agent1_baseline
            act_baseline1 = (
                torch.stack(act_baseline1).squeeze(-1).permute(1, 0).to(self.device)
            )
            act_baseline1 = act_baseline1 * mask.view(
                act_baseline1.size(0), act_baseline1.size(1)
            )

        act_prob2 = model.agent2_prob
        act_prob2 = torch.stack(act_prob2).permute(1, 0).to(self.device)
        act_prob2 = act_prob2 * mask.view(act_prob2.size(0), act_prob2.size(1))
        act_entropy2 = model.agent2_entropy
        act_entropy2 = torch.stack(act_entropy2).permute(1, 0).to(self.device)
        act_entropy2 = act_entropy2 * mask.view(
            act_entropy2.size(0), act_entropy2.size(1)
        )
        if self.use_baseline == True:
            act_baseline2 = model.agent2_baseline
            act_baseline2 = (
                torch.stack(act_baseline2).squeeze(-1).permute(1, 0).to(self.device)
            )
            act_baseline2 = act_baseline2 * mask.view(
                act_baseline2.size(0), act_baseline2.size(1)
            )

        running_rewards = []
        discounted_rewards = 0
        for i in reversed(range(act_prob1.size(1))):
            if i == act_prob1.size(1) - 1:
                discounted_rewards = rewards + gamma * discounted_rewards
            else:
                discounted_rewards = (
                    torch.zeros_like(rewards) + gamma * discounted_rewards
                )
            running_rewards.insert(0, discounted_rewards)
        rewards = torch.stack(running_rewards).permute(1, 0)
        # rewards = (rewards - rewards.mean(dim=1).unsqueeze(-1)) / (
        #     rewards.std(dim=1) + 1e-7
        # ).unsqueeze(-1)
        rewards = rewards.detach()

        if self.use_baseline == True:
            loss_value1 = torch.sum((rewards - act_baseline1) ** 2, dim=1) / torch.sum(
                mask, dim=1
            )
            loss_value1 = torch.mean(loss_value1)
            loss_value2 = torch.sum((rewards - act_baseline2) ** 2, dim=1) / torch.sum(
                mask, dim=1
            )
            loss_value2 = torch.mean(loss_value2)
            loss_value = loss_value1 + loss_value2
            loss_RL1 = -torch.sum(
                act_prob1 * (rewards - act_baseline1) + entropy_term * act_entropy1,
                dim=1,
            ) / torch.sum(mask, dim=1)
            loss_RL1 = torch.mean(loss_RL1)
            loss_RL2 = -torch.sum(
                act_prob2 * (rewards - act_baseline2) + entropy_term * act_entropy2,
                dim=1,
            ) / torch.sum(mask, dim=1)
            loss_RL2 = torch.mean(loss_RL2)
            loss_RL = loss_RL1 + loss_RL2
            loss = loss_RL + loss_value
        else:
            loss_RL1 = -torch.sum(
                act_prob1 * rewards + entropy_term * act_entropy1, dim=1
            ) / torch.sum(mask, dim=1)
            loss_RL1 = torch.mean(loss_RL1)
            loss_RL2 = -torch.sum(
                act_prob2 * rewards + entropy_term * act_entropy2, dim=1
            ) / torch.sum(mask, dim=1)
            loss_RL2 = torch.mean(loss_RL2)
            loss_RL = loss_RL1 + loss_RL2
            loss = loss_RL

        return loss

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        The label `kwargs[self.label_key]` is a list of labels for each patient.

        Args:
            **kwargs: keyword arguments for the model. The keys must contain
                all the feature keys and the label key.

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the final loss.
                loss_task: a scalar tensor representing the task loss.
                loss_RL: a scalar tensor representing the RL loss.
                y_prob: a tensor representing the predicted probabilities.
                y_true: a tensor representing the true labels.
        """
        patient_emb = []
        mask_dict = {}
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            dim_, type_ = input_info["dim"], input_info["type"]

            # for case 1: [code1, code2, code3, ...]
            if (dim_ == 2) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_2d(
                    kwargs[feature_key]
                )
                # (patient, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, event)
                mask = torch.any(x !=0, dim=2)
                mask_dict[feature_key] = mask

            # for case 2: [[code1, code2], [code3, ...], ...]
            elif (dim_ == 3) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_3d(
                    kwargs[feature_key]
                )
                # (patient, visit, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, visit, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                # (patient, visit)
                mask = torch.any(x !=0, dim=2)
                mask_dict[feature_key] = mask

            # for case 3: [[1.5, 2.0, 0.0], ...]
            elif (dim_ == 2) and (type_ in [float, int]):
                x, mask = self.padding2d(kwargs[feature_key])
                # (patient, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, event, embedding_dim)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask.bool().to(self.device)
                mask_dict[feature_key] = mask

            # for case 4: [[[1.5, 2.0, 0.0], [1.8, 2.4, 6.0]], ...]
            elif (dim_ == 3) and (type_ in [float, int]):
                x, mask = self.padding3d(kwargs[feature_key])
                # (patient, visit, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask[:, :, 0]
                mask = mask.bool().to(self.device)
                mask_dict[feature_key] = mask

            else:
                raise NotImplementedError

            if self.static_dim > 0:
                static = torch.tensor(
                    kwargs[self.static_key], dtype=torch.float, device=self.device
                )
                x, _ = self.agent[feature_key](x, static=static, mask=mask)
            else:
                x, _ = self.agent[feature_key](x, mask=mask)
            patient_emb.append(x)

        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss_task = self.get_loss_function()(logits, y_true)

        loss_rl = 0
        for feature_key in self.feature_keys:
            cur_loss = self.get_loss(
                self.agent[feature_key], logits, y_true, mask_dict[feature_key]
            )
            loss_rl += cur_loss
        loss = loss_task + loss_rl
        y_prob = self.prepare_y_prob(logits)
        results = {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": y_true,
            "logit": logits,
        }
        if kwargs.get("embed", False):
            results["embed"] = patient_emb
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleEHRDataset

    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            # "single_vector": [1, 2, 3],
            "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
            "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
            "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
            "list_list_vectors": [
                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
                [[7.7, 8.5, 9.4]],
            ],
            "label": 1,
            "demographic": [1.0, 2.0, 1.3],
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-1",
            # "single_vector": [1, 5, 8],
            "list_codes": [
                "55154191800",
                "551541928",
                "55154192800",
                "705182798",
                "70518279800",
            ],
            "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
            "list_list_codes": [["A04A", "B035", "C129"]],
            "list_list_vectors": [
                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
            ],
            "label": 0,
            "demographic": [1.0, 2.0, 1.3],
        },
    ]

    # dataset
    dataset = SampleEHRDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = Agent(
        dataset=dataset,
        feature_keys=[
            "list_codes",
            "list_vectors",
            "list_list_codes",
            # "list_list_vectors",
        ],
        static_key="demographic",
        label_key="label",
        mode="binary",
    )

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for base_model.py:
from abc import ABC
from typing import List, Dict, Union, Callable

import torch
import torch.nn as nn
import torch.nn.functional as F

from pyhealth.datasets import SampleBaseDataset
from pyhealth.models.utils import batch_to_multihot
from pyhealth.tokenizer import Tokenizer

# TODO: add support for regression
VALID_MODE = ["binary", "multiclass", "multilabel"]


class BaseModel(ABC, nn.Module):
    """Abstract class for PyTorch models.

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys: list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
    """

    def __init__(
        self,
        dataset: SampleBaseDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
    ):
        super(BaseModel, self).__init__()
        assert mode in VALID_MODE, f"mode must be one of {VALID_MODE}"
        self.dataset = dataset
        self.feature_keys = feature_keys
        self.label_key = label_key
        self.mode = mode
        # used to query the device of the model
        self._dummy_param = nn.Parameter(torch.empty(0))
        return

    @property
    def device(self):
        """Gets the device of the model."""
        return self._dummy_param.device

    def get_feature_tokenizers(self, special_tokens=None) -> Dict[str, Tokenizer]:
        """Gets the default feature tokenizers using `self.feature_keys`.

        These function is used for specific healthcare models, such as gamenet, safedrug, etc.

        Args:
            special_tokens: a list of special tokens to add to the tokenizer.
                Default is ["<pad>", "<unk>"].

        Returns:
            feature_tokenizers: a dictionary of feature tokenizers with keys
                corresponding to self.feature_keys.
        """
        if special_tokens is None:
            special_tokens = ["<pad>", "<unk>"]
        feature_tokenizers = {}
        for feature_key in self.feature_keys:
            feature_tokenizers[feature_key] = Tokenizer(
                tokens=self.dataset.get_all_tokens(key=feature_key),
                special_tokens=special_tokens,
            )
        return feature_tokenizers

    @staticmethod
    def get_embedding_layers(
        feature_tokenizers: Dict[str, Tokenizer],
        embedding_dim: int,
    ) -> nn.ModuleDict:
        """Gets the default embedding layers using the feature tokenizers.

        These function is used for specific healthcare models, such as gamenet, safedrug, etc.

        Args:
            feature_tokenizers: a dictionary of feature tokenizers with keys
                corresponding to `self.feature_keys`.
            embedding_dim: the dimension of the embedding.

        Returns:
            embedding_layers: a module dictionary of embedding layers with keys
                corresponding to `self.feature_keys`.
        """
        embedding_layers = nn.ModuleDict()
        for key, tokenizer in feature_tokenizers.items():
            embedding_layers[key] = nn.Embedding(
                tokenizer.get_vocabulary_size(),
                embedding_dim,
                padding_idx=tokenizer.get_padding_index(),
            )
        return embedding_layers

    @staticmethod
    def padding2d(batch):
        """
        Similar to pyhealth.tokenizer.Tokenizer.padding2d, but no mapping
        Args:
            batch: a list of list of list obj
                - 1-level: number of samples/patients
                - 2-level: number of visit, length maybe not equal
                - 3-level: number of features per visit, length must be equal
        Returns:
            padded_batch: a padded list of list of list obj
            e.g.,
                batch = [[[1.3, 2.5], [3.2, 4.4]], [[5.1, 6.0], [7.7, 8.3]]] -> [[[1.3, 2.5], [3.2, 4.4]], [[5.1, 6.0], [7.7, 8.3]]]
                batch = [[[1.3, 2.5], [3.2, 4.4]], [[5.1, 6.0]]] -> [[[1.3, 2.5], [3.2, 4.4]], [[5.1, 6.0], [0.0, 0.0]]]
        """
        batch_max_length = max([len(x) for x in batch])

        # get mask
        mask = torch.zeros(len(batch), batch_max_length, dtype=torch.bool)
        for i, x in enumerate(batch):
            mask[i, : len(x)] = 1

        # level-2 padding
        batch = [x + [[0.0] * len(x[0])] * (batch_max_length - len(x)) for x in batch]

        return batch, mask

    @staticmethod
    def padding3d(batch):
        """
        Similar to pyhealth.tokenizer.Tokenizer.padding2d, but no mapping
        Args:
            batch: a list of list of list obj
                - 1-level: number of samples/patients
                - 2-level: number of visit, length maybe not equal
                - 3-level: number of features per visit, length must be equal
        Returns:
            padded_batch: a padded list of list of list obj. No examples, just one more dimension higher than self.padding2d
        """
        batch_max_length_level2 = max([len(x) for x in batch])
        batch_max_length_level3 = max(
            [max([len(x) for x in visits]) for visits in batch]
        )

        # the most inner vector length
        vec_len = len(batch[0][0][0])

        # get mask
        mask = torch.zeros(
            len(batch),
            batch_max_length_level2,
            batch_max_length_level3,
            dtype=torch.bool,
        )
        for i, visits in enumerate(batch):
            for j, x in enumerate(visits):
                mask[i, j, : len(x)] = 1

        # level-2 padding
        batch = [
            x + [[[0.0] * vec_len]] * (batch_max_length_level2 - len(x)) for x in batch
        ]

        # level-3 padding
        batch = [
            [x + [[0.0] * vec_len] * (batch_max_length_level3 - len(x)) for x in visits]
            for visits in batch
        ]

        return batch, mask

    def add_feature_transform_layer(self, feature_key: str, info, special_tokens=None):
        if info["type"] == str:
            # feature tokenizer
            if special_tokens is None:
                special_tokens = ["<pad>", "<unk>"]
            tokenizer = Tokenizer(
                tokens=self.dataset.get_all_tokens(key=feature_key),
                special_tokens=special_tokens,
            )
            self.feat_tokenizers[feature_key] = tokenizer
            # feature embedding
            self.embeddings[feature_key] = nn.Embedding(
                tokenizer.get_vocabulary_size(),
                self.embedding_dim,
                padding_idx=tokenizer.get_padding_index(),
            )
        elif info["type"] in [float, int]:
            self.linear_layers[feature_key] = nn.Linear(info["len"], self.embedding_dim)
        else:
            raise ValueError("Unsupported feature type: {}".format(info["type"]))

    def get_label_tokenizer(self, special_tokens=None) -> Tokenizer:
        """Gets the default label tokenizers using `self.label_key`.

        Args:
            special_tokens: a list of special tokens to add to the tokenizer.
                Default is empty list.

        Returns:
            label_tokenizer: the label tokenizer.
        """
        if special_tokens is None:
            special_tokens = []
        label_tokenizer = Tokenizer(
            self.dataset.get_all_tokens(key=self.label_key),
            special_tokens=special_tokens,
        )
        return label_tokenizer

    def get_output_size(self, label_tokenizer: Tokenizer) -> int:
        """Gets the default output size using the label tokenizer and `self.mode`.

        If the mode is "binary", the output size is 1. If the mode is "multiclass"
        or "multilabel", the output size is the number of classes or labels.

        Args:
            label_tokenizer: the label tokenizer.

        Returns:
            output_size: the output size of the model.
        """
        output_size = label_tokenizer.get_vocabulary_size()
        if self.mode == "binary":
            assert output_size == 2
            output_size = 1
        return output_size

    def get_loss_function(self) -> Callable:
        """Gets the default loss function using `self.mode`.

        The default loss functions are:
            - binary: `F.binary_cross_entropy_with_logits`
            - multiclass: `F.cross_entropy`
            - multilabel: `F.binary_cross_entropy_with_logits`

        Returns:
            The default loss function.
        """
        if self.mode == "binary":
            return F.binary_cross_entropy_with_logits
        elif self.mode == "multiclass":
            return F.cross_entropy
        elif self.mode == "multilabel":
            return F.binary_cross_entropy_with_logits
        else:
            raise ValueError("Invalid mode: {}".format(self.mode))

    def prepare_labels(
        self,
        labels: Union[List[str], List[List[str]]],
        label_tokenizer: Tokenizer,
    ) -> torch.Tensor:
        """Prepares the labels for model training and evaluation.

        This function converts the labels to different formats depending on the
        mode. The default formats are:
            - binary: a tensor of shape (batch_size, 1)
            - multiclass: a tensor of shape (batch_size,)
            - multilabel: a tensor of shape (batch_size, num_labels)

        Args:
            labels: the raw labels from the samples. It should be
                - a list of str for binary and multiclass classificationa
                - a list of list of str for multilabel classification
            label_tokenizer: the label tokenizer.

        Returns:
            labels: the processed labels.
        """
        if self.mode in ["binary"]:
            labels = label_tokenizer.convert_tokens_to_indices(labels)
            labels = torch.FloatTensor(labels).unsqueeze(-1)
        elif self.mode in ["multiclass"]:
            labels = label_tokenizer.convert_tokens_to_indices(labels)
            labels = torch.LongTensor(labels)
        elif self.mode in ["multilabel"]:
            # convert to indices
            labels_index = label_tokenizer.batch_encode_2d(
                labels, padding=False, truncation=False
            )
            # convert to multihot
            num_labels = label_tokenizer.get_vocabulary_size()
            labels = batch_to_multihot(labels_index, num_labels)
        else:
            raise NotImplementedError
        labels = labels.to(self.device)
        return labels

    def prepare_y_prob(self, logits: torch.Tensor) -> torch.Tensor:
        """Prepares the predicted probabilities for model evaluation.

        This function converts the predicted logits to predicted probabilities
        depending on the mode. The default formats are:
            - binary: a tensor of shape (batch_size, 1) with values in [0, 1],
                which is obtained with `torch.sigmoid()`
            - multiclass: a tensor of shape (batch_size, num_classes) with
                values in [0, 1] and sum to 1, which is obtained with
                `torch.softmax()`
            - multilabel: a tensor of shape (batch_size, num_labels) with values
                in [0, 1], which is obtained with `torch.sigmoid()`

        Args:
            logits: the predicted logit tensor.

        Returns:
            y_prob: the predicted probability tensor.
        """
        if self.mode in ["binary"]:
            y_prob = torch.sigmoid(logits)
        elif self.mode in ["multiclass"]:
            y_prob = F.softmax(logits, dim=-1)
        elif self.mode in ["multilabel"]:
            y_prob = torch.sigmoid(logits)
        else:
            raise NotImplementedError
        return y_prob


Here is the code content for cnn.py:
from typing import Dict, List, Tuple

import torch
import torch.nn as nn

from pyhealth.datasets import SampleEHRDataset
from pyhealth.models import BaseModel

VALID_OPERATION_LEVEL = ["visit", "event"]


class CNNBlock(nn.Module):
    """Convolutional neural network block.

    This block wraps the PyTorch convolutional neural network layer with batch
    normalization and residual connection. It is used in the CNN layer.

    Args:
        in_channels: number of input channels.
        out_channels: number of output channels.
    """

    def __init__(self, in_channels: int, out_channels: int):
        super(CNNBlock, self).__init__()
        self.conv1 = nn.Sequential(
            # stride=1 by default
            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm1d(out_channels),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            # stride=1 by default
            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm1d(out_channels),
        )
        self.downsample = None
        if in_channels != out_channels:
            self.downsample = nn.Sequential(
                # stride=1, padding=0 by default
                nn.Conv1d(in_channels, out_channels, kernel_size=1),
                nn.BatchNorm1d(out_channels),
            )
        self.relu = nn.ReLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward propagation.

        Args:
            x: input tensor of shape [batch size, in_channels, *].

        Returns:
            output tensor of shape [batch size, out_channels, *].
        """
        residual = x
        out = self.conv1(x)
        out = self.conv2(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out


class CNNLayer(nn.Module):
    """Convolutional neural network layer.

    This layer stacks multiple CNN blocks and applies adaptive average pooling
    at the end. It is used in the CNN model. But it can also be used as a
    standalone layer.

    Args:
        input_size: input feature size.
        hidden_size: hidden feature size.
        num_layers: number of convolutional layers. Default is 1.

    Examples:
        >>> from pyhealth.models import CNNLayer
        >>> input = torch.randn(3, 128, 5)  # [batch size, sequence len, input_size]
        >>> layer = CNNLayer(5, 64)
        >>> outputs, last_outputs = layer(input)
        >>> outputs.shape
        torch.Size([3, 128, 64])
        >>> last_outputs.shape
        torch.Size([3, 64])
    """

    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int = 1,
    ):
        super(CNNLayer, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.cnn = nn.ModuleDict()
        for i in range(num_layers):
            in_channels = input_size if i == 0 else hidden_size
            out_channels = hidden_size
            self.cnn[f"CNN-{i}"] = CNNBlock(in_channels, out_channels)
        self.pooling = nn.AdaptiveAvgPool1d(1)

    def forward(self, x: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:
        """Forward propagation.

        Args:
            x: a tensor of shape [batch size, sequence len, input size].

        Returns:
            outputs: a tensor of shape [batch size, sequence len, hidden size],
                containing the output features for each time step.
            pooled_outputs: a tensor of shape [batch size, hidden size], containing
                the pooled output features.
        """
        # [batch size, input size, sequence len]
        x = x.permute(0, 2, 1)
        for idx in range(len(self.cnn)):
            x = self.cnn[f"CNN-{idx}"](x)
        outputs = x.permute(0, 2, 1)
        # pooling
        pooled_outputs = self.pooling(x).squeeze(-1)
        return outputs, pooled_outputs


class CNN(BaseModel):
    """Convolutional neural network model.

    This model applies a separate CNN layer for each feature, and then concatenates
    the final hidden states of each CNN layer. The concatenated hidden states are
    then fed into a fully connected layer to make predictions.

    Note:
        We use separate CNN layers for different feature_keys.
        Currentluy, we automatically support different input formats:
            - code based input (need to use the embedding table later)
            - float/int based value input
        We follow the current convention for the CNN model:
            - case 1. [code1, code2, code3, ...]
                - we will assume the code follows the order; our model will encode
                each code into a vector and apply CNN on the code level
            - case 2. [[code1, code2]] or [[code1, code2], [code3, code4, code5], ...]
                - we will assume the inner bracket follows the order; our model first
                use the embedding table to encode each code into a vector and then use
                average/mean pooling to get one vector for one inner bracket; then use
                CNN one the braket level
            - case 3. [[1.5, 2.0, 0.0]] or [[1.5, 2.0, 0.0], [8, 1.2, 4.5], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run CNN directly
                on the inner bracket level, similar to case 1 after embedding table
            - case 4. [[[1.5, 2.0, 0.0]]] or [[[1.5, 2.0, 0.0], [8, 1.2, 4.5]], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run CNN directly
                on the inner bracket level, similar to case 2 after embedding table

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension. Default is 128.
        **kwargs: other parameters for the CNN layer.

    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
        ...             "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        ...             "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
        ...             "list_list_vectors": [
        ...                 [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
        ...                 [[7.7, 8.5, 9.4]],
        ...             ],
        ...             "label": 1,
        ...         },
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-1",
        ...             "list_codes": [
        ...                 "55154191800",
        ...                 "551541928",
        ...                 "55154192800",
        ...                 "705182798",
        ...                 "70518279800",
        ...             ],
        ...             "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7]],
        ...             "list_list_codes": [["A04A", "B035", "C129"], ["A07B", "A07C"]],
        ...             "list_list_vectors": [
        ...                 [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6]],
        ...                 [[7.7, 8.4, 1.3]],
        ...             ],
        ...             "label": 0,
        ...         },
        ...     ]
        >>> dataset = SampleEHRDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import CNN
        >>> model = CNN(
        ...         dataset=dataset,
        ...         feature_keys=[
        ...             "list_codes",
        ...             "list_vectors",
        ...             "list_list_codes",
        ...             "list_list_vectors",
        ...         ],
        ...         label_key="label",
        ...         mode="binary",
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(0.8872, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),
            'y_prob': tensor([[0.5008], [0.6614]], grad_fn=<SigmoidBackward0>),
            'y_true': tensor([[1.], [0.]]),
            'logit': tensor([[0.0033], [0.6695]], grad_fn=<AddmmBackward0>)
        }
        >>>
    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        **kwargs,
    ):
        super(CNN, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

        # validate kwargs for CNN layer
        if "input_size" in kwargs:
            raise ValueError("input_size is determined by embedding_dim")
        if "hidden_size" in kwargs:
            raise ValueError("hidden_size is determined by hidden_dim")

        # the key of self.feat_tokenizers only contains the code based inputs
        self.feat_tokenizers = {}
        self.label_tokenizer = self.get_label_tokenizer()
        # the key of self.embeddings only contains the code based inputs
        self.embeddings = nn.ModuleDict()
        # the key of self.linear_layers only contains the float/int based inputs
        self.linear_layers = nn.ModuleDict()

        # add feature CNN layers
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            # sanity check
            if input_info["type"] not in [str, float, int]:
                raise ValueError(
                    "CNN only supports str code, float and int as input types"
                )
            elif (input_info["type"] == str) and (input_info["dim"] not in [2, 3]):
                raise ValueError(
                    "CNN only supports 2-dim or 3-dim str code as input types"
                )
            elif (input_info["type"] in [float, int]) and (
                input_info["dim"] not in [2, 3]
            ):
                raise ValueError(
                    "CNN only supports 2-dim or 3-dim float and int as input types"
                )
            # for code based input, we need Type
            # for float/int based input, we need Type, input_dim
            self.add_feature_transform_layer(feature_key, input_info)

        self.cnn = nn.ModuleDict()
        for feature_key in feature_keys:
            self.cnn[feature_key] = CNNLayer(
                input_size=embedding_dim, hidden_size=hidden_dim, **kwargs
            )
        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(len(self.feature_keys) * self.hidden_dim, output_size)

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        The label `kwargs[self.label_key]` is a list of labels for each patient.

        Args:
            **kwargs: keyword arguments for the model. The keys must contain
                all the feature keys and the label key.

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the loss.
                y_prob: a tensor representing the predicted probabilities.
                y_true: a tensor representing the true labels.
        """
        patient_emb = []
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            dim_, type_ = input_info["dim"], input_info["type"]

            # for case 1: [code1, code2, code3, ...]
            if (dim_ == 2) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_2d(
                    kwargs[feature_key]
                )
                # (patient, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, event, embedding_dim)
                x = self.embeddings[feature_key](x)

            # for case 2: [[code1, code2], [code3, ...], ...]
            elif (dim_ == 3) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_3d(
                    kwargs[feature_key]
                )
                # (patient, visit, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, visit, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)

            # for case 3: [[1.5, 2.0, 0.0], ...]
            elif (dim_ == 2) and (type_ in [float, int]):
                x, _ = self.padding2d(kwargs[feature_key])
                # (patient, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, event, embedding_dim)
                x = self.linear_layers[feature_key](x)

            # for case 4: [[[1.5, 2.0, 0.0], [1.8, 2.4, 6.0]], ...]
            elif (dim_ == 3) and (type_ in [float, int]):
                x, _ = self.padding3d(kwargs[feature_key])
                # (patient, visit, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                x = self.linear_layers[feature_key](x)

            else:
                raise NotImplementedError

            _, x = self.cnn[feature_key](x)
            patient_emb.append(x)

        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)
        y_prob = self.prepare_y_prob(logits)
        results = {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": y_true,
            "logit": logits,
        }
        if kwargs.get("embed", False):
            results["embed"] = patient_emb
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleEHRDataset

    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            # "single_vector": [1, 2, 3],
            "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
            "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
            "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
            "list_list_vectors": [
                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
                [[7.7, 8.5, 9.4]],
            ],
            "label": 1,
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-1",
            # "single_vector": [1, 5, 8],
            "list_codes": [
                "55154191800",
                "551541928",
                "55154192800",
                "705182798",
                "70518279800",
            ],
            "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
            "list_list_codes": [["A04A", "B035", "C129"]],
            "list_list_vectors": [
                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
            ],
            "label": 0,
        },
    ]

    # dataset
    dataset = SampleEHRDataset(samples=samples, dataset_name="test")
    print(dataset.input_info)

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = CNN(
        dataset=dataset,
        feature_keys=[
            "list_codes",
            "list_vectors",
            "list_list_codes",
            "list_list_vectors",
        ],
        label_key="label",
        mode="binary",
    )

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for concare.py:
import math
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.utils.rnn as rnn_utils

from pyhealth.datasets import SampleEHRDataset
from pyhealth.models import BaseModel
from pyhealth.models.utils import get_last_visit


class FinalAttentionQKV(nn.Module):
    def __init__(
        self,
        attention_input_dim: int,
        attention_hidden_dim: int,
        attention_type: str = "add",
        dropout: float = 0.5,
    ):
        super(FinalAttentionQKV, self).__init__()

        self.attention_type = attention_type
        self.attention_hidden_dim = attention_hidden_dim
        self.attention_input_dim = attention_input_dim

        self.W_q = nn.Linear(attention_input_dim, attention_hidden_dim)
        self.W_k = nn.Linear(attention_input_dim, attention_hidden_dim)
        self.W_v = nn.Linear(attention_input_dim, attention_hidden_dim)

        self.W_out = nn.Linear(attention_hidden_dim, 1)

        self.b_in = nn.Parameter(
            torch.zeros(
                1,
            )
        )
        self.b_out = nn.Parameter(
            torch.zeros(
                1,
            )
        )

        nn.init.kaiming_uniform_(self.W_q.weight, a=math.sqrt(5))
        nn.init.kaiming_uniform_(self.W_k.weight, a=math.sqrt(5))
        nn.init.kaiming_uniform_(self.W_v.weight, a=math.sqrt(5))
        nn.init.kaiming_uniform_(self.W_out.weight, a=math.sqrt(5))

        self.Wh = nn.Parameter(
            torch.randn(2 * attention_input_dim, attention_hidden_dim)
        )
        self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))
        self.ba = nn.Parameter(
            torch.zeros(
                1,
            )
        )

        nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))
        nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))

        self.dropout = nn.Dropout(p=dropout)
        self.tanh = nn.Tanh()
        self.softmax = nn.Softmax(dim=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, input):

        (
            batch_size,
            time_step,
            input_dim,
        ) = input.size()  # batch_size * input_dim + 1 * hidden_dim(i)
        input_q = self.W_q(input[:, -1, :])  # b h
        input_k = self.W_k(input)  # b t h
        input_v = self.W_v(input)  # b t h

        if self.attention_type == "add":  # B*T*I  @ H*I

            q = torch.reshape(
                input_q, (batch_size, 1, self.attention_hidden_dim)
            )  # B*1*H
            h = q + input_k + self.b_in  # b t h
            h = self.tanh(h)  # B*T*H
            e = self.W_out(h)  # b t 1
            e = torch.reshape(e, (batch_size, time_step))  # b t

        elif self.attention_type == "mul":
            q = torch.reshape(
                input_q, (batch_size, self.attention_hidden_dim, 1)
            )  # B*h 1
            e = torch.matmul(input_k, q).squeeze()  # b t

        elif self.attention_type == "concat":
            q = input_q.unsqueeze(1).repeat(1, time_step, 1)  # b t h
            k = input_k
            c = torch.cat((q, k), dim=-1)  # B*T*2I
            h = torch.matmul(c, self.Wh)
            h = self.tanh(h)
            e = torch.matmul(h, self.Wa) + self.ba  # B*T*1
            e = torch.reshape(e, (batch_size, time_step))  # b t
        else:
            raise ValueError(
                "Unknown attention type: {}, please use add, mul, concat".format(
                    self.attention_type
                )
            )

        a = self.softmax(e)  # B*T
        if self.dropout is not None:
            a = self.dropout(a)
        v = torch.matmul(a.unsqueeze(1), input_v).squeeze()  # B*I

        return v, a


class PositionwiseFeedForward(nn.Module):  # new added
    "Implements FFN equation."

    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(torch.relu(self.w_1(x)))), None


class PositionalEncoding(nn.Module):  # new added / not use anymore
    "Implement the PE function."

    def __init__(self, d_model, dropout, max_len=400):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0.0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0.0, d_model, 2) * -(math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x):
        pos = self.pe[:, : x.size(1)].clone().requires_grad_(False)
        x = x + pos
        return self.dropout(x)


class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0):
        "Take in model size and number of heads."
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = nn.ModuleList(
            [nn.Linear(d_model, self.d_k * self.h) for _ in range(3)]
        )
        self.final_linear = nn.Linear(d_model, d_model)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def attention(self, query, key, value, mask=None, dropout=None):
        "Compute 'Scaled Dot Product Attention'"
        d_k = query.size(-1)  # b h t d_k
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)  # b h t t
        if mask is not None:  # 1 1 t t
            scores = scores.masked_fill(mask == 0, -1e9)  # b h t t 下三角
        p_attn = torch.softmax(scores, dim=-1)  # b h t t
        if dropout is not None:
            p_attn = dropout(p_attn)
        return torch.matmul(p_attn, value), p_attn  # b h t v (d_k)

    def cov(self, m, y=None):
        if y is not None:
            m = torch.cat((m, y), dim=0)
        m_exp = torch.mean(m, dim=1)
        x = m - m_exp[:, None]
        cov = 1 / (x.size(1) - 1) * x.mm(x.t())
        return cov

    def forward(self, query, key, value, mask=None):
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)  # 1 1 t t

        nbatches = query.size(0)  # b
        input_dim = query.size(1)  # i+1
        feature_dim = query.size(1)  # i+1

        # input size -> # batch_size * d_input * hidden_dim

        # d_model => h * d_k
        query, key, value = [
            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
            for l, x in zip(self.linears, (query, key, value))
        ]  # b num_head d_input d_k

        x, self.attn = self.attention(
            query, key, value, mask=mask, dropout=self.dropout
        )  # b num_head d_input d_v (d_k)

        x = (
            x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        )  # batch_size * d_input * hidden_dim

        # DeCov
        DeCov_contexts = x.transpose(0, 1).transpose(1, 2)  # I+1 H B
        Covs = self.cov(DeCov_contexts[0, :, :])
        DeCov_loss = 0.5 * (
            torch.norm(Covs, p="fro") ** 2 - torch.norm(torch.diag(Covs)) ** 2
        )
        for i in range(feature_dim - 1):
            Covs = self.cov(DeCov_contexts[i + 1, :, :])
            DeCov_loss += 0.5 * (
                torch.norm(Covs, p="fro") ** 2 - torch.norm(torch.diag(Covs)) ** 2
            )

        return self.final_linear(x), DeCov_loss


class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-7):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2


class SublayerConnection(nn.Module):
    """
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.
    """

    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        "Apply residual connection to any sublayer with the same size."
        returned_value = sublayer(self.norm(x))
        return x + self.dropout(returned_value[0]), returned_value[1]


class SingleAttention(nn.Module):
    def __init__(
        self,
        attention_input_dim,
        attention_hidden_dim,
        attention_type="add",
        time_aware=False,
    ):
        super(SingleAttention, self).__init__()

        self.attention_type = attention_type
        self.attention_hidden_dim = attention_hidden_dim
        self.attention_input_dim = attention_input_dim
        self.time_aware = time_aware

        # batch_time = torch.arange(0, batch_mask.size()[1], dtype=torch.float32).reshape(1, batch_mask.size()[1], 1)
        # batch_time = batch_time.repeat(batch_mask.size()[0], 1, 1)

        if attention_type == "add":
            if self.time_aware:
                # self.Wx = nn.Parameter(torch.randn(attention_input_dim+1, attention_hidden_dim))
                self.Wx = nn.Parameter(
                    torch.randn(attention_input_dim, attention_hidden_dim)
                )
                self.Wtime_aware = nn.Parameter(torch.randn(1, attention_hidden_dim))
                nn.init.kaiming_uniform_(self.Wtime_aware, a=math.sqrt(5))
            else:
                self.Wx = nn.Parameter(
                    torch.randn(attention_input_dim, attention_hidden_dim)
                )
            self.Wt = nn.Parameter(
                torch.randn(attention_input_dim, attention_hidden_dim)
            )
            self.bh = nn.Parameter(
                torch.zeros(
                    attention_hidden_dim,
                )
            )
            self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))
            self.ba = nn.Parameter(
                torch.zeros(
                    1,
                )
            )

            nn.init.kaiming_uniform_(self.Wd, a=math.sqrt(5))
            nn.init.kaiming_uniform_(self.Wx, a=math.sqrt(5))
            nn.init.kaiming_uniform_(self.Wt, a=math.sqrt(5))
            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))
        elif attention_type == "mul":
            self.Wa = nn.Parameter(
                torch.randn(attention_input_dim, attention_input_dim)
            )
            self.ba = nn.Parameter(
                torch.zeros(
                    1,
                )
            )

            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))
        elif attention_type == "concat":
            if self.time_aware:
                self.Wh = nn.Parameter(
                    torch.randn(2 * attention_input_dim + 1, attention_hidden_dim)
                )
            else:
                self.Wh = nn.Parameter(
                    torch.randn(2 * attention_input_dim, attention_hidden_dim)
                )

            self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))
            self.ba = nn.Parameter(
                torch.zeros(
                    1,
                )
            )

            nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))
            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))

        elif attention_type == "new":
            self.Wt = nn.Parameter(
                torch.randn(attention_input_dim, attention_hidden_dim)
            )
            self.Wx = nn.Parameter(
                torch.randn(attention_input_dim, attention_hidden_dim)
            )

            self.rate = nn.Parameter(torch.zeros(1) + 0.8)
            nn.init.kaiming_uniform_(self.Wx, a=math.sqrt(5))
            nn.init.kaiming_uniform_(self.Wt, a=math.sqrt(5))

        else:
            raise RuntimeError(
                "Wrong attention type. Please use 'add', 'mul', 'concat' or 'new'."
            )

        self.tanh = nn.Tanh()
        self.softmax = nn.Softmax(dim=1)
        self.sigmoid = nn.Sigmoid()
        self.relu = nn.ReLU()

    def forward(self, input, mask, device):

        (
            batch_size,
            time_step,
            input_dim,
        ) = input.size()  # batch_size * time_step * hidden_dim(i)

        time_decays = (
            torch.tensor(range(time_step - 1, -1, -1), dtype=torch.float32)
            .unsqueeze(-1)
            .unsqueeze(0)
            .to(device=device)
        )  # 1*t*1
        b_time_decays = time_decays.repeat(batch_size, 1, 1) + 1  # b t 1

        if self.attention_type == "add":  # B*T*I  @ H*I
            last_visit = get_last_visit(input, mask)
            q = torch.matmul(last_visit, self.Wt)  # b h
            q = torch.reshape(q, (batch_size, 1, self.attention_hidden_dim))  # B*1*H
            if self.time_aware == True:
                k = torch.matmul(input, self.Wx)  # b t h
                time_hidden = torch.matmul(b_time_decays, self.Wtime_aware)  # b t h
            else:
                k = torch.matmul(input, self.Wx)  # b t h
            h = q + k + self.bh  # b t h
            if self.time_aware:
                h += time_hidden
            h = self.tanh(h)  # B*T*H
            e = torch.matmul(h, self.Wa) + self.ba  # B*T*1
            e = torch.reshape(e, (batch_size, time_step))  # b t
        elif self.attention_type == "mul":
            last_visit = get_last_visit(input, mask)
            e = torch.matmul(last_visit, self.Wa)  # b i
            e = (
                torch.matmul(e.unsqueeze(1), input.permute(0, 2, 1)).reshape(
                    batch_size, time_step
                )
                + self.ba
            )  # b t
        elif self.attention_type == "concat":
            last_visit = get_last_visit(input, mask)
            q = last_visit.unsqueeze(1).repeat(1, time_step, 1)  # b t i
            k = input
            c = torch.cat((q, k), dim=-1)  # B*T*2I
            if self.time_aware:
                c = torch.cat((c, b_time_decays), dim=-1)  # B*T*2I+1
            h = torch.matmul(c, self.Wh)
            h = self.tanh(h)
            e = torch.matmul(h, self.Wa) + self.ba  # B*T*1
            e = torch.reshape(e, (batch_size, time_step))  # b t

        elif self.attention_type == "new":
            last_visit = get_last_visit(input, mask)
            q = torch.matmul(last_visit, self.Wt)  # b h
            q = torch.reshape(q, (batch_size, 1, self.attention_hidden_dim))  # B*1*H
            k = torch.matmul(input, self.Wx)  # b t h
            dot_product = torch.matmul(q, k.transpose(1, 2)).reshape(
                batch_size, time_step
            )  # b t
            denominator = self.sigmoid(self.rate) * (
                torch.log(2.72 + (1 - self.sigmoid(dot_product)))
                * (b_time_decays.reshape(batch_size, time_step))
            )
            e = self.relu(self.sigmoid(dot_product) / (denominator))  # b * t
        else:
            raise ValueError(
                "Wrong attention type. Plase use 'add', 'mul', 'concat' or 'new'."
            )

        if mask is not None:
            e = e.masked_fill(mask == 0, -1e9)
        a = self.softmax(e)  # B*T
        v = torch.matmul(a.unsqueeze(1), input).reshape(batch_size, input_dim)  # B*I

        return v, a


class ConCareLayer(nn.Module):
    """ConCare layer.

    Paper: Liantao Ma et al. Concare: Personalized clinical feature embedding via capturing the healthcare context. AAAI 2020.

    This layer is used in the ConCare model. But it can also be used as a
    standalone layer.

    Args:
        input_dim: dynamic feature size.
        static_dim: static feature size, if 0, then no static feature is used.
        hidden_dim: hidden dimension of the channel-wise GRU, default 128.
        transformer_hidden: hidden dimension of the transformer, default 128.
        num_head: number of heads in the transformer, default 4.
        pe_hidden: hidden dimension of the positional encoding, default 64.
        dropout: dropout rate, default 0.5.

    Examples:
        >>> from pyhealth.models import ConCareLayer
        >>> input = torch.randn(3, 128, 64)  # [batch size, sequence len, feature_size]
        >>> layer = ConCareLayer(64)
        >>> c, _ = layer(input)
        >>> c.shape
        torch.Size([3, 128])
    """

    def __init__(
        self,
        input_dim: int,
        static_dim: int = 0,
        hidden_dim: int = 128,
        num_head: int = 4,
        pe_hidden: int = 64,
        dropout: int = 0.5,
    ):
        super(ConCareLayer, self).__init__()

        # hyperparameters
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim  # d_model
        self.transformer_hidden = hidden_dim
        self.num_head = num_head
        self.pe_hidden = pe_hidden
        # self.output_dim = output_dim
        self.dropout = dropout
        self.static_dim = static_dim

        # layers
        self.PositionalEncoding = PositionalEncoding(
            self.transformer_hidden, dropout=0, max_len=400
        )

        self.GRUs = nn.ModuleList(
            [
                nn.GRU(1, self.hidden_dim, batch_first=True)
                for _ in range(self.input_dim)
            ]
        )
        self.LastStepAttentions = nn.ModuleList(
            [
                SingleAttention(
                    self.hidden_dim,
                    8,
                    attention_type="new",
                    time_aware=True,
                )
                for _ in range(self.input_dim)
            ]
        )

        self.FinalAttentionQKV = FinalAttentionQKV(
            self.hidden_dim,
            self.hidden_dim,
            attention_type="mul",
            dropout=self.dropout,
        )

        self.MultiHeadedAttention = MultiHeadedAttention(
            self.num_head, self.transformer_hidden, dropout=self.dropout
        )
        self.SublayerConnection = SublayerConnection(
            self.transformer_hidden, dropout=self.dropout
        )

        self.PositionwiseFeedForward = PositionwiseFeedForward(
            self.transformer_hidden, self.pe_hidden, dropout=0.1
        )

        if self.static_dim > 0:
            self.demo_proj_main = nn.Linear(self.static_dim, self.hidden_dim)

        self.dropout = nn.Dropout(p=self.dropout)
        self.tanh = nn.Tanh()
        self.softmax = nn.Softmax()
        self.sigmoid = nn.Sigmoid()
        self.relu = nn.ReLU()

    def concare_encoder(self, input, static=None, mask=None):
        # input shape [batch_size, timestep, feature_dim]

        if self.static_dim > 0:
            demo_main = self.tanh(self.demo_proj_main(static)).unsqueeze(
                1
            )  # b hidden_dim

        batch_size = input.size(0)
        time_step = input.size(1)
        feature_dim = input.size(2)

        if self.transformer_hidden % self.num_head != 0:
            raise ValueError("transformer_hidden must be divisible by num_head")

        # forward
        GRU_embeded_input = self.GRUs[0](
            input[:, :, 0].unsqueeze(-1).to(device=input.device),
            torch.zeros(batch_size, self.hidden_dim)
            .to(device=input.device)
            .unsqueeze(0),
        )[
            0
        ]  # b t h
        Attention_embeded_input = self.LastStepAttentions[0](
            GRU_embeded_input, mask, input.device
        )[0].unsqueeze(
            1
        )  # b 1 h

        for i in range(feature_dim - 1):
            embeded_input = self.GRUs[i + 1](
                input[:, :, i + 1].unsqueeze(-1),
                torch.zeros(batch_size, self.hidden_dim)
                .to(device=input.device)
                .unsqueeze(0),
            )[
                0
            ]  # b 1 h
            embeded_input = self.LastStepAttentions[i + 1](
                embeded_input, mask, input.device
            )[0].unsqueeze(
                1
            )  # b 1 h
            Attention_embeded_input = torch.cat(
                (Attention_embeded_input, embeded_input), 1
            )  # b i h

        if self.static_dim > 0:
            Attention_embeded_input = torch.cat(
                (Attention_embeded_input, demo_main), 1
            )  # b i+1 h
        posi_input = self.dropout(
            Attention_embeded_input
        )  # batch_size * d_input+1 * hidden_dim

        contexts = self.SublayerConnection(
            posi_input,
            lambda x: self.MultiHeadedAttention(
                posi_input, posi_input, posi_input, None
            ),
        )  # # batch_size * d_input * hidden_dim

        DeCov_loss = contexts[1]
        contexts = contexts[0]

        contexts = self.SublayerConnection(
            contexts, lambda x: self.PositionwiseFeedForward(contexts)
        )[0]

        weighted_contexts, a = self.FinalAttentionQKV(contexts)
        return weighted_contexts, DeCov_loss

    def forward(
        self,
        x: torch.tensor,
        static: Optional[torch.tensor] = None,
        mask: Optional[torch.tensor] = None,
    ) -> Tuple[torch.tensor]:
        """Forward propagation.

        Args:
            x: a tensor of shape [batch size, sequence len, input_dim].
            static: a tensor of shape [batch size, static_dim].
            mask: an optional tensor of shape [batch size, sequence len], where
                1 indicates valid and 0 indicates invalid.

        Returns:
            output: a tensor of shape [batch size, fusion_dim] representing the
                patient embedding.
            decov: the decov loss value
        """
        # rnn will only apply dropout between layers
        batch_size, time_steps, _ = x.size()
        out = torch.zeros((batch_size, time_steps, self.hidden_dim))
        out, decov = self.concare_encoder(x, static, mask)
        out = self.dropout(out)
        return out, decov


class ConCare(BaseModel):
    """ConCare model.

    Paper: Liantao Ma et al. Concare: Personalized clinical feature embedding via capturing the healthcare context. AAAI 2020.

    Note:
        We use separate ConCare layers for different feature_keys.
        Currently, we automatically support different input formats:
            - code based input (need to use the embedding table later)
            - float/int based value input
        If you need the interpretable feature correlations provided by the ConCare model calculates the , we do not recommend use embeddings for the input features.
        We follow the current convention for the ConCare model:
            - case 1. [code1, code2, code3, ...]
                - we will assume the code follows the order; our model will encode
                each code into a vector and apply ConCare on the code level
            - case 2. [[code1, code2]] or [[code1, code2], [code3, code4, code5], ...]
                - we will assume the inner bracket follows the order; our model first
                use the embedding table to encode each code into a vector and then use
                average/mean pooling to get one vector for one inner bracket; then use
                ConCare one the braket level
            - case 3. [[1.5, 2.0, 0.0]] or [[1.5, 2.0, 0.0], [8, 1.2, 4.5], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run ConCare directly
                on the inner bracket level, similar to case 1 after embedding table
            - case 4. [[[1.5, 2.0, 0.0]]] or [[[1.5, 2.0, 0.0], [8, 1.2, 4.5]], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run ConCare directly
                on the inner bracket level, similar to case 2 after embedding table

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        static_keys: the key in samples to use as static features, e.g. "demographics". Default is None.
                     we only support numerical static features.
        use_embedding: list of bools indicating whether to use embedding for each feature type,
            e.g. [True, False].
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension. Default is 128.
        **kwargs: other parameters for the ConCare layer.


    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
        ...             "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        ...             "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
        ...             "list_list_vectors": [
        ...                 [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
        ...                 [[7.7, 8.5, 9.4]],
        ...             ],
        ...             "demographic": [0.0, 2.0, 1.5],
        ...             "label": 1,
        ...         },
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-1",
        ...             "list_codes": [
        ...                 "55154191800",
        ...                 "551541928",
        ...                 "55154192800",
        ...                 "705182798",
        ...                 "70518279800",
        ...             ],
        ...             "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
        ...             "list_list_codes": [["A04A", "B035", "C129"]],
        ...             "list_list_vectors": [
        ...                 [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
        ...             ],
        ...             "demographic": [0.0, 2.0, 1.5],
        ...             "label": 0,
        ...         },
        ...     ]
        >>> dataset = SampleEHRDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import ConCare
        >>> model = ConCare(
        ...         dataset=dataset,
        ...         feature_keys=[
        ...             "list_codes",
        ...             "list_vectors",
        ...             "list_list_codes",
        ...             "list_list_vectors",
        ...         ],
        ...         label_key="label",
        ...         static_key="demographic",
        ...         use_embedding=[True, False, True, False],
        ...         mode="binary"
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(9.5541, grad_fn=<AddBackward0>), 
            'y_prob': tensor([[0.5323], [0.5363]], grad_fn=<SigmoidBackward0>), 
            'y_true': tensor([[1.], [0.]]), 
            'logit': tensor([[0.1293], [0.1454]], grad_fn=<AddmmBackward0>)
        }
        >>>

    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        use_embedding: List[bool],
        static_key: Optional[str] = None,
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        **kwargs,
    ):
        super(ConCare, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim
        self.use_embedding = use_embedding
        self.hidden_dim = hidden_dim

        # validate kwargs for ConCare layer
        if "feature_size" in kwargs:
            raise ValueError("feature_size is determined by embedding_dim")

        # the key of self.feat_tokenizers only contains the code based inputs
        self.feat_tokenizers = {}
        self.static_key = static_key
        self.label_tokenizer = self.get_label_tokenizer()
        # the key of self.embeddings only contains the code based inputs
        self.embeddings = nn.ModuleDict()
        # the key of self.linear_layers only contains the float/int based inputs
        self.linear_layers = nn.ModuleDict()

        self.static_dim = 0
        if self.static_key is not None:
            self.static_dim = self.dataset.input_info[self.static_key]["len"]

        self.concare = nn.ModuleDict()
        # add feature ConCare layers
        for idx, feature_key in enumerate(self.feature_keys):
            input_info = self.dataset.input_info[feature_key]
            # sanity check
            if input_info["type"] not in [str, float, int]:
                raise ValueError(
                    "ConCare only supports str code, float and int as input types"
                )
            elif (input_info["type"] == str) and (input_info["dim"] not in [2, 3]):
                raise ValueError(
                    "ConCare only supports 2-dim or 3-dim str code as input types"
                )
            elif (input_info["type"] == str) and (use_embedding[idx] == False):
                raise ValueError(
                    "ConCare only supports embedding for str code as input types"
                )
            elif (input_info["type"] in [float, int]) and (
                input_info["dim"] not in [2, 3]
            ):
                raise ValueError(
                    "ConCare only supports 2-dim or 3-dim float and int as input types"
                )

            # for code based input, we need Type
            # for float/int based input, we need Type, input_dim
            if use_embedding[idx]:
                self.add_feature_transform_layer(feature_key, input_info)
                self.concare[feature_key] = ConCareLayer(
                    input_dim=embedding_dim,
                    static_dim=self.static_dim,
                    hidden_dim=self.hidden_dim,
                    **kwargs,
                )
            else:
                self.concare[feature_key] = ConCareLayer(
                    input_dim=input_info["len"],
                    static_dim=self.static_dim,
                    hidden_dim=self.hidden_dim,
                    **kwargs,
                )

        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(len(self.feature_keys) * self.hidden_dim, output_size)

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        The label `kwargs[self.label_key]` is a list of labels for each patient.

        Args:
            **kwargs: keyword arguments for the model. The keys must contain
                all the feature keys and the label key.

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the final loss.
                loss_task: a scalar tensor representing the task loss.
                loss_decov: a scalar tensor representing the decov loss.
                y_prob: a tensor representing the predicted probabilities.
                y_true: a tensor representing the true labels.
        """
        patient_emb = []
        decov_loss = 0
        for idx, feature_key in enumerate(self.feature_keys):
            input_info = self.dataset.input_info[feature_key]
            dim_, type_ = input_info["dim"], input_info["type"]

            # for case 1: [code1, code2, code3, ...]
            if (dim_ == 2) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_2d(
                    kwargs[feature_key]
                )
                # (patient, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, event)
                mask = torch.any(x !=0, dim=2)

            # for case 2: [[code1, code2], [code3, ...], ...]
            elif (dim_ == 3) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_3d(
                    kwargs[feature_key]
                )
                # (patient, visit, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, visit, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                # (patient, visit)
                mask = torch.any(x !=0, dim=2)

            # for case 3: [[1.5, 2.0, 0.0], ...]
            elif (dim_ == 2) and (type_ in [float, int]):
                x, mask = self.padding2d(kwargs[feature_key])
                # (patient, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, event, embedding_dim)
                if self.use_embedding[idx]:
                    x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask.bool().to(self.device)

            # for case 4: [[[1.5, 2.0, 0.0], [1.8, 2.4, 6.0]], ...]
            elif (dim_ == 3) and (type_ in [float, int]):
                x, mask = self.padding3d(kwargs[feature_key])
                # (patient, visit, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)

                if self.use_embedding[idx]:
                    x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask[:, :, 0]
                mask = mask.bool().to(self.device)
            else:
                raise NotImplementedError

            if self.static_dim > 0:
                static = torch.tensor(
                    kwargs[self.static_key], dtype=torch.float, device=self.device
                )
                x, decov = self.concare[feature_key](x, static=static, mask=mask)
            else:
                x, decov = self.concare[feature_key](x, mask=mask)
            patient_emb.append(x)
            decov_loss += decov

        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss_task = self.get_loss_function()(logits, y_true)
        loss = decov_loss + loss_task
        y_prob = self.prepare_y_prob(logits)
        results = {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": y_true,
            'logit': logits,
        }
        if kwargs.get('embed', False):
            results['embed'] = patient_emb
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleEHRDataset

    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            # "single_vector": [1, 2, 3],
            "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
            "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
            "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
            "list_list_vectors": [
                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
                [[7.7, 8.5, 9.4]],
            ],
            "label": 1,
            "demographic": [1.0, 2.0, 1.3],
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-1",
            # "single_vector": [1, 5, 8],
            "list_codes": [
                "55154191800",
                "551541928",
                "55154192800",
                "705182798",
                "70518279800",
            ],
            "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
            "list_list_codes": [["A04A", "B035", "C129"]],
            "list_list_vectors": [
                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
            ],
            "label": 0,
            "demographic": [1.0, 2.0, 1.3],
        },
    ]

    # dataset
    dataset = SampleEHRDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = ConCare(
        dataset=dataset,
        feature_keys=[
            "list_codes",
            "list_vectors",
            "list_list_codes",
            # "list_list_vectors",
        ],
        static_key="demographic",
        label_key="label",
        use_embedding=[True, False, True],
        mode="binary",
        hidden_dim=64,
    )

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for contrawr.py:
import functools
import math
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn

from pyhealth.datasets import BaseSignalDataset
from pyhealth.models import BaseModel


class ResBlock2D(nn.Module):
    """Convolutional Residual Block 2D

    This block stacks two convolutional layers with batch normalization,
    max pooling, dropout, and residual connection.

    Args:
        in_channels: number of input channels.
        out_channels: number of output channels.
        stride: stride of the convolutional layers.
        downsample: whether to use a downsampling residual connection.
        pooling: whether to use max pooling.

    Example:
        >>> import torch
        >>> from pyhealth.models import ResBlock2D
        >>>
        >>> model = ResBlock2D(6, 16, 1, True, True)
        >>> input_ = torch.randn((16, 6, 28, 150))  # (batch, channel, height, width)
        >>> output = model(input_)
        >>> output.shape
        torch.Size([16, 16, 14, 75])
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        stride: int = 2,
        downsample: bool = True,
        pooling: bool = True,
    ):
        super(ResBlock2D, self).__init__()
        self.conv1 = nn.Conv2d(
            in_channels, out_channels, kernel_size=3, stride=stride, padding=1
        )
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ELU()
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.maxpool = nn.MaxPool2d(2, stride=2)
        self.downsample = nn.Sequential(
            nn.Conv2d(
                in_channels, out_channels, kernel_size=3, stride=stride, padding=1
            ),
            nn.BatchNorm2d(out_channels),
        )
        self.downsampleOrNot = downsample
        self.pooling = pooling
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        """Forward propagation.

        Args:
            x: input tensor of shape (batch_size, in_channels, height, width).

        Returns:
            out: output tensor of shape (batch_size, out_channels, *, *).
        """
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if self.downsampleOrNot:
            residual = self.downsample(x)
            out += residual
        if self.pooling:
            out = self.maxpool(out)
        out = self.dropout(out)
        return out


class ContraWR(BaseModel):
    """The encoder model of ContraWR (a supervised model, STFT + 2D CNN layers)

    Paper: Yang, Chaoqi, Danica Xiao, M. Brandon Westover, and Jimeng Sun.
    "Self-supervised eeg representation learning for automatic sleep staging."
    arXiv preprint arXiv:2110.15278 (2021).

    Note:
        We use one encoder to handle multiple channel together.

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension. Default is 128.
        **kwargs: other parameters for the Deepr layer.

    Examples:
        >>> from pyhealth.datasets import SampleSignalDataset
        >>> samples = [
        ...         {
        ...             "record_id": "SC4001-0",
        ...             "patient_id": "SC4001",
        ...             "epoch_path": "/home/chaoqiy2/.cache/pyhealth/datasets/2f06a9232e54254cbcb4b62624294d71/SC4001-0.pkl",
        ...             "label": "W",
        ...         },
        ...         {
        ...             "record_id": "SC4001-1",
        ...             "patient_id": "SC4001",
        ...             "epoch_path": "/home/chaoqiy2/.cache/pyhealth/datasets/2f06a9232e54254cbcb4b62624294d71/SC4001-1.pkl",
        ...             "label": "R",
        ...         }
        ...     ]
        >>> dataset = SampleSignalDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import ContraWR
        >>> model = ContraWR(
        ...         dataset=dataset,
        ...         feature_keys=["signal"], # dataloader will load the signal from "epoch_path" and put it in "signal"
        ...         label_key="label",
        ...         mode="multiclass",
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(2.8425, device='cuda:0', grad_fn=<NllLossBackward0>),
            'y_prob': tensor([[0.9345, 0.0655],
                            [0.9482, 0.0518]], device='cuda:0', grad_fn=<SoftmaxBackward0>),
            'y_true': tensor([1, 1], device='cuda:0'),
            'logit': tensor([[ 0.1472, -2.5104],
                            [2.1584, -0.7481]], device='cuda:0', grad_fn=<AddmmBackward0>)
        }
        >>>
    """

    def __init__(
        self,
        dataset: BaseSignalDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        n_fft: int = 128,
        **kwargs,
    ):
        super(ContraWR, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.n_fft = n_fft

        # TODO: Use more tokens for <gap> for different lengths once the input has such information
        self.label_tokenizer = self.get_label_tokenizer()

        # the ContraWR encoder
        channels, emb_size = self.cal_encoder_stat()
        self.encoder = nn.Sequential(
            *[
                ResBlock2D(channels[i], channels[i + 1], 2, True, True)
                for i in range(len(channels) - 1)
            ]
        )

        output_size = self.get_output_size(self.label_tokenizer)
        # the fully connected layer
        self.fc = nn.Linear(emb_size, output_size)

    def cal_encoder_stat(self):
        """obtain the convolution encoder initialization statistics

        Note:
            We show an example to illustrate the encoder statistics.
            input x:
                - torch.Size([5, 7, 3000])
            after stft transform
                - torch.Size([5, 7, 65, 90])
            we design the first CNN (out_channels = 8)
                - torch.Size([5, 8, 16, 22])
                - here: 8 * 16 * 22 > 256, we continute the convolution
            we design the second CNN (out_channels = 16)
                - torch.Size([5, 16, 4, 5])
                - here: 16 * 4 * 5 > 256, we continute the convolution
            we design the second CNN (out_channels = 32)
                - torch.Size([5, 32, 1, 1])
                - here: 32 * 1 * 1, we stop the convolution
            output:
                - channels = [7, 8, 16, 32]
                - emb_size = 32 * 1 * 1 = 32
        """

        print(f"\n=== Input data statistics ===")
        # obtain input signal size
        signal_info = self.dataset.input_info["signal"]
        in_channels, length = signal_info["n_channels"], signal_info["length"]
        # input signal size (batch, n_channels, length)
        print(f"n_channels: {in_channels}")
        print(f"length: {length}")

        # after stft transform (batch, n_channels, freq, time_steps)
        freq = self.n_fft // 2 + 1
        time_steps = (length - self.n_fft) // (self.n_fft // 4) + 1
        print(f"=== Spectrogram statistics ===")
        print(f"n_channels: {in_channels}")
        print(f"freq_dim: {freq}")
        print(f"time_steps: {time_steps}")

        if freq < 4 or time_steps < 4:
            raise ValueError("The input signal is too short or n_fft is too small.")

        # obtain stats at each cnn layer
        channels = [in_channels]
        cur_freq_dim = freq
        cur_time_dim = time_steps

        print(f"=== Convolution Statistics ===")
        while (cur_freq_dim >= 4 and cur_time_dim >= 4) and (
            len(channels) == 1 or cur_freq_dim * cur_time_dim * channels[-1] > 256
        ):
            channels.append(2 ** (math.floor(np.log2(channels[-1])) + 1))
            cur_freq_dim = (cur_freq_dim + 1) // 4
            cur_time_dim = (cur_time_dim + 1) // 4

            print(
                f"in_channels: {channels[-2]}, out_channels: {channels[-1]}, freq_dim: {cur_freq_dim}, time_steps: {cur_time_dim}"
            )
        print()

        emb_size = cur_freq_dim * cur_time_dim * channels[-1]
        return channels, emb_size

    def torch_stft(self, X):
        """torch short time fourier transform (STFT)

        Args:
            X: (batch, n_channels, length)

        Returns:
            signal: (batch, n_channels, freq, time_steps)
        """
        signal = []
        for s in range(X.shape[1]):
            spectral = torch.stft(
                X[:, s, :],
                n_fft=self.n_fft,
                hop_length=self.n_fft // 4,
                center=False,
                onesided=True,
                return_complex=False,
            )
            signal.append(spectral)

        signal1 = torch.stack(signal)[:, :, :, :, 0].permute(1, 0, 2, 3)
        signal2 = torch.stack(signal)[:, :, :, :, 1].permute(1, 0, 2, 3)
        signal = (signal1**2 + signal2**2) ** 0.5
        return signal

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:

        """Forward propagation."""
        # concat the info within one batch (batch, channel, length)
        x = torch.tensor(
            np.array(kwargs[self.feature_keys[0]]), device=self.device
        ).float()
        # obtain the stft spectrogram (batch, channel, freq, time step)
        x_spectrogram = self.torch_stft(x)
        # final layer embedding (batch, embedding)
        emb = self.encoder(x_spectrogram).view(x.shape[0], -1)

        # (patient, label_size)
        logits = self.fc(emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)
        y_prob = self.prepare_y_prob(logits)
        results = {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": y_true,
            "logit": logits,
        }
        if kwargs.get("embed", False):
            results["embed"] = emb
        return results


if __name__ == "__main__":
    """
    test the ResBlock2D
    """
    # import torch

    # input_ = torch.randn((16, 6, 3, 75))  # (batch, channel, height, width)
    # model = ResBlock2D(6, 16, 1, True, True)
    # output = model(input_)
    # print("input shape: ", input_.shape)
    # print("output.shape:", output.shape)

    # """
    # test ContraWR
    # """
    # from pyhealth.datasets import split_by_patient, get_dataloader
    # from pyhealth.trainer import Trainer
    # from pyhealth.datasets import SleepEDFDataset
    # from pyhealth.tasks import sleep_staging_sleepedf_fn

    # # step 1: load signal data
    # dataset = SleepEDFDataset(
    #     root="/srv/local/data/SLEEPEDF/sleep-edf-database-expanded-1.0.0/sleep-cassette",
    #     dev=True,
    #     refresh_cache=False,
    # )

    # # step 2: set task
    # sleep_staging_ds = dataset.set_task(sleep_staging_sleepedf_fn)
    # sleep_staging_ds.stat()
    # print(sleep_staging_ds.input_info)

    # # split dataset
    # train_dataset, val_dataset, test_dataset = split_by_patient(
    #     sleep_staging_ds, [0.6, 0.2, 0.2]
    # )
    # train_dataloader = get_dataloader(train_dataset, batch_size=5, shuffle=True)
    # val_dataloader = get_dataloader(val_dataset, batch_size=5, shuffle=False)
    # test_dataloader = get_dataloader(test_dataset, batch_size=5, shuffle=False)
    # print(
    #     "loader size: train/val/test",
    #     len(train_dataset),
    #     len(val_dataset),
    #     len(test_dataset),
    # )

    # batch = next(iter(train_dataloader))

    # # step 3: define model
    # model = ContraWR(
    #     sleep_staging_ds,
    #     feature_keys=["signal"],
    #     label_key="label",
    #     mode="multiclass",
    #     n_fft=128,
    # )

    # result = model(**batch)
    # print(result)

    """
    test ContraWR 2
    """
    from pyhealth.datasets import SampleSignalDataset, get_dataloader

    samples = [
        {
            "record_id": "SC4001-0",
            "patient_id": "SC4001",
            "epoch_path": "/home/chaoqiy2/.cache/pyhealth/datasets/2f06a9232e54254cbcb4b62624294d71/SC4001-0.pkl",
            "label": "W",
        },
        {
            "record_id": "SC4001-0",
            "patient_id": "SC4001",
            "epoch_path": "/home/chaoqiy2/.cache/pyhealth/datasets/2f06a9232e54254cbcb4b62624294d71/SC4001-1.pkl",
            "label": "R",
        },
    ]

    # dataset
    dataset = SampleSignalDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = ContraWR(
        dataset=dataset,
        feature_keys=["signal"],
        label_key="label",
        mode="multiclass",
    ).to("cuda:0")

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for deepr.py:
import functools
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn as nn

from pyhealth.datasets import BaseEHRDataset
from pyhealth.models import BaseModel


class DeeprLayer(nn.Module):
    """Deepr layer.

    Paper: P. Nguyen, T. Tran, N. Wickramasinghe and S. Venkatesh,
    " Deepr : A Convolutional Net for Medical Records," in IEEE Journal
    of Biomedical and Health Informatics, vol. 21, no. 1, pp. 22-30,
    Jan. 2017, doi: 10.1109/JBHI.2016.2633963.

    This layer is used in the Deepr model.

    Args:
        feature_size: embedding dim of codes (m in the original paper).
        window: sliding window (d in the original paper)
        hidden_size: number of conv filters (motif size, p, in the original paper)
    Examples:
        >>> from pyhealth.models import DeeprLayer
        >>> input = torch.randn(3, 128, 5)  # [batch size, sequence len, input_size]
        >>> layer = DeeprLayer(5, window=4, hidden_size=7) # window does not impact the output shape
        >>> outputs = layer(input)
        >>> outputs.shape
        torch.Size([3, 7])
    """

    def __init__(
        self,
        feature_size: int = 100,
        window: int = 1,
        hidden_size: int = 3,
    ):
        super(DeeprLayer, self).__init__()

        self.conv = torch.nn.Conv1d(
            feature_size, hidden_size, kernel_size=2 * window + 1
        )

    def forward(
        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Forward propagation.

        Args:
            x: a Tensor of shape [batch size, sequence len, input size].
            mask: an optional tensor of shape [batch size, sequence len], where
                            1 indicates valid and 0 indicates invalid.
        Returns:
            c: a Tensor of shape [batch size, hidden_size] representing the
                summarized vector.
        """
        if mask is not None:
            x = x * mask.unsqueeze(-1)
        x = x.permute(0, 2, 1)  # [batch size, input size, sequence len]
        x = torch.relu(self.conv(x))
        x = x.max(-1)[0]
        return x


def _flatten_and_fill_gap(gap_embedding, batch, device):
    """Helper function to fill <gap> embedding into a batch of data."""
    embed_dim = gap_embedding.shape[-1]
    batch = [
        [
            [torch.tensor(_, device=device, dtype=torch.float) for _ in _visit_x]
            for _visit_x in _pat_x
        ]
        for _pat_x in batch
    ]
    batch = [
        torch.stack(functools.reduce(lambda a, b: a + [gap_embedding] + b, _), 0)
        for _ in batch
    ]
    batch_max_length = max(map(len, batch))
    mask = torch.tensor(
        [[1] * len(x) + [0] * (batch_max_length - len(x)) for x in batch],
        dtype=torch.long,
        device=device,
    )
    out = torch.zeros(
        [len(batch), batch_max_length, embed_dim], device=device, dtype=torch.float
    )
    for i, x in enumerate(batch):
        out[i, : len(x)] = x
    return out, mask


class Deepr(BaseModel):
    """Deepr model.

    Paper: P. Nguyen, T. Tran, N. Wickramasinghe and S. Venkatesh,
    " Deepr : A Convolutional Net for Medical Records," in IEEE Journal
    of Biomedical and Health Informatics, vol. 21, no. 1, pp. 22-30,
    Jan. 2017, doi: 10.1109/JBHI.2016.2633963.

    Note:
        We use separate Deepr layers for different feature_keys.

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension. Default is 128.
        **kwargs: other parameters for the Deepr layer.

    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
        ...             "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        ...             "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
        ...             "list_list_vectors": [
        ...                 [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
        ...                 [[7.7, 8.5, 9.4]],
        ...             ],
        ...             "label": 1,
        ...         },
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-1",
        ...             "list_codes": [
        ...                 "55154191800",
        ...                 "551541928",
        ...                 "55154192800",
        ...                 "705182798",
        ...                 "70518279800",
        ...             ],
        ...             "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
        ...             "list_list_codes": [["A04A", "B035", "C129"]],
        ...             "list_list_vectors": [
        ...                 [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
        ...             ],
        ...             "label": 0,
        ...         },
        ...     ]
        >>> dataset = SampleEHRDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import Deepr
        >>> model = Deepr(
        ...         dataset=dataset,
        ...         feature_keys=[
        ...             "list_list_codes",
        ...             "list_list_vectors",
        ...         ],
        ...         label_key="label",
        ...         mode="binary",
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(0.8908, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),
            'y_prob': tensor([[0.2295],
                        [0.2665]], device='cuda:0', grad_fn=<SigmoidBackward0>),
            'y_true': tensor([[1.],
                        [0.]], device='cuda:0'),
            'logit': tensor([[-1.2110],
                        [-1.0126]], device='cuda:0', grad_fn=<AddmmBackward0>)
        }
    """

    def __init__(
        self,
        dataset: BaseEHRDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        **kwargs,
    ):
        super(Deepr, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

        # TODO: Use more tokens for <gap> for different lengths once the input has such information
        self.feat_tokenizers = {}
        self.label_tokenizer = self.get_label_tokenizer()
        # TODO: Pretrain this embeddings with word2vec?
        self.embeddings = nn.ModuleDict()
        # the key of self.linear_layers only contains the float/int based inputs
        self.linear_layers = nn.ModuleDict()

        # add feature Deepr layers
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            # sanity check
            if input_info["type"] not in [str, float, int]:
                raise ValueError(
                    "Deepr only supports str code, float and int as input types"
                )
            if (input_info["type"] == str) and (input_info["dim"] != 3):
                raise ValueError("Deepr only supports 2-level str code as input types")
            if (input_info["type"] in [float, int]) and (input_info["dim"] != 3):
                raise ValueError(
                    "Deepr only supports 3-level float and int as input types"
                )
            # for code based input, we need Type
            # for float/int based input, we need Type, input_dim
            self.add_feature_transform_layer(
                feature_key, input_info, special_tokens=["<pad>", "<unk>", "<gap>"]
            )
            if input_info["type"] != str:
                self.embeddings[feature_key] = torch.nn.Embedding(1, input_info["len"])

        self.cnn = nn.ModuleDict()
        for feature_key in feature_keys:
            self.cnn[feature_key] = DeeprLayer(
                feature_size=embedding_dim, hidden_size=hidden_dim, **kwargs
            )
        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(len(self.feature_keys) * self.hidden_dim, output_size)

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:

        """Forward propagation."""
        patient_emb = []
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            dim_, type_ = input_info["dim"], input_info["type"]

            # for case 2: [[code1, code2], [code3, ...], ...]
            if (dim_ == 3) and (type_ == str):
                feature_vals = [
                    functools.reduce(lambda a, b: a + ["<gap>"] + b, _)
                    for _ in kwargs[feature_key]
                ]
                x = self.feat_tokenizers[feature_key].batch_encode_2d(
                    feature_vals, padding=True, truncation=False
                )
                pad_idx = self.feat_tokenizers[feature_key].vocabulary("<pad>")
                mask = torch.tensor(
                    [[_code != pad_idx for _code in _pat] for _pat in x],
                    dtype=torch.long,
                    device=self.device,
                )
                # (patient, code)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, event, embedding_dim)
                x = self.embeddings[feature_key](x)
            # for case 4: [[[1.5, 2.0, 0.0], [1.8, 2.4, 6.0]], ...]
            elif (dim_ == 3) and (type_ in [float, int]):
                gap_embedding = self.embeddings[feature_key](
                    torch.zeros(1, dtype=torch.long, device=self.device)
                )[0]
                x, mask = _flatten_and_fill_gap(
                    gap_embedding, kwargs[feature_key], self.device
                )
                # (patient, event, embedding_dim)
                x = self.linear_layers[feature_key](x)
            else:
                raise NotImplementedError(
                    f"Deepr does not support this input format (dim={dim_}, type={type_})."
                )
            # (patient, hidden_dim)
            x = self.cnn[feature_key](x, mask)
            patient_emb.append(x)

        # (patient, features * hidden_dim)
        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)
        y_prob = self.prepare_y_prob(logits)
        results = {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": y_true,
            "logit": logits,
        }
        if kwargs.get("embed", False):
            results["embed"] = patient_emb
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleEHRDataset

    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            "single_vector": [1, 2, 3],
            "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
            "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
            "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
            "list_list_vectors": [
                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
                [[7.7, 8.5, 9.4]],
            ],
            "label": 1,
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-1",
            "single_vector": [1, 5, 8],
            "list_codes": [
                "55154191800",
                "551541928",
                "55154192800",
                "705182798",
                "70518279800",
            ],
            "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
            "list_list_codes": [["A04A", "B035", "C129"]],
            "list_list_vectors": [
                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
            ],
            "label": 0,
        },
    ]

    # dataset
    dataset = SampleEHRDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = Deepr(
        dataset=dataset,
        # feature_keys=["procedures"],
        feature_keys=["list_list_codes", "list_list_vectors"],
        label_key="label",
        mode="binary",
    ).to("cuda:0")

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for gamenet.py:
import math
from typing import Tuple, List, Dict, Optional

import torch
import torch.nn as nn

from pyhealth.datasets import SampleEHRDataset
from pyhealth.medcode import ATC
from pyhealth.models import BaseModel
from pyhealth.models.utils import get_last_visit, batch_to_multihot


class GCNLayer(nn.Module):
    """GCN layer.

    Paper: Thomas N. Kipf et al. Semi-Supervised Classification with Graph
    Convolutional Networks. ICLR 2017.

    This layer is used in the GCN model.

    Args:
        in_features: input feature size.
        out_features: output feature size.
        bias: whether to use bias. Default is True.
    """

    def __init__(self, in_features: int, out_features: int, bias=True):
        super(GCNLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))
        if bias:
            self.bias = nn.Parameter(torch.FloatTensor(out_features))
        else:
            self.register_parameter("bias", None)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input: torch.tensor, adj: torch.tensor) -> torch.tensor:
        """
        Args:
            input: input feature tensor of shape [num_nodes, in_features].
            adj: adjacency tensor of shape [num_nodes, num_nodes].

        Returns:
            Output tensor of shape [num_nodes, out_features].
        """
        support = torch.mm(input, self.weight)
        output = torch.mm(adj, support)
        if self.bias is not None:
            return output + self.bias
        else:
            return output


class GCN(nn.Module):
    """GCN model.

    Paper: Thomas N. Kipf et al. Semi-Supervised Classification with Graph
    Convolutional Networks. ICLR 2017.

    This model is used in the GAMENet layer.

    Args:
        hidden_size: hidden feature size.
        adj: adjacency tensor of shape [num_nodes, num_nodes].
        dropout: dropout rate. Default is 0.5.
    """

    def __init__(self, adj: torch.tensor, hidden_size: int, dropout: float = 0.5):
        super(GCN, self).__init__()
        self.emb_dim = hidden_size
        self.dropout = dropout

        voc_size = adj.shape[0]
        adj = adj + torch.eye(adj.shape[0])
        adj = self.normalize(adj)
        self.adj = torch.nn.Parameter(adj, requires_grad=False)
        self.x = torch.nn.Parameter(torch.eye(voc_size), requires_grad=False)

        self.gcn1 = GCNLayer(voc_size, hidden_size)
        self.dropout_layer = nn.Dropout(p=dropout)
        self.gcn2 = GCNLayer(hidden_size, hidden_size)

    def normalize(self, mx: torch.tensor) -> torch.tensor:
        """Normalizes the matrix row-wise."""
        rowsum = mx.sum(1)
        r_inv = torch.pow(rowsum, -1).flatten()
        r_inv[torch.isinf(r_inv)] = 0.0
        r_mat_inv = torch.diagflat(r_inv)
        mx = torch.mm(r_mat_inv, mx)
        return mx

    def forward(self) -> torch.tensor:
        """Forward propagation.

        Returns:
            Output tensor of shape [num_nodes, hidden_size].
        """
        node_embedding = self.gcn1(self.x, self.adj)
        node_embedding = torch.relu(node_embedding)
        node_embedding = self.dropout_layer(node_embedding)
        node_embedding = self.gcn2(node_embedding, self.adj)
        return node_embedding


class GAMENetLayer(nn.Module):
    """GAMENet layer.

    Paper: Junyuan Shang et al. GAMENet: Graph Augmented MEmory Networks for
    Recommending Medication Combination AAAI 2019.

    This layer is used in the GAMENet model. But it can also be used as a
    standalone layer.

    Args:
        hidden_size: hidden feature size.
        ehr_adj: an adjacency tensor of shape [num_drugs, num_drugs].
        ddi_adj: an adjacency tensor of shape [num_drugs, num_drugs].
        dropout : the dropout rate. Default is 0.5.

    Examples:
        >>> from pyhealth.models import GAMENetLayer
        >>> queries = torch.randn(3, 5, 32) # [patient, visit, hidden_size]
        >>> prev_drugs = torch.randint(0, 2, (3, 4, 50)).float()
        >>> curr_drugs = torch.randint(0, 2, (3, 50)).float()
        >>> ehr_adj = torch.randint(0, 2, (50, 50)).float()
        >>> ddi_adj = torch.randint(0, 2, (50, 50)).float()
        >>> layer = GAMENetLayer(32, ehr_adj, ddi_adj)
        >>> loss, y_prob = layer(queries, prev_drugs, curr_drugs)
        >>> loss.shape
        torch.Size([])
        >>> y_prob.shape
        torch.Size([3, 50])
    """

    def __init__(
        self,
        hidden_size: int,
        ehr_adj: torch.tensor,
        ddi_adj: torch.tensor,
        dropout: float = 0.5,
    ):
        super(GAMENetLayer, self).__init__()
        self.hidden_size = hidden_size
        self.ehr_adj = ehr_adj
        self.ddi_adj = ddi_adj

        num_labels = ehr_adj.shape[0]
        self.ehr_gcn = GCN(adj=ehr_adj, hidden_size=hidden_size, dropout=dropout)
        self.ddi_gcn = GCN(adj=ddi_adj, hidden_size=hidden_size, dropout=dropout)
        self.beta = nn.Parameter(torch.FloatTensor(1))
        self.fc = nn.Linear(3 * hidden_size, num_labels)

        self.bce_loss_fn = nn.BCEWithLogitsLoss()

    def forward(
        self,
        queries: torch.tensor,
        prev_drugs: torch.tensor,
        curr_drugs: torch.tensor,
        mask: Optional[torch.tensor] = None,
    ) -> Tuple[torch.tensor, torch.tensor]:
        """Forward propagation.

        Args:
            queries: query tensor of shape [patient, visit, hidden_size].
            prev_drugs: multihot tensor indicating drug usage in all previous
                visits of shape [patient, visit - 1, num_drugs].
            curr_drugs: multihot tensor indicating drug usage in the current
                visit of shape [patient, num_drugs].
            mask: an optional mask tensor of shape [patient, visit] where 1
                indicates valid visits and 0 indicates invalid visits.

        Returns:
            loss: a scalar tensor representing the loss.
            y_prob: a tensor of shape [patient, num_labels] representing
                the probability of each drug.
        """
        if mask is None:
            mask = torch.ones_like(queries[:, :, 0])

        """I: Input memory representation"""
        query = get_last_visit(queries, mask)

        """G: Generalization"""
        # memory bank
        MB = self.ehr_gcn() - self.ddi_gcn() * torch.sigmoid(self.beta)

        # dynamic memory
        DM_keys = queries[:, :-1, :]
        DM_values = prev_drugs[:, :-1, :]

        """O: Output memory representation"""
        a_c = torch.softmax(torch.mm(query, MB.t()), dim=-1)
        o_b = torch.mm(a_c, MB)

        a_s = torch.softmax(torch.einsum("bd,bvd->bv", query, DM_keys), dim=1)
        a_m = torch.einsum("bv,bvz->bz", a_s, DM_values.float())
        o_d = torch.mm(a_m, MB)

        """R: Response"""
        memory_output = torch.cat([query, o_b, o_d], dim=-1)
        logits = self.fc(memory_output)

        loss = self.bce_loss_fn(logits, curr_drugs)
        y_prob = torch.sigmoid(logits)

        return loss, y_prob


class GAMENet(BaseModel):
    """GAMENet model.

    Paper: Junyuan Shang et al. GAMENet: Graph Augmented MEmory Networks for
    Recommending Medication Combination AAAI 2019.

    Note:
        This model is only for medication prediction which takes conditions
        and procedures as feature_keys, and drugs as label_key.
        It only operates on the visit level. Thus, we have disable the 
        feature_keys, label_key, and mode arguments.

    Note:
        This model only accepts ATC level 3 as medication codes.

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension. Default is 128.
        num_layers: the number of layers used in RNN. Default is 1.
        dropout: the dropout rate. Default is 0.5.
        **kwargs: other parameters for the GAMENet layer.
    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        num_layers: int = 1,
        dropout: float = 0.5,
        **kwargs
    ):
        super(GAMENet, self).__init__(
            dataset=dataset,
            feature_keys=["conditions", "procedures"],
            label_key="drugs",
            mode="multilabel",
        )
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.dropout = dropout

        self.feat_tokenizers = self.get_feature_tokenizers()
        self.label_tokenizer = self.get_label_tokenizer()
        self.embeddings = self.get_embedding_layers(self.feat_tokenizers, embedding_dim)

        ehr_adj = self.generate_ehr_adj()
        ddi_adj = self.generate_ddi_adj()

        self.cond_rnn = nn.GRU(
            embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
        )
        self.proc_rnn = nn.GRU(
            embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
        )
        self.query = nn.Sequential(
            nn.ReLU(),
            nn.Linear(hidden_dim * 2, hidden_dim),
        )

        # validate kwargs for GAMENet layer
        if "hidden_size" in kwargs:
            raise ValueError("hidden_size is determined by hidden_dim")
        if "ehr_adj" in kwargs:
            raise ValueError("ehr_adj is determined by the dataset")
        if "ddi_adj" in kwargs:
            raise ValueError("ddi_adj is determined by the dataset")
        self.gamenet = GAMENetLayer(
            hidden_size=hidden_dim,
            ehr_adj=ehr_adj,
            ddi_adj=ddi_adj,
            dropout=dropout,
            **kwargs,
        )

    def generate_ehr_adj(self) -> torch.tensor:
        """Generates the EHR graph adjacency matrix."""
        label_size = self.label_tokenizer.get_vocabulary_size()
        ehr_adj = torch.zeros((label_size, label_size))
        for sample in self.dataset:
            curr_drugs = sample["drugs"]
            encoded_drugs = self.label_tokenizer.convert_tokens_to_indices(curr_drugs)
            for idx1, med1 in enumerate(encoded_drugs):
                for idx2, med2 in enumerate(encoded_drugs):
                    if idx1 >= idx2:
                        continue
                    ehr_adj[med1, med2] = 1
                    ehr_adj[med2, med1] = 1
        return ehr_adj

    def generate_ddi_adj(self) -> torch.tensor:
        """Generates the DDI graph adjacency matrix."""
        atc = ATC()
        ddi = atc.get_ddi(gamenet_ddi=True)
        label_size = self.label_tokenizer.get_vocabulary_size()
        vocab_to_index = self.label_tokenizer.vocabulary
        ddi_adj = torch.zeros((label_size, label_size))
        ddi_atc3 = [
            [ATC.convert(l[0], level=3), ATC.convert(l[1], level=3)] for l in ddi
        ]
        for atc_i, atc_j in ddi_atc3:
            if atc_i in vocab_to_index and atc_j in vocab_to_index:
                ddi_adj[vocab_to_index(atc_i), vocab_to_index(atc_j)] = 1
                ddi_adj[vocab_to_index(atc_j), vocab_to_index(atc_i)] = 1
        return ddi_adj

    def forward(
        self,
        conditions: List[List[List[str]]],
        procedures: List[List[List[str]]],
        drugs_hist: List[List[List[str]]],
        drugs: List[List[str]],
        **kwargs
    ) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        Args:
            conditions: a nested list in three levels [patient, visit, condition].
            procedures: a nested list in three levels [patient, visit, procedure].
            drugs_hist: a nested list in three levels [patient, visit, drug], up to visit (N-1)
            drugs: a nested list in two levels [patient, drug], at visit N

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the loss.
                y_prob: a tensor of shape [patient, visit, num_labels] representing
                    the probability of each drug.
                y_true: a tensor of shape [patient, visit, num_labels] representing
                    the ground truth of each drug.

        """
        conditions = self.feat_tokenizers["conditions"].batch_encode_3d(conditions)
        # (patient, visit, code)
        conditions = torch.tensor(conditions, dtype=torch.long, device=self.device)
        # (patient, visit, code, embedding_dim)
        conditions = self.embeddings["conditions"](conditions)
        # (patient, visit, embedding_dim)
        conditions = torch.sum(conditions, dim=2)
        # (batch, visit, hidden_size)
        conditions, _ = self.cond_rnn(conditions)

        procedures = self.feat_tokenizers["procedures"].batch_encode_3d(procedures)
        # (patient, visit, code)
        procedures = torch.tensor(procedures, dtype=torch.long, device=self.device)
        # (patient, visit, code, embedding_dim)
        procedures = self.embeddings["procedures"](procedures)
        # (patient, visit, embedding_dim)
        procedures = torch.sum(procedures, dim=2)
        # (batch, visit, hidden_size)
        procedures, _ = self.proc_rnn(procedures)

        # (batch, visit, 2 * hidden_size)
        patient_representations = torch.cat([conditions, procedures], dim=-1)
        # (batch, visit, hidden_size)
        queries = self.query(patient_representations)

        label_size = self.label_tokenizer.get_vocabulary_size()
        drugs_hist = self.label_tokenizer.batch_encode_3d(
            drugs_hist, padding=(False, False), truncation=(True, False)
        )

        curr_drugs = self.prepare_labels(drugs, self.label_tokenizer)

        prev_drugs = drugs_hist
        max_num_visit = max([len(p) for p in prev_drugs])
        prev_drugs = [p + [[]] * (max_num_visit - len(p)) for p in prev_drugs]
        prev_drugs = [batch_to_multihot(p, label_size) for p in prev_drugs]
        prev_drugs = torch.stack(prev_drugs, dim=0)
        prev_drugs = prev_drugs.to(self.device)

        # get mask
        mask = torch.sum(conditions, dim=2) != 0

        # process drugs
        loss, y_prob = self.gamenet(queries, prev_drugs, curr_drugs, mask)

        return {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": curr_drugs,
        }


Here is the code content for grasp.py:
import copy
import math
import random
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.utils.rnn as rnn_utils
from sklearn.neighbors import kneighbors_graph

from pyhealth.datasets import SampleEHRDataset
from pyhealth.models import BaseModel, ConCareLayer, RNNLayer
from pyhealth.models.utils import get_last_visit


def random_init(dataset, num_centers, device):
    num_points = dataset.size(0)
    dimension = dataset.size(1)
    # print("random size", dataset.size())
    # print("numcenter", num_centers)

    indices = torch.tensor(
        np.array(random.sample(range(num_points), k=num_centers)), dtype=torch.long
    )

    centers = torch.gather(
        dataset, 0, indices.view(-1, 1).expand(-1, dimension).to(device=device)
    )
    return centers


# Compute for each data point the closest center
def compute_codes(dataset, centers):
    num_points = dataset.size(0)
    dimension = dataset.size(1)
    num_centers = centers.size(0)

    # print("size:", dataset.size(), centers.size())
    # 5e8 should vary depending on the free memory on the GPU
    # Ideally, automatically ;)
    chunk_size = int(5e8 / num_centers)
    codes = torch.zeros(num_points, dtype=torch.long)
    centers_t = torch.transpose(centers, 0, 1)
    centers_norms = torch.sum(centers**2, dim=1).view(1, -1)
    for i in range(0, num_points, chunk_size):
        begin = i
        end = min(begin + chunk_size, num_points)
        dataset_piece = dataset[begin:end, :]
        dataset_norms = torch.sum(dataset_piece**2, dim=1).view(-1, 1)
        distances = torch.mm(dataset_piece, centers_t)
        distances *= -2.0
        distances += dataset_norms
        distances += centers_norms
        _, min_ind = torch.min(distances, dim=1)
        codes[begin:end] = min_ind
    return codes


# Compute new centers as means of the data points forming the clusters
def update_centers(dataset, codes, num_centers, device):
    num_points = dataset.size(0)
    dimension = dataset.size(1)
    centers = torch.zeros(num_centers, dimension, dtype=torch.float).to(device=device)
    cnt = torch.zeros(num_centers, dtype=torch.float)
    centers.scatter_add_(
        0, codes.view(-1, 1).expand(-1, dimension).to(device=device), dataset
    )
    cnt.scatter_add_(0, codes, torch.ones(num_points, dtype=torch.float))
    # Avoiding division by zero
    # Not necessary if there are no duplicates among the data points
    cnt = torch.where(cnt > 0.5, cnt, torch.ones(num_centers, dtype=torch.float))
    centers /= cnt.view(-1, 1).to(device=device)
    return centers


def cluster(dataset, num_centers, device):
    centers = random_init(dataset, num_centers, device)
    codes = compute_codes(dataset, centers)
    num_iterations = 0
    while True:
        num_iterations += 1
        centers = update_centers(dataset, codes, num_centers, device)
        new_codes = compute_codes(dataset, centers)
        # Waiting until the clustering stops updating altogether
        # This is too strict in practice
        if torch.equal(codes, new_codes):
            break
        if num_iterations > 1000:
            break
        codes = new_codes
    return centers, codes


class GraphConvolution(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.Tensor(in_features, out_features).float())
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_features).float())
        else:
            self.register_parameter("bias", None)
        self.initialize_parameters()

    def initialize_parameters(self):
        std = 1.0 / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-std, std)
        if self.bias is not None:
            self.bias.data.uniform_(-std, std)

    def forward(self, adj, x, device):
        y = torch.mm(x.float(), self.weight.float())
        output = torch.mm(adj.float(), y.float())
        if self.bias is not None:
            return output + self.bias.float().to(device=device)
        else:
            return output


def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


class GRASPLayer(nn.Module):
    """GRASPLayer layer.

    Paper: Liantao Ma et al. GRASP: generic framework for health status representation learning based on incorporating knowledge from similar patients. AAAI 2021.

    This layer is used in the GRASP model. But it can also be used as a
    standalone layer.

    Args:
        input_dim: dynamic feature size.
        static_dim: static feature size, if 0, then no static feature is used.
        hidden_dim: hidden dimension of the GRASP layer, default 128.
        cluster_num: number of clusters, default 12. The cluster_num should be no more than the number of samples.
        dropout: dropout rate, default 0.5.
        block: the backbone model used in the GRASP layer ('ConCare', 'LSTM' or 'GRU'), default 'ConCare'.

    Examples:
        >>> from pyhealth.models import GRASPLayer
        >>> input = torch.randn(3, 128, 64)  # [batch size, sequence len, feature_size]
        >>> layer = GRASPLayer(64, cluster_num=2)
        >>> c = layer(input)
        >>> c.shape
        torch.Size([3, 128])
    """

    def __init__(
        self,
        input_dim: int,
        static_dim: int = 0,
        hidden_dim: int = 128,
        cluster_num: int = 2,
        dropout: int = 0.5,
        block: str = "ConCare",
    ):
        super(GRASPLayer, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.cluster_num = cluster_num
        self.dropout = dropout
        self.block = block

        if self.block == "ConCare":
            self.backbone = ConCareLayer(
                input_dim, static_dim, hidden_dim, hidden_dim, dropout=0
            )
        elif self.block == "GRU":
            self.backbone = RNNLayer(input_dim, hidden_dim, rnn_type="GRU", dropout=0)
        elif self.block == "LSTM":
            self.backbone = RNNLayer(input_dim, hidden_dim, rnn_type="LSTM", dropout=0)

        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(dropout)
        self.weight1 = nn.Linear(self.hidden_dim, 1)
        self.weight2 = nn.Linear(self.hidden_dim, 1)
        self.GCN = GraphConvolution(self.hidden_dim, self.hidden_dim, bias=True)
        self.GCN.initialize_parameters()
        self.GCN_2 = GraphConvolution(self.hidden_dim, self.hidden_dim, bias=True)
        self.GCN_2.initialize_parameters()
        self.A_mat = None

        self.bn = nn.BatchNorm1d(self.hidden_dim)

    def sample_gumbel(self, shape, eps=1e-20):
        U = torch.rand(shape)

        return -torch.log(-torch.log(U + eps) + eps)

    def gumbel_softmax_sample(self, logits, temperature, device):
        y = logits + self.sample_gumbel(logits.size()).to(device=device)
        return torch.softmax(y / temperature, dim=-1)

    def gumbel_softmax(self, logits, temperature, device, hard=False):
        """
        ST-gumple-softmax
        input: [*, n_class]
        return: flatten --> [*, n_class] an one-hot vector
        """
        y = self.gumbel_softmax_sample(logits, temperature, device)

        if not hard:
            return y.view(-1, self.cluster_num)

        shape = y.size()
        _, ind = y.max(dim=-1)
        y_hard = torch.zeros_like(y).view(-1, shape[-1])
        y_hard.scatter_(1, ind.view(-1, 1), 1)
        y_hard = y_hard.view(*shape)
        # Set gradients w.r.t. y_hard gradients w.r.t. y
        y_hard = (y_hard - y).detach() + y
        return y_hard

    def grasp_encoder(self, input, static=None, mask=None):

        if self.block == "ConCare":
            hidden_t, _ = self.backbone(input, mask=mask, static=static)
        else:
            _, hidden_t = self.backbone(input, mask)
        hidden_t = torch.squeeze(hidden_t, 0)

        centers, codes = cluster(hidden_t, self.cluster_num, input.device)

        if self.A_mat == None:
            A_mat = np.eye(self.cluster_num)
        else:
            A_mat = kneighbors_graph(
                np.array(centers.detach().cpu().numpy()),
                20,
                mode="connectivity",
                include_self=False,
            ).toarray()

        adj_mat = torch.tensor(A_mat).to(device=input.device)

        e = self.relu(torch.matmul(hidden_t, centers.transpose(0, 1)))  # b clu_num

        scores = self.gumbel_softmax(e, temperature=1, device=input.device, hard=True)
        digits = torch.argmax(scores, dim=-1)  #  b

        h_prime = self.relu(self.GCN(adj_mat, centers, input.device))
        h_prime = self.relu(self.GCN_2(adj_mat, h_prime, input.device))

        clu_appendix = torch.matmul(scores, h_prime)

        weight1 = torch.sigmoid(self.weight1(clu_appendix))
        weight2 = torch.sigmoid(self.weight2(hidden_t))
        weight1 = weight1 / (weight1 + weight2)
        weight2 = 1 - weight1

        final_h = weight1 * clu_appendix + weight2 * hidden_t
        out = final_h
        return out

    def forward(
        self,
        x: torch.tensor,
        static: Optional[torch.tensor] = None,
        mask: Optional[torch.tensor] = None,
    ) -> torch.tensor:
        """Forward propagation.

        Args:
            x: a tensor of shape [batch size, sequence len, input_dim].
            static: a tensor of shape [batch size, static_dim].
            mask: an optional tensor of shape [batch size, sequence len], where
                1 indicates valid and 0 indicates invalid.

        Returns:
            output: a tensor of shape [batch size, fusion_dim] representing the
                patient embedding.
        """
        # rnn will only apply dropout between layers
        out = self.grasp_encoder(x, static, mask)
        out = self.dropout(out)
        return out


class GRASP(BaseModel):
    """GRASP model.

    Paper: Liantao Ma et al. GRASP: generic framework for health status representation learning based on incorporating knowledge from similar patients. AAAI 2021.

    Note:
        We use separate GRASP layers for different feature_keys.
        Currently, we automatically support different input formats:
            - code based input (need to use the embedding table later)
            - float/int based value input
        We follow the current convention for the GRASP model:
            - case 1. [code1, code2, code3, ...]
                - we will assume the code follows the order; our model will encode
                each code into a vector and apply GRASP on the code level
            - case 2. [[code1, code2]] or [[code1, code2], [code3, code4, code5], ...]
                - we will assume the inner bracket follows the order; our model first
                use the embedding table to encode each code into a vector and then use
                average/mean pooling to get one vector for one inner bracket; then use
                GRASP one the braket level
            - case 3. [[1.5, 2.0, 0.0]] or [[1.5, 2.0, 0.0], [8, 1.2, 4.5], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run GRASP directly
                on the inner bracket level, similar to case 1 after embedding table
            - case 4. [[[1.5, 2.0, 0.0]]] or [[[1.5, 2.0, 0.0], [8, 1.2, 4.5]], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run GRASP directly
                on the inner bracket level, similar to case 2 after embedding table

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        static_keys: the key in samples to use as static features, e.g. "demographics". Default is None.
                     we only support numerical static features.
        use_embedding: list of bools indicating whether to use embedding for each feature type,
            e.g. [True, False].
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension of the GRASP layer. Default is 128.
        cluster_num: the number of clusters. Default is 10. Note that batch size should be greater than cluster_num.
        **kwargs: other parameters for the GRASP layer.


    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
        ...             "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        ...             "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
        ...             "list_list_vectors": [
        ...                 [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
        ...                 [[7.7, 8.5, 9.4]],
        ...             ],
        ...             "demographic": [0.0, 2.0, 1.5],
        ...             "label": 1,
        ...         },
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-1",
        ...             "list_codes": [
        ...                 "55154191800",
        ...                 "551541928",
        ...                 "55154192800",
        ...                 "705182798",
        ...                 "70518279800",
        ...             ],
        ...             "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
        ...             "list_list_codes": [["A04A", "B035", "C129"]],
        ...             "list_list_vectors": [
        ...                 [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
        ...             ],
        ...             "demographic": [0.0, 2.0, 1.5],
        ...             "label": 0,
        ...         },
        ...     ]
        >>> dataset = SampleEHRDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import GRASP
        >>> model = GRASP(
        ...         dataset=dataset,
        ...         feature_keys=[
        ...             "list_codes",
        ...             "list_vectors",
        ...             "list_list_codes",
        ...             "list_list_vectors",
        ...         ],
        ...         label_key="label",
        ...         static_key="demographic",
        ...         use_embedding=[True, False, True, False],
        ...         mode="binary"
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),
            'y_prob': tensor([[0.4983],
                        [0.4947]], grad_fn=<SigmoidBackward0>),
            'y_true': tensor([[1.],
                        [0.]]),
            'logit': tensor([[-0.0070],
                        [-0.0213]], grad_fn=<AddmmBackward0>)
        }
        >>>

    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        use_embedding: List[bool],
        static_key: Optional[str] = None,
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        **kwargs,
    ):
        super(GRASP, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.use_embedding = use_embedding

        # validate kwargs for GRASP layer
        if "feature_size" in kwargs:
            raise ValueError("feature_size is determined by embedding_dim")
        if len(dataset) < 12 and "cluster_num" not in kwargs:
            raise ValueError("cluster_num is required for small dataset, default 12")
        if "cluster_num" in kwargs and kwargs["cluster_num"] > len(dataset):
            raise ValueError("cluster_num must be no larger than dataset size")

        cluster_num = kwargs.get("cluster_num", 12)

        # the key of self.feat_tokenizers only contains the code based inputs
        self.feat_tokenizers = {}
        self.static_key = static_key
        self.label_tokenizer = self.get_label_tokenizer()
        # the key of self.embeddings only contains the code based inputs
        self.embeddings = nn.ModuleDict()
        # the key of self.linear_layers only contains the float/int based inputs
        self.linear_layers = nn.ModuleDict()

        self.static_dim = 0
        if self.static_key is not None:
            self.static_dim = self.dataset.input_info[self.static_key]["len"]

        self.grasp = nn.ModuleDict()
        # add feature GRASP layers
        for idx, feature_key in enumerate(self.feature_keys):
            input_info = self.dataset.input_info[feature_key]
            # sanity check
            if input_info["type"] not in [str, float, int]:
                raise ValueError(
                    "GRASP only supports str code, float and int as input types"
                )
            elif (input_info["type"] == str) and (input_info["dim"] not in [2, 3]):
                raise ValueError(
                    "GRASP only supports 2-dim or 3-dim str code as input types"
                )
            elif (input_info["type"] == str) and (use_embedding[idx] == False):
                raise ValueError(
                    "GRASP only supports embedding for str code as input types"
                )
            elif (input_info["type"] in [float, int]) and (
                input_info["dim"] not in [2, 3]
            ):
                raise ValueError(
                    "GRASP only supports 2-dim or 3-dim float and int as input types"
                )

            # for code based input, we need Type
            # for float/int based input, we need Type, input_dim
            if use_embedding[idx]:
                self.add_feature_transform_layer(feature_key, input_info)
                self.grasp[feature_key] = GRASPLayer(
                    input_dim=embedding_dim,
                    static_dim=self.static_dim,
                    hidden_dim=self.hidden_dim,
                    **kwargs,
                )
            else:
                self.grasp[feature_key] = GRASPLayer(
                    input_dim=input_info["len"],
                    static_dim=self.static_dim,
                    hidden_dim=self.hidden_dim,
                    **kwargs,
                )

        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(len(self.feature_keys) * self.hidden_dim, output_size)

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        The label `kwargs[self.label_key]` is a list of labels for each patient.

        Args:
            **kwargs: keyword arguments for the model. The keys must contain
                all the feature keys and the label key.

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the final loss.
                y_prob: a tensor representing the predicted probabilities.
                y_true: a tensor representing the true labels.
        """
        patient_emb = []
        for idx, feature_key in enumerate(self.feature_keys):
            input_info = self.dataset.input_info[feature_key]
            dim_, type_ = input_info["dim"], input_info["type"]

            # for case 1: [code1, code2, code3, ...]
            if (dim_ == 2) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_2d(
                    kwargs[feature_key]
                )
                # (patient, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, event)
                mask = torch.any(x !=0, dim=2)

            # for case 2: [[code1, code2], [code3, ...], ...]
            elif (dim_ == 3) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_3d(
                    kwargs[feature_key]
                )
                # (patient, visit, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, visit, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                # (patient, visit)
                mask = torch.any(x !=0, dim=2)

            # for case 3: [[1.5, 2.0, 0.0], ...]
            elif (dim_ == 2) and (type_ in [float, int]):
                x, mask = self.padding2d(kwargs[feature_key])
                # (patient, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, event, embedding_dim)
                if self.use_embedding[idx]:
                    x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask.bool().to(self.device)

            # for case 4: [[[1.5, 2.0, 0.0], [1.8, 2.4, 6.0]], ...]
            elif (dim_ == 3) and (type_ in [float, int]):
                x, mask = self.padding3d(kwargs[feature_key])
                # (patient, visit, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)

                if self.use_embedding[idx]:
                    x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask[:, :, 0]
                mask = mask.bool().to(self.device)
            else:
                raise NotImplementedError

            if self.static_dim > 0:
                static = torch.tensor(
                    kwargs[self.static_key], dtype=torch.float, device=self.device
                )
                x = self.grasp[feature_key](x, static=static, mask=mask)
            else:
                x = self.grasp[feature_key](x, mask=mask)
            patient_emb.append(x)

        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)
        y_prob = self.prepare_y_prob(logits)
        results = {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": y_true,
            "logit": logits,
        }
        if kwargs.get("embed", False):
            results["embed"] = patient_emb
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleEHRDataset

    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            # "single_vector": [1, 2, 3],
            "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
            "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
            "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
            "list_list_vectors": [
                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
                [[7.7, 8.5, 9.4]],
            ],
            "label": 1,
            "demographic": [1.0, 2.0, 1.3],
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-1",
            # "single_vector": [1, 5, 8],
            "list_codes": [
                "55154191800",
                "551541928",
                "55154192800",
                "705182798",
                "70518279800",
            ],
            "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
            "list_list_codes": [["A04A", "B035", "C129"]],
            "list_list_vectors": [
                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
            ],
            "label": 0,
            "demographic": [1.0, 2.0, 1.3],
        },
    ]

    # dataset
    dataset = SampleEHRDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = GRASP(
        dataset=dataset,
        feature_keys=[
            "list_codes",
            "list_vectors",
            "list_list_codes",
            # "list_list_vectors",
        ],
        static_key="demographic",
        label_key="label",
        use_embedding=[True, False, True],
        mode="binary",
        cluster_num=2,
    )

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for micron.py:
from typing import List, Tuple, Dict, Optional

import torch
import torch.nn as nn

from pyhealth.datasets import SampleEHRDataset
from pyhealth.models import BaseModel
from pyhealth.models.utils import get_last_visit


class MICRONLayer(nn.Module):
    """MICRON layer.

    Paper: Chaoqi Yang et al. Change Matters: Medication Change Prediction
    with Recurrent Residual Networks. IJCAI 2021.

    This layer is used in the MICRON model. But it can also be used as a
    standalone layer.

    Args:
        input_size: input feature size.
        hidden_size: hidden feature size.
        num_drugs: total number of drugs to recommend.
        lam: regularization parameter for the reconstruction loss. Default is 0.1.

    Examples:
        >>> from pyhealth.models import MICRONLayer
        >>> patient_emb = torch.randn(3, 5, 32) # [patient, visit, input_size]
        >>> drugs = torch.randint(0, 2, (3, 50)).float()
        >>> layer = MICRONLayer(32, 64, 50)
        >>> loss, y_prob = layer(patient_emb, drugs)
        >>> loss.shape
        torch.Size([])
        >>> y_prob.shape
        torch.Size([3, 50])
    """

    def __init__(
        self, input_size: int, hidden_size: int, num_drugs: int, lam: float = 0.1
    ):
        super(MICRONLayer, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_labels = num_drugs
        self.lam = lam

        self.health_net = nn.Linear(input_size, hidden_size)
        self.prescription_net = nn.Linear(hidden_size, hidden_size)
        self.fc = nn.Linear(hidden_size, num_drugs)

        self.bce_loss_fn = nn.BCEWithLogitsLoss()

    @staticmethod
    def compute_reconstruction_loss(
        logits: torch.tensor, logits_residual: torch.tensor, mask: torch.tensor
    ) -> torch.tensor:
        rec_loss = torch.mean(
            torch.square(
                torch.sigmoid(logits[:, 1:, :])
                - torch.sigmoid(logits[:, :-1, :] + logits_residual)
            )
            * mask[:, 1:].unsqueeze(2)
        )
        return rec_loss

    def forward(
        self,
        patient_emb: torch.tensor,
        drugs: torch.tensor,
        mask: Optional[torch.tensor] = None,
    ) -> Tuple[torch.tensor, torch.tensor]:
        """Forward propagation.

        Args:
            patient_emb: a tensor of shape [patient, visit, input_size].
            drugs: a multihot tensor of shape [patient, num_labels].
            mask: an optional tensor of shape [patient, visit] where
                1 indicates valid visits and 0 indicates invalid visits.

        Returns:
            loss: a scalar tensor representing the loss.
            y_prob: a tensor of shape [patient, num_labels] representing
                the probability of each drug.
        """
        if mask is None:
            mask = torch.ones_like(patient_emb[:, :, 0])

        # (patient, visit, hidden_size)
        health_rep = self.health_net(patient_emb)
        drug_rep = self.prescription_net(health_rep)
        logits = self.fc(drug_rep)
        logits_last_visit = get_last_visit(logits, mask)
        bce_loss = self.bce_loss_fn(logits_last_visit, drugs)

        # (batch, visit-1, input_size)
        health_rep_last = health_rep[:, :-1, :]
        # (batch, visit-1, input_size)
        health_rep_cur = health_rep[:, 1:, :]
        # (batch, visit-1, input_size)
        health_rep_residual = health_rep_cur - health_rep_last
        drug_rep_residual = self.prescription_net(health_rep_residual)
        logits_residual = self.fc(drug_rep_residual)
        rec_loss = self.compute_reconstruction_loss(logits, logits_residual, mask)

        loss = bce_loss + self.lam * rec_loss
        y_prob = torch.sigmoid(logits_last_visit)

        return loss, y_prob


class MICRON(BaseModel):
    """MICRON model.

    Paper: Chaoqi Yang et al. Change Matters: Medication Change Prediction
    with Recurrent Residual Networks. IJCAI 2021.

    Note:
        This model is only for medication prediction which takes conditions
        and procedures as feature_keys, and drugs as label_key. It only operates
        on the visit level.

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension. Default is 128.
        **kwargs: other parameters for the MICRON layer.
    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        **kwargs
    ):
        super(MICRON, self).__init__(
            dataset=dataset,
            feature_keys=["conditions", "procedures"],
            label_key="drugs",
            mode="multilabel",
        )
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

        self.feat_tokenizers = self.get_feature_tokenizers()
        self.label_tokenizer = self.get_label_tokenizer()
        self.embeddings = self.get_embedding_layers(self.feat_tokenizers, embedding_dim)

        # validate kwargs for MICRON layer
        if "input_size" in kwargs:
            raise ValueError("input_size is determined by embedding_dim")
        if "hidden_size" in kwargs:
            raise ValueError("hidden_size is determined by hidden_dim")
        if "num_drugs" in kwargs:
            raise ValueError("num_drugs is determined by the dataset")
        self.micron = MICRONLayer(
            input_size=embedding_dim * 2,
            hidden_size=hidden_dim,
            num_drugs=self.label_tokenizer.get_vocabulary_size(),
            **kwargs
        )

    def forward(
        self,
        conditions: List[List[List[str]]],
        procedures: List[List[List[str]]],
        drugs: List[List[str]],
        **kwargs
    ) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        Args:
            conditions: a nested list in three levels [patient, visit, condition].
            procedures: a nested list in three levels [patient, visit, procedure].
            drugs: a nested list in two levels [patient, drug].

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the loss.
                y_prob: a tensor of shape [patient, visit, num_labels] representing
                    the probability of each drug.
                y_true: a tensor of shape [patient, visit, num_labels] representing
                    the ground truth of each drug.
        """
        conditions = self.feat_tokenizers["conditions"].batch_encode_3d(conditions)
        # (patient, visit, code)
        conditions = torch.tensor(conditions, dtype=torch.long, device=self.device)
        # (patient, visit, code, embedding_dim)
        conditions = self.embeddings["conditions"](conditions)
        # (patient, visit, embedding_dim)
        conditions = torch.sum(conditions, dim=2)

        procedures = self.feat_tokenizers["procedures"].batch_encode_3d(procedures)
        # (patient, visit, code)
        procedures = torch.tensor(procedures, dtype=torch.long, device=self.device)
        # (patient, visit, code, embedding_dim)
        procedures = self.embeddings["procedures"](procedures)
        # (patient, visit, embedding_dim)
        procedures = torch.sum(procedures, dim=2)

        # (patient, visit, embedding_dim * 2)
        patient_emb = torch.cat([conditions, procedures], dim=2)
        # (patient, visit)
        mask = torch.sum(patient_emb, dim=2) != 0
        # (patient, num_labels)
        drugs = self.prepare_labels(drugs, self.label_tokenizer)

        loss, y_prob = self.micron(patient_emb, drugs, mask)

        return {"loss": loss, "y_prob": y_prob, "y_true": drugs}


Here is the code content for mlp.py:
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn as nn

from pyhealth.datasets import SampleEHRDataset
from pyhealth.models import BaseModel


class MLP(BaseModel):
    """Multi-layer perceptron model.

    This model applies a separate MLP layer for each feature, and then concatenates
    the final hidden states of each MLP layer. The concatenated hidden states are
    then fed into a fully connected layer to make predictions.

    Note:
        We use separate MLP layers for different feature_keys.
        Currentluy, we automatically support different input formats:
            - code based input (need to use the embedding table later)
            - float/int based value input
        We follow the current convention for the rnn model:
            - case 1. [code1, code2, code3, ...]
                - we will assume the code follows the order; our model will encode
                each code into a vector; we use mean/sum pooling and then MLP
            - case 2. [[code1, code2]] or [[code1, code2], [code3, code4, code5], ...]
                - we first use the embedding table to encode each code into a vector
                and then use mean/sum pooling to get one vector for each sample; we then
                use MLP layers
            - case 3. [1.5, 2.0, 0.0]
                - we run MLP directly
            - case 4. [[1.5, 2.0, 0.0]] or [[1.5, 2.0, 0.0], [8, 1.2, 4.5], ...]
                - This case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we use mean/sum pooling
                within each outer bracket and use MLP, similar to case 1 after embedding table
            - case 5. [[[1.5, 2.0, 0.0]]] or [[[1.5, 2.0, 0.0], [8, 1.2, 4.5]], ...]
                - This case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we use mean/sum pooling
                within each outer bracket and use MLP, similar to case 2 after embedding table

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension. Default is 128.
        n_layers: the number of layers. Default is 2.
        activation: the activation function. Default is "relu".
        **kwargs: other parameters for the RNN layer.

    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "conditions": ["cond-33", "cond-86", "cond-80"],
        ...             "procedures": [1.0, 2.0, 3.5, 4],
        ...             "label": 0,
        ...         },
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "conditions": ["cond-33", "cond-86", "cond-80"],
        ...             "procedures": [5.0, 2.0, 3.5, 4],
        ...             "label": 1,
        ...         },
        ...     ]
        >>> dataset = SampleEHRDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import MLP
        >>> model = MLP(
        ...         dataset=dataset,
        ...         feature_keys=["conditions", "procedures"],
        ...         label_key="label",
        ...         mode="binary",
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(0.6659, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),
            'y_prob': tensor([[0.5680],
                            [0.5352]], grad_fn=<SigmoidBackward0>),
            'y_true': tensor([[1.],
                            [0.]]),
            'logit': tensor([[0.2736],
                            [0.1411]], grad_fn=<AddmmBackward0>)
        }
        >>>

    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        n_layers: int = 2,
        activation: str = "relu",
        **kwargs,
    ):
        super(MLP, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers

        # validate kwargs for RNN layer
        if "input_size" in kwargs:
            raise ValueError("input_size is determined by embedding_dim")
        if "hidden_size" in kwargs:
            raise ValueError("hidden_size is determined by hidden_dim")

        # the key of self.feat_tokenizers only contains the code based inputs
        self.feat_tokenizers = {}
        self.label_tokenizer = self.get_label_tokenizer()
        # the key of self.embeddings only contains the code based inputs
        self.embeddings = nn.ModuleDict()
        # the key of self.linear_layers only contains the float/int based inputs
        self.linear_layers = nn.ModuleDict()

        # add feature MLP layers
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            # sanity check
            if input_info["type"] not in [str, float, int]:
                raise ValueError(
                    "MLP only supports str code, float and int as input types"
                )
            elif (input_info["type"] == str) and (input_info["dim"] not in [1, 2]):
                raise ValueError(
                    "MLP only supports 1-dim or 2-dim str code as input types"
                )
            elif (input_info["type"] in [float, int]) and (
                input_info["dim"] not in [1, 2, 3]
            ):
                raise ValueError(
                    "MLP only supports 1-dim, 2-dim or 3-dim float and int as input types"
                )
            # for code based input, we need Type
            # for float/int based input, we need Type, input_dim
            self.add_feature_transform_layer(feature_key, input_info)

        if activation == "relu":
            self.activation = nn.ReLU()
        elif activation == "tanh":
            self.activation = nn.Tanh()
        elif activation == "sigmoid":
            self.activation = nn.Sigmoid()
        elif activation == "leaky_relu":
            self.activation = nn.LeakyReLU()
        elif activation == "elu":
            self.activation = nn.ELU()
        else:
            raise ValueError(f"Unsupported activation function {activation}")

        self.mlp = nn.ModuleDict()
        for feature_key in feature_keys:
            Modules = []
            Modules.append(nn.Linear(self.embedding_dim, self.hidden_dim))
            for _ in range(self.n_layers - 1):
                Modules.append(self.activation)
                Modules.append(nn.Linear(self.hidden_dim, self.hidden_dim))
            self.mlp[feature_key] = nn.Sequential(*Modules)

        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(len(self.feature_keys) * self.hidden_dim, output_size)

    @staticmethod
    def mean_pooling(x, mask):
        """Mean pooling over the middle dimension of the tensor.
        Args:
            x: tensor of shape (batch_size, seq_len, embedding_dim)
            mask: tensor of shape (batch_size, seq_len)
        Returns:
            x: tensor of shape (batch_size, embedding_dim)
        Examples:
            >>> x.shape
            [128, 5, 32]
            >>> mean_pooling(x).shape
            [128, 32]
        """
        return x.sum(dim=1) / mask.sum(dim=1, keepdim=True)

    @staticmethod
    def sum_pooling(x):
        """Mean pooling over the middle dimension of the tensor.
        Args:
            x: tensor of shape (batch_size, seq_len, embedding_dim)
            mask: tensor of shape (batch_size, seq_len)
        Returns:
            x: tensor of shape (batch_size, embedding_dim)
        Examples:
            >>> x.shape
            [128, 5, 32]
            >>> sum_pooling(x).shape
            [128, 32]
        """
        return x.sum(dim=1)

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        The label `kwargs[self.label_key]` is a list of labels for each patient.

        Args:
            **kwargs: keyword arguments for the model. The keys must contain
                all the feature keys and the label key.

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the loss.
                y_prob: a tensor representing the predicted probabilities.
                y_true: a tensor representing the true labels.
        """
        patient_emb = []
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            dim_, type_ = input_info["dim"], input_info["type"]

            # for case 1: [code1, code2, code3, ...]
            if (dim_ == 2) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_2d(
                    kwargs[feature_key]
                )
                # (patient, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, event)
                mask = torch.any(x !=0, dim=2)
                # (patient, embedding_dim)
                x = self.mean_pooling(x, mask)

            # for case 2: [[code1, code2], [code3, ...], ...]
            elif (dim_ == 3) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_3d(
                    kwargs[feature_key]
                )
                # (patient, visit, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, visit, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                # (patient, visit)
                mask = torch.any(x !=0, dim=2)
                # (patient, embedding_dim)
                x = self.mean_pooling(x, mask)

            # for case 3: [1.5, 2.0, 0.0]
            elif (dim_ == 1) and (type_ in [float, int]):
                # (patient, values)
                x = torch.tensor(
                    kwargs[feature_key], dtype=torch.float, device=self.device
                )
                # (patient, embedding_dim)
                x = self.linear_layers[feature_key](x)

            # for case 4: [[1.5, 2.0, 0.0], ...]
            elif (dim_ == 2) and (type_ in [float, int]):
                x, mask = self.padding2d(kwargs[feature_key])
                # (patient, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, event, embedding_dim)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = torch.tensor(mask, dtype=torch.bool, device=self.device)
                # (patient, embedding_dim)
                x = self.mean_pooling(x, mask)

            # for case 5: [[[1.5, 2.0, 0.0], [1.8, 2.4, 6.0]], ...]
            elif (dim_ == 3) and (type_ in [float, int]):
                x, mask = self.padding3d(kwargs[feature_key])
                # (patient, visit, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, visit, embedding_dim)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = torch.tensor(mask, dtype=torch.bool, device=self.device)
                # (patient, embedding_dim)
                x = self.mean_pooling(x, mask)

            else:
                raise NotImplementedError

            x = self.mlp[feature_key](x)
            patient_emb.append(x)

        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)
        y_prob = self.prepare_y_prob(logits)
        results = {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": y_true,
            "logit": logits,
        }
        if kwargs.get("embed", False):
            results["embed"] = patient_emb
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleEHRDataset

    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            "conditions": ["cond-33", "cond-86", "cond-80"],
            "procedures": [1.0, 2.0, 3.5, 4],
            "label": 0,
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            "conditions": ["cond-33", "cond-86", "cond-80"],
            "procedures": [5.0, 2.0, 3.5, 4],
            "label": 1,
        },
    ]

    # dataset
    dataset = SampleEHRDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = MLP(
        dataset=dataset,
        feature_keys=["conditions", "procedures"],
        label_key="label",
        mode="binary",
    )

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # TODO: the loss back propagation step seems slow.
    # try loss backward
    ret["loss"].backward()


Here is the code content for molerec.py:
import torch
import math
import pkg_resources
import numpy as np
from typing import Any, Dict, List, Tuple, Optional, Union
from rdkit import Chem
from torch.nn.functional import binary_cross_entropy_with_logits
from torch.nn.functional import multilabel_margin_loss

from pyhealth.models import BaseModel
from pyhealth.models.utils import get_last_visit
from pyhealth.models.utils import batch_to_multihot
from pyhealth.metrics import ddi_rate_score
from pyhealth.medcode import ATC
from pyhealth.datasets import SampleEHRDataset


def graph_batch_from_smiles(smiles_list, device=torch.device("cpu")):
    edge_idxes, edge_feats, node_feats, lstnode, batch = [], [], [], 0, []
    graphs = [smiles2graph(x) for x in smiles_list]

    for idx, graph in enumerate(graphs):
        edge_idxes.append(graph["edge_index"] + lstnode)
        edge_feats.append(graph["edge_feat"])
        node_feats.append(graph["node_feat"])
        lstnode += graph["num_nodes"]
        batch.append(np.ones(graph["num_nodes"], dtype=np.int64) * idx)

    result = {
        "edge_index": np.concatenate(edge_idxes, axis=-1),
        "edge_attr": np.concatenate(edge_feats, axis=0),
        "batch": np.concatenate(batch, axis=0),
        "x": np.concatenate(node_feats, axis=0),
    }
    result = {k: torch.from_numpy(v).to(device) for k, v in result.items()}
    result["num_nodes"] = lstnode
    result["num_edges"] = result["edge_index"].shape[1]
    return result


class StaticParaDict(torch.nn.Module):
    def __init__(self, **kwargs):
        super(StaticParaDict, self).__init__()
        for k, v in kwargs.items():
            if isinstance(v, torch.Tensor):
                setattr(self, k, torch.nn.Parameter(v, requires_grad=False))
            elif isinstance(v, np.ndarray):
                v = torch.from_numpy(v)
                setattr(self, k, torch.nn.Parameter(v, requires_grad=False))
            else:
                setattr(self, k, v)

    def forward(self, key: str) -> Any:
        return getattr(self, key)

    def __getitem__(self, key: str) -> Any:
        return self(key)

    def __setitem__(self, key: str, value: Any):
        if isinstance(value, np.ndarray):
            value = torch.from_numpy(value)
        if isinstance(value, torch.Tensor):
            value = torch.nn.Parameter(value, requires_grad=False)
        setattr(self, key, value)


class GINConv(torch.nn.Module):
    def __init__(self, embedding_dim: int = 64):
        super(GINConv, self).__init__()
        self.mlp = torch.nn.Sequential(
            torch.nn.Linear(embedding_dim, 2 * embedding_dim),
            torch.nn.BatchNorm1d(2 * embedding_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(2 * embedding_dim, embedding_dim),
        )
        self.eps = torch.nn.Parameter(torch.Tensor([0]))
        self.bond_encoder = BondEncoder(emb_dim=embedding_dim)

    def forward(
        self,
        node_feats: torch.Tensor,
        edge_feats: torch.Tensor,
        edge_index: torch.Tensor,
        num_nodes: int,
        num_edges: int,
    ) -> torch.Tensor:
        edge_feats = self.bond_encoder(edge_feats)
        message_node = torch.index_select(input=node_feats, dim=0, index=edge_index[1])
        message = torch.relu(message_node + edge_feats)
        dim = message.shape[-1]

        message_reduce = torch.zeros(num_nodes, dim).to(message)
        index = edge_index[0].unsqueeze(-1).repeat(1, dim)
        message_reduce.scatter_add_(dim=0, index=index, src=message)

        return self.mlp((1 + self.eps) * node_feats + message_reduce)


class GINGraph(torch.nn.Module):
    def __init__(
        self, num_layers: int = 4, embedding_dim: int = 64, dropout: float = 0.7
    ):
        super(GINGraph, self).__init__()
        if num_layers < 2:
            raise ValueError("Number of GNN layers must be greater than 1.")

        self.atom_encoder = AtomEncoder(emb_dim=embedding_dim)
        self.convs = torch.nn.ModuleList()
        self.batch_norms = torch.nn.ModuleList()
        self.num_layers = num_layers
        self.dropout_fun = torch.nn.Dropout(dropout)
        for layer in range(self.num_layers):
            self.convs.append(GINConv(embedding_dim))
            self.batch_norms.append(torch.nn.BatchNorm1d(embedding_dim))

    def forward(self, graph: Dict[str, Union[int, torch.Tensor]]) -> torch.Tensor:
        h_list = [self.atom_encoder(graph["x"])]
        for layer in range(self.num_layers):
            h = self.batch_norms[layer](
                self.convs[layer](
                    node_feats=h_list[layer],
                    edge_feats=graph["edge_attr"],
                    edge_index=graph["edge_index"],
                    num_nodes=graph["num_nodes"],
                    num_edges=graph["num_edges"],
                )
            )
            if layer != self.num_layers - 1:
                h = self.dropout_fun(torch.relu(h))
            else:
                h = self.dropout_fun(h)
            h_list.append(h)

        batch_size, dim = graph["batch"].max().item() + 1, h_list[-1].shape[-1]
        out_feat = torch.zeros(batch_size, dim).to(h_list[-1])
        cnt = torch.zeros_like(out_feat).to(out_feat)
        index = graph["batch"].unsqueeze(-1).repeat(1, dim)

        out_feat.scatter_add_(dim=0, index=index, src=h_list[-1])
        cnt.scatter_add_(
            dim=0, index=index, src=torch.ones_like(h_list[-1]).to(h_list[-1])
        )

        return out_feat / (cnt + 1e-9)


class MAB(torch.nn.Module):
    def __init__(
        self, Qdim: int, Kdim: int, Vdim: int, number_heads: int, use_ln: bool = False
    ):
        super(MAB, self).__init__()
        self.Vdim = Vdim
        self.number_heads = number_heads

        assert (
            self.Vdim % self.number_heads == 0
        ), "the dim of features should be divisible by number_heads"

        self.Qdense = torch.nn.Linear(Qdim, self.Vdim)
        self.Kdense = torch.nn.Linear(Kdim, self.Vdim)
        self.Vdense = torch.nn.Linear(Kdim, self.Vdim)
        self.Odense = torch.nn.Linear(self.Vdim, self.Vdim)

        self.use_ln = use_ln
        if self.use_ln:
            self.ln1 = torch.nn.LayerNorm(self.Vdim)
            self.ln2 = torch.nn.LayerNorm(self.Vdim)

    def forward(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:
        Q, K, V = self.Qdense(X), self.Kdense(Y), self.Vdense(Y)
        batch_size, dim_split = Q.shape[0], self.Vdim // self.number_heads

        Q_split = torch.cat(Q.split(dim_split, 2), 0)
        K_split = torch.cat(K.split(dim_split, 2), 0)
        V_split = torch.cat(V.split(dim_split, 2), 0)

        Attn = torch.matmul(Q_split, K_split.transpose(1, 2))
        Attn = torch.softmax(Attn / math.sqrt(dim_split), dim=-1)
        O = Q_split + torch.matmul(Attn, V_split)
        O = torch.cat(O.split(batch_size, 0), 2)

        O = O if not self.use_ln else self.ln1(O)
        O = self.Odense(O)
        O = O if not self.use_ln else self.ln2(O)

        return O


class SAB(torch.nn.Module):
    def __init__(
        self, in_dim: int, out_dim: int, number_heads: int, use_ln: bool = False
    ):
        super(SAB, self).__init__()
        self.net = MAB(in_dim, in_dim, out_dim, number_heads, use_ln)

    def forward(self, X: torch.Tensor) -> torch.Tensor:
        return self.net(X, X)


class AttnAgg(torch.nn.Module):
    def __init__(self, Qdim: int, Kdim: int, mid_dim: int):
        super(AttnAgg, self).__init__()
        self.model_dim = mid_dim
        self.Qdense = torch.nn.Linear(Qdim, mid_dim)
        self.Kdense = torch.nn.Linear(Kdim, mid_dim)
        # self.use_ln = use_ln

    def forward(
        self,
        main_feat: torch.Tensor,
        other_feat: torch.Tensor,
        fix_feat: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """Forward propagation.

        Adjusted Attention Aggregator

        Args:
            main_feat (torch.Tensor): shape of [main_num, Q_dim]
            other_feat (torch.Tensor): shape of [other_num, K_dim]
            fix_feat (torch.Tensor): shape of [batch, other_num],
                adjust parameter for attention weight
            mask (torch.Tensor): shape of [main_num, other_num] a mask
                representing where main object should have attention
                with other obj 1 means no attention should be done.
                (default: `None`)

        Returns:
            torch.Tensor: aggregated features, shape of
                [batch, main_num, K_dim]
        """
        Q = self.Qdense(main_feat)
        K = self.Kdense(other_feat)
        Attn = torch.matmul(Q, K.transpose(0, 1)) / math.sqrt(self.model_dim)

        if mask is not None:
            Attn = torch.masked_fill(Attn, mask, -(1 << 32))
        Attn = torch.softmax(Attn, dim=-1)

        batch_size = fix_feat.shape[0]
        # [batch_size, other_num, other_num]
        fix_feat = torch.diag_embed(fix_feat)
        # [batch_size, other_num, K_dim]
        other_feat = other_feat.repeat(batch_size, 1, 1)
        other_feat = torch.matmul(fix_feat, other_feat)
        Attn = Attn.repeat(batch_size, 1, 1)

        return torch.matmul(Attn, other_feat)


class MoleRecLayer(torch.nn.Module):
    """MoleRec model.

    Paper: Nianzu Yang et al. MoleRec: Combinatorial Drug Recommendation
    with Substructure-Aware Molecular Representation Learning. WWW 2023.

    This layer is used in the MoleRec model. But it can also be used as a
    standalone layer.

    Args:
        hidden_size: hidden feature size.
        coef: coefficient of ddi loss weight annealing. larger coefficient
            means higher penalty to the drug-drug-interaction. Default is 2.5.
        target_ddi: DDI acceptance rate. Default is 0.06.
        GNN_layers: the number of layers of GNNs encoding molecule and
            substructures. Default is 4.
        dropout: the dropout ratio of model. Default is 0.7.
        multiloss_weight: the weight of multilabel_margin_loss for
            multilabel classification. Value should be set between [0, 1].
            Default is 0.05
    """

    def __init__(
        self,
        hidden_size: int,
        coef: float = 2.5,
        target_ddi: float = 0.08,
        GNN_layers: int = 4,
        dropout: float = 0.5,
        multiloss_weight: float = 0.05,
        **kwargs,
    ):
        super(MoleRecLayer, self).__init__()

        dependencies = ["ogb>=1.3.5"]

        # test whether the ogb and torch_scatter packages are ready
        try:
            pkg_resources.require(dependencies)
            global smiles2graph, AtomEncoder, BondEncoder
            from ogb.utils import smiles2graph
            from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder
        except Exception as e:
            print(
                "Please follow the error message and install the [ogb>=1.3.5] packages first."
            )
            print(e)

        self.hidden_size = hidden_size
        self.coef, self.target_ddi = coef, target_ddi
        GNN_para = {
            "num_layers": GNN_layers,
            "dropout": dropout,
            "embedding_dim": hidden_size,
        }
        self.substructure_encoder = GINGraph(**GNN_para)
        self.molecule_encoder = GINGraph(**GNN_para)
        self.substructure_interaction_module = SAB(
            hidden_size, hidden_size, 2, use_ln=True
        )
        self.combination_feature_aggregator = AttnAgg(
            hidden_size, hidden_size, hidden_size
        )
        score_extractor = [
            torch.nn.Linear(hidden_size, hidden_size // 2),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_size // 2, 1),
        ]
        self.score_extractor = torch.nn.Sequential(*score_extractor)
        self.multiloss_weight = multiloss_weight

    def calc_loss(
        self,
        logits: torch.Tensor,
        y_prob: torch.Tensor,
        ddi_adj: torch.Tensor,
        labels: torch.Tensor,
        label_index: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        mul_pred_prob = y_prob.T @ y_prob  # (voc_size, voc_size)
        ddi_loss = (mul_pred_prob * ddi_adj).sum() / (ddi_adj.shape[0] ** 2)

        y_pred = y_prob.detach().cpu().numpy()
        y_pred[y_pred >= 0.5] = 1
        y_pred[y_pred < 0.5] = 0
        y_pred = [np.where(sample == 1)[0] for sample in y_pred]

        loss_cls = binary_cross_entropy_with_logits(logits, labels)
        if self.multiloss_weight > 0 and label_index is not None:
            loss_multi = multilabel_margin_loss(y_prob, label_index)
            loss_cls = (
                self.multiloss_weight * loss_multi
                + (1 - self.multiloss_weight) * loss_cls
            )

        cur_ddi_rate = ddi_rate_score(y_pred, ddi_adj.cpu().numpy())
        if cur_ddi_rate > self.target_ddi:
            beta = self.coef * (1 - (cur_ddi_rate / self.target_ddi))
            beta = min(math.exp(beta), 1)
            loss = beta * loss_cls + (1 - beta) * ddi_loss
        else:
            loss = loss_cls
        return loss

    def forward(
        self,
        patient_emb: torch.Tensor,
        drugs: torch.Tensor,
        average_projection: torch.Tensor,
        ddi_adj: torch.Tensor,
        substructure_mask: torch.Tensor,
        substructure_graph: Union[StaticParaDict, Dict[str, Union[int, torch.Tensor]]],
        molecule_graph: Union[StaticParaDict, Dict[str, Union[int, torch.Tensor]]],
        mask: Optional[torch.tensor] = None,
        drug_indexes: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Forward propagation.

        Args:
            patient_emb: a tensor of shape [patient, visit, num_substructures],
                representating the relation between each patient visit and
                each substructures.
            drugs: a multihot tensor of shape [patient, num_labels].
            mask: an optional tensor of shape [patient, visit] where 1
                indicates valid visits and 0 indicates invalid visits.
            substructure_mask: tensor of shape [num_drugs, num_substructures],
                representing whether a substructure shows up in one of the
                molecule of each drug.
            average_projection: a tensor of shape [num_drugs, num_molecules]
                representing the average projection for aggregating multiple
                molecules of the same drug into one vector.
            substructure_graph: a dictionary representating a graph batch
                of all substructures, where each graph is extracted via
                'smiles2graph' api of ogb library.
            molecule_graph: dictionary with same form of substructure_graph,
                representing the graph batch of all molecules.
            ddi_adj: an adjacency tensor for drug drug interaction
                of shape [num_drugs, num_drugs].
            drug_indexes: the index version of drugs (ground truth) of shape
                [patient, num_labels], padded with -1
        Returns:
            loss: a scalar tensor representing the loss.
            y_prob: a tensor of shape [patient, num_labels] representing
                the probability of each drug.
        """
        if mask is None:
            mask = torch.ones_like(patient_emb[:, :, 0])
        substructure_relation = get_last_visit(patient_emb, mask)
        # [patient, num_substructures]

        substructure_embedding = self.substructure_interaction_module(
            self.substructure_encoder(substructure_graph).unsqueeze(0)
        ).squeeze(0)

        if substructure_relation.shape[-1] != substructure_embedding.shape[0]:
            raise RuntimeError(
                "the substructure relation vector of each patient should have "
                "the same dimension as the number of substructure"
            )

        molecule_embedding = self.molecule_encoder(molecule_graph)
        molecule_embedding = torch.mm(average_projection, molecule_embedding)

        combination_embedding = self.combination_feature_aggregator(
            molecule_embedding,
            substructure_embedding,
            substructure_relation,
            torch.logical_not(substructure_mask > 0),
        )
        # [patient, num_drugs, hidden]
        logits = self.score_extractor(combination_embedding).squeeze(-1)

        y_prob = torch.sigmoid(logits)

        loss = self.calc_loss(logits, y_prob, ddi_adj, drugs, drug_indexes)

        return loss, y_prob


class MoleRec(BaseModel):
    """MoleRec model.

    Paper: Nianzu Yang et al. MoleRec: Combinatorial Drug Recommendation
    with Substructure-Aware Molecular Representation Learning. WWW 2023.

    Note:
        This model is only for medication prediction which takes conditions
        and procedures as feature_keys, and drugs as label_key. It only
        operates on the visit level.

    Note:
        This model only accepts ATC level 3 as medication codes.

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension. Default is 128.
        num_rnn_layers: the number of layers used in RNN. Default is 1.
        num_gnn_layers: the number of layers used in GNN. Default is 4.
        dropout: the dropout rate. Default is 0.7.
        **kwargs: other parameters for the MoleRec layer.
    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        embedding_dim: int = 64,
        hidden_dim: int = 64,
        num_rnn_layers: int = 1,
        num_gnn_layers: int = 4,
        dropout: float = 0.5,
        **kwargs,
    ):
        super(MoleRec, self).__init__(
            dataset=dataset,
            feature_keys=["conditions", "procedures"],
            label_key="drugs",
            mode="multilabel",
        )

        dependencies = ["ogb>=1.3.5"]

        # test whether the ogb and torch_scatter packages are ready
        try:
            pkg_resources.require(dependencies)
            global smiles2graph, AtomEncoder, BondEncoder
            from ogb.utils import smiles2graph
            from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder
        except Exception as e:
            print(
                "Please follow the error message and install the [ogb>=1.3.5] packages first."
            )
            print(e)

        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_rnn_layers = num_rnn_layers
        self.num_gnn_layers = num_gnn_layers
        self.dropout = dropout
        self.dropout_fn = torch.nn.Dropout(dropout)

        self.feat_tokenizers = self.get_feature_tokenizers()
        self.label_tokenizer = self.get_label_tokenizer()
        self.embeddings = self.get_embedding_layers(self.feat_tokenizers, embedding_dim)

        self.label_size = self.label_tokenizer.get_vocabulary_size()

        self.ddi_adj = torch.nn.Parameter(self.generate_ddi_adj(), requires_grad=False)
        self.all_smiles_list = self.generate_smiles_list()

        substructure_mask, self.substructure_smiles = self.generate_substructure_mask()

        self.substructure_mask = torch.nn.Parameter(
            substructure_mask, requires_grad=False
        )

        average_projection, self.all_smiles_flatten = self.generate_average_projection()

        self.average_projection = torch.nn.Parameter(
            average_projection, requires_grad=False
        )
        self.substructure_graphs = StaticParaDict(
            **graph_batch_from_smiles(self.substructure_smiles)
        )
        self.molecule_graphs = StaticParaDict(
            **graph_batch_from_smiles(self.all_smiles_flatten)
        )

        self.rnns = torch.nn.ModuleDict(
            {
                x: torch.nn.GRU(
                    embedding_dim,
                    hidden_dim,
                    num_layers=num_rnn_layers,
                    dropout=dropout if num_rnn_layers > 1 else 0,
                    batch_first=True,
                )
                for x in ["conditions", "procedures"]
            }
        )
        num_substructures = substructure_mask.shape[1]
        self.substructure_relation = torch.nn.Sequential(
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim * 2, hidden_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, num_substructures),
        )
        self.layer = MoleRecLayer(
            hidden_size=hidden_dim, dropout=dropout, GNN_layers=num_gnn_layers, **kwargs
        )

        if "GNN_layers" in kwargs:
            raise ValueError("number of GNN layers is determined by num_gnn_layers")
        if "hidden_size" in kwargs:
            raise ValueError("hidden_size is determined by hidden_dim")

    def generate_ddi_adj(self) -> torch.FloatTensor:
        """Generates the DDI graph adjacency matrix."""
        atc = ATC()
        ddi = atc.get_ddi(gamenet_ddi=True)
        vocab_to_index = self.label_tokenizer.vocabulary
        ddi_adj = np.zeros((self.label_size, self.label_size))
        ddi_atc3 = [
            [ATC.convert(l[0], level=3), ATC.convert(l[1], level=3)] for l in ddi
        ]
        for atc_i, atc_j in ddi_atc3:
            if atc_i in vocab_to_index and atc_j in vocab_to_index:
                ddi_adj[vocab_to_index(atc_i), vocab_to_index(atc_j)] = 1
                ddi_adj[vocab_to_index(atc_j), vocab_to_index(atc_i)] = 1
        ddi_adj = torch.FloatTensor(ddi_adj)
        return ddi_adj

    def generate_substructure_mask(self) -> Tuple[torch.Tensor, List[str]]:
        # Generates the molecular segmentation mask H and substructure smiles.
        all_substructures_list = [[] for _ in range(self.label_size)]
        for index, smiles_list in enumerate(self.all_smiles_list):
            for smiles in smiles_list:
                mol = Chem.MolFromSmiles(smiles)
                if mol is None:
                    continue
                substructures = Chem.BRICS.BRICSDecompose(mol)
                all_substructures_list[index] += substructures
        # all segment set
        substructures_set = list(set(sum(all_substructures_list, [])))
        # mask_H
        mask_H = np.zeros((self.label_size, len(substructures_set)))
        for index, substructures in enumerate(all_substructures_list):
            for s in substructures:
                mask_H[index, substructures_set.index(s)] = 1
        mask_H = torch.from_numpy(mask_H)
        return mask_H, substructures_set

    def generate_smiles_list(self) -> List[List[str]]:
        """Generates the list of SMILES strings."""
        atc3_to_smiles = {}
        atc = ATC()
        for code in atc.graph.nodes:
            if len(code) != 7:
                continue
            code_atc3 = ATC.convert(code, level=3)
            smiles = atc.graph.nodes[code]["smiles"]
            if smiles != smiles:
                continue
            atc3_to_smiles[code_atc3] = atc3_to_smiles.get(code_atc3, []) + [smiles]
        # just take first one for computational efficiency
        atc3_to_smiles = {k: v[:1] for k, v in atc3_to_smiles.items()}
        all_smiles_list = [[] for _ in range(self.label_size)]
        vocab_to_index = self.label_tokenizer.vocabulary
        for atc3, smiles_list in atc3_to_smiles.items():
            if atc3 in vocab_to_index:
                index = vocab_to_index(atc3)
                all_smiles_list[index] += smiles_list
        return all_smiles_list

    def generate_average_projection(self) -> Tuple[torch.Tensor, List[str]]:
        molecule_set, average_index = [], []
        for smiles_list in self.all_smiles_list:
            """Create each data with the above defined functions."""
            counter = 0  # counter how many drugs are under that ATC-3
            for smiles in smiles_list:
                mol = Chem.MolFromSmiles(smiles)
                if mol is None:
                    continue
                molecule_set.append(smiles)
                counter += 1
            average_index.append(counter)
        average_projection = np.zeros((len(average_index), sum(average_index)))
        col_counter = 0
        for i, item in enumerate(average_index):
            if item <= 0:
                continue
            average_projection[i, col_counter : col_counter + item] = 1 / item
            col_counter += item
        average_projection = torch.FloatTensor(average_projection)
        return average_projection, molecule_set

    def encode_patient(
        self, feature_key: str, raw_values: List[List[List[str]]]
    ) -> torch.Tensor:
        codes = self.feat_tokenizers[feature_key].batch_encode_3d(raw_values)
        codes = torch.tensor(codes, dtype=torch.long, device=self.device)
        embeddings = self.embeddings[feature_key](codes)
        embeddings = torch.sum(self.dropout_fn(embeddings), dim=2)
        outputs, _ = self.rnns[feature_key](embeddings)
        return outputs

    def forward(
        self,
        conditions: List[List[List[str]]],
        procedures: List[List[List[str]]],
        drugs: List[List[str]],
        **kwargs,
    ) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        Args:
            conditions: a nested list in three levels with
                shape [patient, visit, condition].
            procedures: a nested list in three levels with
                shape [patient, visit, procedure].
            drugs: a nested list in two levels [patient, drug].

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the loss.
                y_prob: a tensor of shape [patient, visit, num_labels]
                    representing the probability of each drug.
                y_true: a tensor of shape [patient, visit, num_labels]
                    representing the ground truth of each drug.
        """

        # prepare labels
        labels_index = self.label_tokenizer.batch_encode_2d(
            drugs, padding=False, truncation=False
        )
        # convert to multihot
        labels = batch_to_multihot(labels_index, self.label_size)
        index_labels = -np.ones((len(labels), self.label_size), dtype=np.int64)
        for idx, cont in enumerate(labels_index):
            # remove redundant labels
            cont = list(set(cont))
            index_labels[idx, : len(cont)] = cont
        index_labels = torch.from_numpy(index_labels)

        labels = labels.to(self.device)
        index_labels = index_labels.to(self.device)

        # encoding procs and diags
        condition_emb = self.encode_patient("conditions", conditions)
        procedure_emb = self.encode_patient("procedures", procedures)
        mask = torch.sum(condition_emb, dim=2) != 0

        patient_emb = torch.cat([condition_emb, procedure_emb], dim=-1)
        substruct_rela = self.substructure_relation(patient_emb)

        # calculate loss
        loss, y_prob = self.layer(
            patient_emb=substruct_rela,
            drugs=labels,
            ddi_adj=self.ddi_adj,
            average_projection=self.average_projection,
            substructure_mask=self.substructure_mask,
            substructure_graph=self.substructure_graphs,
            molecule_graph=self.molecule_graphs,
            mask=mask,
            drug_indexes=index_labels,
        )

        return {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": labels,
        }


Here is the code content for retain.py:
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.utils.rnn as rnn_utils

from pyhealth.datasets import SampleEHRDataset
from pyhealth.models import BaseModel

# VALID_OPERATION_LEVEL = ["visit", "event"]


class RETAINLayer(nn.Module):
    """RETAIN layer.

    Paper: Edward Choi et al. RETAIN: An Interpretable Predictive Model for
    Healthcare using Reverse Time Attention Mechanism. NIPS 2016.

    This layer is used in the RETAIN model. But it can also be used as a
    standalone layer.

    Args:
        feature_size: the hidden feature size.
        dropout: dropout rate. Default is 0.5.

    Examples:
        >>> from pyhealth.models import RETAINLayer
        >>> input = torch.randn(3, 128, 64)  # [batch size, sequence len, feature_size]
        >>> layer = RETAINLayer(64)
        >>> c = layer(input)
        >>> c.shape
        torch.Size([3, 64])
    """

    def __init__(
        self,
        feature_size: int,
        dropout: float = 0.5,
    ):
        super(RETAINLayer, self).__init__()
        self.feature_size = feature_size
        self.dropout = dropout
        self.dropout_layer = nn.Dropout(p=self.dropout)

        self.alpha_gru = nn.GRU(feature_size, feature_size, batch_first=True)
        self.beta_gru = nn.GRU(feature_size, feature_size, batch_first=True)

        self.alpha_li = nn.Linear(feature_size, 1)
        self.beta_li = nn.Linear(feature_size, feature_size)

    @staticmethod
    def reverse_x(input, lengths):
        """Reverses the input."""
        reversed_input = input.new(input.size())
        for i, length in enumerate(lengths):
            reversed_input[i, :length] = input[i, :length].flip(dims=[0])
        return reversed_input

    def compute_alpha(self, rx, lengths):
        """Computes alpha attention."""
        rx = rnn_utils.pack_padded_sequence(
            rx, lengths, batch_first=True, enforce_sorted=False
        )
        g, _ = self.alpha_gru(rx)
        g, _ = rnn_utils.pad_packed_sequence(g, batch_first=True)
        attn_alpha = torch.softmax(self.alpha_li(g), dim=1)
        return attn_alpha

    def compute_beta(self, rx, lengths):
        """Computes beta attention."""
        rx = rnn_utils.pack_padded_sequence(
            rx, lengths, batch_first=True, enforce_sorted=False
        )
        h, _ = self.beta_gru(rx)
        h, _ = rnn_utils.pad_packed_sequence(h, batch_first=True)
        attn_beta = torch.tanh(self.beta_li(h))
        return attn_beta

    def forward(
        self,
        x: torch.tensor,
        mask: Optional[torch.tensor] = None,
    ) -> Tuple[torch.tensor, torch.tensor]:
        """Forward propagation.

        Args:
            x: a tensor of shape [batch size, sequence len, feature_size].
            mask: an optional tensor of shape [batch size, sequence len], where
                1 indicates valid and 0 indicates invalid.

        Returns:
            c: a tensor of shape [batch size, feature_size] representing the
                context vector.
        """
        # rnn will only apply dropout between layers
        x = self.dropout_layer(x)
        batch_size = x.size(0)
        if mask is None:
            lengths = torch.full(
                size=(batch_size,), fill_value=x.size(1), dtype=torch.int64
            )
        else:
            lengths = torch.sum(mask.int(), dim=-1).cpu()
        rx = self.reverse_x(x, lengths)
        attn_alpha = self.compute_alpha(rx, lengths)
        attn_beta = self.compute_beta(rx, lengths)
        c = attn_alpha * attn_beta * x  # (patient, sequence len, feature_size)
        c = torch.sum(c, dim=1)  # (patient, feature_size)
        return c


class RETAIN(BaseModel):
    """RETAIN model.

    Paper: Edward Choi et al. RETAIN: An Interpretable Predictive Model for
    Healthcare using Reverse Time Attention Mechanism. NIPS 2016.

    Note:
        We use separate Retain layers for different feature_keys.
        Currentluy, we automatically support different input formats:
            - code based input (need to use the embedding table later)
            - float/int based value input
        We follow the current convention for the Retain model:
            - case 1. [code1, code2, code3, ...]
                - we will assume the code follows the order; our model will encode
                each code into a vector and apply Retain on the code level
            - case 2. [[code1, code2]] or [[code1, code2], [code3, code4, code5], ...]
                - we will assume the inner bracket follows the order; our model first
                use the embedding table to encode each code into a vector and then use
                average/mean pooling to get one vector for one inner bracket; then use
                Retain one the braket level
            - case 3. [[1.5, 2.0, 0.0]] or [[1.5, 2.0, 0.0], [8, 1.2, 4.5], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run Retain directly
                on the inner bracket level, similar to case 1 after embedding table
            - case 4. [[[1.5, 2.0, 0.0]]] or [[[1.5, 2.0, 0.0], [8, 1.2, 4.5]], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run Retain directly
                on the inner bracket level, similar to case 2 after embedding table

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        embedding_dim: the embedding dimension. Default is 128.
        **kwargs: other parameters for the RETAIN layer.


    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
        ...             "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        ...             "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
        ...             "list_list_vectors": [
        ...                 [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
        ...                 [[7.7, 8.5, 9.4]],
        ...             ],
        ...             "label": 1,
        ...         },
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-1",
        ...             "list_codes": [
        ...                 "55154191800",
        ...                 "551541928",
        ...                 "55154192800",
        ...                 "705182798",
        ...                 "70518279800",
        ...             ],
        ...             "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
        ...             "list_list_codes": [["A04A", "B035", "C129"]],
        ...             "list_list_vectors": [
        ...                 [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
        ...             ],
        ...             "label": 0,
        ...         },
        ...     ]
        >>> dataset = SampleEHRDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import RETAIN
        >>> model = RETAIN(
        ...         dataset=dataset,
        ...         feature_keys=[
        ...             "list_codes",
        ...             "list_vectors",
        ...             "list_list_codes",
        ...             "list_list_vectors",
        ...         ],
        ...         label_key="label",
        ...         mode="binary",
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(0.5640, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),
            'y_prob': tensor([[0.5325],
                            [0.3922]], grad_fn=<SigmoidBackward0>),
            'y_true': tensor([[1.],
                            [0.]]),
            'logit': tensor([[ 0.1303],
                            [-0.4382]], grad_fn=<AddmmBackward0>)
        }
        >>>

    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        embedding_dim: int = 128,
        **kwargs,
    ):
        super(RETAIN, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim

        # validate kwargs for RETAIN layer
        if "feature_size" in kwargs:
            raise ValueError("feature_size is determined by embedding_dim")

        # the key of self.feat_tokenizers only contains the code based inputs
        self.feat_tokenizers = {}
        self.label_tokenizer = self.get_label_tokenizer()
        # the key of self.embeddings only contains the code based inputs
        self.embeddings = nn.ModuleDict()
        # the key of self.linear_layers only contains the float/int based inputs
        self.linear_layers = nn.ModuleDict()

        # add feature RETAIN layers
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            # sanity check
            if input_info["type"] not in [str, float, int]:
                raise ValueError(
                    "RETAIN only supports str code, float and int as input types"
                )
            elif (input_info["type"] == str) and (input_info["dim"] not in [2, 3]):
                raise ValueError(
                    "RETAIN only supports 2-dim or 3-dim str code as input types"
                )
            elif (input_info["type"] in [float, int]) and (
                input_info["dim"] not in [2, 3]
            ):
                raise ValueError(
                    "RETAIN only supports 2-dim or 3-dim float and int as input types"
                )
            # for code based input, we need Type
            # for float/int based input, we need Type, input_dim
            self.add_feature_transform_layer(feature_key, input_info)

        self.retain = nn.ModuleDict()
        for feature_key in feature_keys:
            self.retain[feature_key] = RETAINLayer(feature_size=embedding_dim, **kwargs)

        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(len(self.feature_keys) * self.embedding_dim, output_size)

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        The label `kwargs[self.label_key]` is a list of labels for each patient.

        Args:
            **kwargs: keyword arguments for the model. The keys must contain
                all the feature keys and the label key.

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the loss.
                y_prob: a tensor representing the predicted probabilities.
                y_true: a tensor representing the true labels.
        """
        patient_emb = []
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            dim_, type_ = input_info["dim"], input_info["type"]

            # for case 1: [code1, code2, code3, ...]
            if (dim_ == 2) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_2d(
                    kwargs[feature_key]
                )
                # (patient, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, event)
                mask = torch.sum(x, dim=2) != 0

            # for case 2: [[code1, code2], [code3, ...], ...]
            elif (dim_ == 3) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_3d(
                    kwargs[feature_key]
                )
                # (patient, visit, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, visit, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                # (patient, visit)
                mask = torch.sum(x, dim=2) != 0

            # for case 3: [[1.5, 2.0, 0.0], ...]
            elif (dim_ == 2) and (type_ in [float, int]):
                x, mask = self.padding2d(kwargs[feature_key])
                # (patient, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, event, embedding_dim)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask.bool().to(self.device)

            # for case 4: [[[1.5, 2.0, 0.0], [1.8, 2.4, 6.0]], ...]
            elif (dim_ == 3) and (type_ in [float, int]):
                x, mask = self.padding3d(kwargs[feature_key])
                # (patient, visit, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask[:, :, 0]
                mask = mask.bool().to(self.device)

            else:
                raise NotImplementedError

            x = self.retain[feature_key](x, mask)
            patient_emb.append(x)

        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)
        y_prob = self.prepare_y_prob(logits)
        results = {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": y_true,
            "logit": logits,
        }
        if kwargs.get("embed", False):
            results["embed"] = patient_emb
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleEHRDataset

    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            # "single_vector": [1, 2, 3],
            "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
            "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
            "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
            "list_list_vectors": [
                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
                [[7.7, 8.5, 9.4]],
            ],
            "label": 1,
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-1",
            # "single_vector": [1, 5, 8],
            "list_codes": [
                "55154191800",
                "551541928",
                "55154192800",
                "705182798",
                "70518279800",
            ],
            "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
            "list_list_codes": [["A04A", "B035", "C129"]],
            "list_list_vectors": [
                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
            ],
            "label": 0,
        },
    ]

    # dataset
    dataset = SampleEHRDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = RETAIN(
        dataset=dataset,
        feature_keys=[
            "list_codes",
            "list_vectors",
            "list_list_codes",
            # "list_list_vectors",
        ],
        label_key="label",
        mode="binary",
    )

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for rnn.py:
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.utils.rnn as rnn_utils

from pyhealth.datasets import SampleEHRDataset
from pyhealth.models import BaseModel

# VALID_OPERATION_LEVEL = ["visit", "event"]


class RNNLayer(nn.Module):
    """Recurrent neural network layer.

    This layer wraps the PyTorch RNN layer with masking and dropout support. It is
    used in the RNN model. But it can also be used as a standalone layer.

    Args:
        input_size: input feature size.
        hidden_size: hidden feature size.
        rnn_type: type of rnn, one of "RNN", "LSTM", "GRU". Default is "GRU".
        num_layers: number of recurrent layers. Default is 1.
        dropout: dropout rate. If non-zero, introduces a Dropout layer before each
            RNN layer. Default is 0.5.
        bidirectional: whether to use bidirectional recurrent layers. If True,
            a fully-connected layer is applied to the concatenation of the forward
            and backward hidden states to reduce the dimension to hidden_size.
            Default is False.

    Examples:
        >>> from pyhealth.models import RNNLayer
        >>> input = torch.randn(3, 128, 5)  # [batch size, sequence len, input_size]
        >>> layer = RNNLayer(5, 64)
        >>> outputs, last_outputs = layer(input)
        >>> outputs.shape
        torch.Size([3, 128, 64])
        >>> last_outputs.shape
        torch.Size([3, 64])
    """

    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        rnn_type: str = "GRU",
        num_layers: int = 1,
        dropout: float = 0.5,
        bidirectional: bool = False,
    ):
        super(RNNLayer, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.rnn_type = rnn_type
        self.num_layers = num_layers
        self.dropout = dropout
        self.bidirectional = bidirectional

        self.dropout_layer = nn.Dropout(dropout)
        self.num_directions = 2 if bidirectional else 1
        rnn_module = getattr(nn, rnn_type)
        self.rnn = rnn_module(
            input_size,
            hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=bidirectional,
            batch_first=True,
        )
        if bidirectional:
            self.down_projection = nn.Linear(hidden_size * 2, hidden_size)

    def forward(
        self,
        x: torch.tensor,
        mask: Optional[torch.tensor] = None,
    ) -> Tuple[torch.tensor, torch.tensor]:
        """Forward propagation.

        Args:
            x: a tensor of shape [batch size, sequence len, input size].
            mask: an optional tensor of shape [batch size, sequence len], where
                1 indicates valid and 0 indicates invalid.

        Returns:
            outputs: a tensor of shape [batch size, sequence len, hidden size],
                containing the output features for each time step.
            last_outputs: a tensor of shape [batch size, hidden size], containing
                the output features for the last time step.
        """
        # pytorch's rnn will only apply dropout between layers
        x = self.dropout_layer(x)
        batch_size = x.size(0)
        if mask is None:
            lengths = torch.full(
                size=(batch_size,), fill_value=x.size(1), dtype=torch.int64
            )
        else:
            lengths = torch.sum(mask.int(), dim=-1).cpu()
        x = rnn_utils.pack_padded_sequence(
            x, lengths, batch_first=True, enforce_sorted=False
        )
        outputs, _ = self.rnn(x)
        outputs, _ = rnn_utils.pad_packed_sequence(outputs, batch_first=True)
        if not self.bidirectional:
            last_outputs = outputs[torch.arange(batch_size), (lengths - 1), :]
            return outputs, last_outputs
        else:
            outputs = outputs.view(batch_size, outputs.shape[1], 2, -1)
            f_last_outputs = outputs[torch.arange(batch_size), (lengths - 1), 0, :]
            b_last_outputs = outputs[:, 0, 1, :]
            last_outputs = torch.cat([f_last_outputs, b_last_outputs], dim=-1)
            outputs = outputs.view(batch_size, outputs.shape[1], -1)
            last_outputs = self.down_projection(last_outputs)
            outputs = self.down_projection(outputs)
            return outputs, last_outputs


class RNN(BaseModel):
    """Recurrent neural network model.

    This model applies a separate RNN layer for each feature, and then concatenates
    the final hidden states of each RNN layer. The concatenated hidden states are
    then fed into a fully connected layer to make predictions.

    Note:
        We use separate rnn layers for different feature_keys.
        Currently, we automatically support different input formats:
            - code based input (need to use the embedding table later)
            - float/int based value input
        We follow the current convention for the rnn model:
            - case 1. [code1, code2, code3, ...]
                - we will assume the code follows the order; our model will encode
                each code into a vector and apply rnn on the code level
            - case 2. [[code1, code2]] or [[code1, code2], [code3, code4, code5], ...]
                - we will assume the inner bracket follows the order; our model first
                use the embedding table to encode each code into a vector and then use
                average/mean pooling to get one vector for one inner bracket; then use
                rnn one the braket level
            - case 3. [[1.5, 2.0, 0.0]] or [[1.5, 2.0, 0.0], [8, 1.2, 4.5], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run rnn directly
                on the inner bracket level, similar to case 1 after embedding table
            - case 4. [[[1.5, 2.0, 0.0]]] or [[[1.5, 2.0, 0.0], [8, 1.2, 4.5]], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run rnn directly
                on the inner bracket level, similar to case 2 after embedding table

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension. Default is 128.
        **kwargs: other parameters for the RNN layer.

    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
        ...             "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        ...             "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
        ...             "list_list_vectors": [
        ...                 [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
        ...                 [[7.7, 8.5, 9.4]],
        ...             ],
        ...             "label": 1,
        ...         },
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-1",
        ...             "list_codes": [
        ...                 "55154191800",
        ...                 "551541928",
        ...                 "55154192800",
        ...                 "705182798",
        ...                 "70518279800",
        ...             ],
        ...             "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
        ...             "list_list_codes": [["A04A", "B035", "C129"]],
        ...             "list_list_vectors": [
        ...                 [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
        ...             ],
        ...             "label": 0,
        ...         },
        ...     ]
        >>> dataset = SampleEHRDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import RNN
        >>> model = RNN(
        ...         dataset=dataset,
        ...         feature_keys=[
        ...             "list_codes",
        ...             "list_vectors",
        ...             "list_list_codes",
        ...             "list_list_vectors",
        ...         ],
        ...         label_key="label",
        ...         mode="binary",
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(0.8056, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),
            'y_prob': tensor([[0.5906],
                            [0.6620]], grad_fn=<SigmoidBackward0>),
            'y_true': tensor([[1.],
                            [0.]]),
            'logit': tensor([[0.3666],
                            [0.6721]], grad_fn=<AddmmBackward0>)
        }
        >>>


    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        **kwargs
    ):
        super(RNN, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

        # validate kwargs for RNN layer
        if "input_size" in kwargs:
            raise ValueError("input_size is determined by embedding_dim")
        if "hidden_size" in kwargs:
            raise ValueError("hidden_size is determined by hidden_dim")

        # the key of self.feat_tokenizers only contains the code based inputs
        self.feat_tokenizers = {}
        self.label_tokenizer = self.get_label_tokenizer()
        # the key of self.embeddings only contains the code based inputs
        self.embeddings = nn.ModuleDict()
        # the key of self.linear_layers only contains the float/int based inputs
        self.linear_layers = nn.ModuleDict()

        # add feature RNN layers
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            # sanity check
            if input_info["type"] not in [str, float, int]:
                raise ValueError(
                    "RNN only supports str code, float and int as input types"
                )
            elif (input_info["type"] == str) and (input_info["dim"] not in [2, 3]):
                raise ValueError(
                    "RNN only supports 2-dim or 3-dim str code as input types"
                )
            elif (input_info["type"] in [float, int]) and (
                input_info["dim"] not in [2, 3]
            ):
                raise ValueError(
                    "RNN only supports 2-dim or 3-dim float and int as input types"
                )
            # for code based input, we need Type
            # for float/int based input, we need Type, input_dim
            self.add_feature_transform_layer(feature_key, input_info)

        self.rnn = nn.ModuleDict()
        for feature_key in feature_keys:
            self.rnn[feature_key] = RNNLayer(
                input_size=embedding_dim, hidden_size=hidden_dim, **kwargs
            )
        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(len(self.feature_keys) * self.hidden_dim, output_size)

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        The label `kwargs[self.label_key]` is a list of labels for each patient.

        Args:
            **kwargs: keyword arguments for the model. The keys must contain
                all the feature keys and the label key.

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the loss.
                y_prob: a tensor representing the predicted probabilities.
                y_true: a tensor representing the true labels.
        """
        patient_emb = []
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            dim_, type_ = input_info["dim"], input_info["type"]

            # for case 1: [code1, code2, code3, ...]
            if (dim_ == 2) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_2d(
                    kwargs[feature_key]
                )
                # (patient, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, event)
                mask = torch.any(x !=0, dim=2)

            # for case 2: [[code1, code2], [code3, ...], ...]
            elif (dim_ == 3) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_3d(
                    kwargs[feature_key]
                )
                # (patient, visit, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, visit, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                # (patient, visit)
                mask = torch.any(x !=0, dim=2)

            # for case 3: [[1.5, 2.0, 0.0], ...]
            elif (dim_ == 2) and (type_ in [float, int]):
                x, mask = self.padding2d(kwargs[feature_key])
                # (patient, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, event, embedding_dim)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask.bool().to(self.device)

            # for case 4: [[[1.5, 2.0, 0.0], [1.8, 2.4, 6.0]], ...]
            elif (dim_ == 3) and (type_ in [float, int]):
                x, mask = self.padding3d(kwargs[feature_key])
                # (patient, visit, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask[:, :, 0]
                mask = mask.bool().to(self.device)

            else:
                raise NotImplementedError

            _, x = self.rnn[feature_key](x, mask)
            patient_emb.append(x)

        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)
        y_prob = self.prepare_y_prob(logits)
        results = {"loss": loss, "y_prob": y_prob, "y_true": y_true, "logit": logits}
        if kwargs.get("embed", False):
            results["embed"] = patient_emb
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleEHRDataset

    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            # "single_vector": [1, 2, 3],
            "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
            "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
            "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
            "list_list_vectors": [
                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
                [[7.7, 8.5, 9.4]],
            ],
            "label": 1,
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-1",
            # "single_vector": [1, 5, 8],
            "list_codes": [
                "55154191800",
                "551541928",
                "55154192800",
                "705182798",
                "70518279800",
            ],
            "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
            "list_list_codes": [["A04A", "B035", "C129"]],
            "list_list_vectors": [
                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
            ],
            "label": 0,
        },
    ]

    # dataset
    dataset = SampleEHRDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = RNN(
        dataset=dataset,
        feature_keys=[
            "list_codes",
            "list_vectors",
            "list_list_codes",
            "list_list_vectors",
        ],
        label_key="label",
        mode="binary",
    )

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for safedrug.py:
from collections import defaultdict
from copy import deepcopy
from typing import List, Tuple, Dict, Optional

import numpy as np
import rdkit.Chem.BRICS as BRICS
import torch
import torch.nn as nn
from rdkit import Chem

from pyhealth.datasets import SampleEHRDataset
from pyhealth.medcode import ATC
from pyhealth.metrics import ddi_rate_score
from pyhealth.models import BaseModel
from pyhealth.models.utils import get_last_visit


class MaskLinear(nn.Module):
    """MaskLinear layer.

    This layer wraps the PyTorch linear layer and adds a hard mask for
    the parameter matrix. It is used in the SafeDrug model.

    Args:
        in_features: input feature size.
        out_features: output feature size.
        bias: whether to use bias. Default is True.
    """

    def __init__(self, in_features: int, out_features: int, bias=True):
        super(MaskLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))
        if bias:
            self.bias = nn.Parameter(torch.FloatTensor(out_features))
        else:
            self.register_parameter("bias", None)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1.0 / self.weight.size(1) ** 0.5
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input: torch.tensor, mask: torch.tensor) -> torch.tensor:
        """
        Args:
            input: input feature tensor of shape [batch size, ..., input_size].
            mask: mask tensor of shape [input_size, output_size], i.e., the same
                size as the weight matrix.

        Returns:
            Output tensor of shape [batch size, ..., output_size].
        """
        weight = torch.mul(self.weight, mask)
        output = torch.mm(input, weight)
        if self.bias is not None:
            return output + self.bias
        else:
            return output


class MolecularGraphNeuralNetwork(nn.Module):
    """Molecular Graph Neural Network.

    Paper: Masashi Tsubaki et al. Compound-protein interaction
    prediction with end-to-end learning of neural networks for
    graphs and sequences. Bioinformatics, 2019.

    Args:
        num_fingerprints: total number of fingerprints.
        dim: embedding dimension of the fingerprint vectors.
        layer_hidden: number of hidden layers.
    """

    def __init__(self, num_fingerprints, dim, layer_hidden):
        super(MolecularGraphNeuralNetwork, self).__init__()
        self.layer_hidden = layer_hidden
        self.embed_fingerprint = nn.Embedding(num_fingerprints, dim)
        self.W_fingerprint = nn.ModuleList(
            [nn.Linear(dim, dim) for _ in range(layer_hidden)]
        )

    def update(self, matrix, vectors, layer):
        hidden_vectors = torch.relu(self.W_fingerprint[layer](vectors))
        return hidden_vectors + torch.mm(matrix, hidden_vectors)

    def sum(self, vectors, axis):
        sum_vectors = [torch.sum(v, 0) for v in torch.split(vectors, axis)]
        return torch.stack(sum_vectors)

    def mean(self, vectors, axis):
        mean_vectors = [torch.mean(v, 0) for v in torch.split(vectors, axis)]
        return torch.stack(mean_vectors)

    def forward(self, fingerprints, adjacencies, molecular_sizes):
        """
        Args:
            fingerprints: a list of fingerprints
            adjacencies: a list of adjacency matrices
            molecular_sizes: a list of the number of atoms in each molecule
        """
        """MPNN layer (update the fingerprint vectors)."""
        fingerprint_vectors = self.embed_fingerprint(fingerprints)
        for layer in range(self.layer_hidden):
            hs = self.update(adjacencies, fingerprint_vectors, layer)
            # fingerprint_vectors = F.normalize(hs, 2, 1)  # normalize.
            fingerprint_vectors = hs

        """Molecular vector by sum or mean of the fingerprint vectors."""
        molecular_vectors = self.sum(fingerprint_vectors, molecular_sizes)
        # molecular_vectors = self.mean(fingerprint_vectors, molecular_sizes)

        return molecular_vectors


class SafeDrugLayer(nn.Module):
    """SafeDrug model.

    Paper: Chaoqi Yang et al. SafeDrug: Dual Molecular Graph Encoders for
    Recommending Effective and Safe Drug Combinations. IJCAI 2021.

    This layer is used in the SafeDrug model. But it can also be used as a
    standalone layer.

    Args:
        hidden_size: hidden feature size.
        mask_H: the mask matrix H of shape [num_drugs, num_substructures].
        ddi_adj: an adjacency tensor of shape [num_drugs, num_drugs].
        num_fingerprints: total number of different fingerprints.
        molecule_set: a list of molecule tuples (A, B, C) of length num_molecules.
            - A <torch.tensor>: fingerprints of atoms in the molecule
            - B <torch.tensor>: adjacency matrix of the molecule
            - C <int>: molecular_size
        average_projection: a tensor of shape [num_drugs, num_molecules] representing
            the average projection for aggregating multiple molecules of the
            same drug into one vector.
        kp: correcting factor for the proportional signal. Default is 0.5.
        target_ddi: DDI acceptance rate. Default is 0.08.
    """

    def __init__(
        self,
        hidden_size: int,
        mask_H: torch.Tensor,
        ddi_adj: torch.Tensor,
        num_fingerprints: int,
        molecule_set: List[Tuple],
        average_projection: torch.Tensor,
        kp: float = 0.05,
        target_ddi: float = 0.08,
    ):
        super(SafeDrugLayer, self).__init__()
        self.hidden_size = hidden_size
        self.kp = kp
        self.target_ddi = target_ddi

        self.mask_H = nn.Parameter(mask_H, requires_grad=False)
        self.ddi_adj = nn.Parameter(ddi_adj, requires_grad=False)

        # medication space size
        label_size = mask_H.shape[0]

        # local bipartite encoder
        self.bipartite_transform = nn.Linear(hidden_size, mask_H.shape[1])
        self.bipartite_output = MaskLinear(mask_H.shape[1], label_size, False)
        # self.bipartite_output = nn.Linear(mask_H.shape[1], hidden_size)

        # global MPNN encoder (add fingerprints and adjacency matrix to parameter list)
        mpnn_molecule_set = list(zip(*molecule_set))

        # process three parts of information
        fingerprints = torch.cat(mpnn_molecule_set[0])
        self.fingerprints = nn.Parameter(fingerprints, requires_grad=False)
        adjacencies = self.pad(mpnn_molecule_set[1], 0)
        self.adjacencies = nn.Parameter(adjacencies, requires_grad=False)
        self.molecule_sizes = mpnn_molecule_set[2]
        self.average_projection = nn.Parameter(average_projection, requires_grad=False)

        self.mpnn = MolecularGraphNeuralNetwork(
            num_fingerprints, hidden_size, layer_hidden=2
        )
        self.mpnn_output = nn.Linear(label_size, label_size)
        self.mpnn_layernorm = nn.LayerNorm(label_size)

        self.loss_fn = nn.BCEWithLogitsLoss()

    def pad(self, matrices, pad_value):
        """Pads the list of matrices.

        Padding with a pad_value (e.g., 0) for batch processing.
        For example, given a list of matrices [A, B, C], we obtain a new
        matrix [A00, 0B0, 00C], where 0 is the zero (i.e., pad value) matrix.
        """
        shapes = [m.shape for m in matrices]
        M, N = sum([s[0] for s in shapes]), sum([s[1] for s in shapes])
        zeros = torch.FloatTensor(np.zeros((M, N)))
        pad_matrices = pad_value + zeros
        i, j = 0, 0
        for k, matrix in enumerate(matrices):
            m, n = shapes[k]
            pad_matrices[i : i + m, j : j + n] = matrix
            i += m
            j += n
        return pad_matrices

    def calculate_loss(
        self, logits: torch.Tensor, y_prob: torch.Tensor, labels: torch.Tensor
    ) -> torch.Tensor:
        mul_pred_prob = y_prob.T @ y_prob  # (voc_size, voc_size)
        batch_ddi_loss = (
            torch.sum(mul_pred_prob.mul(self.ddi_adj)) / self.ddi_adj.shape[0] ** 2
        )

        y_pred = y_prob.detach().cpu().numpy()
        y_pred[y_pred >= 0.5] = 1
        y_pred[y_pred < 0.5] = 0
        y_pred = [np.where(sample == 1)[0] for sample in y_pred]

        cur_ddi_rate = ddi_rate_score(y_pred, self.ddi_adj.cpu().numpy())
        if cur_ddi_rate > self.target_ddi:
            beta = max(0.0, 1 + (self.target_ddi - cur_ddi_rate) / self.kp)
            add_loss, beta = batch_ddi_loss, beta
        else:
            add_loss, beta = 0, 1

        # obtain target, loss, prob, pred
        bce_loss = self.loss_fn(logits, labels)

        loss = beta * bce_loss + (1 - beta) * add_loss
        return loss

    def forward(
        self,
        patient_emb: torch.tensor,
        drugs: torch.tensor,
        mask: Optional[torch.tensor] = None,
    ) -> Tuple[torch.tensor, torch.tensor]:
        """Forward propagation.

        Args:
            patient_emb: a tensor of shape [patient, visit, input_size].
            drugs: a multihot tensor of shape [patient, num_labels].
            mask: an optional tensor of shape [patient, visit] where 1
                indicates valid visits and 0 indicates invalid visits.

        Returns:
            loss: a scalar tensor representing the loss.
            y_prob: a tensor of shape [patient, num_labels] representing
                the probability of each drug.
        """
        if mask is None:
            mask = torch.ones_like(patient_emb[:, :, 0])

        query = get_last_visit(patient_emb, mask)  # (batch, dim)

        # MPNN Encoder
        MPNN_emb = self.mpnn(
            self.fingerprints, self.adjacencies, self.molecule_sizes
        )  # (#molecule, hidden_size)
        MPNN_emb = torch.mm(self.average_projection, MPNN_emb)  # (#med, hidden_size)
        MPNN_match = torch.sigmoid(torch.mm(query, MPNN_emb.T))  # (patient, #med)
        MPNN_att = self.mpnn_layernorm(
            MPNN_match + self.mpnn_output(MPNN_match)
        )  # (batch, #med)

        # Bipartite Encoder (use the bipartite encoder only for now)
        bipartite_emb = self.bipartite_transform(query)  # (batch, dim)
        bipartite_att = self.bipartite_output(
            bipartite_emb, self.mask_H.T
        )  # (batch, hidden_size)

        # combine
        logits = bipartite_att * MPNN_att

        # calculate the ddi_loss by PID stragegy and add to final loss
        y_prob = torch.sigmoid(logits)

        loss = self.calculate_loss(logits, y_prob, drugs)

        return loss, y_prob


class SafeDrug(BaseModel):
    """SafeDrug model.

    Paper: Chaoqi Yang et al. SafeDrug: Dual Molecular Graph Encoders for
    Recommending Effective and Safe Drug Combinations. IJCAI 2021.

    Note:
        This model is only for medication prediction which takes conditions
        and procedures as feature_keys, and drugs as label_key. It only operates
        on the visit level.

    Note:
        This model only accepts ATC level 3 as medication codes.

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        embedding_dim: the embedding dimension. Default is 128.
        hidden_dim: the hidden dimension. Default is 128.
        num_layers: the number of layers used in RNN. Default is 1.
        dropout: the dropout rate. Default is 0.5.
        **kwargs: other parameters for the SafeDrug layer.
    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        num_layers: int = 1,
        dropout: float = 0.5,
        **kwargs,
    ):
        super(SafeDrug, self).__init__(
            dataset=dataset,
            feature_keys=["conditions", "procedures"],
            label_key="drugs",
            mode="multilabel",
        )
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.dropout = dropout

        self.feat_tokenizers = self.get_feature_tokenizers()
        self.label_tokenizer = self.get_label_tokenizer()
        self.embeddings = self.get_embedding_layers(self.feat_tokenizers, embedding_dim)

        # drug space size
        self.label_size = self.label_tokenizer.get_vocabulary_size()

        self.all_smiles_list = self.generate_smiles_list()
        mask_H = self.generate_mask_H()
        (
            molecule_set,
            num_fingerprints,
            average_projection,
        ) = self.generate_molecule_info()
        ddi_adj = self.generate_ddi_adj()

        self.cond_rnn = nn.GRU(
            embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
        )
        self.proc_rnn = nn.GRU(
            embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
        )
        self.query = nn.Sequential(
            nn.ReLU(),
            nn.Linear(hidden_dim * 2, hidden_dim),
        )

        # validate kwargs for GAMENet layer
        if "hidden_size" in kwargs:
            raise ValueError("hidden_size is determined by hidden_dim")
        if "mask_H" in kwargs:
            raise ValueError("mask_H is determined by the dataset")
        if "ddi_adj" in kwargs:
            raise ValueError("ddi_adj is determined by the dataset")
        if "num_fingerprints" in kwargs:
            raise ValueError("num_fingerprints is determined by the dataset")
        if "molecule_set" in kwargs:
            raise ValueError("molecule_set is determined by the dataset")
        if "average_projection" in kwargs:
            raise ValueError("average_projection is determined by the dataset")
        self.safedrug = SafeDrugLayer(
            hidden_size=hidden_dim,
            mask_H=mask_H,
            ddi_adj=ddi_adj,
            num_fingerprints=num_fingerprints,
            molecule_set=molecule_set,
            average_projection=average_projection,
            **kwargs,
        )

    def generate_ddi_adj(self) -> torch.tensor:
        """Generates the DDI graph adjacency matrix."""
        atc = ATC()
        ddi = atc.get_ddi(gamenet_ddi=True)
        label_size = self.label_tokenizer.get_vocabulary_size()
        vocab_to_index = self.label_tokenizer.vocabulary
        ddi_adj = np.zeros((label_size, label_size))
        ddi_atc3 = [
            [ATC.convert(l[0], level=3), ATC.convert(l[1], level=3)] for l in ddi
        ]
        for atc_i, atc_j in ddi_atc3:
            if atc_i in vocab_to_index and atc_j in vocab_to_index:
                ddi_adj[vocab_to_index(atc_i), vocab_to_index(atc_j)] = 1
                ddi_adj[vocab_to_index(atc_j), vocab_to_index(atc_i)] = 1
        ddi_adj = torch.FloatTensor(ddi_adj)
        return ddi_adj

    def generate_smiles_list(self) -> List[List[str]]:
        """Generates the list of SMILES strings."""
        atc3_to_smiles = {}
        atc = ATC()
        for code in atc.graph.nodes:
            if len(code) != 7:
                continue
            code_atc3 = ATC.convert(code, level=3)
            smiles = atc.graph.nodes[code]["smiles"]
            if smiles != smiles:
                continue
            atc3_to_smiles[code_atc3] = atc3_to_smiles.get(code_atc3, []) + [smiles]
        # just take first one for computational efficiency
        atc3_to_smiles = {k: v[:1] for k, v in atc3_to_smiles.items()}
        all_smiles_list = [[] for _ in range(self.label_size)]
        vocab_to_index = self.label_tokenizer.vocabulary
        for atc3, smiles_list in atc3_to_smiles.items():
            if atc3 in vocab_to_index:
                index = vocab_to_index(atc3)
                all_smiles_list[index] += smiles_list
        return all_smiles_list

    def generate_mask_H(self) -> torch.tensor:
        """Generates the molecular segmentation mask H."""
        all_substructures_list = [[] for _ in range(self.label_size)]
        for index, smiles_list in enumerate(self.all_smiles_list):
            for smiles in smiles_list:
                mol = Chem.MolFromSmiles(smiles)
                if mol is None:
                    continue
                substructures = BRICS.BRICSDecompose(mol)
                all_substructures_list[index] += substructures
        # all segment set
        substructures_set = list(set(sum(all_substructures_list, [])))
        # mask_H
        mask_H = np.zeros((self.label_size, len(substructures_set)))
        for index, substructures in enumerate(all_substructures_list):
            for s in substructures:
                mask_H[index, substructures_set.index(s)] = 1
        mask_H = torch.FloatTensor(mask_H)
        return mask_H

    def generate_molecule_info(self, radius: int = 1):
        """Generates the molecule information."""

        def create_atoms(mol, atom2idx):
            """Transform the atom types in a molecule (e.g., H, C, and O)
            into the indices (e.g., H=0, C=1, and O=2). Note that each atom
            index considers the aromaticity.
            """
            atoms = [a.GetSymbol() for a in mol.GetAtoms()]
            for a in mol.GetAromaticAtoms():
                i = a.GetIdx()
                atoms[i] = (atoms[i], "aromatic")
            atoms = [atom2idx[a] for a in atoms]
            return np.array(atoms)

        def create_ijbonddict(mol, bond2idx):
            """Create a dictionary, in which each key is a node ID
            and each value is the tuples of its neighboring node
            and chemical bond (e.g., single and double) IDs.
            """
            i_jbond_dict = defaultdict(lambda: [])
            for b in mol.GetBonds():
                i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()
                bond = bond2idx[str(b.GetBondType())]
                i_jbond_dict[i].append((j, bond))
                i_jbond_dict[j].append((i, bond))
            return i_jbond_dict

        def extract_fingerprints(r, atoms, i_jbond_dict, fingerprint2idx, edge2idx):
            """Extract the fingerprints from a molecular graph
            based on Weisfeiler-Lehman algorithm.
            """
            nodes = [fingerprint2idx[a] for a in atoms]
            i_jedge_dict = i_jbond_dict

            for _ in range(r):

                """Update each node ID considering its neighboring nodes and edges.
                The updated node IDs are the fingerprint IDs.
                """
                nodes_ = deepcopy(nodes)
                for i, j_edge in i_jedge_dict.items():
                    neighbors = [(nodes[j], edge) for j, edge in j_edge]
                    fingerprint = (nodes[i], tuple(sorted(neighbors)))
                    nodes_[i] = fingerprint2idx[fingerprint]

                """Also update each edge ID considering
                its two nodes on both sides.
                """
                i_jedge_dict_ = defaultdict(list)
                for i, j_edge in i_jedge_dict.items():
                    for j, edge in j_edge:
                        both_side = tuple(sorted((nodes[i], nodes[j])))
                        edge = edge2idx[(both_side, edge)]
                        i_jedge_dict_[i].append((j, edge))

                nodes = deepcopy(nodes_)
                i_jedge_dict = deepcopy(i_jedge_dict_)
                del nodes_, i_jedge_dict_

            return np.array(nodes)

        atom2idx = defaultdict(lambda: len(atom2idx))
        bond2idx = defaultdict(lambda: len(bond2idx))
        fingerprint2idx = defaultdict(lambda: len(fingerprint2idx))
        edge2idx = defaultdict(lambda: len(edge2idx))
        molecule_set, average_index = [], []

        for smiles_list in self.all_smiles_list:
            """Create each data with the above defined functions."""
            counter = 0  # counter how many drugs are under that ATC-3
            for smiles in smiles_list:
                mol = Chem.MolFromSmiles(smiles)
                if mol is None:
                    continue
                mol = Chem.AddHs(mol)
                atoms = create_atoms(mol, atom2idx)
                molecular_size = len(atoms)
                i_jbond_dict = create_ijbonddict(mol, bond2idx)
                fingerprints = extract_fingerprints(
                    radius, atoms, i_jbond_dict, fingerprint2idx, edge2idx
                )
                adjacency = Chem.GetAdjacencyMatrix(mol)
                """Transform the above each data of numpy to pytorch tensor."""
                fingerprints = torch.LongTensor(fingerprints)
                adjacency = torch.FloatTensor(adjacency)
                molecule_set.append((fingerprints, adjacency, molecular_size))
                counter += 1
            average_index.append(counter)

        num_fingerprints = len(fingerprint2idx)
        # transform into projection matrix
        n_col = sum(average_index)
        n_row = len(average_index)
        average_projection = np.zeros((n_row, n_col))
        col_counter = 0
        for i, item in enumerate(average_index):
            if item > 0:
                average_projection[i, col_counter : col_counter + item] = 1 / item
            col_counter += item
        average_projection = torch.FloatTensor(average_projection)
        return molecule_set, num_fingerprints, average_projection

    def forward(
        self,
        conditions: List[List[List[str]]],
        procedures: List[List[List[str]]],
        drugs: List[List[str]],
        **kwargs,
    ) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        Args:
            conditions: a nested list in three levels [patient, visit, condition].
            procedures: a nested list in three levels [patient, visit, procedure].
            drugs: a nested list in two levels [patient, drug].

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the loss.
                y_prob: a tensor of shape [patient, visit, num_labels] representing
                    the probability of each drug.
                y_true: a tensor of shape [patient, visit, num_labels] representing
                    the ground truth of each drug.
        """
        conditions = self.feat_tokenizers["conditions"].batch_encode_3d(conditions)
        # (patient, visit, code)
        conditions = torch.tensor(conditions, dtype=torch.long, device=self.device)
        # (patient, visit, code, embedding_dim)
        conditions = self.embeddings["conditions"](conditions)
        # (patient, visit, embedding_dim)
        conditions = torch.sum(conditions, dim=2)
        # (batch, visit, hidden_size)
        conditions, _ = self.cond_rnn(conditions)

        procedures = self.feat_tokenizers["procedures"].batch_encode_3d(procedures)
        # (patient, visit, code)
        procedures = torch.tensor(procedures, dtype=torch.long, device=self.device)
        # (patient, visit, code, embedding_dim)
        procedures = self.embeddings["procedures"](procedures)
        # (patient, visit, embedding_dim)
        procedures = torch.sum(procedures, dim=2)
        # (batch, visit, hidden_size)
        procedures, _ = self.proc_rnn(procedures)

        # (batch, visit, 2 * hidden_size)
        patient_emb = torch.cat([conditions, procedures], dim=-1)
        # (batch, visit, hidden_size)
        patient_emb = self.query(patient_emb)

        # get mask
        mask = torch.sum(conditions, dim=2) != 0

        drugs = self.prepare_labels(drugs, self.label_tokenizer)

        loss, y_prob = self.safedrug(patient_emb, drugs, mask)

        return {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": drugs,
        }


Here is the code content for sparcnet.py:
import math
from collections import OrderedDict
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from pyhealth.datasets import BaseSignalDataset
from pyhealth.models import BaseModel


class DenseLayer(nn.Sequential):
    """Densely connected layer
    Args:
        input_channels: number of input channels
        growth_rate: rate of growth of channels in this layer
        bn_size: multiplicative factor for the bottleneck layer (does not affect the output size)
        drop_rate: dropout rate
        conv_bias: whether to use bias in convolutional layers
        batch_norm: whether to use batch normalization

    Example:
        >>> x = torch.randn(128, 5, 1000)
        >>> batch, channels, length = x.shape
        >>> model = DenseLayer(channels, 5, 2)
        >>> y = model(x)
        >>> y.shape
        torch.Size([128, 10, 1000])
    """

    def __init__(
        self,
        input_channels,
        growth_rate,
        bn_size,
        drop_rate=0.5,
        conv_bias=True,
        batch_norm=True,
    ):
        super(DenseLayer, self).__init__()
        if batch_norm:
            self.add_module("norm1", nn.BatchNorm1d(input_channels)),
        self.add_module("elu1", nn.ELU()),
        self.add_module(
            "conv1",
            nn.Conv1d(
                input_channels,
                bn_size * growth_rate,
                kernel_size=1,
                stride=1,
                bias=conv_bias,
            ),
        ),
        if batch_norm:
            self.add_module("norm2", nn.BatchNorm1d(bn_size * growth_rate)),
        self.add_module("elu2", nn.ELU()),
        self.add_module(
            "conv2",
            nn.Conv1d(
                bn_size * growth_rate,
                growth_rate,
                kernel_size=3,
                stride=1,
                padding=1,
                bias=conv_bias,
            ),
        ),
        self.drop_rate = drop_rate

    def forward(self, x):
        new_features = super(DenseLayer, self).forward(x)
        new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)
        return torch.cat([x, new_features], 1)


class DenseBlock(nn.Sequential):
    """Densely connected block
    Args:
        num_layers: number of layers in this block
        input_channls: number of input channels
        growth_rate: rate of growth of channels in this layer
        bn_size: multiplicative factor for the bottleneck layer (does not affect the output size)
        drop_rate: dropout rate
        conv_bias: whether to use bias in convolutional layers
        batch_norm: whether to use batch normalization

    Example:
        >>> x = torch.randn(128, 5, 1000)
        >>> batch, channels, length = x.shape
        >>> model = DenseBlock(3, channels, 5, 2)
        >>> y = model(x)
        >>> y.shape
        torch.Size([128, 20, 1000])
    """

    def __init__(
        self,
        num_layers,
        input_channels,
        growth_rate,
        bn_size,
        drop_rate=0.5,
        conv_bias=True,
        batch_norm=True,
    ):
        super(DenseBlock, self).__init__()
        for idx_layer in range(num_layers):
            layer = DenseLayer(
                input_channels + idx_layer * growth_rate,
                growth_rate,
                bn_size,
                drop_rate,
                conv_bias,
                batch_norm,
            )
            self.add_module("denselayer%d" % (idx_layer + 1), layer)


class TransitionLayer(nn.Sequential):
    """pooling transition layer

    Args:
        input_channls: number of input channels
        output_channels: number of output channels
        conv_bias: whether to use bias in convolutional layers
        batch_norm: whether to use batch normalization

    Example:
        >>> x = torch.randn(128, 5, 1000)
        >>> model = TransitionLayer(5, 18)
        >>> y = model(x)
        >>> y.shape
        torch.Size([128, 18, 500])

    """

    def __init__(
        self, input_channels, output_channels, conv_bias=True, batch_norm=True
    ):
        super(TransitionLayer, self).__init__()
        if batch_norm:
            self.add_module("norm", nn.BatchNorm1d(input_channels))
        self.add_module("elu", nn.ELU())
        self.add_module(
            "conv",
            nn.Conv1d(
                input_channels,
                output_channels,
                kernel_size=1,
                stride=1,
                bias=conv_bias,
            ),
        )
        self.add_module("pool", nn.AvgPool1d(kernel_size=2, stride=2))


class SparcNet(BaseModel):
    """The SparcNet model for sleep staging.

    Paper: Jin Jing, et al. Development of Expert-level Classification of Seizures and Rhythmic and
    Periodic Patterns During EEG Interpretation. Neurology 2023.

    Note:
        We use one encoder to handle multiple channel together.

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        embedding_dim: (not used now) the embedding dimension. Default is 128.
        hidden_dim: (not used now) the hidden dimension. Default is 128.
        block_layer: the number of layers in each dense block. Default is 4.
        growth_rate: the growth rate of each dense layer. Default is 16.
        bn_size: the bottleneck size of each dense layer. Default is 16.
        conv_bias: whether to use bias in convolutional layers. Default is True.
        batch_norm: whether to use batch normalization. Default is True.
        **kwargs: other parameters for the Deepr layer.

    Examples:
        >>> from pyhealth.datasets import SampleSignalDataset
        >>> samples = [
        ...         {
        ...             "record_id": "SC4001-0",
        ...             "patient_id": "SC4001",
        ...             "epoch_path": "/home/chaoqiy2/.cache/pyhealth/datasets/2f06a9232e54254cbcb4b62624294d71/SC4001-0.pkl",
        ...             "label": "W",
        ...         },
        ...         {
        ...             "record_id": "SC4001-1",
        ...             "patient_id": "SC4001",
        ...             "epoch_path": "/home/chaoqiy2/.cache/pyhealth/datasets/2f06a9232e54254cbcb4b62624294d71/SC4001-1.pkl",
        ...             "label": "R",
        ...         }
        ...     ]
        >>> dataset = SampleSignalDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import SparcNet
        >>> model = SparcNet(
        ...         dataset=dataset,
        ...         feature_keys=["signal"], # dataloader will load the signal from "epoch_path" and put it in "signal"
        ...         label_key="label",
        ...         mode="multiclass",
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(0.6530, device='cuda:0', grad_fn=<NllLossBackward0>),
            'y_prob': tensor([[0.4459, 0.5541],
                            [0.5111, 0.4889]], device='cuda:0', grad_fn=<SoftmaxBackward0>),
            'y_true': tensor([1, 1], device='cuda:0'),
            'logit': tensor([[-0.2750, -0.0577],
                            [-0.1319, -0.1763]], device='cuda:0', grad_fn=<AddmmBackward0>)
        }

    """

    def __init__(
        self,
        dataset: BaseSignalDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        embedding_dim: int = 128,
        hidden_dim: int = 128,
        block_layers=4,
        growth_rate=16,
        bn_size=16,
        drop_rate=0.5,
        conv_bias=True,
        batch_norm=True,
        **kwargs,
    ):

        super(SparcNet, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )

        """ common """
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

        # TODO: Use more tokens for <gap> for different lengths once the input has such information
        self.label_tokenizer = self.get_label_tokenizer()

        """ input statistics """
        print(f"\n=== Input data statistics ===")
        # obtain input signal size
        signal_info = self.dataset.input_info["signal"]
        in_channels, length = signal_info["n_channels"], signal_info["length"]
        # input signal size (batch, n_channels, length)
        print(f"n_channels: {in_channels}")
        print(f"length: {length}")

        """ define sparcnet """
        # add initial convolutional layer
        out_channels = 2 ** (math.floor(np.log2(in_channels)) + 1)
        first_conv = OrderedDict(
            [
                (
                    "conv0",
                    nn.Conv1d(
                        in_channels,
                        out_channels,
                        kernel_size=7,
                        stride=2,
                        padding=3,
                        bias=conv_bias,
                    ),
                )
            ]
        )
        first_conv["norm0"] = nn.BatchNorm1d(out_channels)
        first_conv["elu0"] = nn.ELU()
        first_conv["pool0"] = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)
        self.encoder = nn.Sequential(first_conv)

        n_channels = out_channels

        # add dense blocks
        for n_layer in np.arange(math.floor(np.log2(length // 4))):
            block = DenseBlock(
                num_layers=block_layers,
                input_channels=n_channels,
                growth_rate=growth_rate,
                bn_size=bn_size,
                drop_rate=drop_rate,
                conv_bias=conv_bias,
                batch_norm=batch_norm,
            )
            self.encoder.add_module("denseblock%d" % (n_layer + 1), block)
            # update number of channels after each dense block
            n_channels = n_channels + block_layers * growth_rate

            trans = TransitionLayer(
                input_channels=n_channels,
                output_channels=n_channels // 2,
                conv_bias=conv_bias,
                batch_norm=batch_norm,
            )
            self.encoder.add_module("transition%d" % (n_layer + 1), trans)
            # update number of channels after each transition layer
            n_channels = n_channels // 2

        """ prediction layer """
        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(n_channels, output_size)

        # Official init from torch repo.
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight.data)
            elif isinstance(m, nn.BatchNorm1d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.bias.data.zero_()

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation."""
        # concat the info within one batch (batch, channel, length)
        x = torch.tensor(
            np.array(kwargs[self.feature_keys[0]]), device=self.device
        ).float()

        # final layer embedding (batch, embedding)
        emb = self.encoder(x).view(x.shape[0], -1)
        # (patient, label_size)
        logits = self.fc(emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)
        y_prob = self.prepare_y_prob(logits)
        results = {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": y_true,
            "logit": logits,
        }
        if kwargs.get("embed", False):
            results["embed"] = emb
        return results


if __name__ == "__main__":
    """
    For dense layer
    """
    # x = torch.randn(128, 5, 1000)
    # batch, channels, length = x.shape
    # model = DenseLayer(channels, 5, 2)
    # y = model(x)
    # print(y.shape)

    """
    For dense block
    """
    # x = torch.randn(128, 5, 1000)
    # batch, channels, length = x.shape
    # model = DenseBlock(3, channels, 5, 2)
    # y = model(x)
    # print(y.shape)

    """
    For transition layer
    """
    # x = torch.randn(128, 5, 1000)
    # batch, channels, length = x.shape
    # model = TransitionLayer(channels, 18)
    # y = model(x)
    # print(y.shape)

    """
    For sparcenet
    """
    from pyhealth.datasets import SampleSignalDataset, get_dataloader

    samples = [
        {
            "record_id": "SC4001-0",
            "patient_id": "SC4001",
            "epoch_path": "/home/chaoqiy2/.cache/pyhealth/datasets/2f06a9232e54254cbcb4b62624294d71/SC4001-0.pkl",
            "label": "W",
        },
        {
            "record_id": "SC4001-0",
            "patient_id": "SC4001",
            "epoch_path": "/home/chaoqiy2/.cache/pyhealth/datasets/2f06a9232e54254cbcb4b62624294d71/SC4001-1.pkl",
            "label": "R",
        },
    ]

    # dataset
    dataset = SampleSignalDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = SparcNet(
        dataset=dataset,
        feature_keys=["signal"],
        label_key="label",
        mode="multiclass",
    ).to("cuda:0")

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for stagenet.py:
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.rnn as rnn_utils

from pyhealth.datasets import SampleEHRDataset
from pyhealth.models import BaseModel
from pyhealth.models.utils import get_last_visit

# VALID_OPERATION_LEVEL = ["visit", "event"]


class StageNetLayer(nn.Module):
    """StageNet layer.

    Paper: Stagenet: Stage-aware neural networks for health risk prediction. WWW 2020.

    This layer is used in the StageNet model. But it can also be used as a
    standalone layer.

    Args:
        input_dim: dynamic feature size.
        chunk_size: the chunk size for the StageNet layer. Default is 128.
        levels: the number of levels for the StageNet layer. levels * chunk_size = hidden_dim in the RNN. Smaller chunk size and more levels can capture more detailed patient status variations. Default is 3.
        conv_size: the size of the convolutional kernel. Default is 10.
        dropconnect: the dropout rate for the dropconnect. Default is 0.3.
        dropout: the dropout rate for the dropout. Default is 0.3.
        dropres: the dropout rate for the residual connection. Default is 0.3.

    Examples:
        >>> from pyhealth.models import StageNetLayer
        >>> input = torch.randn(3, 128, 64)  # [batch size, sequence len, feature_size]
        >>> layer = StageNetLayer(64)
        >>> c, _, _ = layer(input)
        >>> c.shape
        torch.Size([3, 384])
    """

    def __init__(
        self,
        input_dim: int,
        chunk_size: int = 128,
        conv_size: int = 10,
        levels: int = 3,
        dropconnect: int = 0.3,
        dropout: int = 0.3,
        dropres: int = 0.3,
    ):
        super(StageNetLayer, self).__init__()

        self.dropout = dropout
        self.dropconnect = dropconnect
        self.dropres = dropres
        self.input_dim = input_dim
        self.hidden_dim = chunk_size * levels
        self.conv_dim = self.hidden_dim
        self.conv_size = conv_size
        # self.output_dim = output_dim
        self.levels = levels
        self.chunk_size = chunk_size

        self.kernel = nn.Linear(
            int(input_dim + 1), int(self.hidden_dim * 4 + levels * 2)
        )
        nn.init.xavier_uniform_(self.kernel.weight)
        nn.init.zeros_(self.kernel.bias)
        self.recurrent_kernel = nn.Linear(
            int(self.hidden_dim + 1), int(self.hidden_dim * 4 + levels * 2)
        )
        nn.init.orthogonal_(self.recurrent_kernel.weight)
        nn.init.zeros_(self.recurrent_kernel.bias)

        self.nn_scale = nn.Linear(int(self.hidden_dim), int(self.hidden_dim // 6))
        self.nn_rescale = nn.Linear(int(self.hidden_dim // 6), int(self.hidden_dim))
        self.nn_conv = nn.Conv1d(
            int(self.hidden_dim), int(self.conv_dim), int(conv_size), 1
        )
        # self.nn_output = nn.Linear(int(self.conv_dim), int(output_dim))

        if self.dropconnect:
            self.nn_dropconnect = nn.Dropout(p=dropconnect)
            self.nn_dropconnect_r = nn.Dropout(p=dropconnect)
        if self.dropout:
            self.nn_dropout = nn.Dropout(p=dropout)
            self.nn_dropres = nn.Dropout(p=dropres)

    def cumax(self, x, mode="l2r"):
        if mode == "l2r":
            x = torch.softmax(x, dim=-1)
            x = torch.cumsum(x, dim=-1)
            return x
        elif mode == "r2l":
            x = torch.flip(x, [-1])
            x = torch.softmax(x, dim=-1)
            x = torch.cumsum(x, dim=-1)
            return torch.flip(x, [-1])
        else:
            return x

    def step(self, inputs, c_last, h_last, interval, device):
        x_in = inputs.to(device=device)

        # Integrate inter-visit time intervals
        interval = interval.unsqueeze(-1).to(device=device)
        x_out1 = self.kernel(torch.cat((x_in, interval), dim=-1)).to(device)
        x_out2 = self.recurrent_kernel(
            torch.cat((h_last.to(device=device), interval), dim=-1)
        )

        if self.dropconnect:
            x_out1 = self.nn_dropconnect(x_out1)
            x_out2 = self.nn_dropconnect_r(x_out2)
        x_out = x_out1 + x_out2
        f_master_gate = self.cumax(x_out[:, : self.levels], "l2r")
        f_master_gate = f_master_gate.unsqueeze(2).to(device=device)
        i_master_gate = self.cumax(x_out[:, self.levels : self.levels * 2], "r2l")
        i_master_gate = i_master_gate.unsqueeze(2)
        x_out = x_out[:, self.levels * 2 :]
        x_out = x_out.reshape(-1, self.levels * 4, self.chunk_size)
        f_gate = torch.sigmoid(x_out[:, : self.levels]).to(device=device)
        i_gate = torch.sigmoid(x_out[:, self.levels : self.levels * 2]).to(
            device=device
        )
        o_gate = torch.sigmoid(x_out[:, self.levels * 2 : self.levels * 3])
        c_in = torch.tanh(x_out[:, self.levels * 3 :]).to(device=device)
        c_last = c_last.reshape(-1, self.levels, self.chunk_size).to(device=device)
        overlap = (f_master_gate * i_master_gate).to(device=device)
        c_out = (
            overlap * (f_gate * c_last + i_gate * c_in)
            + (f_master_gate - overlap) * c_last
            + (i_master_gate - overlap) * c_in
        )
        h_out = o_gate * torch.tanh(c_out)
        c_out = c_out.reshape(-1, self.hidden_dim)
        h_out = h_out.reshape(-1, self.hidden_dim)
        out = torch.cat([h_out, f_master_gate[..., 0], i_master_gate[..., 0]], 1)
        return out, c_out, h_out

    def forward(
        self,
        x: torch.tensor,
        time: Optional[torch.tensor] = None,
        mask: Optional[torch.tensor] = None,
    ) -> Tuple[torch.tensor]:
        """Forward propagation.

        Args:
            x: a tensor of shape [batch size, sequence len, input_dim].
            static: a tensor of shape [batch size, static_dim].
            mask: an optional tensor of shape [batch size, sequence len], where
                1 indicates valid and 0 indicates invalid.

        Returns:
            last_output: a tensor of shape [batch size, chunk_size*levels] representing the
                patient embedding.
            outputs: a tensor of shape [batch size, sequence len, chunk_size*levels] representing the patient at each time step.
        """
        # rnn will only apply dropout between layers
        batch_size, time_step, feature_dim = x.size()
        device = x.device
        if time == None:
            time = torch.ones(batch_size, time_step)
        time = time.reshape(batch_size, time_step)
        c_out = torch.zeros(batch_size, self.hidden_dim)
        h_out = torch.zeros(batch_size, self.hidden_dim)

        tmp_h = (
            torch.zeros_like(h_out, dtype=torch.float32)
            .view(-1)
            .repeat(self.conv_size)
            .view(self.conv_size, batch_size, self.hidden_dim)
        )
        tmp_dis = torch.zeros((self.conv_size, batch_size))
        h = []
        origin_h = []
        distance = []
        for t in range(time_step):
            out, c_out, h_out = self.step(x[:, t, :], c_out, h_out, time[:, t], device)
            cur_distance = 1 - torch.mean(
                out[..., self.hidden_dim : self.hidden_dim + self.levels], -1
            )
            origin_h.append(out[..., : self.hidden_dim])
            tmp_h = torch.cat(
                (
                    tmp_h[1:].to(device=device),
                    out[..., : self.hidden_dim].unsqueeze(0).to(device=device),
                ),
                0,
            )
            tmp_dis = torch.cat(
                (
                    tmp_dis[1:].to(device=device),
                    cur_distance.unsqueeze(0).to(device=device),
                ),
                0,
            )
            distance.append(cur_distance)

            # Re-weighted convolution operation
            local_dis = tmp_dis.permute(1, 0)
            local_dis = torch.cumsum(local_dis, dim=1)
            local_dis = torch.softmax(local_dis, dim=1)
            local_h = tmp_h.permute(1, 2, 0)
            local_h = local_h * local_dis.unsqueeze(1)

            # Re-calibrate Progression patterns
            local_theme = torch.mean(local_h, dim=-1)
            local_theme = self.nn_scale(local_theme).to(device)
            local_theme = torch.relu(local_theme)
            local_theme = self.nn_rescale(local_theme).to(device)
            local_theme = torch.sigmoid(local_theme)

            local_h = self.nn_conv(local_h).squeeze(-1)
            local_h = local_theme * local_h
            h.append(local_h)

        origin_h = torch.stack(origin_h).permute(1, 0, 2)
        rnn_outputs = torch.stack(h).permute(1, 0, 2)
        if self.dropres > 0.0:
            origin_h = self.nn_dropres(origin_h)
        rnn_outputs = rnn_outputs + origin_h
        rnn_outputs = rnn_outputs.contiguous().view(-1, rnn_outputs.size(-1))
        if self.dropout > 0.0:
            rnn_outputs = self.nn_dropout(rnn_outputs)

        output = rnn_outputs.contiguous().view(batch_size, time_step, self.hidden_dim)
        last_output = get_last_visit(output, mask)

        return last_output, output, torch.stack(distance)


class StageNet(BaseModel):
    """StageNet model.

    Paper: Junyi Gao et al. Stagenet: Stage-aware neural networks for health risk prediction. WWW 2020.

    Note:
        We use separate StageNet layers for different feature_keys.
        Currently, we automatically support different input formats:
            - code based input (need to use the embedding table later)
            - float/int based value input
        We follow the current convention for the StageNet model:
            - case 1. [code1, code2, code3, ...]
                - we will assume the code follows the order; our model will encode
                each code into a vector and apply StageNet on the code level
            - case 2. [[code1, code2]] or [[code1, code2], [code3, code4, code5], ...]
                - we will assume the inner bracket follows the order; our model first
                use the embedding table to encode each code into a vector and then use
                average/mean pooling to get one vector for one inner bracket; then use
                StageNet one the braket level
            - case 3. [[1.5, 2.0, 0.0]] or [[1.5, 2.0, 0.0], [8, 1.2, 4.5], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run StageNet directly
                on the inner bracket level, similar to case 1 after embedding table
            - case 4. [[[1.5, 2.0, 0.0]]] or [[[1.5, 2.0, 0.0], [8, 1.2, 4.5]], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run StageNet directly
                on the inner bracket level, similar to case 2 after embedding table
        The time interval information specified by time_keys will be used to calculate the memory decay between each visit. If time_keys is None, all visits are treated as the same time interval. For each feature, the time interval should be a two-dimensional float array with shape (time_step, 1).

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        time_keys: list of keys in samples to use as time interval information for each feature, Default is None. If none, all visits are treated as the same time interval.
        embedding_dim: the embedding dimension. Default is 128.
        chunk_size: the chunk size for the StageNet layer. Default is 128.
        levels: the number of levels for the StageNet layer. levels * chunk_size = hidden_dim in the RNN. Smaller chunk size and more levels can capture more detailed patient status variations. Default is 3.
        **kwargs: other parameters for the StageNet layer.


    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...     {
        ...         "patient_id": "patient-0",
        ...         "visit_id": "visit-0",
        ...         # "single_vector": [1, 2, 3],
        ...         "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
        ...         "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        ...         "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
        ...         "list_list_vectors": [
        ...             [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
        ...             [[7.7, 8.5, 9.4]],
        ...         ],
        ...         "label": 1,
        ...         "list_vectors_time": [[0.0], [1.3]],
        ...         "list_codes_time": [[0.0], [2.0], [1.3]],
        ...         "list_list_codes_time": [[0.0], [1.5]],
        ...     },
        ...     {
        ...         "patient_id": "patient-0",
        ...         "visit_id": "visit-1",
        ...         # "single_vector": [1, 5, 8],
        ...         "list_codes": [
        ...             "55154191800",
        ...             "551541928",
        ...             "55154192800",
        ...             "705182798",
        ...             "70518279800",
        ...         ],
        ...         "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
        ...         "list_list_codes": [["A04A", "B035", "C129"]],
        ...         "list_list_vectors": [
        ...             [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
        ...         ],
        ...         "label": 0,
        ...         "list_vectors_time": [[0.0], [2.0], [1.0]],
        ...         "list_codes_time": [[0.0], [2.0], [1.3], [1.0], [2.0]],
        ...         "list_list_codes_time": [[0.0]],
        ...     },
        ... ]
        >>>
        >>> # dataset
        >>> dataset = SampleEHRDataset(samples=samples, dataset_name="test")
        >>>
        >>> # data loader
        >>> from pyhealth.datasets import get_dataloader
        >>>
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>>
        >>> # model
        >>> model = StageNet(
        ...     dataset=dataset,
        ...     feature_keys=[
        ...         "list_codes",
        ...         "list_vectors",
        ...         "list_list_codes",
        ...         # "list_list_vectors",
        ...     ],
        ...     time_keys=["list_codes_time", "list_vectors_time", "list_list_codes_time"],
        ...     label_key="label",
        ...     mode="binary",
        ... )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(0.7111, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),
            'y_prob': tensor([[0.4815],
                        [0.4991]], grad_fn=<SigmoidBackward0>),
            'y_true': tensor([[1.],
                        [0.]]),
            'logit': tensor([[-0.0742],
                        [-0.0038]], grad_fn=<AddmmBackward0>)
        }
        >>>

    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        time_keys: List[str] = None,
        embedding_dim: int = 128,
        chunk_size: int = 128,
        levels: int = 3,
        **kwargs,
    ):
        super(StageNet, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim
        self.chunk_size = chunk_size
        self.levels = levels

        # validate kwargs for StageNet layer
        if "feature_size" in kwargs:
            raise ValueError("feature_size is determined by embedding_dim")
        if time_keys is not None:
            if len(time_keys) != len(feature_keys):
                raise ValueError(
                    "time_keys should have the same length as feature_keys"
                )

        # the key of self.feat_tokenizers only contains the code based inputs
        self.feat_tokenizers = {}
        self.time_keys = time_keys
        self.label_tokenizer = self.get_label_tokenizer()
        # the key of self.embeddings only contains the code based inputs
        self.embeddings = nn.ModuleDict()
        # the key of self.linear_layers only contains the float/int based inputs
        self.linear_layers = nn.ModuleDict()

        self.stagenet = nn.ModuleDict()
        # add feature StageNet layers
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            # sanity check
            if input_info["type"] not in [str, float, int]:
                raise ValueError(
                    "StageNet only supports str code, float and int as input types"
                )
            elif (input_info["type"] == str) and (input_info["dim"] not in [2, 3]):
                raise ValueError(
                    "StageNet only supports 2-dim or 3-dim str code as input types"
                )
            elif (input_info["type"] in [float, int]) and (
                input_info["dim"] not in [2, 3]
            ):
                raise ValueError(
                    "StageNet only supports 2-dim or 3-dim float and int as input types"
                )

            # for code based input, we need Type
            # for float/int based input, we need Type, input_dim
            self.add_feature_transform_layer(feature_key, input_info)
            self.stagenet[feature_key] = StageNetLayer(
                input_dim=embedding_dim,
                chunk_size=self.chunk_size,
                levels=self.levels,
                **kwargs,
            )

        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(
            len(self.feature_keys) * self.chunk_size * self.levels, output_size
        )

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        The label `kwargs[self.label_key]` is a list of labels for each patient.

        Args:
            **kwargs: keyword arguments for the model. The keys must contain
                all the feature keys and the label key.

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the final loss.
                distance: list of tensors representing the stage variation of the patient.
                y_prob: a tensor representing the predicted probabilities.
                y_true: a tensor representing the true labels.
        """
        patient_emb = []
        distance = []
        mask_dict = {}
        for idx, feature_key in enumerate(self.feature_keys):
            input_info = self.dataset.input_info[feature_key]
            dim_, type_ = input_info["dim"], input_info["type"]

            # for case 1: [code1, code2, code3, ...]
            if (dim_ == 2) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_2d(
                    kwargs[feature_key]
                )
                # (patient, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, event)
                mask = torch.any(x !=0, dim=2)
                mask_dict[feature_key] = mask

            # for case 2: [[code1, code2], [code3, ...], ...]
            elif (dim_ == 3) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_3d(
                    kwargs[feature_key]
                )
                # (patient, visit, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, visit, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                # (patient, visit)
                mask = torch.any(x !=0, dim=2)
                mask_dict[feature_key] = mask

            # for case 3: [[1.5, 2.0, 0.0], ...]
            elif (dim_ == 2) and (type_ in [float, int]):
                x, mask = self.padding2d(kwargs[feature_key])
                # (patient, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, event, embedding_dim)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask.bool().to(self.device)
                mask_dict[feature_key] = mask

            # for case 4: [[[1.5, 2.0, 0.0], [1.8, 2.4, 6.0]], ...]
            elif (dim_ == 3) and (type_ in [float, int]):
                x, mask = self.padding3d(kwargs[feature_key])
                # (patient, visit, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask[:, :, 0]
                mask = mask.bool().to(self.device)
                mask_dict[feature_key] = mask

            else:
                raise NotImplementedError

            time = None
            if self.time_keys is not None:
                input_info = self.dataset.input_info[self.time_keys[idx]]
                dim_, type_ = input_info["dim"], input_info["type"]
                if (dim_ != 2) or (type_ not in [float, int]):
                    raise ValueError("Time interval must be 2-dim float or int.")
                time, _ = self.padding2d(kwargs[self.time_keys[idx]])
                time = torch.tensor(time, dtype=torch.float, device=self.device)
            x, _, cur_dis = self.stagenet[feature_key](x, time=time, mask=mask)
            patient_emb.append(x)
            distance.append(cur_dis)

        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)

        y_prob = self.prepare_y_prob(logits)
        results = {
            "loss": loss,
            "y_prob": y_prob,
            "y_true": y_true,
            "logit": logits,
        }
        if kwargs.get("embed", False):
            results["embed"] = patient_emb
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleEHRDataset

    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            # "single_vector": [1, 2, 3],
            "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
            "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
            "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
            "list_list_vectors": [
                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
                [[7.7, 8.5, 9.4]],
            ],
            "label": 1,
            "list_vectors_time": [[0.0], [1.3]],
            "list_codes_time": [[0.0], [2.0], [1.3]],
            "list_list_codes_time": [[0.0], [1.5]],
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-1",
            # "single_vector": [1, 5, 8],
            "list_codes": [
                "55154191800",
                "551541928",
                "55154192800",
                "705182798",
                "70518279800",
            ],
            "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
            "list_list_codes": [["A04A", "B035", "C129"]],
            "list_list_vectors": [
                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
            ],
            "label": 0,
            "list_vectors_time": [[0.0], [2.0], [1.0]],
            "list_codes_time": [[0.0], [2.0], [1.3], [1.0], [2.0]],
            "list_list_codes_time": [[0.0]],
        },
    ]

    # dataset
    dataset = SampleEHRDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = StageNet(
        dataset=dataset,
        feature_keys=[
            "list_codes",
            "list_vectors",
            "list_list_codes",
            # "list_list_vectors",
        ],
        time_keys=["list_codes_time", "list_vectors_time", "list_list_codes_time"],
        label_key="label",
        mode="binary",
    )

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for transformer.py:
import math
from typing import Dict, List, Optional, Tuple

import torch
from torch import nn

from pyhealth.datasets import SampleEHRDataset
from pyhealth.models import BaseModel
from pyhealth.tokenizer import Tokenizer

# VALID_OPERATION_LEVEL = ["visit", "event"]


class Attention(nn.Module):
    def forward(self, query, key, value, mask=None, dropout=None):
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        p_attn = torch.softmax(scores, dim=-1)
        if mask is not None:
            p_attn = p_attn.masked_fill(mask == 0, 0)
        if dropout is not None:
            p_attn = dropout(p_attn)
        return torch.matmul(p_attn, value), p_attn


class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0

        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h

        self.linear_layers = nn.ModuleList(
            [nn.Linear(d_model, d_model, bias=False) for _ in range(3)]
        )
        self.output_linear = nn.Linear(d_model, d_model, bias=False)
        self.attention = Attention()

        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)

        # 1) Do all the linear projections in batch from d_model => h x d_k
        query, key, value = [
            l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
            for l, x in zip(self.linear_layers, (query, key, value))
        ]

        # 2) Apply attention on all the projected vectors in batch.
        if mask is not None:
            mask = mask.unsqueeze(1)
        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)

        # 3) "Concat" using a view and apply a final linear.
        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)

        return self.output_linear(x)


class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.GELU()

    def forward(self, x, mask=None):
        x = self.w_2(self.dropout(self.activation(self.w_1(x))))
        if mask is not None:
            mask = mask.sum(dim=-1) > 0
            x[~mask] = 0
        return x


class SublayerConnection(nn.Module):
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = nn.LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))


class TransformerBlock(nn.Module):
    """Transformer block.

    MultiHeadedAttention + PositionwiseFeedForward + SublayerConnection

    Args:
        hidden: hidden size of transformer.
        attn_heads: head sizes of multi-head attention.
        dropout: dropout rate.
    """

    def __init__(self, hidden, attn_heads, dropout):
        super(TransformerBlock, self).__init__()
        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)
        self.feed_forward = PositionwiseFeedForward(
            d_model=hidden, d_ff=4 * hidden, dropout=dropout
        )
        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)
        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x, mask=None):
        """Forward propagation.

        Args:
            x: [batch_size, seq_len, hidden]
            mask: [batch_size, seq_len, seq_len]

        Returns:
            A tensor of shape [batch_size, seq_len, hidden]
        """
        x = self.input_sublayer(x, lambda _x: self.attention(_x, _x, _x, mask=mask))
        x = self.output_sublayer(x, lambda _x: self.feed_forward(_x, mask=mask))
        return self.dropout(x)


class TransformerLayer(nn.Module):
    """Transformer layer.

    Paper: Ashish Vaswani et al. Attention is all you need. NIPS 2017.

    This layer is used in the Transformer model. But it can also be used
    as a standalone layer.

    Args:
        feature_size: the hidden feature size.
        heads: the number of attention heads. Default is 1.
        dropout: dropout rate. Default is 0.5.
        num_layers: number of transformer layers. Default is 1.

    Examples:
        >>> from pyhealth.models import TransformerLayer
        >>> input = torch.randn(3, 128, 64)  # [batch size, sequence len, feature_size]
        >>> layer = TransformerLayer(64)
        >>> emb, cls_emb = layer(input)
        >>> emb.shape
        torch.Size([3, 128, 64])
        >>> cls_emb.shape
        torch.Size([3, 64])
    """

    def __init__(self, feature_size, heads=1, dropout=0.5, num_layers=1):
        super(TransformerLayer, self).__init__()
        self.transformer = nn.ModuleList(
            [TransformerBlock(feature_size, heads, dropout) for _ in range(num_layers)]
        )

    def forward(
        self, x: torch.tensor, mask: Optional[torch.tensor] = None
    ) -> Tuple[torch.tensor, torch.tensor]:
        """Forward propagation.

        Args:
            x: a tensor of shape [batch size, sequence len, feature_size].
            mask: an optional tensor of shape [batch size, sequence len], where
                1 indicates valid and 0 indicates invalid.

        Returns:
            emb: a tensor of shape [batch size, sequence len, feature_size],
                containing the output features for each time step.
            cls_emb: a tensor of shape [batch size, feature_size], containing
                the output features for the first time step.
        """
        if mask is not None:
            mask = torch.einsum("ab,ac->abc", mask, mask)
        for transformer in self.transformer:
            x = transformer(x, mask)
        emb = x
        cls_emb = x[:, 0, :]
        return emb, cls_emb


class Transformer(BaseModel):
    """Transformer model.

    This model applies a separate Transformer layer for each feature, and then
    concatenates the final hidden states of each Transformer layer. The concatenated
    hidden states are then fed into a fully connected layer to make predictions.

    Note:
        We use separate Transformer layers for different feature_keys.
        Currentluy, we automatically support different input formats:
            - code based input (need to use the embedding table later)
            - float/int based value input
        We follow the current convention for the transformer model:
            - case 1. [code1, code2, code3, ...]
                - we will assume the code follows the order; our model will encode
                each code into a vector and apply transformer on the code level
            - case 2. [[code1, code2]] or [[code1, code2], [code3, code4, code5], ...]
                - we will assume the inner bracket follows the order; our model first
                use the embedding table to encode each code into a vector and then use
                average/mean pooling to get one vector for one inner bracket; then use
                transformer one the braket level
            - case 3. [[1.5, 2.0, 0.0]] or [[1.5, 2.0, 0.0], [8, 1.2, 4.5], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run transformer directly
                on the inner bracket level, similar to case 1 after embedding table
            - case 4. [[[1.5, 2.0, 0.0]]] or [[[1.5, 2.0, 0.0], [8, 1.2, 4.5]], ...]
                - this case only makes sense when each inner bracket has the same length;
                we assume each dimension has the same meaning; we run transformer directly
                on the inner bracket level, similar to case 2 after embedding table

        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        embedding_dim: the embedding dimension. Default is 128.
        **kwargs: other parameters for the Transformer layer.

    Examples:
        >>> from pyhealth.datasets import SampleEHRDataset
        >>> samples = [
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-0",
        ...             "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
        ...             "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        ...             "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
        ...             "list_list_vectors": [
        ...                 [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
        ...                 [[7.7, 8.5, 9.4]],
        ...             ],
        ...             "label": 1,
        ...         },
        ...         {
        ...             "patient_id": "patient-0",
        ...             "visit_id": "visit-1",
        ...             "list_codes": [
        ...                 "55154191800",
        ...                 "551541928",
        ...                 "55154192800",
        ...                 "705182798",
        ...                 "70518279800",
        ...             ],
        ...             "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
        ...             "list_list_codes": [["A04A", "B035", "C129"]],
        ...             "list_list_vectors": [
        ...                 [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
        ...             ],
        ...             "label": 0,
        ...         },
        ...     ]
        >>> dataset = SampleEHRDataset(samples=samples, dataset_name="test")
        >>>
        >>> from pyhealth.models import Transformer
        >>> model = Transformer(
        ...         dataset=dataset,
        ...         feature_keys=[
        ...             "list_codes",
        ...             "list_vectors",
        ...             "list_list_codes",
        ...             "list_list_vectors",
        ...         ],
        ...         label_key="label",
        ...         mode="multiclass",
        ...     )
        >>>
        >>> from pyhealth.datasets import get_dataloader
        >>> train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)
        >>> data_batch = next(iter(train_loader))
        >>>
        >>> ret = model(**data_batch)
        >>> print(ret)
        {
            'loss': tensor(4.0555, grad_fn=<NllLossBackward0>),
            'y_prob': tensor([[1.0000e+00, 1.8206e-06],
                        [9.9970e-01, 3.0020e-04]], grad_fn=<SoftmaxBackward0>),
            'y_true': tensor([0, 1]),
            'logit': tensor([[ 7.6283, -5.5881],
                        [ 1.0898, -7.0210]], grad_fn=<AddmmBackward0>)
        }
        >>>

    """

    def __init__(
        self,
        dataset: SampleEHRDataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        embedding_dim: int = 128,
        **kwargs
    ):
        super(Transformer, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.embedding_dim = embedding_dim

        # validate kwargs for Transformer layer
        if "feature_size" in kwargs:
            raise ValueError("feature_size is determined by embedding_dim")

        # the key of self.feat_tokenizers only contains the code based inputs
        self.feat_tokenizers = {}
        self.label_tokenizer = self.get_label_tokenizer()
        # the key of self.embeddings only contains the code based inputs
        self.embeddings = nn.ModuleDict()
        # the key of self.linear_layers only contains the float/int based inputs
        self.linear_layers = nn.ModuleDict()

        # add feature transformation layers
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            # sanity check
            if input_info["type"] not in [str, float, int]:
                raise ValueError(
                    "Transformer only supports str code, float and int as input types"
                )
            elif (input_info["type"] == str) and (input_info["dim"] not in [2, 3]):
                raise ValueError(
                    "Transformer only supports 2-dim or 3-dim str code as input types"
                )
            elif (input_info["type"] in [float, int]) and (
                input_info["dim"] not in [2, 3]
            ):
                raise ValueError(
                    "Transformer only supports 2-dim or 3-dim float and int as input types"
                )
            # for code based input, we need Type
            # for float/int based input, we need Type, input_dim
            self.add_feature_transform_layer(feature_key, input_info)

        self.transformer = nn.ModuleDict()
        for feature_key in feature_keys:
            self.transformer[feature_key] = TransformerLayer(
                feature_size=embedding_dim, **kwargs
            )
        output_size = self.get_output_size(self.label_tokenizer)
        # transformer's output feature size is still embedding_dim
        self.fc = nn.Linear(len(self.feature_keys) * self.embedding_dim, output_size)

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation.

        The label `kwargs[self.label_key]` is a list of labels for each patient.

        Args:
            **kwargs: keyword arguments for the model. The keys must contain
                all the feature keys and the label key.

        Returns:
            A dictionary with the following keys:
                loss: a scalar tensor representing the loss.
                y_prob: a tensor representing the predicted probabilities.
                y_true: a tensor representing the true labels.
        """
        patient_emb = []
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            dim_, type_ = input_info["dim"], input_info["type"]

            # for case 1: [code1, code2, code3, ...]
            if (dim_ == 2) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_2d(
                    kwargs[feature_key]
                )
                # (patient, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, event)
                mask = torch.any(x !=0, dim=2)

            # for case 2: [[code1, code2], [code3, ...], ...]
            elif (dim_ == 3) and (type_ == str):
                x = self.feat_tokenizers[feature_key].batch_encode_3d(
                    kwargs[feature_key]
                )
                # (patient, visit, event)
                x = torch.tensor(x, dtype=torch.long, device=self.device)
                # (patient, visit, event, embedding_dim)
                x = self.embeddings[feature_key](x)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                # (patient, visit)
                mask = torch.any(x !=0, dim=2)

            # for case 3: [[1.5, 2.0, 0.0], ...]
            elif (dim_ == 2) and (type_ in [float, int]):
                x, mask = self.padding2d(kwargs[feature_key])
                # (patient, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, event, embedding_dim)
                x = self.linear_layers[feature_key](x)
                # (patient, event)
                mask = mask.bool().to(self.device)

            # for case 4: [[[1.5, 2.0, 0.0], [1.8, 2.4, 6.0]], ...]
            elif (dim_ == 3) and (type_ in [float, int]):
                x, mask = self.padding3d(kwargs[feature_key])
                # (patient, visit, event, values)
                x = torch.tensor(x, dtype=torch.float, device=self.device)
                # (patient, visit, embedding_dim)
                x = torch.sum(x, dim=2)
                x = self.linear_layers[feature_key](x)
                mask = mask[:, :, 0]
                mask = mask.bool().to(self.device)

            else:
                raise NotImplementedError

            _, x = self.transformer[feature_key](x, mask)
            patient_emb.append(x)

        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)
        y_prob = self.prepare_y_prob(logits)
        results = {"loss": loss, "y_prob": y_prob, "y_true": y_true, "logit": logits}
        if kwargs.get("embed", False):
            results["embed"] = patient_emb
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleEHRDataset

    samples = [
        {
            "patient_id": "patient-0",
            "visit_id": "visit-0",
            "single_vector": [1, 2, 3],
            "list_codes": ["505800458", "50580045810", "50580045811"],  # NDC
            "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
            "list_list_codes": [["A05B", "A05C", "A06A"], ["A11D", "A11E"]],  # ATC-4
            "list_list_vectors": [
                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],
                [[7.7, 8.5, 9.4]],
            ],
            "label": 1,
        },
        {
            "patient_id": "patient-0",
            "visit_id": "visit-1",
            "single_vector": [1, 5, 8],
            "list_codes": [
                "55154191800",
                "551541928",
                "55154192800",
                "705182798",
                "70518279800",
            ],
            "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7], [4.5, 5.9, 1.7]],
            "list_list_codes": [["A04A", "B035", "C129"]],
            "list_list_vectors": [
                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6], [7.7, 8.4, 1.3], [7.7, 8.4, 1.3]],
            ],
            "label": 0,
        },
    ]

    # dataset
    dataset = SampleEHRDataset(samples=samples, dataset_name="test")

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)

    # model
    model = Transformer(
        dataset=dataset,
        feature_keys=[
            "list_codes",
            "list_vectors",
            "list_list_codes",
            "list_list_vectors",
        ],
        label_key="label",
        mode="multiclass",
    )

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()


Here is the code content for __init__.py:
from .base_model import BaseModel
from .rnn import RNN, RNNLayer
from .transformer import Transformer, TransformerLayer
from .retain import RETAIN, RETAINLayer
from .cnn import CNN, CNNLayer
from .micron import MICRON, MICRONLayer
from .gamenet import GAMENet, GAMENetLayer
from .safedrug import SafeDrug, SafeDrugLayer
from .mlp import MLP
from .deepr import Deepr, DeeprLayer
from .contrawr import ResBlock2D, ContraWR
from .adacare import AdaCare, AdaCareLayer
from .concare import ConCare, ConCareLayer
from .agent import Agent, AgentLayer
from .grasp import GRASP, GRASPLayer
from .stagenet import StageNet, StageNetLayer
from .tcn import TCN, TCNLayer
from .sparcnet import SparcNet, DenseBlock, DenseLayer, TransitionLayer
from .adacare import AdaCare, AdaCareLayer
from .concare import ConCare, ConCareLayer
from .agent import Agent, AgentLayer
from .grasp import GRASP, GRASPLayer
from .stagenet import StageNet, StageNetLayer
from .tcn import TCN, TCNLayer
from .molerec import MoleRec, MoleRecLayer


Here is the code content for length_of_stay_prediction.py:
from pyhealth.data import Patient


def categorize_los(days: int):
    """Categorizes length of stay into 10 categories.

    One for ICU stays shorter than a day, seven day-long categories for each day of
    the first week, one for stays of over one week but less than two,
    and one for stays of over two weeks.

    Args:
        days: int, length of stay in days

    Returns:
        category: int, category of length of stay
    """
    # ICU stays shorter than a day
    if days < 1:
        return 0
    # each day of the first week
    elif 1 <= days <= 7:
        return days
    # stays of over one week but less than two
    elif 7 < days <= 14:
        return 8
    # stays of over two weeks
    else:
        return 9


def length_of_stay_prediction_mimic3_fn(patient: Patient):
    """Processes a single patient for the length-of-stay prediction task.

    Length of stay prediction aims at predicting the length of stay (in days) of the
    current hospital visit based on the clinical information from the visit
    (e.g., conditions and procedures).

    Args:
        patient: a Patient object.

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key.

    Note that we define the task as a multi-class classification task.

    Examples:
        >>> from pyhealth.datasets import MIMIC3Dataset
        >>> mimic3_base = MIMIC3Dataset(
        ...    root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        ...    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        ...    code_mapping={"ICD9CM": "CCSCM"},
        ... )
        >>> from pyhealth.tasks import length_of_stay_prediction_mimic3_fn
        >>> mimic3_sample = mimic3_base.set_task(length_of_stay_prediction_mimic3_fn)
        >>> mimic3_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '19', '122', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 4}]
    """
    samples = []

    for visit in patient:

        conditions = visit.get_code_list(table="DIAGNOSES_ICD")
        procedures = visit.get_code_list(table="PROCEDURES_ICD")
        drugs = visit.get_code_list(table="PRESCRIPTIONS")
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue

        los_days = (visit.discharge_time - visit.encounter_time).days
        los_category = categorize_los(los_days)

        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": los_category,
            }
        )
    # no cohort selection
    return samples


def length_of_stay_prediction_mimic4_fn(patient: Patient):
    """Processes a single patient for the length-of-stay prediction task.

    Length of stay prediction aims at predicting the length of stay (in days) of the
    current hospital visit based on the clinical information from the visit
    (e.g., conditions and procedures).

    Args:
        patient: a Patient object.

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key.

    Note that we define the task as a multi-class classification task.

    Examples:
        >>> from pyhealth.datasets import MIMIC4Dataset
        >>> mimic4_base = MIMIC4Dataset(
        ...     root="/srv/local/data/physionet.org/files/mimiciv/2.0/hosp",
        ...     tables=["diagnoses_icd", "procedures_icd"],
        ...     code_mapping={"ICD10PROC": "CCSPROC"},
        ... )
        >>> from pyhealth.tasks import length_of_stay_prediction_mimic4_fn
        >>> mimic4_sample = mimic4_base.set_task(length_of_stay_prediction_mimic4_fn)
        >>> mimic4_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '19', '122', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 2}]
    """
    samples = []

    for visit in patient:

        conditions = visit.get_code_list(table="diagnoses_icd")
        procedures = visit.get_code_list(table="procedures_icd")
        drugs = visit.get_code_list(table="prescriptions")
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue

        los_days = (visit.discharge_time - visit.encounter_time).days
        los_category = categorize_los(los_days)

        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": los_category,
            }
        )
    # no cohort selection
    return samples


def length_of_stay_prediction_eicu_fn(patient: Patient):
    """Processes a single patient for the length-of-stay prediction task.

    Length of stay prediction aims at predicting the length of stay (in days) of the
    current hospital visit based on the clinical information from the visit
    (e.g., conditions and procedures).

    Args:
        patient: a Patient object.

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key.

    Note that we define the task as a multi-class classification task.

    Examples:
        >>> from pyhealth.datasets import eICUDataset
        >>> eicu_base = eICUDataset(
        ...     root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        ...     tables=["diagnosis", "medication"],
        ...     code_mapping={},
        ...     dev=True
        ... )
        >>> from pyhealth.tasks import length_of_stay_prediction_eicu_fn
        >>> eicu_sample = eicu_base.set_task(length_of_stay_prediction_eicu_fn)
        >>> eicu_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 5}]
    """
    samples = []

    for visit in patient:

        conditions = visit.get_code_list(table="diagnosis")
        procedures = visit.get_code_list(table="physicalExam")
        drugs = visit.get_code_list(table="medication")
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue

        los_days = (visit.discharge_time - visit.encounter_time).days
        los_category = categorize_los(los_days)

        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": los_category,
            }
        )
    # no cohort selection
    return samples


def length_of_stay_prediction_omop_fn(patient: Patient):
    """Processes a single patient for the length-of-stay prediction task.

    Length of stay prediction aims at predicting the length of stay (in days) of the
    current hospital visit based on the clinical information from the visit
    (e.g., conditions and procedures).

    Args:
        patient: a Patient object.

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key.

    Note that we define the task as a multi-class classification task.

    Examples:
        >>> from pyhealth.datasets import OMOPDataset
        >>> omop_base = OMOPDataset(
        ...     root="https://storage.googleapis.com/pyhealth/synpuf1k_omop_cdm_5.2.2",
        ...     tables=["condition_occurrence", "procedure_occurrence"],
        ...     code_mapping={},
        ... )
        >>> from pyhealth.tasks import length_of_stay_prediction_omop_fn
        >>> omop_sample = omop_base.set_task(length_of_stay_prediction_eicu_fn)
        >>> omop_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 7}]
    """
    samples = []

    for visit in patient:

        conditions = visit.get_code_list(table="condition_occurrence")
        procedures = visit.get_code_list(table="procedure_occurrence")
        drugs = visit.get_code_list(table="drug_exposure")
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue

        los_days = (visit.discharge_time - visit.encounter_time).days
        los_category = categorize_los(los_days)

        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": los_category,
            }
        )
        # no cohort selection
    return samples


if __name__ == "__main__":
    from pyhealth.datasets import MIMIC3Dataset

    base_dataset = MIMIC3Dataset(
        root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        dev=True,
        code_mapping={"ICD9CM": "CCSCM", "NDC": "ATC"},
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=length_of_stay_prediction_mimic3_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)

    from pyhealth.datasets import MIMIC4Dataset

    base_dataset = MIMIC4Dataset(
        root="/srv/local/data/physionet.org/files/mimiciv/2.0/hosp",
        tables=["diagnoses_icd", "procedures_icd", "prescriptions"],
        dev=True,
        code_mapping={"NDC": "ATC"},
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=length_of_stay_prediction_mimic4_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)

    from pyhealth.datasets import eICUDataset

    base_dataset = eICUDataset(
        root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        tables=["diagnosis", "medication", "physicalExam"],
        dev=True,
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=length_of_stay_prediction_eicu_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)

    from pyhealth.datasets import OMOPDataset

    base_dataset = OMOPDataset(
        root="/srv/local/data/zw12/pyhealth/raw_data/synpuf1k_omop_cdm_5.2.2",
        tables=["condition_occurrence", "procedure_occurrence", "drug_exposure"],
        dev=True,
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=length_of_stay_prediction_omop_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)


Here is the code content for EEG_abnormal.py:
import os
import pickle
import pkg_resources
import mne
import pandas as pd
import numpy as np


def EEG_isAbnormal_fn(record):
    """Processes a single patient for the abnormal EEG detection task on TUAB.

    Abnormal EEG detection aims at determining whether a EEG is abnormal.

    Args:
        record: a singleton list of one subject from the TUABDataset.
            The (single) record is a dictionary with the following keys:
                load_from_path, patient_id, visit_id, signal_file, label_file, save_to_path

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id, record_id,
            and epoch_path (the path to the saved epoch {"signal": signal, "label": label} as key.

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import TUABDataset
        >>> isabnormal = TUABDataset(
        ...         root="/srv/local/data/TUH/tuh_eeg_abnormal/v3.0.0/edf/", download=True,
        ...     )
        >>> from pyhealth.tasks import EEG_isabnormal_fn
        >>> EEG_abnormal_ds = isabnormal.set_task(EEG_isAbnormal_fn)
        >>> EEG_abnormal_ds.samples[0]
        {
            'patient_id': 'aaaaamye',
            'visit_id': 's001',
            'record_id': '1',
            'epoch_path': '/home/zhenlin4/.cache/pyhealth/datasets/832afe6e6e8a5c9ea5505b47e7af8125/10-1/1/0.pkl',
            'label': 1
        }
    """
    
    samples = []
    for visit in record:
        root, pid, visit_id, signal, label, save_path = (
            visit["load_from_path"],
            visit["patient_id"],
            visit["visit_id"],
            visit["signal_file"],
            visit["label_file"],
            visit["save_to_path"],
        )

        raw = mne.io.read_raw_edf(os.path.join(root, signal), preload=True)
        raw.resample(200)
        ch_name = raw.ch_names
        raw_data = raw.get_data()
        channeled_data = raw_data.copy()[:16]
        try:
            channeled_data[0] = (
                raw_data[ch_name.index("EEG FP1-REF")]
                - raw_data[ch_name.index("EEG F7-REF")]
            )
            channeled_data[1] = (
                raw_data[ch_name.index("EEG F7-REF")]
                - raw_data[ch_name.index("EEG T3-REF")]
            )
            channeled_data[2] = (
                raw_data[ch_name.index("EEG T3-REF")]
                - raw_data[ch_name.index("EEG T5-REF")]
            )
            channeled_data[3] = (
                raw_data[ch_name.index("EEG T5-REF")]
                - raw_data[ch_name.index("EEG O1-REF")]
            )
            channeled_data[4] = (
                raw_data[ch_name.index("EEG FP2-REF")]
                - raw_data[ch_name.index("EEG F8-REF")]
            )
            channeled_data[5] = (
                raw_data[ch_name.index("EEG F8-REF")]
                - raw_data[ch_name.index("EEG T4-REF")]
            )
            channeled_data[6] = (
                raw_data[ch_name.index("EEG T4-REF")]
                - raw_data[ch_name.index("EEG T6-REF")]
            )
            channeled_data[7] = (
                raw_data[ch_name.index("EEG T6-REF")]
                - raw_data[ch_name.index("EEG O2-REF")]
            )
            channeled_data[8] = (
                raw_data[ch_name.index("EEG FP1-REF")]
                - raw_data[ch_name.index("EEG F3-REF")]
            )
            channeled_data[9] = (
                raw_data[ch_name.index("EEG F3-REF")]
                - raw_data[ch_name.index("EEG C3-REF")]
            )
            channeled_data[10] = (
                raw_data[ch_name.index("EEG C3-REF")]
                - raw_data[ch_name.index("EEG P3-REF")]
            )
            channeled_data[11] = (
                raw_data[ch_name.index("EEG P3-REF")]
                - raw_data[ch_name.index("EEG O1-REF")]
            )
            channeled_data[12] = (
                raw_data[ch_name.index("EEG FP2-REF")]
                - raw_data[ch_name.index("EEG F4-REF")]
            )
            channeled_data[13] = (
                raw_data[ch_name.index("EEG F4-REF")]
                - raw_data[ch_name.index("EEG C4-REF")]
            )
            channeled_data[14] = (
                raw_data[ch_name.index("EEG C4-REF")]
                - raw_data[ch_name.index("EEG P4-REF")]
            )
            channeled_data[15] = (
                raw_data[ch_name.index("EEG P4-REF")]
                - raw_data[ch_name.index("EEG O2-REF")]
            )
        except:
            with open("tuab-process-error-files.txt", "a") as f:
                f.write(os.path.join(root, signal) + "\n")
            continue

        # get the label
        data_field = pid.split("_")[0]
        if data_field == "0" or data_field == "2":
            label = 1
        else:
            label = 0

        # load data
        for i in range(channeled_data.shape[1] // 2000):
            dump_path = os.path.join(
                save_path, pid + "_" + visit_id + "_" + str(i) + ".pkl"
            )
            pickle.dump(
                {"signal": channeled_data[:, i * 2000 : (i + 1) * 2000], "label": label},
                open(dump_path, "wb"),
            )

            samples.append(
                    {   
                        "patient_id": pid,
                        "visit_id": visit_id,
                        "record_id": i,
                        "epoch_path": dump_path,
                        "label": label,
                    }
            )

        return samples



if __name__ == "__main__":
    from pyhealth.datasets import TUABDataset
    
    dataset = TUABDataset(
        root="/srv/local/data/TUH/tuh_eeg_abnormal/v3.0.0/edf/",
        dev=True,
        refresh_cache=True,
    )
    EEG_abnormal_ds = dataset.set_task(EEG_isAbnormal_fn)
    print(EEG_abnormal_ds.samples[0])
    print(EEG_abnormal_ds.input_info)
    
    
    
    


Here is the code content for EEG_events.py:
import os
import pickle
import pkg_resources
import mne
import pandas as pd
import numpy as np

def EEG_events_fn(record):
    """Processes a single patient for the EEG events task on TUEV.

    This task aims at annotating of EEG segments as one of six classes: (1) spike and sharp wave (SPSW), (2) generalized periodic epileptiform discharges (GPED), (3) periodic lateralized epileptiform discharges (PLED), (4) eye movement (EYEM), (5) artifact (ARTF) and (6) background (BCKG).

    Args:
        record: a singleton list of one subject from the TUEVDataset.
            The (single) record is a dictionary with the following keys:
                load_from_path, patient_id, visit_id, signal_file, label_file, save_to_path

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id, record_id, label, offending_channel,
            and epoch_path (the path to the saved epoch {"signal": signal, "label": label} as key.

    Note that we define the task as a multiclass classification task.

    Examples:
        >>> from pyhealth.datasets import TUEVDataset
        >>> EEGevents = TUEVDataset(
        ...         root="/srv/local/data/TUH/tuh_eeg_events/v2.0.0/edf/", download=True,
        ...     )
        >>> from pyhealth.tasks import EEG_events_fn
        >>> EEG_events_ds = EEGevents.set_task(EEG_events_fn)
        >>> EEG_events_ds.samples[0]
        {
            'patient_id': '0_00002265',
            'visit_id': '00000001',
            'record_id': 0,
            'epoch_path': '/Users/liyanjing/.cache/pyhealth/datasets/d8f3cb92cc444d481444d3414fb5240c/0_00002265_00000001_0.pkl',
            'label': 6,
            'offending_channel': array([4.])
        }
    """
    
    samples = []
    for visit in record:
        root, pid, visit_id, signal, label, save_path = (
            visit["load_from_path"],
            visit["patient_id"],
            visit["visit_id"],
            visit["signal_file"],
            visit["label_file"],
            visit["save_to_path"],
        )

        
        # load data
        try:
            [signals, times, event, Rawdata] = readEDF(
                os.path.join(root, signal)
            )  # event is the .rec file in the form of an array
            signals = convert_signals(signals, Rawdata)
        except (ValueError, KeyError):
            print("something funky happened in " + os.path.join(root, signal))
            continue
        signals, offending_channels, labels = BuildEvents(signals, times, event)

        for idx, (signal, offending_channel, label) in enumerate(
            zip(signals, offending_channels, labels)
        ):
            dump_path = os.path.join(
                save_path, pid + "_" + visit_id + "_" + str(idx) + ".pkl"
            )

            pickle.dump(
                    {"signal": signal, "label": int(label[0])},
                    open(dump_path, "wb"),
                )
            
            samples.append(
                {
                    "patient_id": pid,
                    "visit_id": visit_id,
                    "record_id": idx,
                    "epoch_path": dump_path,
                    "label": int(label[0]),
                    "offending_channel": offending_channel,
                }
            )

    return samples

def BuildEvents(signals, times, EventData):
    [numEvents, z] = EventData.shape  # numEvents is equal to # of rows of the .rec file
    fs = 250.0
    [numChan, numPoints] = signals.shape

    features = np.zeros([numEvents, numChan, int(fs) * 5])
    offending_channel = np.zeros([numEvents, 1])  # channel that had the detected thing
    labels = np.zeros([numEvents, 1])
    offset = signals.shape[1]
    signals = np.concatenate([signals, signals, signals], axis=1)
    for i in range(numEvents):  # for each event
        chan = int(EventData[i, 0])  # chan is channel
        start = np.where((times) >= EventData[i, 1])[0][0]
        end = np.where((times) >= EventData[i, 2])[0][0]
        features[i, :] = signals[
            :, offset + start - 2 * int(fs) : offset + end + 2 * int(fs)
        ]
        offending_channel[i, :] = int(chan)
        labels[i, :] = int(EventData[i, 3])
    return [features, offending_channel, labels]


def convert_signals(signals, Rawdata):
    signal_names = {
        k: v
        for (k, v) in zip(
            Rawdata.info["ch_names"], list(range(len(Rawdata.info["ch_names"])))
        )
    }
    new_signals = np.vstack(
        (
            signals[signal_names["EEG FP1-REF"]]
            - signals[signal_names["EEG F7-REF"]],  # 0
            (
                signals[signal_names["EEG F7-REF"]]
                - signals[signal_names["EEG T3-REF"]]
            ),  # 1
            (
                signals[signal_names["EEG T3-REF"]]
                - signals[signal_names["EEG T5-REF"]]
            ),  # 2
            (
                signals[signal_names["EEG T5-REF"]]
                - signals[signal_names["EEG O1-REF"]]
            ),  # 3
            (
                signals[signal_names["EEG FP2-REF"]]
                - signals[signal_names["EEG F8-REF"]]
            ),  # 4
            (
                signals[signal_names["EEG F8-REF"]]
                - signals[signal_names["EEG T4-REF"]]
            ),  # 5
            (
                signals[signal_names["EEG T4-REF"]]
                - signals[signal_names["EEG T6-REF"]]
            ),  # 6
            (
                signals[signal_names["EEG T6-REF"]]
                - signals[signal_names["EEG O2-REF"]]
            ),  # 7
            (
                signals[signal_names["EEG FP1-REF"]]
                - signals[signal_names["EEG F3-REF"]]
            ),  # 14
            (
                signals[signal_names["EEG F3-REF"]]
                - signals[signal_names["EEG C3-REF"]]
            ),  # 15
            (
                signals[signal_names["EEG C3-REF"]]
                - signals[signal_names["EEG P3-REF"]]
            ),  # 16
            (
                signals[signal_names["EEG P3-REF"]]
                - signals[signal_names["EEG O1-REF"]]
            ),  # 17
            (
                signals[signal_names["EEG FP2-REF"]]
                - signals[signal_names["EEG F4-REF"]]
            ),  # 18
            (
                signals[signal_names["EEG F4-REF"]]
                - signals[signal_names["EEG C4-REF"]]
            ),  # 19
            (
                signals[signal_names["EEG C4-REF"]]
                - signals[signal_names["EEG P4-REF"]]
            ),  # 20
            (signals[signal_names["EEG P4-REF"]] - signals[signal_names["EEG O2-REF"]]),
        )
    )  # 21
    return new_signals


def readEDF(fileName):
    Rawdata = mne.io.read_raw_edf(fileName)
    signals, times = Rawdata[:]
    RecFile = fileName[0:-3] + "rec"
    eventData = np.genfromtxt(RecFile, delimiter=",")
    Rawdata.close()
    return [signals, times, eventData, Rawdata]



if __name__ == "__main__":
    from pyhealth.datasets import TUEVDataset
    
    dataset = TUEVDataset(
        root="/srv/local/data/TUH/tuh_eeg_events/v2.0.0/edf/",
        dev=True,
        refresh_cache=True,
    )
    EEG_events_ds = dataset.set_task(EEG_events_fn)
    print(EEG_events_ds.samples[0])
    print(EEG_events_ds.input_info)
    
    
    
    


Here is the code content for __init__.py:
from .drug_recommendation import (
    drug_recommendation_eicu_fn,
    drug_recommendation_mimic3_fn,
    drug_recommendation_mimic4_fn,
    drug_recommendation_omop_fn,
)
from .cardiology_detect import (
    cardiology_isAR_fn,
    cardiology_isBBBFB_fn,
    cardiology_isAD_fn,
    cardiology_isCD_fn,
    cardiology_isWA_fn,
)
from .temple_university_EEG_tasks import (
    EEG_isAbnormal_fn, EEG_events_fn,
)
from .length_of_stay_prediction import (
    length_of_stay_prediction_eicu_fn,
    length_of_stay_prediction_mimic3_fn,
    length_of_stay_prediction_mimic4_fn,
    length_of_stay_prediction_omop_fn,
)
from .mortality_prediction import (
    mortality_prediction_eicu_fn,
    mortality_prediction_eicu_fn2,
    mortality_prediction_mimic3_fn,
    mortality_prediction_mimic4_fn,
    mortality_prediction_omop_fn,
)
from .readmission_prediction import (
    readmission_prediction_eicu_fn,
    readmission_prediction_eicu_fn2,
    readmission_prediction_mimic3_fn,
    readmission_prediction_mimic4_fn,
    readmission_prediction_omop_fn,
)
from .sleep_staging import (
    sleep_staging_sleepedf_fn,
    sleep_staging_isruc_fn,
    sleep_staging_shhs_fn,
)




Here is the code content for cardiology_detect.py:
import os
import pickle
import pkg_resources
from scipy.io import loadmat
import pandas as pd
import numpy as np


def cardiology_isAR_fn(record, epoch_sec=10, shift=5):
    """Processes a single patient for the Arrhythmias symptom in cardiology on the CardiologyDataset

    Cardiology symptoms can be divided into six categories. The task focuses on Arrhythmias and is defined as a binary classification.

    Args:
        record: a singleton list of one subject from the CardiologyDataset.
            The (single) record is a dictionary with the following keys:
                load_from_path, signal_file, label1_file, label2_file, save_to_path, subject_id
        epoch_sec: how long will each epoch be (in seconds). 
        shift: the step size for the sampling window (with a width of epoch_sec)
        

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, record_id,
            and epoch_path (the path to the saved epoch {"X": signal, "Sex": gender, "Age": age, Y": label} as key.

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import CardiologyDataset
        >>> isAR = CardiologyDataset(
        ...         root="physionet.org/files/challenge-2020/1.0.2/training",
                    chosen_dataset=[1,1,1,1,1,1], 
        ...     )
        >>> from pyhealth.tasks import cardiology_isAR_fn
        >>> cardiology_ds = isAR.set_task(cardiology_isAR_fn)
        >>> cardiology_ds.samples[0]
        {
            'patient_id': '0_0',
            'visit_id': 'A0033',
            'record_id': 1,
            'Sex': ['Female'],
            'Age': ['34'],
            'epoch_path': '/Users/liyanjing/.cache/pyhealth/datasets/46c18f2a1a18803b4707a934a577a331/0_0-0.pkl',
            'label': '0'
        }
    """

    # these are the AR diseases codes
    AR_space = list(
        map(
            str,
            [
                164889003,
                164890007,
                426627000,
                284470004,
                427172004,
                427393009,
                426177001,
                427084000,
                63593006,
                17338001,
            ],
        )
    )
    
    samples = []
    for visit in record:
        root, pid, signal, label, save_path = (
            visit["load_from_path"],
            visit["patient_id"],
            visit["signal_file"],
            visit["label_file"],
            visit["save_to_path"],
        )
        
        # X load
        X = loadmat(os.path.join(root, signal))["val"]
        label_content =  open(os.path.join(root, label), "r").readlines()
        Dx, Sex, Age = label_content[-4].split(" ")[-1][:-1].split(","), \
                label_content[-5].split(" ")[-1][:-1].split(","), \
                label_content[-6].split(" ")[-1][:-1].split(",")

        y = 1 if set(Dx).intersection(AR_space) else 0
       
        
        # frequency * seconds (500 * 10)
        if X.shape[1] >= 500 * epoch_sec:
            for index in range((X.shape[1] - 500 * epoch_sec) // (500 * shift) + 1):
                save_file_path = os.path.join(save_path, f"{pid}-AR-{index}.pkl")
            
                pickle.dump(
                    {"signal": X[:, (500 * shift) * index : (500 * shift) * index + 5000], "label": y},
                    open(save_file_path, "wb"),
                )
                
                samples.append(
                    {   
                        "patient_id": pid,
                        "visit_id": signal.split(".")[0],
                        "record_id": len(samples) + 1,
                        "Sex": Sex,
                        "Age": Age,
                        "epoch_path": save_file_path,
                        "label": y,
                    }
                )
    return samples

def cardiology_isBBBFB_fn(record, epoch_sec=10, shift=5):
    """Processes a single patient for the Bundle branch blocks and fascicular blocks symptom in cardiology on the CardiologyDataset

    Cardiology symptoms can be divided into six categories. The task focuses on Bundle branch blocks and fascicular blocks and is defined as a binary classification.

    Args:
        record: a singleton list of one subject from the CardiologyDataset.
            The (single) record is a dictionary with the following keys:
                load_from_path, signal_file, label1_file, label2_file, save_to_path, subject_id
        epoch_sec: how long will each epoch be (in seconds). 
        shift: the step size for the sampling window (with a width of epoch_sec)
        

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, record_id,
            and epoch_path (the path to the saved epoch {"X": signal, "Sex": gender, "Age": age, Y": label} as key.

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import CardiologyDataset
        >>> isBBBFB = CardiologyDataset(
        ...         root="physionet.org/files/challenge-2020/1.0.2/training",
                    chosen_dataset=[1,1,1,1,1,1], 
        ...     )
        >>> from pyhealth.tasks import cardiology_isBBBFB_fn
        >>> cardiology_ds = isBBBFB.set_task(cardiology_isBBBFB_fn)
        >>> cardiology_ds.samples[0]
        {
            'patient_id': '0_0',
            'visit_id': 'A0033',
            'record_id': 1,
            'Sex': ['Female'],
            'Age': ['34'],
            'epoch_path': '/Users/liyanjing/.cache/pyhealth/datasets/46c18f2a1a18803b4707a934a577a331/0_0-0.pkl',
            'label': '0'
        }
    """

    # these are the diseases codes for Bundle branch blocks and fascicular blocks symptom
    BBBFB_space = list(
        map(
            str,
            [
                713427006,
                713426002,
                445118002,
                164909002,
                59118001,
            ],
        )
    )
    
    samples = []
    for visit in record:
        root, pid, signal, label, save_path = (
            visit["load_from_path"],
            visit["patient_id"],
            visit["signal_file"],
            visit["label_file"],
            visit["save_to_path"],
        )
        
        # X load
        X = loadmat(os.path.join(root, signal))["val"]
        label_content =  open(os.path.join(root, label), "r").readlines()
        Dx, Sex, Age = label_content[-4].split(" ")[-1][:-1].split(","), label_content[-5].split(" ")[-1][:-1].split(","), label_content[-6].split(" ")[-1][:-1].split(",")

        y = 1 if set(Dx).intersection(BBBFB_space) else 0
       
        
        # frequency * seconds (500 * 10)
        if X.shape[1] >= 500 * epoch_sec:
            for index in range((X.shape[1] - 500 * epoch_sec) // (500 * shift) + 1):
                save_file_path = os.path.join(save_path, f"{pid}-BBBFB-{index}.pkl")
            
                pickle.dump(
                    {"signal": X[:, (500 * shift) * index : (500 * shift) * index + 5000], "label": y},
                    open(save_file_path, "wb"),
                )
                
                samples.append(
                    {   
                        "patient_id": pid,
                        "visit_id": signal.split(".")[0],
                        "record_id": len(samples) + 1,
                        "Sex": Sex,
                        "Age": Age,
                        "epoch_path": save_file_path,
                        "label": y,
                    }
                )
                
    return samples


def cardiology_isAD_fn(record, epoch_sec=10, shift=5):
    """Processes a single patient for the Axis deviations symptom in cardiology on the CardiologyDataset

    Cardiology symptoms can be divided into six categories. The task focuses on Axis deviations and is defined as a binary classification.

    Args:
        record: a singleton list of one subject from the CardiologyDataset.
            The (single) record is a dictionary with the following keys:
                load_from_path, signal_file, label1_file, label2_file, save_to_path, subject_id
        epoch_sec: how long will each epoch be (in seconds). 
        shift: the step size for the sampling window (with a width of epoch_sec)
        

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, record_id,
            and epoch_path (the path to the saved epoch {"X": signal, "Sex": gender, "Age": age, Y": label} as key.

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import CardiologyDataset
        >>> isAD = CardiologyDataset(
        ...         root="physionet.org/files/challenge-2020/1.0.2/training",
                    chosen_dataset=[1,1,1,1,1,1], 
        ...     )
        >>> from pyhealth.tasks import cardiology_isAD_fn
        >>> cardiology_ds = isAD.set_task(cardiology_isAD_fn)
        >>> cardiology_ds.samples[0]
        {
            'patient_id': '0_0',
            'visit_id': 'A0033',
            'record_id': 1,
            'Sex': ['Female'],
            'Age': ['34'],
            'epoch_path': '/Users/liyanjing/.cache/pyhealth/datasets/46c18f2a1a18803b4707a934a577a331/0_0-0.pkl',
            'label': '0'
        }
    """
    
    
    # these are the diseases codes for Axis deviations symptom 
    AD_space = list(
        map(
            str,
            [
                39732003,
                47665007,
            ],
        )
    )
    
    samples = []
    for visit in record:
        root, pid, signal, label, save_path = (
            visit["load_from_path"],
            visit["patient_id"],
            visit["signal_file"],
            visit["label_file"],
            visit["save_to_path"],
        )
        
        # X load
        X = loadmat(os.path.join(root, signal))["val"]
        label_content =  open(os.path.join(root, label), "r").readlines()
        Dx, Sex, Age = label_content[-4].split(" ")[-1][:-1].split(","), label_content[-5].split(" ")[-1][:-1].split(","), label_content[-6].split(" ")[-1][:-1].split(",")

        y = 1 if set(Dx).intersection(AD_space) else 0
       
        
        # frequency * seconds (500 * 10)
        if X.shape[1] >= 500 * epoch_sec:
            for index in range((X.shape[1] - 500 * epoch_sec) // (500 * shift) + 1):
                save_file_path = os.path.join(save_path, f"{pid}-AD-{index}.pkl")
            
                pickle.dump(
                    {"signal": X[:, (500 * shift) * index : (500 * shift) * index + 5000], "label": y},
                    open(save_file_path, "wb"),
                )
                
                samples.append(
                    {   
                        "patient_id": pid,
                        "visit_id": signal.split(".")[0],
                        "record_id": len(samples) + 1,
                        "Sex": Sex,
                        "Age": Age,
                        "epoch_path": save_file_path,
                        "label": y,
                    }
                )

    return samples

def cardiology_isCD_fn(record, epoch_sec=10, shift=5):
    """Processes a single patient for the Conduction delays symptom in cardiology on the CardiologyDataset

    Cardiology symptoms can be divided into six categories. The task focuses on Conduction delays and is defined as a binary classification.

    Args:
        record: a singleton list of one subject from the CardiologyDataset.
            The (single) record is a dictionary with the following keys:
                load_from_path, signal_file, label1_file, label2_file, save_to_path, subject_id
        epoch_sec: how long will each epoch be (in seconds). 
        shift: the step size for the sampling window (with a width of epoch_sec)
        

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, record_id,
            and epoch_path (the path to the saved epoch {"X": signal, "Sex": gender, "Age": age, Y": label} as key.

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import CardiologyDataset
        >>> isCD = CardiologyDataset(
        ...         root="physionet.org/files/challenge-2020/1.0.2/training",
                    chosen_dataset=[1,1,1,1,1,1], 
        ...     )
        >>> from pyhealth.tasks import cardiology_isCD_fn
        >>> cardiology_ds = isCD.set_task(cardiology_isCD_fn)
        >>> cardiology_ds.samples[0]
        {
            'patient_id': '0_0',
            'visit_id': 'A0033',
            'record_id': 1,
            'Sex': ['Female'],
            'Age': ['34'],
            'epoch_path': '/Users/liyanjing/.cache/pyhealth/datasets/46c18f2a1a18803b4707a934a577a331/0_0-0.pkl',
            'label': '0'
        }
    """

    # these are the diseases codes for Conduction delays symptom
    CD_space = list(
        map(
            str,
            [
                270492004,
                698252002,
                164947007,
                111975006,
            ],
        )
    )
    
    samples = []
    for visit in record:
        root, pid, signal, label, save_path = (
            visit["load_from_path"],
            visit["patient_id"],
            visit["signal_file"],
            visit["label_file"],
            visit["save_to_path"],
        )
        
        # X load
        X = loadmat(os.path.join(root, signal))["val"]
        label_content =  open(os.path.join(root, label), "r").readlines()
        Dx, Sex, Age = label_content[-4].split(" ")[-1][:-1].split(","), label_content[-5].split(" ")[-1][:-1].split(","), label_content[-6].split(" ")[-1][:-1].split(",")

        y = 1 if set(Dx).intersection(CD_space) else 0
       
        
        # frequency * seconds (500 * 10)
        if X.shape[1] >= 500 * epoch_sec:
            for index in range((X.shape[1] - 500 * epoch_sec) // (500 * shift) + 1):
                save_file_path = os.path.join(save_path, f"{pid}-CD-{index}.pkl")
            
                pickle.dump(
                    {"signal": X[:, (500 * shift) * index : (500 * shift) * index + 5000], "label": y},
                    open(save_file_path, "wb"),
                )
                
                samples.append(
                    {   
                        "patient_id": pid,
                        "visit_id": signal.split(".")[0],
                        "record_id": len(samples) + 1,
                        "Sex": Sex,
                        "Age": Age,
                        "epoch_path": save_file_path,
                        "label": y,
                    }
                )

    return samples


def cardiology_isWA_fn(record, epoch_sec=10, shift=5):
    """Processes a single patient for the Wave abnormalities symptom in cardiology on the CardiologyDataset

    Cardiology symptoms can be divided into six categories. The task focuses on Wave abnormalities and is defined as a binary classification.

    Args:
        record: a singleton list of one subject from the CardiologyDataset.
            The (single) record is a dictionary with the following keys:
                load_from_path, signal_file, label1_file, label2_file, save_to_path, subject_id
        epoch_sec: how long will each epoch be (in seconds). 
        shift: the step size for the sampling window (with a width of epoch_sec)
        

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, record_id,
            and epoch_path (the path to the saved epoch {"X": signal, "Sex": gender, "Age": age, Y": label} as key.

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import CardiologyDataset
        >>> isWA = CardiologyDataset(
        ...         root="physionet.org/files/challenge-2020/1.0.2/training",
                    chosen_dataset=[1,1,1,1,1,1], 
        ...     )
        >>> from pyhealth.tasks import cardiology_isWA_fn
        >>> cardiology_ds = isWA.set_task(cardiology_isWA_fn)
        >>> cardiology_ds.samples[0]
        {
            'patient_id': '0_0',
            'visit_id': 'A0033',
            'record_id': 1,
            'Sex': ['Female'],
            'Age': ['34'],
            'epoch_path': '/Users/liyanjing/.cache/pyhealth/datasets/46c18f2a1a18803b4707a934a577a331/0_0-0.pkl',
            'label': '0'
        }
    """

    # these are the diseases codes for Wave abnormalities symptom
    WA_space = list(
        map(
            str,
            [
                164917005,
                164934002,
                59931005,
            ],
        )
    )
    
    samples = []
    for visit in record:
        root, pid, signal, label, save_path = (
            visit["load_from_path"],
            visit["patient_id"],
            visit["signal_file"],
            visit["label_file"],
            visit["save_to_path"],
        )
        
        # X load
        X = loadmat(os.path.join(root, signal))["val"]
        label_content =  open(os.path.join(root, label), "r").readlines()
        Dx, Sex, Age = label_content[-4].split(" ")[-1][:-1].split(","), label_content[-5].split(" ")[-1][:-1].split(","), label_content[-6].split(" ")[-1][:-1].split(",")


        y = 1 if set(Dx).intersection(WA_space) else 0
       
        
        # frequency * seconds (500 * 10)
        if X.shape[1] >= 500 * epoch_sec:
            for index in range((X.shape[1] - 500 * epoch_sec) // (500 * shift) + 1):
                save_file_path = os.path.join(save_path, f"{pid}-WA-{index}.pkl")
            
                pickle.dump(
                    {"signal": X[:, (500 * shift) * index : (500 * shift) * index + 5000], "label": y},
                    open(save_file_path, "wb"),
                )
                
                samples.append(
                    {   
                        "patient_id": pid,
                        "visit_id": signal.split(".")[0],
                        "record_id": len(samples) + 1,
                        "Sex": Sex,
                        "Age": Age,
                        "epoch_path": save_file_path,
                        "label": y,
                    }
                )

    return samples




if __name__ == "__main__":
    from pyhealth.datasets import CardiologyDataset

    #index for cardiology symptoms
    """
    Arrhythmias
    Bundle branch blocks and fascicular blocks
    Axis deviations
    Conduction delays
    Wave abnormalities
    """

    dataset = CardiologyDataset(
        root="/srv/local/data/physionet.org/files/challenge-2020/1.0.2/training",
        dev=True,
        refresh_cache=True,
    )
    sleep_staging_ds = dataset.set_task(cardiology_isAR_fn)
    print(sleep_staging_ds.samples[0])
    # print(sleep_staging_ds.patient_to_index)
    # print(sleep_staging_ds.record_to_index)
    print(sleep_staging_ds.input_info)
    
    
    
    


Here is the code content for drug_recommendation.py:
from pyhealth.data import Patient, Visit


def drug_recommendation_mimic3_fn(patient: Patient):
    """Processes a single patient for the drug recommendation task.

    Drug recommendation aims at recommending a set of drugs given the patient health
    history  (e.g., conditions and procedures).

    Args:
        patient: a Patient object

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key, like this
            {
                "patient_id": xxx,
                "visit_id": xxx,
                "conditions": [list of diag in visit 1, list of diag in visit 2, ..., list of diag in visit N],
                "procedures": [list of prod in visit 1, list of prod in visit 2, ..., list of prod in visit N],
                "drugs_hist": [list of drug in visit 1, list of drug in visit 2, ..., list of drug in visit (N-1)],
                "drugs": list of drug in visit N, # this is the predicted target
            }

    Examples:
        >>> from pyhealth.datasets import MIMIC3Dataset
        >>> mimic3_base = MIMIC3Dataset(
        ...    root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        ...    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        ...    code_mapping={"ICD9CM": "CCSCM"},
        ... )
        >>> from pyhealth.tasks import drug_recommendation_mimic3_fn
        >>> mimic3_sample = mimic3_base.set_task(drug_recommendation_mimic3_fn)
        >>> mimic3_sample.samples[0]
        {
            'visit_id': '174162',
            'patient_id': '107',
            'conditions': [['139', '158', '237', '99', '60', '101', '51', '54', '53', '133', '143', '140', '117', '138', '55']],
            'procedures': [['4443', '4513', '3995']],
            'drugs_hist': [[]],
            'drugs': ['0000', '0033', '5817', '0057', '0090', '0053', '0', '0012', '6332', '1001', '6155', '1001', '6332', '0033', '5539', '6332', '5967', '0033', '0040', '5967', '5846', '0016', '5846', '5107', '5551', '6808', '5107', '0090', '5107', '5416', '0033', '1150', '0005', '6365', '0090', '6155', '0005', '0090', '0000', '6373'],
        }
    """
    samples = []
    for i in range(len(patient)):
        visit: Visit = patient[i]
        conditions = visit.get_code_list(table="DIAGNOSES_ICD")
        procedures = visit.get_code_list(table="PROCEDURES_ICD")
        drugs = visit.get_code_list(table="PRESCRIPTIONS")
        # ATC 3 level
        drugs = [drug[:4] for drug in drugs]
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": conditions,
                "procedures": procedures,
                "drugs": drugs,
                "drugs_hist": drugs,
            }
        )
    # exclude: patients with less than 2 visit
    if len(samples) < 2:
        return []
    # add history
    samples[0]["conditions"] = [samples[0]["conditions"]]
    samples[0]["procedures"] = [samples[0]["procedures"]]
    samples[0]["drugs_hist"] = [samples[0]["drugs_hist"]]

    for i in range(1, len(samples)):
        samples[i]["conditions"] = samples[i - 1]["conditions"] + [
            samples[i]["conditions"]
        ]
        samples[i]["procedures"] = samples[i - 1]["procedures"] + [
            samples[i]["procedures"]
        ]
        samples[i]["drugs_hist"] = samples[i - 1]["drugs_hist"] + [
            samples[i]["drugs_hist"]
        ]

    # remove the target drug from the history
    for i in range(len(samples)):
        samples[i]["drugs_hist"][i] = []

    return samples


def drug_recommendation_mimic4_fn(patient: Patient):
    """Processes a single patient for the drug recommendation task.

    Drug recommendation aims at recommending a set of drugs given the patient health
    history  (e.g., conditions and procedures).

    Args:
        patient: a Patient object

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key
            {
                "patient_id": xxx,
                "visit_id": xxx,
                "conditions": [list of diag in visit 1, list of diag in visit 2, ..., list of diag in visit N],
                "procedures": [list of prod in visit 1, list of prod in visit 2, ..., list of prod in visit N],
                "drugs_hist": [list of drug in visit 1, list of drug in visit 2, ..., list of drug in visit (N-1)],
                "drugs": list of drug in visit N, # this is the predicted target
            }

    Examples:
        >>> from pyhealth.datasets import MIMIC4Dataset
        >>> mimic4_base = MIMIC4Dataset(
        ...     root="/srv/local/data/physionet.org/files/mimiciv/2.0/hosp",
        ...     tables=["diagnoses_icd", "procedures_icd"],
        ...     code_mapping={"ICD10PROC": "CCSPROC"},
        ... )
        >>> from pyhealth.tasks import drug_recommendation_mimic4_fn
        >>> mimic4_sample = mimic4_base.set_task(drug_recommendation_mimic4_fn)
        >>> mimic4_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '19', '122', '98', '663', '58', '51']], 'procedures': [['1']], 'label': [['2', '3', '4']]}]
    """
    samples = []
    for i in range(len(patient)):
        visit: Visit = patient[i]
        conditions = visit.get_code_list(table="diagnoses_icd")
        procedures = visit.get_code_list(table="procedures_icd")
        drugs = visit.get_code_list(table="prescriptions")
        # ATC 3 level
        drugs = [drug[:4] for drug in drugs]
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": conditions,
                "procedures": procedures,
                "drugs": drugs,
                "drugs_hist": drugs,
            }
        )
    # exclude: patients with less than 2 visit
    if len(samples) < 2:
        return []
    # add history
    samples[0]["conditions"] = [samples[0]["conditions"]]
    samples[0]["procedures"] = [samples[0]["procedures"]]
    samples[0]["drugs_hist"] = [samples[0]["drugs_hist"]]

    for i in range(1, len(samples)):
        samples[i]["conditions"] = samples[i - 1]["conditions"] + [
            samples[i]["conditions"]
        ]
        samples[i]["procedures"] = samples[i - 1]["procedures"] + [
            samples[i]["procedures"]
        ]
        samples[i]["drugs_hist"] = samples[i - 1]["drugs_hist"] + [
            samples[i]["drugs_hist"]
        ]

    # remove the target drug from the history
    for i in range(len(samples)):
        samples[i]["drugs_hist"][i] = []

    return samples


def drug_recommendation_eicu_fn(patient: Patient):
    """Processes a single patient for the drug recommendation task.

    Drug recommendation aims at recommending a set of drugs given the patient health
    history  (e.g., conditions and procedures).

    Args:
        patient: a Patient object

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key

    Examples:
        >>> from pyhealth.datasets import eICUDataset
        >>> eicu_base = eICUDataset(
        ...     root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        ...     tables=["diagnosis", "medication"],
        ...     code_mapping={},
        ...     dev=True
        ... )
        >>> from pyhealth.tasks import drug_recommendation_eicu_fn
        >>> eicu_sample = eicu_base.set_task(drug_recommendation_eicu_fn)
        >>> eicu_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '98', '663', '58', '51']], 'procedures': [['1']], 'label': [['2', '3', '4']]}]
    """
    samples = []
    for i in range(len(patient)):
        visit: Visit = patient[i]
        conditions = visit.get_code_list(table="diagnosis")
        procedures = visit.get_code_list(table="physicalExam")
        drugs = visit.get_code_list(table="medication")
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": conditions,
                "procedures": procedures,
                "drugs": drugs,
                "drugs_all": drugs,
            }
        )
    # exclude: patients with less than 2 visit
    if len(samples) < 2:
        return []
    # add history
    samples[0]["conditions"] = [samples[0]["conditions"]]
    samples[0]["procedures"] = [samples[0]["procedures"]]
    samples[0]["drugs_all"] = [samples[0]["drugs_all"]]

    for i in range(1, len(samples)):
        samples[i]["conditions"] = samples[i - 1]["conditions"] + [
            samples[i]["conditions"]
        ]
        samples[i]["procedures"] = samples[i - 1]["procedures"] + [
            samples[i]["procedures"]
        ]
        samples[i]["drugs_all"] = samples[i - 1]["drugs_all"] + [
            samples[i]["drugs_all"]
        ]

    return samples


def drug_recommendation_omop_fn(patient: Patient):
    """Processes a single patient for the drug recommendation task.

    Drug recommendation aims at recommending a set of drugs given the patient health
    history  (e.g., conditions and procedures).

    Args:
        patient: a Patient object

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key

    Examples:
        >>> from pyhealth.datasets import OMOPDataset
        >>> omop_base = OMOPDataset(
        ...     root="https://storage.googleapis.com/pyhealth/synpuf1k_omop_cdm_5.2.2",
        ...     tables=["condition_occurrence", "procedure_occurrence"],
        ...     code_mapping={},
        ... )
        >>> from pyhealth.tasks import drug_recommendation_omop_fn
        >>> omop_sample = omop_base.set_task(drug_recommendation_eicu_fn)
        >>> omop_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '98', '663', '58', '51'], ['98', '663', '58', '51']], 'procedures': [['1'], ['2', '3']], 'label': [['2', '3', '4'], ['0', '1', '4', '5']]}]
    """

    samples = []
    for i in range(len(patient)):
        visit: Visit = patient[i]
        conditions = visit.get_code_list(table="condition_occurrence")
        procedures = visit.get_code_list(table="procedure_occurrence")
        drugs = visit.get_code_list(table="drug_exposure")
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": conditions,
                "procedures": procedures,
                "drugs": drugs,
                "drugs_all": drugs,
            }
        )
    # exclude: patients with less than 2 visit
    if len(samples) < 2:
        return []
    # add history
    samples[0]["conditions"] = [samples[0]["conditions"]]
    samples[0]["procedures"] = [samples[0]["procedures"]]
    samples[0]["drugs_all"] = [samples[0]["drugs_all"]]

    for i in range(1, len(samples)):
        samples[i]["conditions"] = samples[i - 1]["conditions"] + [
            samples[i]["conditions"]
        ]
        samples[i]["procedures"] = samples[i - 1]["procedures"] + [
            samples[i]["procedures"]
        ]
        samples[i]["drugs_all"] = samples[i - 1]["drugs_all"] + [
            samples[i]["drugs_all"]
        ]

    return samples


if __name__ == "__main__":
    # from pyhealth.datasets import MIMIC3Dataset
    # base_dataset = MIMIC3Dataset(
    #     root="/srv/local/data/physionet.org/files/mimiciii/1.4",
    #     tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
    #     dev=True,
    #     code_mapping={"ICD9CM": "CCSCM"},
    #     refresh_cache=False,
    # )
    # sample_dataset = base_dataset.set_task(task_fn=drug_recommendation_mimic3_fn)
    # sample_dataset.stat()
    # print(sample_dataset.available_keys)
    # print(sample_dataset.samples[0])

    from pyhealth.datasets import MIMIC4Dataset

    base_dataset = MIMIC4Dataset(
        root="/srv/local/data/physionet.org/files/mimiciv/2.0/hosp",
        tables=["diagnoses_icd", "procedures_icd", "prescriptions"],
        dev=True,
        code_mapping={"NDC": "ATC"},
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=drug_recommendation_mimic4_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)
    print(sample_dataset.samples[0])

    # from pyhealth.datasets import eICUDataset

    # base_dataset = eICUDataset(
    #     root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
    #     tables=["diagnosis", "medication", "physicalExam"],
    #     dev=True,
    #     refresh_cache=False,
    # )
    # sample_dataset = base_dataset.set_task(task_fn=drug_recommendation_eicu_fn)
    # sample_dataset.stat()
    # print(sample_dataset.available_keys)

    # from pyhealth.datasets import OMOPDataset

    # base_dataset = OMOPDataset(
    #     root="/srv/local/data/zw12/pyhealth/raw_data/synpuf1k_omop_cdm_5.2.2",
    #     tables=["condition_occurrence", "procedure_occurrence", "drug_exposure"],
    #     dev=True,
    #     refresh_cache=False,
    # )
    # sample_dataset = base_dataset.set_task(task_fn=drug_recommendation_omop_fn)
    # sample_dataset.stat()
    # print(sample_dataset.available_keys)


Here is the code content for mortality_prediction.py:
from pyhealth.data import Patient, Visit


def mortality_prediction_mimic3_fn(patient: Patient):
    """Processes a single patient for the mortality prediction task.

    Mortality prediction aims at predicting whether the patient will decease in the
    next hospital visit based on the clinical information from current visit
    (e.g., conditions and procedures).

    Args:
        patient: a Patient object

    Returns:
        samples: a list of samples, each sample is a dict with patient_id,
            visit_id, and other task-specific attributes as key

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import MIMIC3Dataset
        >>> mimic3_base = MIMIC3Dataset(
        ...    root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        ...    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        ...    code_mapping={"ICD9CM": "CCSCM"},
        ... )
        >>> from pyhealth.tasks import mortality_prediction_mimic3_fn
        >>> mimic3_sample = mimic3_base.set_task(mortality_prediction_mimic3_fn)
        >>> mimic3_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '19', '122', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 0}]
    """
    samples = []

    # we will drop the last visit
    for i in range(len(patient) - 1):
        visit: Visit = patient[i]
        next_visit: Visit = patient[i + 1]

        if next_visit.discharge_status not in [0, 1]:
            mortality_label = 0
        else:
            mortality_label = int(next_visit.discharge_status)

        conditions = visit.get_code_list(table="DIAGNOSES_ICD")
        procedures = visit.get_code_list(table="PROCEDURES_ICD")
        drugs = visit.get_code_list(table="PRESCRIPTIONS")
        # exclude: visits without condition, procedure, and drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": mortality_label,
            }
        )
    # no cohort selection
    return samples


def mortality_prediction_mimic4_fn(patient: Patient):
    """Processes a single patient for the mortality prediction task.

    Mortality prediction aims at predicting whether the patient will decease in the
    next hospital visit based on the clinical information from current visit
    (e.g., conditions and procedures).

    Args:
        patient: a Patient object

    Returns:
        samples: a list of samples, each sample is a dict with patient_id,
            visit_id, and other task-specific attributes as key

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import MIMIC4Dataset
        >>> mimic4_base = MIMIC4Dataset(
        ...     root="/srv/local/data/physionet.org/files/mimiciv/2.0/hosp",
        ...     tables=["diagnoses_icd", "procedures_icd"],
        ...     code_mapping={"ICD10PROC": "CCSPROC"},
        ... )
        >>> from pyhealth.tasks import mortality_prediction_mimic4_fn
        >>> mimic4_sample = mimic4_base.set_task(mortality_prediction_mimic4_fn)
        >>> mimic4_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '19', '122', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 1}]
    """
    samples = []

    # we will drop the last visit
    for i in range(len(patient) - 1):
        visit: Visit = patient[i]
        next_visit: Visit = patient[i + 1]

        if next_visit.discharge_status not in [0, 1]:
            mortality_label = 0
        else:
            mortality_label = int(next_visit.discharge_status)

        conditions = visit.get_code_list(table="diagnoses_icd")
        procedures = visit.get_code_list(table="procedures_icd")
        drugs = visit.get_code_list(table="prescriptions")
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": mortality_label,
            }
        )
    # no cohort selection
    return samples


def mortality_prediction_eicu_fn(patient: Patient):
    """Processes a single patient for the mortality prediction task.

    Mortality prediction aims at predicting whether the patient will decease in the
    next hospital visit based on the clinical information from current visit
    (e.g., conditions and procedures).

    Features key-value pairs:
    - using diagnosis table (ICD9CM and ICD10CM) as condition codes
    - using physicalExam table as procedure codes
    - using medication table as drugs codes

    Args:
        patient: a Patient object

    Returns:
        samples: a list of samples, each sample is a dict with patient_id,
            visit_id, and other task-specific attributes as key

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import eICUDataset
        >>> eicu_base = eICUDataset(
        ...     root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        ...     tables=["diagnosis", "medication", "physicalExam"],
        ...     code_mapping={},
        ...     dev=True
        ... )
        >>> from pyhealth.tasks import mortality_prediction_eicu_fn
        >>> eicu_sample = eicu_base.set_task(mortality_prediction_eicu_fn)
        >>> eicu_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 0}]
    """
    samples = []
    # we will drop the last visit
    for i in range(len(patient) - 1):
        visit: Visit = patient[i]
        next_visit: Visit = patient[i + 1]

        if next_visit.discharge_status not in ["Alive", "Expired"]:
            mortality_label = 0
        else:
            mortality_label = 0 if next_visit.discharge_status == "Alive" else 1

        conditions = visit.get_code_list(table="diagnosis")
        procedures = visit.get_code_list(table="physicalExam")
        drugs = visit.get_code_list(table="medication")
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": mortality_label,
            }
        )
    # no cohort selection
    return samples


def mortality_prediction_eicu_fn2(patient: Patient):
    """Processes a single patient for the mortality prediction task.

    Mortality prediction aims at predicting whether the patient will decease in the
    next hospital visit based on the clinical information from current visit
    (e.g., conditions and procedures).

    Similar to mortality_prediction_eicu_fn, but with different code mapping:
    - using admissionDx table and diagnosisString under diagnosis table as condition codes
    - using treatment table as procedure codes

    Args:
        patient: a Patient object

    Returns:
        samples: a list of samples, each sample is a dict with patient_id,
            visit_id, and other task-specific attributes as key

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import eICUDataset
        >>> eicu_base = eICUDataset(
        ...     root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        ...     tables=["diagnosis", "admissionDx", "treatment"],
        ...     code_mapping={},
        ...     dev=True
        ... )
        >>> from pyhealth.tasks import mortality_prediction_eicu_fn2
        >>> eicu_sample = eicu_base.set_task(mortality_prediction_eicu_fn2)
        >>> eicu_sample.samples[0]
        {'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 0}
    """
    samples = []
    # we will drop the last visit
    for i in range(len(patient) - 1):
        visit: Visit = patient[i]
        next_visit: Visit = patient[i + 1]

        if next_visit.discharge_status not in ["Alive", "Expired"]:
            mortality_label = 0
        else:
            mortality_label = 0 if next_visit.discharge_status == "Alive" else 1

        admissionDx = visit.get_code_list(table="admissionDx")
        diagnosisString = list(
            set(
                [
                    dx.attr_dict["diagnosisString"]
                    for dx in visit.get_event_list("diagnosis")
                ]
            )
        )
        treatment = visit.get_code_list(table="treatment")

        # exclude: visits without treatment, admissionDx, diagnosisString
        if len(admissionDx) + len(diagnosisString) * len(treatment) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": admissionDx + diagnosisString,
                "procedures": treatment,
                "label": mortality_label,
            }
        )
    print(samples)
    # no cohort selection
    return samples


def mortality_prediction_omop_fn(patient: Patient):
    """Processes a single patient for the mortality prediction task.

    Mortality prediction aims at predicting whether the patient will decease in the
    next hospital visit based on the clinical information from current visit
    (e.g., conditions and procedures).

    Args:
        patient: a Patient object

    Returns:
        samples: a list of samples, each sample is a dict with patient_id,
            visit_id, and other task-specific attributes as key

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import OMOPDataset
        >>> omop_base = OMOPDataset(
        ...     root="https://storage.googleapis.com/pyhealth/synpuf1k_omop_cdm_5.2.2",
        ...     tables=["condition_occurrence", "procedure_occurrence"],
        ...     code_mapping={},
        ... )
        >>> from pyhealth.tasks import mortality_prediction_omop_fn
        >>> omop_sample = omop_base.set_task(mortality_prediction_eicu_fn)
        >>> omop_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 1}]
    """
    samples = []
    # we will drop the last visit
    for i in range(len(patient) - 1):
        visit: Visit = patient[i]
        next_visit: Visit = patient[i + 1]
        mortality_label = int(next_visit.discharge_status)

        conditions = visit.get_code_list(table="condition_occurrence")
        procedures = visit.get_code_list(table="procedure_occurrence")
        drugs = visit.get_code_list(table="drug_exposure")
        # labs = visit.get_code_list(table="measurement")

        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": mortality_label,
            }
        )
    # no cohort selection
    return samples


if __name__ == "__main__":
    from pyhealth.datasets import MIMIC3Dataset

    base_dataset = MIMIC3Dataset(
        root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        dev=True,
        code_mapping={"ICD9CM": "CCSCM", "NDC": "ATC"},
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=mortality_prediction_mimic3_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)

    from pyhealth.datasets import MIMIC4Dataset

    base_dataset = MIMIC4Dataset(
        root="/srv/local/data/physionet.org/files/mimiciv/2.0/hosp",
        tables=["diagnoses_icd", "procedures_icd", "prescriptions"],
        dev=True,
        code_mapping={"NDC": "ATC"},
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=mortality_prediction_mimic4_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)

    from pyhealth.datasets import eICUDataset

    base_dataset = eICUDataset(
        root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        tables=["diagnosis", "medication", "physicalExam"],
        dev=True,
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=mortality_prediction_eicu_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)

    base_dataset = eICUDataset(
        root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        tables=["diagnosis", "admissionDx", "treatment"],
        dev=True,
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=mortality_prediction_eicu_fn2)
    sample_dataset.stat()
    print(sample_dataset.available_keys)

    from pyhealth.datasets import OMOPDataset

    base_dataset = OMOPDataset(
        root="/srv/local/data/zw12/pyhealth/raw_data/synpuf1k_omop_cdm_5.2.2",
        tables=["condition_occurrence", "procedure_occurrence", "drug_exposure"],
        dev=True,
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=mortality_prediction_omop_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)


Here is the code content for readmission_prediction.py:
from pyhealth.data import Patient, Visit


# TODO: time_window cannot be passed in to base_dataset
def readmission_prediction_mimic3_fn(patient: Patient, time_window=15):
    """Processes a single patient for the readmission prediction task.

    Readmission prediction aims at predicting whether the patient will be readmitted
    into hospital within time_window days based on the clinical information from
    current visit (e.g., conditions and procedures).

    Args:
        patient: a Patient object
        time_window: the time window threshold (gap < time_window means label=1 for
            the task)

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import MIMIC3Dataset
        >>> mimic3_base = MIMIC3Dataset(
        ...    root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        ...    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        ...    code_mapping={"ICD9CM": "CCSCM"},
        ... )
        >>> from pyhealth.tasks import readmission_prediction_mimic3_fn
        >>> mimic3_sample = mimic3_base.set_task(readmission_prediction_mimic3_fn)
        >>> mimic3_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '19', '122', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 1}]
    """
    samples = []

    # we will drop the last visit
    for i in range(len(patient) - 1):
        visit: Visit = patient[i]
        next_visit: Visit = patient[i + 1]

        # get time difference between current visit and next visit
        time_diff = (next_visit.encounter_time - visit.encounter_time).days
        readmission_label = 1 if time_diff < time_window else 0

        conditions = visit.get_code_list(table="DIAGNOSES_ICD")
        procedures = visit.get_code_list(table="PROCEDURES_ICD")
        drugs = visit.get_code_list(table="PRESCRIPTIONS")
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": readmission_label,
            }
        )
    # no cohort selection
    return samples


def readmission_prediction_mimic4_fn(patient: Patient, time_window=15):
    """Processes a single patient for the readmission prediction task.

    Readmission prediction aims at predicting whether the patient will be readmitted
    into hospital within time_window days based on the clinical information from
    current visit (e.g., conditions and procedures).

    Args:
        patient: a Patient object
        time_window: the time window threshold (gap < time_window means label=1 for
            the task)

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import MIMIC4Dataset
        >>> mimic4_base = MIMIC4Dataset(
        ...     root="/srv/local/data/physionet.org/files/mimiciv/2.0/hosp",
        ...     tables=["diagnoses_icd", "procedures_icd"],
        ...     code_mapping={"ICD10PROC": "CCSPROC"},
        ... )
        >>> from pyhealth.tasks import readmission_prediction_mimic4_fn
        >>> mimic4_sample = mimic4_base.set_task(readmission_prediction_mimic4_fn)
        >>> mimic4_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '19', '122', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 0}]
    """
    samples = []

    # we will drop the last visit
    for i in range(len(patient) - 1):
        visit: Visit = patient[i]
        next_visit: Visit = patient[i + 1]

        # get time difference between current visit and next visit
        time_diff = (next_visit.encounter_time - visit.encounter_time).days
        readmission_label = 1 if time_diff < time_window else 0

        conditions = visit.get_code_list(table="diagnoses_icd")
        procedures = visit.get_code_list(table="procedures_icd")
        drugs = visit.get_code_list(table="prescriptions")
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": readmission_label,
            }
        )
    # no cohort selection
    return samples


def readmission_prediction_eicu_fn(patient: Patient, time_window=5):
    """Processes a single patient for the readmission prediction task.

    Readmission prediction aims at predicting whether the patient will be readmitted
    into hospital within time_window days based on the clinical information from
    current visit (e.g., conditions and procedures).

    Features key-value pairs:
    - using diagnosis table (ICD9CM and ICD10CM) as condition codes
    - using physicalExam table as procedure codes
    - using medication table as drugs codes

    Args:
        patient: a Patient object
        time_window: the time window threshold (gap < time_window means label=1 for
            the task)

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import eICUDataset
        >>> eicu_base = eICUDataset(
        ...     root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        ...     tables=["diagnosis", "medication", "physicalExam"],
        ...     code_mapping={},
        ...     dev=True
        ... )
        >>> from pyhealth.tasks import readmission_prediction_eicu_fn
        >>> eicu_sample = eicu_base.set_task(readmission_prediction_eicu_fn)
        >>> eicu_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 1}]
    """
    samples = []
    # we will drop the last visit
    for i in range(len(patient) - 1):
        visit: Visit = patient[i]
        next_visit: Visit = patient[i + 1]
        # get time difference between current visit and next visit
        time_diff = (next_visit.encounter_time - visit.encounter_time).days
        readmission_label = 1 if time_diff < time_window else 0

        conditions = visit.get_code_list(table="diagnosis")
        procedures = visit.get_code_list(table="physicalExam")
        drugs = visit.get_code_list(table="medication")
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": readmission_label,
            }
        )
    # no cohort selection
    return samples


def readmission_prediction_eicu_fn2(patient: Patient, time_window=5):
    """Processes a single patient for the readmission prediction task.

    Readmission prediction aims at predicting whether the patient will be readmitted
    into hospital within time_window days based on the clinical information from
    current visit (e.g., conditions and procedures).

    Similar to readmission_prediction_eicu_fn, but with different code mapping:
    - using admissionDx table and diagnosisString under diagnosis table as condition codes
    - using treatment table as procedure codes

    Args:
        patient: a Patient object
        time_window: the time window threshold (gap < time_window means label=1 for
            the task)

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import eICUDataset
        >>> eicu_base = eICUDataset(
        ...     root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        ...     tables=["diagnosis", "treatment", "admissionDx"],
        ...     code_mapping={},
        ...     dev=True
        ... )
        >>> from pyhealth.tasks import readmission_prediction_eicu_fn2
        >>> eicu_sample = eicu_base.set_task(readmission_prediction_eicu_fn2)
        >>> eicu_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 1}]
    """
    samples = []
    # we will drop the last visit
    for i in range(len(patient) - 1):
        visit: Visit = patient[i]
        next_visit: Visit = patient[i + 1]
        # get time difference between current visit and next visit
        time_diff = (next_visit.encounter_time - visit.encounter_time).days
        readmission_label = 1 if time_diff < time_window else 0

        admissionDx = visit.get_code_list(table="admissionDx")
        diagnosisString = list(
            set(
                [
                    dx.attr_dict["diagnosisString"]
                    for dx in visit.get_event_list("diagnosis")
                ]
            )
        )
        treatment = visit.get_code_list(table="treatment")

        # exclude: visits without treatment, admissionDx, diagnosisString
        if len(admissionDx) * len(diagnosisString) * len(treatment) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": admissionDx + diagnosisString,
                "procedures": treatment,
                "label": readmission_label,
            }
        )
    # no cohort selection
    return samples


def readmission_prediction_omop_fn(patient: Patient, time_window=15):
    """Processes a single patient for the readmission prediction task.

    Readmission prediction aims at predicting whether the patient will be readmitted
    into hospital within time_window days based on the clinical information from
    current visit (e.g., conditions and procedures).

    Args:
        patient: a Patient object
        time_window: the time window threshold (gap < time_window means label=1 for
            the task)

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import OMOPDataset
        >>> omop_base = OMOPDataset(
        ...     root="https://storage.googleapis.com/pyhealth/synpuf1k_omop_cdm_5.2.2",
        ...     tables=["condition_occurrence", "procedure_occurrence"],
        ...     code_mapping={},
        ... )
        >>> from pyhealth.tasks import readmission_prediction_omop_fn
        >>> omop_sample = omop_base.set_task(readmission_prediction_eicu_fn)
        >>> omop_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '98', '663', '58', '51']], 'procedures': [['1']], 'label': 1}]
    """
    samples = []
    # we will drop the last visit
    for i in range(len(patient) - 1):
        visit: Visit = patient[i]
        next_visit: Visit = patient[i + 1]
        time_diff = (next_visit.encounter_time - visit.encounter_time).days
        readmission_label = 1 if time_diff < time_window else 0

        conditions = visit.get_code_list(table="condition_occurrence")
        procedures = visit.get_code_list(table="procedure_occurrence")
        drugs = visit.get_code_list(table="drug_exposure")
        # labs = get_code_from_list_of_event(
        #     visit.get_event_list(table="measurement")
        # )

        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": readmission_label,
            }
        )
    # no cohort selection
    return samples


if __name__ == "__main__":
    from pyhealth.datasets import MIMIC3Dataset

    base_dataset = MIMIC3Dataset(
        root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        dev=True,
        code_mapping={"ICD9CM": "CCSCM", "NDC": "ATC"},
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=readmission_prediction_mimic3_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)

    from pyhealth.datasets import MIMIC4Dataset

    base_dataset = MIMIC4Dataset(
        root="/srv/local/data/physionet.org/files/mimiciv/2.0/hosp",
        tables=["diagnoses_icd", "procedures_icd", "prescriptions"],
        dev=True,
        code_mapping={"NDC": "ATC"},
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=readmission_prediction_mimic4_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)

    from pyhealth.datasets import eICUDataset

    base_dataset = eICUDataset(
        root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        tables=["diagnosis", "medication", "physicalExam"],
        dev=True,
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=readmission_prediction_eicu_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)

    base_dataset = eICUDataset(
        root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        tables=["diagnosis", "admissionDx", "treatment"],
        dev=True,
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=readmission_prediction_eicu_fn2)
    sample_dataset.stat()
    print(sample_dataset.available_keys)

    from pyhealth.datasets import OMOPDataset

    base_dataset = OMOPDataset(
        root="/srv/local/data/zw12/pyhealth/raw_data/synpuf1k_omop_cdm_5.2.2",
        tables=["condition_occurrence", "procedure_occurrence", "drug_exposure"],
        dev=True,
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(task_fn=readmission_prediction_omop_fn)
    sample_dataset.stat()
    print(sample_dataset.available_keys)


Here is the code content for sleep_staging.py:
import os
import pickle
import pkg_resources
import mne
import pandas as pd
import numpy as np


def sleep_staging_isruc_fn(record, epoch_seconds=10, label_id=1):
    """Processes a single patient for the sleep staging task on ISRUC.

    Sleep staging aims at predicting the sleep stages (Awake, N1, N2, N3, REM) based on
    the multichannel EEG signals. The task is defined as a multi-class classification.

    Args:
        record: a singleton list of one subject from the ISRUCDataset.
            The (single) record is a dictionary with the following keys:
                load_from_path, signal_file, label1_file, label2_file, save_to_path, subject_id
        epoch_seconds: how long will each epoch be (in seconds).
            It has to be a factor of 30 because the original data was labeled every 30 seconds.
        label_id: which set of labels to use. ISURC is labeled by *two* experts.
            By default we use the first set of labels (label_id=1).

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, record_id,
            and epoch_path (the path to the saved epoch {"X": signal, "Y": label} as key.

    Note that we define the task as a multi-class classification task.

    Examples:
        >>> from pyhealth.datasets import ISRUCDataset
        >>> isruc = ISRUCDataset(
        ...         root="/srv/local/data/data/ISRUC-I", download=True,
        ...     )
        >>> from pyhealth.tasks import sleep_staging_isruc_fn
        >>> sleepstage_ds = isruc.set_task(sleep_staging_isruc_fn)
        >>> sleepstage_ds.samples[0]
        {
            'record_id': '1-0',
            'patient_id': '1',
            'epoch_path': '/home/zhenlin4/.cache/pyhealth/datasets/832afe6e6e8a5c9ea5505b47e7af8125/10-1/1/0.pkl',
            'label': 'W'
        }
    """
    SAMPLE_RATE = 200
    assert 30 % epoch_seconds == 0, "ISRUC is annotated every 30 seconds."
    _channels = [
        "F3",
        "F4",
        "C3",
        "C4",
        "O1",
        "O2",
    ]  # https://arxiv.org/pdf/1910.06100.pdf

    def _find_channels(potential_channels):
        keep = {}
        for c in potential_channels:
            # https://www.ers-education.org/lrmedia/2016/pdf/298830.pdf
            new_c = (
                c.replace("-M2", "")
                .replace("-A2", "")
                .replace("-M1", "")
                .replace("-A1", "")
            )
            if new_c in _channels:
                assert new_c not in keep, f"Unrecognized channels: {potential_channels}"
                keep[new_c] = c
        assert len(keep) == len(
            _channels
        ), f"Unrecognized channels: {potential_channels}"
        return {v: k for k, v in keep.items()}

    record = record[0]
    save_path = os.path.join(
        record["save_to_path"], f"{epoch_seconds}-{label_id}", record["subject_id"]
    )
    if not os.path.isdir(save_path):
        os.makedirs(save_path)
    data = mne.io.read_raw_edf(
        os.path.join(record["load_from_path"], record["signal_file"])
    ).to_data_frame()
    data = (
        data.rename(columns=_find_channels(data.columns))
        .reindex(columns=_channels)
        .values
    )
    ann = pd.read_csv(
        os.path.join(record["load_from_path"], record[f"label{label_id}_file"]),
        header=None,
    )[0]
    ann = ann.map(["W", "N1", "N2", "N3", "Unknown", "R"].__getitem__)
    assert "Unknown" not in ann.values, "bad annotations"
    samples = []
    sample_length = SAMPLE_RATE * epoch_seconds
    for i, epoch_label in enumerate(np.repeat(ann.values, 30 // epoch_seconds)):
        epoch_signal = data[i * sample_length : (i + 1) * sample_length].T
        save_file_path = os.path.join(save_path, f"{i}.pkl")
        pickle.dump(
            {
                "signal": epoch_signal,
                "label": epoch_label,
            },
            open(save_file_path, "wb"),
        )
        samples.append(
            {
                "record_id": f"{record['subject_id']}-{i}",
                "patient_id": record["subject_id"],
                "epoch_path": save_file_path,
                "label": epoch_label,  # use for counting the label tokens
            }
        )
    return samples


def sleep_staging_sleepedf_fn(record, epoch_seconds=30):
    """Processes a single patient for the sleep staging task on Sleep EDF.

    Sleep staging aims at predicting the sleep stages (Awake, REM, N1, N2, N3, N4) based on
    the multichannel EEG signals. The task is defined as a multi-class classification.

    Args:
        patient: a list of (load_from_path, signal_file, label_file, save_to_path) tuples, where PSG is the signal files and the labels are
        in label file
        epoch_seconds: how long will each epoch be (in seconds)

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, record_id,
            and epoch_path (the path to the saved epoch {"X": signal, "Y": label} as key.

    Note that we define the task as a multi-class classification task.

    Examples:
        >>> from pyhealth.datasets import SleepEDFDataset
        >>> sleepedf = SleepEDFDataset(
        ...         root="/srv/local/data/SLEEPEDF/sleep-edf-database-expanded-1.0.0/sleep-cassette",
        ...     )
        >>> from pyhealth.tasks import sleep_staging_sleepedf_fn
        >>> sleepstage_ds = sleepedf.set_task(sleep_staging_sleepedf_fn)
        >>> sleepstage_ds.samples[0]
        {
            'record_id': 'SC4001-0',
            'patient_id': 'SC4001',
            'epoch_path': '/home/chaoqiy2/.cache/pyhealth/datasets/70d6dbb28bd81bab27ae2f271b2cbb0f/SC4001-0.pkl',
            'label': 'W'
        }
    """

    SAMPLE_RATE = 100

    root, psg_file, hypnogram_file, save_path = (
        record[0]["load_from_path"],
        record[0]["signal_file"],
        record[0]["label_file"],
        record[0]["save_to_path"],
    )
    # get patient id
    pid = psg_file[:6]

    # load signal "X" part
    data = mne.io.read_raw_edf(os.path.join(root, psg_file))

    X = data.get_data()
    # load label "Y" part
    ann = mne.read_annotations(os.path.join(root, hypnogram_file))

    labels = []
    for dur, des in zip(ann.duration, ann.description):
        """
        all possible des:
            - 'Sleep stage W'
            - 'Sleep stage 1'
            - 'Sleep stage 2'
            - 'Sleep stage 3'
            - 'Sleep stage 4'
            - 'Sleep stage R'
            - 'Sleep stage ?'
            - 'Movement time'
        """
        for _ in range(int(dur) // 30):
            labels.append(des)

    samples = []
    sample_length = SAMPLE_RATE * epoch_seconds
    # slice the EEG signals into non-overlapping windows
    # window size = sampling rate * second time = 100 * epoch_seconds
    for slice_index in range(min(X.shape[1] // sample_length, len(labels))):
        # ingore the no label epoch
        if labels[slice_index] not in [
            "Sleep stage W",
            "Sleep stage 1",
            "Sleep stage 2",
            "Sleep stage 3",
            "Sleep stage 4",
            "Sleep stage R",
        ]:
            continue

        epoch_signal = X[
            :, slice_index * sample_length : (slice_index + 1) * sample_length
        ]
        epoch_label = labels[slice_index][-1]  # "W", "1", "2", "3", "R"
        save_file_path = os.path.join(save_path, f"{pid}-{slice_index}.pkl")

        pickle.dump(
            {
                "signal": epoch_signal,
                "label": epoch_label,
            },
            open(save_file_path, "wb"),
        )

        samples.append(
            {
                "record_id": f"{pid}-{slice_index}",
                "patient_id": pid,
                "epoch_path": save_file_path,
                "label": epoch_label,  # use for counting the label tokens
            }
        )
    return samples


def sleep_staging_shhs_fn(record, epoch_seconds=30):
    """Processes a single recording for the sleep staging task on SHHS.

    Sleep staging aims at predicting the sleep stages (Awake, REM, N1, N2, N3) based on
    the multichannel EEG signals. The task is defined as a multi-class classification.

    Args:
        patient: a list of (load_from_path, signal file, label file, save_to_path) tuples, where the signal is in edf file and
        the labels are in the label file
        epoch_seconds: how long will each epoch be (in seconds), 30 seconds as default given by the label file

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, record_id,
            and epoch_path (the path to the saved epoch {"X": signal, "Y": label} as key.

    Note that we define the task as a multi-class classification task.

    Examples:
        >>> from pyhealth.datasets import SHHSDataset
        >>> shhs = SleepEDFDataset(
        ...         root="/srv/local/data/SHHS/polysomnography",
        ...         dev=True,
        ...     )
        >>> from pyhealth.tasks import sleep_staging_shhs_fn
        >>> shhs_ds = sleepedf.set_task(sleep_staging_shhs_fn)
        >>> shhs_ds.samples[0]
        {
            'record_id': 'shhs1-200001-0', 
            'patient_id': 'shhs1-200001', 
            'epoch_path': '/home/chaoqiy2/.cache/pyhealth/datasets/76c1ce8195a2e1a654e061cb5df4671a/shhs1-200001-0.pkl', 
            'label': '0'
        }
    """
    
    # test whether the ogb and torch_scatter packages are ready
    dependencies = ["elementpath"]
    try:
        pkg_resources.require(dependencies)
        import xml.etree.ElementTree as ET
    except Exception as e:
        print(e)
        print ('-----------')
        print(
            "Please follow the error message and install the ['elementpath'] packages first."
        )
    
    SAMPLE_RATE = 125

    root, signal_file, label_file, save_path = (
        record[0]["load_from_path"],
        record[0]["signal_file"],
        record[0]["label_file"],
        record[0]["save_to_path"],
    )
    # get file prefix, e.g., shhs1-200001
    pid = signal_file.split("/")[-1].split(".")[0]

    # load signal "X" part
    data = mne.io.read_raw_edf(os.path.join(root, signal_file))

    X = data.get_data()
    
    # some EEG signals have missing channels, we treat them separately
    if X.shape[0] == 16:
        X = X[[0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15], :]
    elif X.shape[0] == 15:
        X = X[[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14], :]
    X = X[[2,7], :]
            
    # load label "Y" part
    with open(os.path.join(root, label_file), "r") as f:
        text = f.read()
        root = ET.fromstring(text)
        Y = [i.text for i in root.find('SleepStages').findall('SleepStage')]

    samples = []
    sample_length = SAMPLE_RATE * epoch_seconds
    
    # slice the EEG signals into non-overlapping windows
    # window size = sampling rate * second time = 125 * epoch_seconds
    for slice_index in range(X.shape[1] // sample_length):

        epoch_signal = X[
            :, slice_index * sample_length : (slice_index + 1) * sample_length
        ]
        epoch_label = Y[slice_index]
        save_file_path = os.path.join(save_path, f"{pid}-{slice_index}.pkl")

        pickle.dump(
            {
                "signal": epoch_signal,
                "label": epoch_label,
            },
            open(save_file_path, "wb"),
        )

        samples.append(
            {
                "record_id": f"{pid}-{slice_index}",
                "patient_id": pid,
                "epoch_path": save_file_path,
                "label": epoch_label,  # use for counting the label tokens
            }
        )
    return samples


if __name__ == "__main__":
    from pyhealth.datasets import SleepEDFDataset, SHHSDataset, ISRUCDataset

    """ test sleep edf"""
    # dataset = SleepEDFDataset(
    #     root="/srv/local/data/SLEEPEDF/sleep-edf-database-expanded-1.0.0/sleep-telemetry",
    #     dev=True,
    #     refresh_cache=True,
    # )

    # sleep_staging_ds = dataset.set_task(sleep_staging_sleepedf_fn)
    # print(sleep_staging_ds.samples[0])
    # # print(sleep_staging_ds.patient_to_index)
    # # print(sleep_staging_ds.record_to_index)
    # print(sleep_staging_ds.input_info)

    # """ test ISRUC"""
    # dataset = ISRUCDataset(
    #     root="/srv/local/data/trash/",
    #     dev=True,
    #     refresh_cache=True,
    #     download=True,
    # )

    # sleep_staging_ds = dataset.set_task(sleep_staging_isruc_fn)
    # print(sleep_staging_ds.samples[0])
    # # print(sleep_staging_ds.patient_to_index)
    # # print(sleep_staging_ds.record_to_index)
    # print(sleep_staging_ds.input_info)
    
    dataset = SHHSDataset(
        root="/srv/local/data/SHHS/polysomnography",
        dev=True,
        refresh_cache=True,
    )
    sleep_staging_ds = dataset.set_task(sleep_staging_shhs_fn)
    print(sleep_staging_ds.samples[0])
    # print(sleep_staging_ds.patient_to_index)
    # print(sleep_staging_ds.record_to_index)
    print(sleep_staging_ds.input_info)
    
    
    
    


Here is the code content for temple_university_EEG_tasks.py:
import os
import pickle
import pkg_resources
import mne
import pandas as pd
import numpy as np


def EEG_isAbnormal_fn(record):
    """Processes a single patient for the abnormal EEG detection task on TUAB.

    Abnormal EEG detection aims at determining whether a EEG is abnormal.

    Args:
        record: a singleton list of one subject from the TUABDataset.
            The (single) record is a dictionary with the following keys:
                load_from_path, patient_id, visit_id, signal_file, label_file, save_to_path

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id, record_id,
            and epoch_path (the path to the saved epoch {"signal": signal, "label": label} as key.

    Note that we define the task as a binary classification task.

    Examples:
        >>> from pyhealth.datasets import TUABDataset
        >>> isabnormal = TUABDataset(
        ...         root="/srv/local/data/TUH/tuh_eeg_abnormal/v3.0.0/edf/", download=True,
        ...     )
        >>> from pyhealth.tasks import EEG_isabnormal_fn
        >>> EEG_abnormal_ds = isabnormal.set_task(EEG_isAbnormal_fn)
        >>> EEG_abnormal_ds.samples[0]
        {
            'patient_id': 'aaaaamye',
            'visit_id': 's001',
            'record_id': '1',
            'epoch_path': '/home/zhenlin4/.cache/pyhealth/datasets/832afe6e6e8a5c9ea5505b47e7af8125/10-1/1/0.pkl',
            'label': 1
        }
    """
    
    samples = []
    for visit in record:
        root, pid, visit_id, signal, label, save_path = (
            visit["load_from_path"],
            visit["patient_id"],
            visit["visit_id"],
            visit["signal_file"],
            visit["label_file"],
            visit["save_to_path"],
        )

        raw = mne.io.read_raw_edf(os.path.join(root, signal), preload=True)
        raw.resample(200)
        ch_name = raw.ch_names
        raw_data = raw.get_data()
        channeled_data = raw_data.copy()[:16]
        try:
            channeled_data[0] = (
                raw_data[ch_name.index("EEG FP1-REF")]
                - raw_data[ch_name.index("EEG F7-REF")]
            )
            channeled_data[1] = (
                raw_data[ch_name.index("EEG F7-REF")]
                - raw_data[ch_name.index("EEG T3-REF")]
            )
            channeled_data[2] = (
                raw_data[ch_name.index("EEG T3-REF")]
                - raw_data[ch_name.index("EEG T5-REF")]
            )
            channeled_data[3] = (
                raw_data[ch_name.index("EEG T5-REF")]
                - raw_data[ch_name.index("EEG O1-REF")]
            )
            channeled_data[4] = (
                raw_data[ch_name.index("EEG FP2-REF")]
                - raw_data[ch_name.index("EEG F8-REF")]
            )
            channeled_data[5] = (
                raw_data[ch_name.index("EEG F8-REF")]
                - raw_data[ch_name.index("EEG T4-REF")]
            )
            channeled_data[6] = (
                raw_data[ch_name.index("EEG T4-REF")]
                - raw_data[ch_name.index("EEG T6-REF")]
            )
            channeled_data[7] = (
                raw_data[ch_name.index("EEG T6-REF")]
                - raw_data[ch_name.index("EEG O2-REF")]
            )
            channeled_data[8] = (
                raw_data[ch_name.index("EEG FP1-REF")]
                - raw_data[ch_name.index("EEG F3-REF")]
            )
            channeled_data[9] = (
                raw_data[ch_name.index("EEG F3-REF")]
                - raw_data[ch_name.index("EEG C3-REF")]
            )
            channeled_data[10] = (
                raw_data[ch_name.index("EEG C3-REF")]
                - raw_data[ch_name.index("EEG P3-REF")]
            )
            channeled_data[11] = (
                raw_data[ch_name.index("EEG P3-REF")]
                - raw_data[ch_name.index("EEG O1-REF")]
            )
            channeled_data[12] = (
                raw_data[ch_name.index("EEG FP2-REF")]
                - raw_data[ch_name.index("EEG F4-REF")]
            )
            channeled_data[13] = (
                raw_data[ch_name.index("EEG F4-REF")]
                - raw_data[ch_name.index("EEG C4-REF")]
            )
            channeled_data[14] = (
                raw_data[ch_name.index("EEG C4-REF")]
                - raw_data[ch_name.index("EEG P4-REF")]
            )
            channeled_data[15] = (
                raw_data[ch_name.index("EEG P4-REF")]
                - raw_data[ch_name.index("EEG O2-REF")]
            )
        except:
            with open("tuab-process-error-files.txt", "a") as f:
                f.write(os.path.join(root, signal) + "\n")
            continue

        # get the label
        data_field = pid.split("_")[0]
        if data_field == "0" or data_field == "2":
            label = 1
        else:
            label = 0

        # load data
        for i in range(channeled_data.shape[1] // 2000):
            dump_path = os.path.join(
                save_path, pid + "_" + visit_id + "_" + str(i) + ".pkl"
            )
            pickle.dump(
                {"signal": channeled_data[:, i * 2000 : (i + 1) * 2000], "label": label},
                open(dump_path, "wb"),
            )

            samples.append(
                    {   
                        "patient_id": pid,
                        "visit_id": visit_id,
                        "record_id": i,
                        "epoch_path": dump_path,
                        "label": label,
                    }
            )

        return samples


def EEG_events_fn(record):
    """Processes a single patient for the EEG events task on TUEV.

    This task aims at annotating of EEG segments as one of six classes: (1) spike and sharp wave (SPSW), (2) generalized periodic epileptiform discharges (GPED), (3) periodic lateralized epileptiform discharges (PLED), (4) eye movement (EYEM), (5) artifact (ARTF) and (6) background (BCKG).

    Args:
        record: a singleton list of one subject from the TUEVDataset.
            The (single) record is a dictionary with the following keys:
                load_from_path, patient_id, visit_id, signal_file, label_file, save_to_path

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id, record_id, label, offending_channel,
            and epoch_path (the path to the saved epoch {"signal": signal, "label": label} as key.

    Note that we define the task as a multiclass classification task.

    Examples:
        >>> from pyhealth.datasets import TUEVDataset
        >>> EEGevents = TUEVDataset(
        ...         root="/srv/local/data/TUH/tuh_eeg_events/v2.0.0/edf/", download=True,
        ...     )
        >>> from pyhealth.tasks import EEG_events_fn
        >>> EEG_events_ds = EEGevents.set_task(EEG_events_fn)
        >>> EEG_events_ds.samples[0]
        {
            'patient_id': '0_00002265',
            'visit_id': '00000001',
            'record_id': 0,
            'epoch_path': '/Users/liyanjing/.cache/pyhealth/datasets/d8f3cb92cc444d481444d3414fb5240c/0_00002265_00000001_0.pkl',
            'label': 6,
            'offending_channel': array([4.])
        }
    """
    
    samples = []
    for visit in record:
        root, pid, visit_id, signal, label, save_path = (
            visit["load_from_path"],
            visit["patient_id"],
            visit["visit_id"],
            visit["signal_file"],
            visit["label_file"],
            visit["save_to_path"],
        )

        
        # load data
        try:
            [signals, times, event, Rawdata] = readEDF(
                os.path.join(root, signal)
            )  # event is the .rec file in the form of an array
            signals = convert_signals(signals, Rawdata)
        except (ValueError, KeyError):
            print("something funky happened in " + os.path.join(root, signal))
            continue
        signals, offending_channels, labels = BuildEvents(signals, times, event)

        for idx, (signal, offending_channel, label) in enumerate(
            zip(signals, offending_channels, labels)
        ):
            dump_path = os.path.join(
                save_path, pid + "_" + visit_id + "_" + str(idx) + ".pkl"
            )

            pickle.dump(
                    {"signal": signal, "label": int(label[0])},
                    open(dump_path, "wb"),
                )
            
            samples.append(
                {
                    "patient_id": pid,
                    "visit_id": visit_id,
                    "record_id": idx,
                    "epoch_path": dump_path,
                    "label": int(label[0]),
                    "offending_channel": offending_channel,
                }
            )

    return samples


def BuildEvents(signals, times, EventData):
    [numEvents, z] = EventData.shape  # numEvents is equal to # of rows of the .rec file
    fs = 250.0
    [numChan, numPoints] = signals.shape

    features = np.zeros([numEvents, numChan, int(fs) * 5])
    offending_channel = np.zeros([numEvents, 1])  # channel that had the detected thing
    labels = np.zeros([numEvents, 1])
    offset = signals.shape[1]
    signals = np.concatenate([signals, signals, signals], axis=1)
    for i in range(numEvents):  # for each event
        chan = int(EventData[i, 0])  # chan is channel
        start = np.where((times) >= EventData[i, 1])[0][0]
        end = np.where((times) >= EventData[i, 2])[0][0]
        features[i, :] = signals[
            :, offset + start - 2 * int(fs) : offset + end + 2 * int(fs)
        ]
        offending_channel[i, :] = int(chan)
        labels[i, :] = int(EventData[i, 3])
    return [features, offending_channel, labels]


def convert_signals(signals, Rawdata):
    signal_names = {
        k: v
        for (k, v) in zip(
            Rawdata.info["ch_names"], list(range(len(Rawdata.info["ch_names"])))
        )
    }
    new_signals = np.vstack(
        (
            signals[signal_names["EEG FP1-REF"]]
            - signals[signal_names["EEG F7-REF"]],  # 0
            (
                signals[signal_names["EEG F7-REF"]]
                - signals[signal_names["EEG T3-REF"]]
            ),  # 1
            (
                signals[signal_names["EEG T3-REF"]]
                - signals[signal_names["EEG T5-REF"]]
            ),  # 2
            (
                signals[signal_names["EEG T5-REF"]]
                - signals[signal_names["EEG O1-REF"]]
            ),  # 3
            (
                signals[signal_names["EEG FP2-REF"]]
                - signals[signal_names["EEG F8-REF"]]
            ),  # 4
            (
                signals[signal_names["EEG F8-REF"]]
                - signals[signal_names["EEG T4-REF"]]
            ),  # 5
            (
                signals[signal_names["EEG T4-REF"]]
                - signals[signal_names["EEG T6-REF"]]
            ),  # 6
            (
                signals[signal_names["EEG T6-REF"]]
                - signals[signal_names["EEG O2-REF"]]
            ),  # 7
            (
                signals[signal_names["EEG FP1-REF"]]
                - signals[signal_names["EEG F3-REF"]]
            ),  # 14
            (
                signals[signal_names["EEG F3-REF"]]
                - signals[signal_names["EEG C3-REF"]]
            ),  # 15
            (
                signals[signal_names["EEG C3-REF"]]
                - signals[signal_names["EEG P3-REF"]]
            ),  # 16
            (
                signals[signal_names["EEG P3-REF"]]
                - signals[signal_names["EEG O1-REF"]]
            ),  # 17
            (
                signals[signal_names["EEG FP2-REF"]]
                - signals[signal_names["EEG F4-REF"]]
            ),  # 18
            (
                signals[signal_names["EEG F4-REF"]]
                - signals[signal_names["EEG C4-REF"]]
            ),  # 19
            (
                signals[signal_names["EEG C4-REF"]]
                - signals[signal_names["EEG P4-REF"]]
            ),  # 20
            (signals[signal_names["EEG P4-REF"]] - signals[signal_names["EEG O2-REF"]]),
        )
    )  # 21
    return new_signals


def readEDF(fileName):
    Rawdata = mne.io.read_raw_edf(fileName)
    signals, times = Rawdata[:]
    RecFile = fileName[0:-3] + "rec"
    eventData = np.genfromtxt(RecFile, delimiter=",")
    Rawdata.close()
    return [signals, times, eventData, Rawdata]

if __name__ == "__main__":
    from pyhealth.datasets import TUABDataset, TUEVDataset
    
    # dataset = TUABDataset(
    #     root="/srv/local/data/TUH/tuh_eeg_abnormal/v3.0.0/edf/",
    #     dev=True,
    #     refresh_cache=True,
    # )
    # EEG_abnormal_ds = dataset.set_task(EEG_isAbnormal_fn)
    # print(EEG_abnormal_ds.samples[0])
    # print(EEG_abnormal_ds.input_info)
    
    dataset = TUEVDataset(
        root="/srv/local/data/TUH/tuh_eeg_events/v2.0.0/edf/",
        dev=True,
        refresh_cache=True,
    )
    EEG_events_ds = dataset.set_task(EEG_events_fn)
    print(EEG_events_ds.samples[0])
    print(EEG_events_ds.input_info)
    
    
    
    


Here is the code content for test_medcode.py:
import unittest
import sys 
import os
current = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.dirname(os.path.dirname(current)))

from pyhealth.medcode import InnerMap, CrossMap

class TestInnerMap(unittest.TestCase):

    def setUp(self):
        map_name = "ICD9CM"
        self.inner_map = InnerMap.load(map_name)

    def test_contain(self):
        self.assertTrue(
            "428.0" in self.inner_map, 
            msg="contain function of InnerMap failed"
            )
    
    def test_lookup(self):
        self.assertEqual(
            self.inner_map.lookup("428.0"),
            'Congestive heart failure, unspecified',
            msg="lookup function of InnerMap failed"
            )

    def test_get_ancestors(self):
        self.assertEqual(
            self.inner_map.get_ancestors("428.0"),
            ['428', '420-429.99', '390-459.99', '001-999.99'],
            msg="get_ancestors function of InnerMap failed"
            )

    def test_get_descendants(self):
        self.assertEqual(
            self.inner_map.get_descendants("428"),
            ['428.0', '428.1', '428.2', '428.3', '428.4', '428.9', '428.20', '428.21', '428.22', '428.23', '428.30', '428.31', '428.32', '428.33', '428.40', '428.41', '428.42', '428.43'],            
            msg="get_descendants function of InnerMap failed"        
            )


class TestInnerMapATC(unittest.TestCase):
    def setUp(self):
        self.inner_map = InnerMap.load("ATC")
    
    def test_lookup(self):
        self.assertEqual(
            self.inner_map.lookup("M01AE51"),
            'ibuprofen, combinations',
            msg="lookup function of InnerMap (ATC) failed"
            )
        self.assertEqual(
            self.inner_map.lookup("M01AE51", "drugbank_id"),
            'DB01050',
            msg="lookup function of InnerMap (ATC) failed"
            )
        self.assertEqual(
            self.inner_map.lookup("M01AE51", "smiles"),
            'CC(C)CC1=CC=C(C=C1)C(C)C(O)=O',
            msg="lookup function of InnerMap (ATC) failed"
            )
        
            
    def test_convert(self):
        self.assertEqual(
            self.inner_map.convert("A12CE02", level=3),
            "A12C",
            msg="convert function of InnerMap (ATC) failed"
        )


class TestCrossMap(unittest.TestCase):
    def setUp(self):
        self.cross_map = CrossMap.load(source_vocabulary="ICD9CM", target_vocabulary="CCSCM")

    def test_map(self):
        self.assertEqual(
            self.cross_map.map("428.0"),
            ["108"],
            msg="map function of CrossMap failed"
        )


if __name__ == "__main__":
    unittest.main()


Here is the code content for test_tokenizer.py:
import unittest
import sys 
import os
current = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.dirname(os.path.dirname(current)))

from pyhealth.tokenizer import Tokenizer

token_space = ['A01A', 'A02A', 'A02B', 'A02X', 'A03A', 'A03B', 'A03C', 'A03D', 'A03E', \
          'A03F', 'A04A', 'A05A', 'A05B', 'A05C', 'A06A', 'A07A', 'A07B', 'A07C', \
          'A07D', 'A07E', 'A07F', 'A07X', 'A08A', 'A09A', 'A10A', 'A10B', 'A10X', \
          'A11A', 'A11B', 'A11C', 'A11D', 'A11E', 'A11G', 'A11H', 'A11J', 'A12A', \
          'A12B', 'A12C', 'A13A', 'A14A', 'A14B', 'A16A']


class Test1D(unittest.TestCase):
    def setUp(self):
        self.tokenizer = Tokenizer(tokens=token_space, special_tokens=["<pad>", "<unk>"])
    
    def test_voc_size(self):
        self.assertEqual(
            self.tokenizer.get_vocabulary_size(),
            44,
            msg="get_vocabulary_size function failed"
        )
    
    def test_encode(self):
        tokens = ['A03C', 'A03D', 'A03E', 'A03F', 'A04A', 'A05A', 'A05B', 'B035', 'C129']
        indices = self.tokenizer.convert_tokens_to_indices(tokens)
        self.assertEqual(
            indices,
            [8, 9, 10, 11, 12, 13, 14, 1, 1],
            msg="convert_tokens_to_indices function failed"
        )

    def test_decode(self):
        indices = [0, 1, 2, 3, 4, 5]
        tokens = self.tokenizer.convert_indices_to_tokens(indices)
        self.assertEqual(
            tokens,
            ['<pad>', '<unk>', 'A01A', 'A02A', 'A02B', 'A02X'],
            msg="convert_indices_to_tokens function failed"
        )


class Test2D(unittest.TestCase):
    def setUp(self):
        self.tokenizer = Tokenizer(tokens=token_space, special_tokens=["<pad>", "<unk>"])

    def test_encode(self):
        tokens = [
            ['A03C', 'A03D', 'A03E', 'A03F'],
            ['A04A', 'B035', 'C129']
        ]

        self.assertEqual(
            self.tokenizer.batch_encode_2d(tokens),
            [[8, 9, 10, 11], [12, 1, 1, 0]],
            msg="batch_encode_2d function failed"
        )        

        self.assertEqual(
            self.tokenizer.batch_encode_2d(tokens, padding=False),
            [[8, 9, 10, 11], [12, 1, 1]],
            msg="batch_encode_2d function - set padding failed"
        )   

        self.assertEqual(
            self.tokenizer.batch_encode_2d(tokens, max_length=3),
            [[9, 10, 11], [12, 1, 1]],
            msg="batch_encode_2d function - set max length failed"
        )   

    def test_decode(self):
        indices = [
            [8, 9, 10, 11],
            [12, 1, 1, 0]
        ]

        self.assertEqual(
            self.tokenizer.batch_decode_2d(indices),
            [['A03C', 'A03D', 'A03E', 'A03F'], ['A04A', '<unk>', '<unk>']],
            msg="batch_decode_2d function failed"
        )   

        self.assertEqual(
            self.tokenizer.batch_decode_2d(indices, padding=True),
            [['A03C', 'A03D', 'A03E', 'A03F'], ['A04A', '<unk>', '<unk>', '<pad>']],
            msg="batch_decode_2d function - set padding failed"
        )


class Test3D(unittest.TestCase):
    def setUp(self):
        self.tokenizer = Tokenizer(tokens=token_space, special_tokens=["<pad>", "<unk>"])

    def test_encode(self):
        tokens = [
            [
                ['A03C', 'A03D', 'A03E', 'A03F'],
                ['A08A', 'A09A'],
            ],
            [
                ['A04A', 'B035', 'C129'],
            ]
        ]

        self.assertEqual(
            self.tokenizer.batch_encode_3d(tokens),
            [[[8, 9, 10, 11], [24, 25, 0, 0]], [[12, 1, 1, 0], [0, 0, 0, 0]]],
            msg="batch_encode_3d function failed"
        )

        self.assertEqual(
            self.tokenizer.batch_encode_3d(tokens, padding=(False, True)),
            [[[8, 9, 10, 11], [24, 25, 0, 0]], [[12, 1, 1, 0]]],
            msg="batch_encode_3d function (no padding on the first dimension) failed"
        )

        self.assertEqual(
            self.tokenizer.batch_encode_3d(tokens, padding=(True, False)),
            [[[8, 9, 10, 11], [24, 25]], [[12, 1, 1], [0]]],
            msg="batch_encode_3d function (no padding on the second dimension) failed"
        )

        self.assertEqual(
            self.tokenizer.batch_encode_3d(tokens, padding=(False, False)),
            [[[8, 9, 10, 11], [24, 25]], [[12, 1, 1]]],
            msg="batch_encode_3d function (no padding on both dimension) failed"
        )

        self.assertEqual(
            self.tokenizer.batch_encode_3d(tokens, max_length=(2,2)),
            [[[10, 11], [24, 25]], [[1, 1], [0, 0]]],
            msg="batch_encode_3d function (truncation) failed"
        )

    def test_decode(self):
        indices = [
            [
                [8, 9, 10, 11], 
                [24, 25, 0, 0]
            ], 
            [
                [12, 1, 1, 0], 
                [0, 0, 0, 0]
            ]
        ]

        self.assertEqual(
            self.tokenizer.batch_decode_3d(indices),
            [[['A03C', 'A03D', 'A03E', 'A03F'], ['A08A', 'A09A']], [['A04A', '<unk>', '<unk>']]],
            msg="batch_decode_3d function failed"
        )

        self.assertEqual(
            self.tokenizer.batch_decode_3d(indices, padding=True),
            [[['A03C', 'A03D', 'A03E', 'A03F'], ['A08A', 'A09A', '<pad>', '<pad>']], [['A04A', '<unk>', '<unk>', '<pad>'], ['<pad>', '<pad>', '<pad>', '<pad>']]],            
            msg="batch_decode_3d function - set padding failed"
        )


if __name__ == "__main__":
    unittest.main()

Here is the code content for test_eicu.py:
import datetime
from sqlite3 import Timestamp
import unittest
from pyhealth.data.data import Event
import pandas

from pyhealth.datasets import eICUDataset
from pyhealth.unittests.test_datasets.utils import EHRDatasetStatAssertion


class TesteICUDataset(unittest.TestCase):

    # to test the file this path needs to be updated
    DATASET_NAME = "eICU-demo"
    ROOT = "https://storage.googleapis.com/pyhealth/eicu-demo/"
    TABLES = ["diagnosis", "medication", "lab", "treatment", "physicalExam"]
    CODE_MAPPING = {}
    DEV = True  # not needed when using demo set since its 100 patients large
    REFRESH_CACHE = True

    dataset = eICUDataset(
        dataset_name=DATASET_NAME,
        root=ROOT,
        tables=TABLES,
        code_mapping=CODE_MAPPING,
        dev=DEV,
        refresh_cache=REFRESH_CACHE,
    )

    def setUp(self):
        pass
    
    def test_patient(self):
        # given parametes:
        selected_patient_id = "002-10009+193705"
        selected_visit_index = 0
        # selected indeces for events defined in `expected_event_data`

        # expect:
        # patient data
        expected_birth_datetime = pandas.Timestamp("1938-01-01 00:00:00")
        expected_death_datetime = None
        expected_ethnicity = "Caucasian"
        expected_gender = "Female"

        # visit data
        expected_visit_len = 1
        expected_visit_id = "224606"
        expected_visit_discharge_status = "Alive"
        expected_discharge_time = datetime.datetime(2014, 1, 4, 0, 45)
        expected_encounter_time = datetime.datetime(2014, 1, 1, 2, 59)

        # visit attribute dict data
        expected_visit_attr_dict_len = 2
        expected_visit_hopital_id = 71
        expected_visit_region = "Midwest"

        # event level data
        expected_event_count = 319

        # during a specified visit assert the event data is correct. Event data is parametrized by tables
        # schema:
        #   event type (from one of the requested tables)
        #       'length': number of events for that event type
        #       'events':
        #           tuple of index of the event in the event array, pyhealth.Event object with hardcoded relevant fields for event at the index
        expected_event_data = {
            "diagnosis": {
                "length": 8,
                "events": [
                    (
                        0,
                        Event(
                            code="567.9",
                            timestamp=pandas.Timestamp("2014-01-01 03:36:00"),
                            vocabulary="ICD9CM",
                        ),
                    ),
                    (
                        1,
                        Event(
                            code="K65.0",
                            timestamp=pandas.Timestamp("2014-01-01 03:36:00"),
                            vocabulary="ICD10CM",
                        ),
                    ),
                ],
            },
            "medication": {
                "length": 38,
                "events": [
                    (
                        0,
                        Event(
                            code="MORPHINE INJ",
                            timestamp=pandas.Timestamp("2013-12-31 21:09:00"),
                            vocabulary="eICU_DRUGNAME",
                        ),
                    ),
                    (
                        5,
                        Event(
                            code="CIPROFLOXACIN IN D5W 400 MG/200ML IV SOLN",
                            timestamp=pandas.Timestamp("2013-12-31 22:43:00"),
                            vocabulary="eICU_DRUGNAME",
                        ),
                    ),
                ],
            },
            "lab": {
                "length": 251,
                "events": [
                    (
                        0,
                        Event(
                            code="sodium",
                            timestamp=pandas.Timestamp("2013-12-31 21:04:00"),
                            vocabulary="eICU_LABNAME",
                        ),
                    ),
                    (
                        2,
                        Event(
                            code="BUN",
                            timestamp=pandas.Timestamp("2013-12-31 21:04:00"),
                            vocabulary="eICU_LABNAME",
                        ),
                    ),
                ],
            },
            "physicalExam": {
                "length": 22,
                "events": [
                    (
                        0,
                        Event(
                            code="notes/Progress Notes/Physical Exam/Physical Exam/Neurologic/GCS/Score/scored",
                            timestamp=pandas.Timestamp("2014-01-01 03:05:00"),
                            vocabulary="eICU_PHYSICALEXAMPATH",
                        ),
                    ),
                    (
                        1,
                        Event(
                            code="notes/Progress Notes/Physical Exam/Physical Exam Obtain Options/Performed - Structured",
                            timestamp=pandas.Timestamp("2014-01-01 03:05:00"),
                            vocabulary="eICU_PHYSICALEXAMPATH",
                        ),
                    ),
                ],
            },
        }

        # patient level information
        actual_patient = self.dataset.patients[selected_patient_id]
        self.assertEqual(expected_visit_len, len(actual_patient.visits))
        self.assertEqual(expected_birth_datetime, actual_patient.birth_datetime)
        self.assertEqual(expected_death_datetime, actual_patient.death_datetime)
        self.assertEqual(expected_ethnicity, actual_patient.ethnicity)
        self.assertEqual(expected_gender, actual_patient.gender)

        # visit level information
        actual_visit_id = actual_patient.index_to_visit_id[selected_visit_index]
        self.assertEqual(expected_visit_id, actual_visit_id)

        actual_visit = actual_patient.visits[actual_visit_id]
        self.assertEqual(expected_event_count, actual_visit.num_events)
        self.assertEqual(expected_visit_discharge_status, actual_visit.discharge_status)
        self.assertEqual(expected_discharge_time, actual_visit.discharge_time)
        self.assertEqual(expected_encounter_time, actual_visit.encounter_time)

        # visit attributes
        actual_visit_attributes = actual_visit.attr_dict
        self.assertEqual(expected_visit_attr_dict_len, len(actual_visit_attributes))
        self.assertEqual(
            expected_visit_hopital_id, actual_visit_attributes["hospital_id"]
        )
        self.assertEqual(expected_visit_region, actual_visit_attributes["region"])

        # event level information
        actual_event_list_dict = actual_visit.event_list_dict
        for event_key in expected_event_data:
            actual_event_array = actual_event_list_dict[event_key]
            expected_event = expected_event_data[event_key]

            self.assertEqual(
                expected_event["length"],
                len(actual_event_array),
                f"incorrect num events for'{event_key}'",
            )
            for selected_index, expected_pyhealth_Event in expected_event["events"]:
                error_message = f"incorrect event code on '{event_key}' event, selected index: {selected_index}"

                actual_event = actual_event_array[selected_index]
                self.assertEqual(
                    expected_pyhealth_Event.code, actual_event.code, error_message
                )
                self.assertEqual(
                    expected_pyhealth_Event.timestamp,
                    actual_event.timestamp,
                    error_message,
                )
                self.assertEqual(
                    expected_pyhealth_Event.vocabulary,
                    actual_event.vocabulary,
                    error_message,
                )

    def test_statistics(self):
        # self.dataset.stat()

        self.assertEqual(sorted(self.TABLES), sorted(self.dataset.available_tables))

        EHRDatasetStatAssertion(self.dataset, 0.01).assertEHRStats(
            expected_num_patients=2174,
            expected_num_visits=2520,
            expected_num_visits_per_patient=1.1592,
            expected_events_per_visit_per_table=[
                16.7202,
                17.8345,
                172.4841,
                15.1944,
                33.3563,
            ],
        )


if __name__ == "__main__":
    unittest.main(verbosity=2)


Here is the code content for test_mimic3.py:
import datetime
import unittest

from pyhealth.datasets import MIMIC3Dataset
from pyhealth.unittests.test_datasets.utils import EHRDatasetStatAssertion
import os, sys

current = os.path.dirname(os.path.realpath(__file__))
repo_root = os.path.dirname(os.path.dirname(os.path.dirname(current)))
sys.path.append(repo_root)


# this test suite verifies the MIMIC3 dataset is consistently parsing the dataset.
# a dataset is qualified if it produces the correct statistics, and if a sample from the dataset
# matches the expected data.
# Synthetic_MIMIC-III dataset provided in the root is a dependancy to the expected values
# used for testing correctness
# like the MIMIC4 dataset, if this test suite fails, it may be due to a regression in the
# code, or due to the dataset at the root chaning.


class TestsMimic3Dataset(unittest.TestCase):
    DATASET_NAME = "mimic3-demo"
    ROOT = "https://storage.googleapis.com/pyhealth/mimiciii-demo/1.4/"
    TABLES = ["DIAGNOSES_ICD", "PRESCRIPTIONS"]
    CODE_MAPPING = {"NDC": ("ATC", {"target_kwargs": {"level": 3}})}
    REFRESH_CACHE = True

    dataset = MIMIC3Dataset(
        dataset_name=DATASET_NAME,
        root=ROOT,
        tables=TABLES,
        code_mapping=CODE_MAPPING,
        refresh_cache=REFRESH_CACHE,
    )

    def setUp(self):
        pass

    # tests that a single event is correctly parsed
    def test_patient(self):

        selected_patient_id = "10035"
        selected_visit_id = "110244"

        expected_geneder = "M"
        expected_ethnicity = "WHITE"
        expected_birth_datetime = datetime.datetime(2053, 4, 13, 0, 0)
        expected_death_datetime = None

        expected_visit_to_id_keys = [0]
        expected_visit_to_id_values = [selected_visit_id]

        expected_visit_id = "110244"
        expected_num_visits = 1
        expected_encounter_time = datetime.datetime(2129, 3, 3, 16, 6)
        expected_discharge_time = datetime.datetime(2129, 3, 7, 18, 19)

        expected_num_events_in_visit = 17
        expected_num_event_types = 2
        expected_num_events_diagnoses_icd = 4
        expected_num_events_prescriptions = 13

        self.assertTrue(selected_patient_id in self.dataset.patients)

        actual_patient = self.dataset.patients[selected_patient_id]

        self.assertEqual(expected_geneder, actual_patient.gender)
        self.assertEqual(expected_ethnicity, actual_patient.ethnicity)
        self.assertEqual(expected_birth_datetime, actual_patient.birth_datetime)
        self.assertEqual(expected_death_datetime, actual_patient.death_datetime)

        self.assertEqual(expected_num_visits, len(actual_patient.visits))

        actual_visit_id = list(actual_patient.visits.keys())[0]

        self.assertEqual(
            expected_visit_to_id_keys, list(actual_patient.index_to_visit_id.keys())
        )
        self.assertEqual(
            expected_visit_to_id_values, list(actual_patient.index_to_visit_id.values())
        )

        self.assertEqual(expected_visit_id, actual_visit_id)
        actual_visit = actual_patient.visits[actual_visit_id]
        self.assertEqual(expected_encounter_time, actual_visit.encounter_time)
        self.assertEqual(expected_discharge_time, actual_visit.discharge_time)
        self.assertEqual(expected_num_events_in_visit, actual_visit.num_events)
        self.assertEqual(
            expected_num_event_types, len(actual_visit.event_list_dict.keys())
        )
        self.assertEqual(
            expected_num_events_diagnoses_icd,
            len(actual_visit.event_list_dict["DIAGNOSES_ICD"]),
        )
        self.assertEqual(
            expected_num_events_prescriptions,
            len(actual_visit.event_list_dict["PRESCRIPTIONS"]),
        )

    def test_statistics(self):
        # self.dataset.stat()

        self.assertEqual(sorted(self.TABLES), sorted(self.dataset.available_tables))

        EHRDatasetStatAssertion(self.dataset, 0.01).assertEHRStats(
            expected_num_patients=100,
            expected_num_visits=129,
            expected_num_visits_per_patient=1.2900,
            expected_events_per_visit_per_table=[13.6512, 56.7597],
        )


if __name__ == "__main__":
    unittest.main(verbosity=2)


Here is the code content for test_mimic4.py:
import datetime
import unittest

from pyhealth.datasets import MIMIC4Dataset
from pyhealth.unittests.test_datasets.utils import EHRDatasetStatAssertion

import os, sys

current = os.path.dirname(os.path.realpath(__file__))
repo_root = os.path.dirname(os.path.dirname(os.path.dirname(current)))
sys.path.append(repo_root)


# this test suite verifies the MIMIC4 demo dataset is parsed correctly and produces
# the correct dataset for demoing. To qualify the units under test we check the dataset statistics,
# and a single sample from the dataset.


class TestMimic4Dataset(unittest.TestCase):

    DATASET_NAME = "mimic4-demo"
    ROOT = "https://storage.googleapis.com/pyhealth/mimiciv-demo/hosp/"
    TABLES = ["diagnoses_icd", "procedures_icd", "labevents"]
    CODE_MAPPING = {}
    DEV = True  # not needed when using demo set since its 100 patients large
    REFRESH_CACHE = True

    dataset = MIMIC4Dataset(
        dataset_name=DATASET_NAME,
        root=ROOT,
        tables=TABLES,
        code_mapping=CODE_MAPPING,
        dev=DEV,
        refresh_cache=REFRESH_CACHE,
    )

    def setUp(self):
        pass

    # test the dataset integrity based on a single sample.
    def test_patient(self):
        expected_patient_id = "10000032"

        expected_visit_count = 4
        expected_visit_to_id_keys = 3
        expected_visit_to_id_values = "29079034"

        expected_diagnoses_icd_event_count = 8
        expected_procedures_icd_event_count = 1
        expected_labevent_event_count = 57
        expected_event_length = 66
        expected_encounter_time = datetime.datetime(2180, 5, 6, 22, 23)
        expected_discharge_time = datetime.datetime(2180, 5, 7, 17, 15)

        actual_patient_id = list(self.dataset.patients.keys())[0]
        self.assertEqual(expected_patient_id, actual_patient_id)

        actual_visits = self.dataset.patients[actual_patient_id]
        self.assertEqual(expected_visit_count, len(actual_visits))
        self.assertEqual(
            expected_visit_to_id_keys, list(actual_visits.index_to_visit_id.keys())[-1]
        )
        self.assertEqual(
            expected_visit_to_id_values,
            list(actual_visits.index_to_visit_id.values())[-1],
        )

        visit = actual_visits[0]
        self.assertEqual(
            expected_diagnoses_icd_event_count,
            len(visit.event_list_dict["diagnoses_icd"]),
        )
        self.assertEqual(
            expected_procedures_icd_event_count,
            len(visit.event_list_dict["procedures_icd"]),
        )
        self.assertEqual(
            expected_labevent_event_count, len(visit.event_list_dict["labevents"])
        )
        self.assertEqual(expected_event_length, visit.num_events)
        self.assertEqual(expected_encounter_time, visit.encounter_time)
        self.assertEqual(expected_discharge_time, visit.discharge_time)

    # checks data integrity based on statistics.
    def test_statistics(self):

        # self.dataset.stat()

        self.assertEqual(sorted(self.TABLES), sorted(self.dataset.available_tables))

        EHRDatasetStatAssertion(self.dataset, 0.01).assertEHRStats(
            expected_num_patients=100,
            expected_num_visits=275,
            expected_num_visits_per_patient=2.7500,
            expected_events_per_visit_per_table=[16.3855, 2.6255, 288.3891],
        )


if __name__ == "__main__":
    unittest.main(verbosity=2)


Here is the code content for test_omop.py:
import datetime
from sqlite3 import Timestamp
import unittest
from pyhealth.data.data import Event
import collections

from pyhealth.datasets import OMOPDataset
from pyhealth.unittests.test_datasets.utils import EHRDatasetStatAssertion


class TestOMOPDataset(unittest.TestCase):
    DATASET_NAME = "omop-demo"
    ROOT = "https://storage.googleapis.com/pyhealth/synpuf1k_omop_cdm_5.2.2/"
    TABLES = [
        "condition_occurrence",
        "procedure_occurrence",
        "drug_exposure",
        "measurement",
    ]
    CODE_MAPPING = {}
    DEV = True  # not needed when using demo set since its 100 patients large
    REFRESH_CACHE = True

    dataset = OMOPDataset(
        dataset_name=DATASET_NAME,
        root=ROOT,
        tables=TABLES,
        code_mapping=CODE_MAPPING,
        dev=DEV,
        refresh_cache=REFRESH_CACHE,
    )

    def setUp(self):
        pass

    def test_patient(self):

        print(self.dataset)

        expected_len_patients = 1000
        selected_patient_id = "100"

        expected_available_tables = [
            "drug_exposure",
            "procedure_occurrence",
            "measurement",
        ]
        expected_birth_datetime = datetime.datetime(1934, 4, 1, 0, 0)
        expected_death_datetime = None
        expected_ethnicity = 8527
        expected_gender = 8532

        expected_len_visit_ids = 47

        selected_visit_index = 16
        expected_visit_id = "5393"

        expected_available_tables_for_visit = [
            "procedure_occurrence",
            "drug_exposure",
            "measurement",
        ]

        expected_visit_discharge_status = 0
        expected_visit_discharge_time = datetime.datetime(2008, 8, 8, 0, 0)
        expected_visit_encounter_time = datetime.datetime(2008, 8, 8, 0, 0)
        expected_visit_len_event_list_dict = 3

        Expected_data = collections.namedtuple(
            "Expected_data_attributes",
            [
                "code",
                "timestamp",
                "vocabulary",
            ],
        )

        expected_event_list_dict = [
            (
                "procedure_occurrence",
                4,
                0,
                Expected_data(
                    "2005280",
                    datetime.datetime(2008, 8, 8, 0, 0),
                    "PROCEDURE_CONCEPT_ID",
                ),
            ),
            (
                "drug_exposure",
                1,
                0,
                Expected_data(
                    "2213483", datetime.datetime(2008, 8, 8, 0, 0), "DRUG_CONCEPT_ID"
                ),
            ),
            (
                "measurement",
                1,
                0,
                Expected_data("2212095", None, "MEASUREMENT_CONCEPT_ID"),
            ),
        ]

        self.assertEqual(expected_len_patients, len(self.dataset.patients))

        actual_patient = self.dataset.patients[selected_patient_id]

        self.assertEqual(
            sorted(expected_available_tables), sorted(actual_patient.available_tables)
        )
        self.assertEqual(expected_birth_datetime, actual_patient.birth_datetime)
        self.assertEqual(expected_death_datetime, actual_patient.death_datetime)
        self.assertEqual(expected_ethnicity, actual_patient.ethnicity)
        self.assertEqual(expected_gender, actual_patient.gender)

        self.assertEqual(expected_len_visit_ids, len(actual_patient.index_to_visit_id))

        self.assertEqual(
            expected_visit_id, actual_patient.index_to_visit_id[selected_visit_index]
        )

        actual_visit_id = actual_patient.index_to_visit_id[selected_visit_index]

        actual_visit = actual_patient.visits[actual_visit_id]

        self.assertEqual(
            expected_available_tables_for_visit, actual_visit.available_tables
        )
        self.assertEqual(expected_visit_discharge_status, actual_visit.discharge_status)
        self.assertEqual(expected_visit_discharge_time, actual_visit.discharge_time)
        self.assertEqual(expected_visit_encounter_time, actual_visit.encounter_time)
        self.assertEqual(
            expected_visit_len_event_list_dict, len(actual_visit.event_list_dict)
        )

        for (
            expected_event_type,
            expected_event_list_len,
            selected_event_index,
            expected_event_data,
        ) in expected_event_list_dict:

            self.assertTrue(expected_event_type in actual_visit.available_tables)

            actual_visit_table = actual_visit.event_list_dict[expected_event_type]

            self.assertEqual(expected_event_list_len, len(actual_visit_table))

            actual_event = actual_visit_table[selected_event_index]

            self.assertEqual(expected_event_data.code, actual_event.code)
            self.assertEqual(expected_event_data.timestamp, actual_event.timestamp)
            self.assertEqual(expected_event_data.vocabulary, actual_event.vocabulary)

    def test_statistics(self):

        EHRDatasetStatAssertion(self.dataset, 0.01).assertEHRStats(
            expected_num_patients=1000,
            expected_num_visits=55261,
            expected_num_visits_per_patient=55.2610,
            expected_events_per_visit_per_table=[0.0000, 2.4886, 0.1387, 0.6253],
        )


if __name__ == "__main__":
    unittest.main(verbosity=2)


Here is the code content for utils.py:
from typing import List
from pyhealth.datasets import BaseEHRDataset

class EHRDatasetStatAssertion:
    
    def __init__(self, dataset: BaseEHRDataset, eps: float):
        self.dataset = dataset
        self.eps = eps
        # return self # builder
    
    def assertEHRStats(
        self,
        expected_num_patients: int,
        expected_num_visits: int,
        expected_num_visits_per_patient: float,
        expected_events_per_visit_per_table: List[float],
    ):
        self.assertNumPatients(expected_num_patients)
        self.assertNumVisits(expected_num_visits)
        self.assertMeanVisitsPerPatient(expected_num_visits_per_patient)
        self.assertTableMeans(expected_events_per_visit_per_table)
        
    def assertNumPatients(self, expected: int):
        actual = len(self.dataset.patients)
        if expected != actual:
            raise AssertionError(f"Expected {expected} patients got {actual}")

    def assertNumVisits(self, expected: int):
        actual = sum([len(patient) for patient in self.dataset.patients.values()])
        if expected != actual:
            raise AssertionError(f"Expected {expected} num visits got {actual}")
        
    def assertMeanVisitsPerPatient(self, expected: int):
        actual_visits = [len(patient) for patient in self.dataset.patients.values()]
        actual = sum(actual_visits) / len(actual_visits)
        if abs(expected - actual) > self.eps:
            raise AssertionError(f"Expected {expected} mean num visits got {actual}")
        
        
    # expected list must be ordered by tables
    def assertTableMeans(self, expected_per_table: List[float]):
        for expected_value, table in zip(expected_per_table, self.dataset.tables):
            actual_num_events = [
                len(v.get_event_list(table))
                for p in self.dataset.patients.values()
                for v in p
            ]

            actual_value = sum(actual_num_events) / len(actual_num_events)
            
            if abs(expected_value - actual_value) > self.eps:
                raise AssertionError(f"Expected {expected_value} mean for events in {table} got {actual_value}")

Here is the code content for test_data.py:
import unittest
import datetime
from pyhealth.data import Event, Visit, Patient


class TestEvent(unittest.TestCase):
    def setUp(self):
        self.event = Event(
            code="428.0",
            table="DIAGNOSES_ICD",
            vocabulary="ICD9CM",
            visit_id="v001",
            patient_id="p001",
            timestamp=datetime.datetime(2012, 1, 1, 0, 0),
            add_attr1="add_attr1",
            add_attr2={"key": "add_attr2"},
        )

    def test_type(self):
        self.assertIsInstance(self.event.timestamp, datetime.datetime)

    def test_attr(self):
        self.assertEqual(self.event.code, "428.0")
        self.assertEqual(self.event.table, "DIAGNOSES_ICD")
        self.assertEqual(self.event.vocabulary, "ICD9CM")
        self.assertEqual(self.event.visit_id, "v001")
        self.assertEqual(self.event.patient_id, "p001")
        self.assertEqual(self.event.timestamp, datetime.datetime(2012, 1, 1, 0, 0))

        attr_dict = self.event.attr_dict
        self.assertEqual(attr_dict["add_attr1"], "add_attr1")
        self.assertEqual(attr_dict["add_attr2"], {"key": "add_attr2"})


class TestVisit(unittest.TestCase):
    def setUp(self):
        self.event1 = Event(
            code="00069153041",
            table="PRESCRIPTIONS",
            vocabulary="NDC",
            visit_id="v001",
            patient_id="p001",
            dosage="250mg",
        )

        self.event2 = Event(
            code="00069153042",
            table="PRESCRIPTIONS",
            vocabulary="NDC",
            visit_id="v001",
            patient_id="p001",
            method="tablet",
        )

        self.visit = Visit(
            visit_id="v001",
            patient_id="p001",
            encounter_time=datetime.datetime(2012, 1, 1, 0, 0),
            discharge_time=datetime.datetime(2012, 1, 8, 0, 0),
            discharge_status="expired",
        )

    def test_methods(self):
        # add the first event
        self.visit.add_event(self.event1)
        self.assertTrue("PRESCRIPTIONS" in self.visit.available_tables)
        self.assertEqual(self.visit.num_events, 1)
        self.assertEqual(self.visit.get_event_list("PRESCRIPTIONS"), [self.event1])
        self.assertEqual(self.visit.get_code_list("PRESCRIPTIONS"), [self.event1.code])

        # add the second event
        self.visit.add_event(self.event2)
        self.assertEqual(self.visit.num_events, 2)
        self.assertEqual(
            self.visit.get_event_list("PRESCRIPTIONS"), [self.event1, self.event2]
        )
        self.assertEqual(
            self.visit.get_code_list("PRESCRIPTIONS"),
            [self.event1.code, self.event2.code],
        )

    def test_attr(self):
        self.visit.add_event(self.event1)
        self.visit.add_event(self.event2)
        self.assertEqual(self.visit.visit_id, "v001")
        self.assertEqual(self.visit.patient_id, "p001")
        self.assertEqual(self.visit.num_events, 2)
        self.assertEqual(self.visit.encounter_time, datetime.datetime(2012, 1, 1, 0, 0))
        self.assertEqual(self.visit.discharge_time, datetime.datetime(2012, 1, 8, 0, 0))
        self.assertEqual(self.visit.discharge_status, "expired")


class TestPatient(unittest.TestCase):
    def setUp(self):
        self.event = Event(
            code="00069153041",
            table="PRESCRIPTIONS",
            vocabulary="NDC",
            visit_id="v001",
            patient_id="p001",
            dosage="250mg",
        )

        self.visit = Visit(
            visit_id="v001",
            patient_id="p001",
            encounter_time=datetime.datetime(2012, 1, 1, 0, 0),
            discharge_time=datetime.datetime(2012, 1, 8, 0, 0),
            discharge_status="expired",
        )

        self.patient = Patient(
            patient_id="p001",
        )

    def test_methods(self):
        self.patient.add_visit(self.visit)
        self.patient.add_event(self.event)
        self.assertTrue("PRESCRIPTIONS" in self.patient.available_tables)
        self.assertEqual(self.patient.get_visit_by_id("v001"), self.visit)
        self.assertEqual(self.patient.get_visit_by_index(0), self.visit)
        self.assertEqual(self.patient.visits["v001"], self.visit)

    def test_attr(self):
        self.patient.add_visit(self.visit)
        self.patient.add_event(self.event)
        self.assertEqual(self.patient.patient_id, "p001")
        self.assertEqual(self.patient.get_visit_by_index(0).patient_id, "p001")
        self.assertEqual(self.patient.visits["v001"].patient_id, "p001")


if __name__ == "__main__":
    unittest.main()


Here is the code content for __init__.py:
from pyhealth.calib import calibration, predictionset


Here is the code content for base_classes.py:
from abc import ABC
from typing import Dict

import torch


class PostHocCalibrator(ABC, torch.nn.Module):
    def __init__(self, model, **kwargs) -> None:
        super().__init__()
        self.model = model

    def calibrate(self, cal_dataset):
        ...

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        ...


    def to(self, device):
        super().to(device)
        self.device = device
        return self


class SetPredictor(ABC, torch.nn.Module):
    def __init__(self, model, **kwargs) -> None:
        super().__init__()
        self.model = model

    def calibrate(self, cal_dataset):
        ...

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        ...

    def to(self, device):
        super().to(device)
        self.device = device
        return self

Here is the code content for utils.py:
from collections import defaultdict
from typing import Optional

import numpy as np
import torch
import tqdm
from torch import Tensor

from pyhealth.datasets import utils as datautils


def agg_loss(loss:torch.Tensor, reduction: str):
    if reduction == 'mean':
        return loss.mean()
    if reduction == 'sum':
        return loss.sum()
    return loss


def one_hot_np(labels, K):
    new_labels = np.zeros((len(labels), K))
    new_labels[np.arange(len(labels)), labels] = 1
    return new_labels

class LogLoss(torch.nn.Module):
    """Cross entropy, but takes in the probability instead of the logits"""
    reduction: str
    def __init__(self, weight: Optional[Tensor] = None,  ignore_index: int = -100, reduction: str = 'mean', clip=1e-10) -> None:
        super(LogLoss, self).__init__()
        self.register_buffer('weight', weight)
        self.ignore_index = ignore_index
        self.reduction = reduction
        self.clip = clip

    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        assert self.weight is None or isinstance(self.weight, Tensor)
        dim = input.dim()
        assert dim == 2, f"Expected 2 dimensions (got {dim})"
        input = input.clip(self.clip)#this weight should be trivial, so I won't normalize
        input = -torch.log(input)
        if self.weight is not None:
            input = input * self.weight.unsqueeze(0)
        loss = torch.gather(input, -1, target.unsqueeze(-1)).squeeze(-1)
        return agg_loss(loss, self.reduction)


def prepare_numpy_dataset(model, dataset, keys, forward_kwargs=None,
                         incl_data_keys=None, debug=False, batch_size=32):
    if forward_kwargs is None:
        forward_kwargs = {}
    if incl_data_keys is None:
        incl_data_keys = []
    loader = datautils.get_dataloader(dataset, batch_size, shuffle=False)

    ret = defaultdict(list)
    with torch.no_grad():
        for _i, data in tqdm.tqdm(enumerate(loader), desc=f"retrieving {keys}", total=len(loader)):
            if debug and _i % 10 != 0:
                continue
            data.update(forward_kwargs)
            res = model(**data)
            for key in keys:
                ret[key].append(res[key].detach().cpu().numpy())
            for key in incl_data_keys:
                ret[key].extend(data[key])
    for key in incl_data_keys:
        ret[key] = np.asarray(ret[key])
    for key in keys:
        ret[key] = np.concatenate(ret[key])
    return ret
    

Here is the code content for __init__.py:
"""Model calibration methods"""
from pyhealth.calib.calibration.dircal import DirichletCalibration
from pyhealth.calib.calibration.hb import HistogramBinning
from pyhealth.calib.calibration.kcal import KCal
from pyhealth.calib.calibration.temperature_scale import TemperatureScaling

__all__ = ['DirichletCalibration', 'HistogramBinning', 'KCal', 'TemperatureScaling']


Here is the code content for dircal.py:
"""
Dirichlet Calibration.

Implementation based on https://github.com/dirichletcal/dirichlet_python
"""

from typing import Dict

import torch
import torch.nn.functional as F
from torch import optim
from torch.utils.data import Subset

from pyhealth.calib.base_classes import PostHocCalibrator
from pyhealth.calib.utils import prepare_numpy_dataset
from pyhealth.models import BaseModel


def _get_identity_weights(n_classes, method="Full"):
    raw_weights = None
    if (method is None) or (method == "Full"):
        raw_weights = torch.zeros((n_classes, n_classes + 1)) + torch.hstack(
            [torch.eye(n_classes), torch.zeros((n_classes, 1))]
        )
    else:
        raise NotImplementedError
    return raw_weights.ravel()


def _softmax(X):
    """Compute the softmax of matrix X in a numerically stable way."""
    shiftx = X - torch.max(X, axis=1).values.reshape(-1, 1)
    exps = torch.exp(shiftx)
    return exps / torch.sum(exps, axis=1).reshape(-1, 1)


def _get_weights(params, k, ref_row, method):
    """Reshapes the given params (weights) into the full matrix including 0"""

    if method in ["Full", None]:
        raw_weights = params.reshape(-1, k + 1)
    if ref_row:
        weights = raw_weights - torch.repeat_interleave(
            raw_weights[-1, :].reshape(1, -1), k, dim=0
        )
    else:
        weights = raw_weights
    return weights


class DirichletCalibration(PostHocCalibrator):
    """Dirichlet Calibration

    Dirichlet calibration is similar to retraining a linear layer mapping from the
    old logits to the new logits with regularizations.
    This is a calibration method for *multiclass* classification only.

    Paper:

        [1] Kull, Meelis, Miquel Perello Nieto, Markus Kängsepp,
        Telmo Silva Filho, Hao Song, and Peter Flach.
        "Beyond temperature scaling: Obtaining well-calibrated multi-class
        probabilities with dirichlet calibration."
        Advances in neural information processing systems 32 (2019).

    :param model: A trained base model.
    :type model: BaseModel

    Examples:
        >>> from pyhealth.datasets import ISRUCDataset, split_by_patient, get_dataloader
        >>> from pyhealth.models import SparcNet
        >>> from pyhealth.tasks import sleep_staging_isruc_fn
        >>> from pyhealth.calib.calibration import DirichletCalibration
        >>> sleep_ds = ISRUCDataset("/srv/scratch1/data/ISRUC-I").set_task(sleep_staging_isruc_fn)
        >>> train_data, val_data, test_data = split_by_patient(sleep_ds, [0.6, 0.2, 0.2])
        >>> model = SparcNet(dataset=sleep_ds, feature_keys=["signal"],
        ...     label_key="label", mode="multiclass")
        >>> # ... Train the model here ...
        >>> # Calibrate
        >>> cal_model = DirichletCalibration(model)
        >>> cal_model.calibrate(cal_dataset=val_data)
        >>> # Evaluate
        >>> from pyhealth.trainer import Trainer
        >>> test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
        >>> print(Trainer(model=cal_model, metrics=['cwECEt_adapt', 'accuracy']).evaluate(test_dl))
        {'accuracy': 0.7096615988229524, 'cwECEt_adapt': 0.05336195546573208}
    """

    def __init__(self, model: BaseModel, debug=False, **kwargs) -> None:
        super().__init__(model, **kwargs)
        self.mode = self.model.mode
        for param in model.parameters():
            param.requires_grad = False

        self.model.eval()
        self.device = model.device
        self.debug = debug

        self.num_classes = None

    def _forward(self, logits):
        logits = torch.hstack(
            [logits, torch.zeros((logits.shape[0], 1), device=self.device)]
        )
        weights = _get_weights(
            self.weights, k=self.num_classes, ref_row=True, method="Full"
        )
        new_logits = torch.matmul(logits, weights.permute(1, 0))
        return new_logits, weights

    def calibrate(self, cal_dataset: Subset, lr=0.01, max_iter=128, reg_lambda=1e-3):
        """Calibrate the base model using a calibration dataset.

        :param cal_dataset: Calibration set.
        :type cal_dataset: Subset
        :param lr: learning rate, defaults to 0.01
        :type lr: float, optional
        :param max_iter: maximum iterations, defaults to 128
        :type max_iter: int, optional
        :param reg_lambda: regularization coefficient on the deviation from identity matrix.
            defaults to 1e-3
        :type reg_lambda: float, optional
        :return: None
        :rtype: None
        """

        _cal_data = prepare_numpy_dataset(
            self.model, cal_dataset, ["y_true", "logit"], debug=self.debug
        )
        if self.num_classes is None:
            self.num_classes = _cal_data["logit"].shape[1]

        self.weights = torch.nn.Parameter(
            torch.tensor(_get_identity_weights(self.num_classes), device=self.device),
            requires_grad=True,
        )
        optimizer = optim.LBFGS([self.weights], lr=lr, max_iter=max_iter)
        logits = torch.tensor(_cal_data["logit"], dtype=torch.float, device=self.device)
        label = torch.tensor(
            F.one_hot(torch.tensor(_cal_data["y_true"]), num_classes=self.num_classes),
            dtype=torch.long if self.model.mode == "multiclass" else torch.float32,
            device=self.device,
        )

        reg_mu = None
        reg_format = "identity"

        def _eval():
            optimizer.zero_grad()
            new_logits, weights = self._forward(logits)
            probs = _softmax(new_logits)
            loss = torch.mean(-torch.log(torch.sum(label * probs, dim=1)))
            if reg_mu is None:
                if reg_format == "identity":
                    reg = torch.hstack(
                        [
                            torch.eye(self.num_classes),
                            torch.zeros((self.num_classes, 1)),
                        ]
                    )
                else:
                    reg = torch.zeros((self.num_classes, self.num_classes + 1))
                loss = loss + reg_lambda * torch.sum(
                    (weights - reg.to(self.device)) ** 2
                )
            else:
                weights_hat = weights - torch.hstack(
                    [
                        weights[:, :-1]
                        * torch.eye(self.num_classes, device=self.device),
                        torch.zeros((self.num_classes, 1), device=self.device),
                    ]
                )
                loss = (
                    loss
                    + reg_lambda * torch.sum(weights_hat[:, :-1] ** 2)
                    + reg_mu * torch.sum(weights_hat[:, -1] ** 2)
                )

            loss.backward()
            return loss

        self.train()
        optimizer.step(_eval)
        self.eval()

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation (just like the original model).

        :param **kwargs: Additional arguments to the base model.

        :return:  A dictionary with all results from the base model, with the following modified:

            ``y_prob``: calibrated predicted probabilities.
            ``loss``: Cross entropy loss with the new y_prob.
            ``logit``: temperature-scaled logits.
        :rtype: Dict[str, torch.Tensor]
        """
        ret = self.model(**kwargs)
        ret["logit"] = self._forward(ret["logit"])[0]
        ret["y_prob"] = self.model.prepare_y_prob(ret["logit"])
        criterion = self.model.get_loss_function()
        ret["loss"] = criterion(ret["logit"], ret["y_true"])
        return ret


if __name__ == "__main__":
    from pyhealth.calib.calibration import DirichletCalibration
    from pyhealth.datasets import (ISRUCDataset, get_dataloader,
                                   split_by_patient)
    from pyhealth.models import SparcNet
    from pyhealth.tasks import sleep_staging_isruc_fn

    sleep_ds = ISRUCDataset(
        root="/srv/local/data/trash/",
        dev=True,
    ).set_task(sleep_staging_isruc_fn)
    train_data, val_data, test_data = split_by_patient(sleep_ds, [0.6, 0.2, 0.2])
    model = SparcNet(
        dataset=sleep_ds, feature_keys=["signal"], label_key="label", mode="multiclass"
    )
    # ... Train the model here ...
    # Calibrate
    cal_model = DirichletCalibration(model)
    cal_model.calibrate(cal_dataset=val_data)
    # Evaluate
    from pyhealth.trainer import Trainer

    test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
    print(
        Trainer(model=cal_model, metrics=["cwECEt_adapt", "accuracy"]).evaluate(test_dl)
    )


Here is the code content for hb.py:
"""
Histogram Binning.

Implementation based on https://github.com/aigen/df-posthoc-calibration

"""
from typing import Dict

import numpy as np
import torch
from torch.utils.data import Subset

from pyhealth.calib.base_classes import PostHocCalibrator
from pyhealth.calib.utils import LogLoss, one_hot_np, prepare_numpy_dataset
from pyhealth.models import BaseModel

__all__ = ["HistogramBinning"]


def _nudge(matrix, delta):
    return (matrix + np.random.uniform(low=0, high=delta, size=matrix.shape)) / (
        1 + delta
    )


def _bin_points(scores, bin_edges):
    assert bin_edges is not None, "Bins have not been defined"
    scores = scores.squeeze()
    assert np.size(scores.shape) < 2, "scores should be a 1D vector or singleton"
    scores = np.reshape(scores, (scores.size, 1))
    bin_edges = np.reshape(bin_edges, (1, bin_edges.size))
    return np.sum(scores > bin_edges, axis=1)


def _get_uniform_mass_bins(probs, n_bins):
    assert probs.size >= n_bins, "Fewer points than bins"

    probs_sorted = np.sort(probs)

    # split probabilities into groups of approx equal size
    groups = np.array_split(probs_sorted, n_bins)
    bin_upper_edges = list()

    for cur_group in range(n_bins - 1):
        bin_upper_edges += [max(groups[cur_group])]
    bin_upper_edges += [np.inf]

    return np.array(bin_upper_edges)


class HB_binary(object):
    def __init__(self, n_bins=15):
        ### Hyperparameters
        self.delta = 1e-10
        self.n_bins = n_bins

        ### Parameters to be learnt
        self.bin_upper_edges = None
        self.mean_pred_values = None
        self.num_calibration_examples_in_bin = None

        ### Internal variables
        self.fitted = False

    def fit(self, y_score, y):
        assert self.n_bins is not None, "Number of bins has to be specified"
        y_score = y_score.squeeze()
        y = y.squeeze()
        assert y_score.size == y.size, "Check dimensions of input matrices"
        assert (
            y.size >= self.n_bins
        ), "Number of bins should be less than the number of calibration points"

        ### All required (hyper-)parameters have been passed correctly
        ### Uniform-mass binning/histogram binning code starts below

        # delta-randomization
        y_score = _nudge(y_score, self.delta)

        # compute uniform-mass-bins using calibration data
        self.bin_upper_edges = _get_uniform_mass_bins(y_score, self.n_bins)

        # assign calibration data to bins
        bin_assignment = _bin_points(y_score, self.bin_upper_edges)

        # compute bias of each bin
        self.num_calibration_examples_in_bin = np.zeros([self.n_bins, 1])
        self.mean_pred_values = np.empty(self.n_bins)
        for i in range(self.n_bins):
            bin_idx = bin_assignment == i
            self.num_calibration_examples_in_bin[i] = sum(bin_idx)

            # nudge performs delta-randomization
            if sum(bin_idx) > 0:
                self.mean_pred_values[i] = _nudge(y[bin_idx].mean(), self.delta)
            else:
                self.mean_pred_values[i] = _nudge(0.5, self.delta)

        # check that my code is correct
        assert np.sum(self.num_calibration_examples_in_bin) == y.size

        # histogram binning done
        self.fitted = True
        return self

    def predict_proba(self, y_score):
        assert self.fitted is True, "Call HB_binary.fit() first"
        y_score = y_score.squeeze()

        # delta-randomization
        y_score = _nudge(y_score, self.delta)

        # assign test data to bins
        y_bins = _bin_points(y_score, self.bin_upper_edges)

        # get calibrated predicted probabilities
        y_pred_prob = self.mean_pred_values[y_bins]
        return y_pred_prob


# HB_toplabel is removed as it has no use in our tasks


class HistogramBinning(PostHocCalibrator):
    """Histogram Binning

    Histogram binning amounts to creating bins and computing the accuracy
    for each bin using the calibration dataset, and then predicting such
    at test time. For multilabel/binary/multiclass classification tasks,
    we calibrate each class independently following [1]. Users could choose
    to renormalize the probability scores for multiclass tasks so they sum
    to 1.


    Paper:

        [1] Gupta, Chirag, and Aaditya Ramdas.
        "Top-label calibration and multiclass-to-binary reductions."
        ICLR 2022.

        [2] Zadrozny, Bianca, and Charles Elkan.
        "Learning and making decisions when costs and probabilities are both unknown."
        In Proceedings of the seventh ACM SIGKDD international conference on Knowledge
        discovery and data mining, pp. 204-213. 2001.

    :param model: A trained base model.
    :type model: BaseModel

    Examples:
        >>> from pyhealth.datasets import ISRUCDataset, get_dataloader, split_by_patient
        >>> from pyhealth.models import SparcNet
        >>> from pyhealth.tasks import sleep_staging_isruc_fn
        >>> from pyhealth.calib.calibration import HistogramBinning
        >>> sleep_ds = ISRUCDataset("/srv/scratch1/data/ISRUC-I").set_task(sleep_staging_isruc_fn)
        >>> train_data, val_data, test_data = split_by_patient(sleep_ds, [0.6, 0.2, 0.2])
        >>> model = SparcNet(dataset=sleep_ds, feature_keys=["signal"],
        ...     label_key="label", mode="multiclass")
        >>> # ... Train the model here ...
        >>> # Calibrate
        >>> cal_model = HistogramBinning(model)
        >>> cal_model.calibrate(cal_dataset=val_data)
        >>> # Evaluate
        >>> from pyhealth.trainer import Trainer
        >>> test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
        >>> print(Trainer(model=cal_model, metrics=['cwECEt_adapt', 'accuracy']).evaluate(test_dl))
        {'accuracy': 0.7189072348464207, 'cwECEt_adapt': 0.04455814993598299}
    """

    def __init__(self, model: BaseModel, debug=False, **kwargs) -> None:
        super().__init__(model, **kwargs)
        self.mode = self.model.mode
        for param in model.parameters():
            param.requires_grad = False

        self.model.eval()
        self.device = model.device
        self.debug = debug

        self.num_classes = None
        self.calib = None

    def calibrate(self, cal_dataset: Subset, nbins: int = 15):
        """Calibrate the base model using a calibration dataset.

        :param cal_dataset: Calibration set.
        :type cal_dataset: Subset
        :param nbins: number of bins to use, defaults to 15
        :type nbins: int, optional
        """
        _cal_data = prepare_numpy_dataset(
            self.model, cal_dataset, ["y_true", "y_prob"], debug=self.debug
        )
        if self.num_classes is None:
            self.num_classes = _cal_data["y_prob"].shape[1]
        if self.mode == "binary":
            self.calib = [
                HB_binary(nbins).fit(
                    _cal_data["y_prob"][:, 0], _cal_data["y_true"][:, 0]
                )
            ]
        else:
            self.calib = []
            y_true = _cal_data["y_true"]
            if len(y_true.shape) == 1:
                y_true = one_hot_np(y_true, self.num_classes)
            for k in range(self.num_classes):
                self.calib.append(
                    HB_binary().fit(_cal_data["y_prob"][:, k], y_true[:, k])
                )

    def forward(self, normalization="sum", **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation (just like the original model).

        :param normalization: how to normalize the calibrated probability.
            Defaults to 'sum' (and only 'sum' is supported for now).
        :type normalization: str, optional
        :param **kwargs: Additional arguments to the base model.

        :return:  A dictionary with all results from the base model, with the following modified:

            ``y_prob``: calibrated predicted probabilities.
            ``loss``: Cross entropy loss  with the new y_prob.
        :rtype: Dict[str, torch.Tensor]
        """
        assert normalization is None or normalization == "sum"
        ret = self.model(**kwargs)
        y_prob = ret["y_prob"].cpu().numpy()
        for k in range(self.num_classes):
            y_prob[:, k] = self.calib[k].predict_proba(y_prob[:, k])
        if normalization == "sum" and self.mode == "multiclass":
            y_prob = y_prob / y_prob.sum(1)[:, np.newaxis]
        ret["y_prob"] = torch.tensor(y_prob, dtype=torch.float, device=self.device)
        if self.mode == "multiclass":
            criterion = LogLoss()
        else:
            criterion = torch.nn.functional.binary_cross_entropy
        ret["loss"] = criterion(ret["y_prob"], ret["y_true"])
        ret.pop("logit")
        return ret


if __name__ == "__main__":
    from pyhealth.datasets import ISRUCDataset, get_dataloader, split_by_patient
    from pyhealth.models import SparcNet
    from pyhealth.tasks import sleep_staging_isruc_fn
    from pyhealth.calib.calibration import HistogramBinning

    sleep_ds = ISRUCDataset(
        root="/srv/local/data/trash/",
        dev=True,
    ).set_task(sleep_staging_isruc_fn)
    train_data, val_data, test_data = split_by_patient(sleep_ds, [0.6, 0.2, 0.2])
    model = SparcNet(
        dataset=sleep_ds,
        feature_keys=["signal"],
        label_key="label",
        mode="multiclass",
    )
    # ... Train the model here ...
    # Calibrate
    cal_model = HistogramBinning(model)
    cal_model.calibrate(cal_dataset=val_data)
    # Evaluate
    from pyhealth.trainer import Trainer

    test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
    print(
        Trainer(model=cal_model, metrics=["cwECEt_adapt", "accuracy"]).evaluate(test_dl)
    )


Here is the code content for temperature_scale.py:
"""
Temperature/Platt Scaling.

Implementation based on https://github.com/gpleiss/temperature_scaling
"""

from typing import Dict

import torch
from torch import optim
from torch.utils.data import Subset

from pyhealth.calib.base_classes import PostHocCalibrator
from pyhealth.calib.utils import prepare_numpy_dataset
from pyhealth.models import BaseModel

__all__ = ["TemperatureScaling"]


class TemperatureScaling(PostHocCalibrator):
    """Temperature Scaling

    Temprature scaling refers to scaling the logits by a "temprature" tuned
    on the calibration set. For binary classification tasks, this amounts to
    Platt scaling. For multilabel classification, users can use one temperature
    for all classes, or one for each. For multiclass classification, this is
    a *confidence* calibration method: It tries to calibrate the predicted
    class' predicted probability.


    Paper:

        [1] Guo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger.
        "On calibration of modern neural networks." ICML 2017.

        [2] Platt, John.
        "Probabilistic outputs for support vector machines and
        comparisons to regularized likelihood methods."
        Advances in large margin classifiers 10, no. 3 (1999): 61-74.

    :param model: A trained base model.
    :type model: BaseModel

    Examples:
        >>> from pyhealth.datasets import ISRUCDataset, get_dataloader, split_by_patient
        >>> from pyhealth.models import SparcNet
        >>> from pyhealth.tasks import sleep_staging_isruc_fn
        >>> from pyhealth.calib.calibration import TemperatureScaling
        >>> sleep_ds = ISRUCDataset("/srv/scratch1/data/ISRUC-I").set_task(sleep_staging_isruc_fn)
        >>> train_data, val_data, test_data = split_by_patient(sleep_ds, [0.6, 0.2, 0.2])
        >>> model = SparcNet(dataset=sleep_ds, feature_keys=["signal"],
        ...     label_key="label", mode="multiclass")
        >>> # ... Train the model here ...
        >>> # Calibrate
        >>> cal_model = TemperatureScaling(model)
        >>> cal_model.calibrate(cal_dataset=val_data)
        >>> # Evaluate
        >>> from pyhealth.trainer import Trainer
        >>> test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
        >>> print(Trainer(model=cal_model, metrics=['cwECEt_adapt', 'accuracy']).evaluate(test_dl))
        {'accuracy': 0.709843241966832, 'cwECEt_adapt': 0.051673596521491505}
    """

    def __init__(self, model: BaseModel, debug=False, **kwargs) -> None:
        super().__init__(model, **kwargs)
        self.mode = self.model.mode
        for param in model.parameters():
            param.requires_grad = False

        self.model.eval()
        self.device = model.device
        self.debug = debug

        self.num_classes = None

        self.temperature = torch.nn.Parameter(
            torch.tensor(1.5, dtype=torch.float32, device=self.device), requires_grad=True
        )

    def calibrate(self, cal_dataset: Subset, lr=0.01, max_iter=50, mult_temp=False):
        """Calibrate the base model using a calibration dataset.

        :param cal_dataset: Calibration set.
        :type cal_dataset: Subset
        :param lr: learning rate, defaults to 0.01
        :type lr: float, optional
        :param max_iter: maximum iterations, defaults to 50
        :type max_iter: int, optional
        :param mult_temp: if mult_temp and mode='multilabel',
            defaults to False
        :type mult_temp: bool, optional
        :return: None
        :rtype: None
        """

        _cal_data = prepare_numpy_dataset(
            self.model, cal_dataset, ["y_true", "logit"], debug=self.debug
        )

        if self.num_classes is None:
            self.num_classes = _cal_data["logit"].shape[1]
        if self.mode == "multilabel" and mult_temp:
            self.temperature = torch.tensor(
                [1.5 for _ in range(self.num_classes)],
                dtype=torch.float32,
                device=self.device,
                requires_grad=True,
            )
        optimizer = optim.LBFGS([self.temperature], lr=lr, max_iter=max_iter)
        criterion = self.model.get_loss_function()
        logits = torch.tensor(_cal_data["logit"], dtype=torch.float, device=self.device)
        label = torch.tensor(
            _cal_data["y_true"],
            dtype=torch.long if self.model.mode == "multiclass" else torch.float32,
            device=self.device,
        )

        def _eval():
            optimizer.zero_grad()
            loss = criterion(logits / self.temperature, label)
            loss.backward()
            return loss

        self.train()
        optimizer.step(_eval)
        self.eval()

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation (just like the original model).

        :param **kwargs: Additional arguments to the base model.

        :return:  A dictionary with all results from the base model, with the following modified:

            ``y_prob``: calibrated predicted probabilities.
            ``loss``: Cross entropy loss with the new y_prob.
            ``logit``: temperature-scaled logits.
        :rtype: Dict[str, torch.Tensor]
        """
        ret = self.model(**kwargs)
        ret["logit"] = ret["logit"] / self.temperature
        ret["y_prob"] = self.model.prepare_y_prob(ret["logit"])
        criterion = self.model.get_loss_function()
        ret["loss"] = criterion(ret["logit"], ret["y_true"])
        return ret


if __name__ == "__main__":
    from pyhealth.calib.calibration import TemperatureScaling
    from pyhealth.datasets import (ISRUCDataset, get_dataloader,
                                   split_by_patient)
    from pyhealth.models import SparcNet
    from pyhealth.tasks import sleep_staging_isruc_fn

    sleep_ds = ISRUCDataset(
        root="/srv/local/data/trash/",
        dev=True,
    ).set_task(sleep_staging_isruc_fn)
    train_data, val_data, test_data = split_by_patient(sleep_ds, [0.6, 0.2, 0.2])
    model = SparcNet(
        dataset=sleep_ds,
        feature_keys=["signal"],
        label_key="label",
        mode="multiclass",
    )
    # ... Train the model here ...
    # Calibrate
    cal_model = TemperatureScaling(model)
    cal_model.calibrate(cal_dataset=val_data)
    # Evaluate
    from pyhealth.trainer import Trainer

    test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
    print(
        Trainer(model=cal_model, metrics=["cwECEt_adapt", "accuracy"]).evaluate(test_dl)
    )


Here is the code content for __init__.py:
"""KCal: Kernel-based Calibration

Implementation based on https://github.com/zlin7/KCal.

Paper:

    Lin, Zhen, Shubhendu Trivedi, and Jimeng Sun.
    "Taking a Step Back with KCal: Multi-Class Kernel-Based Calibration
    for Deep Neural Networks." ICLR 2023.


"""
from typing import Dict

import torch
from torch.utils.data import DataLoader, Subset

from pyhealth.calib.base_classes import PostHocCalibrator
from pyhealth.calib.utils import prepare_numpy_dataset
from pyhealth.trainer import Trainer

from .bw import fit_bandwidth
from .embed_data import _EmbedData
from .kde import KDE_classification, KDECrossEntropyLoss, RBFKernelMean

__all__ = ["KCal"]


class ProjectionWrap(torch.nn.Module):
    """Base class for reprojections."""

    def __init__(self) -> None:
        super().__init__()
        self.criterion = KDECrossEntropyLoss()
        self.mode = "multiclass"

    def embed(self, x):
        """The actual projection"""
        raise NotImplementedError()

    def _forward(self, data, target=None, device=None):
        device = device or self.fc.weight.device

        data["supp_embed"] = self.embed(data["supp_embed"].to(device))
        data["supp_target"] = data["supp_target"].to(device)
        if target is None:
            # no supp vs pred - LOO prediction (for eval)
            assert "pred_embed" not in data
            data["pred_embed"] = None
            target = data["supp_target"]
            assert not self.training
        else:
            # used for train
            data["pred_embed"] = self.embed(data["pred_embed"].to(device))
            if "weights" in data and isinstance(data["weights"], torch.Tensor):
                data["weights"] = data["weights"].to(device)
        loss = self.criterion(
            data, target.to(device), eval_only=data["pred_embed"] is None
        )
        return {
            "loss": loss["loss"],
            "y_prob": loss["extra_output"]["prediction"],
            "y_true": target,
        }


class Identity(ProjectionWrap):
    """The identity reprojection (no reprojection)."""

    def embed(self, x):
        return x

    def forward(self, data, target=None):
        """Foward operations"""
        return self._forward(data, target, data["supp_embed"].device)


class SkipELU(ProjectionWrap):
    """The default reprojection module with 2 layers and a skip connection."""

    def __init__(self, input_features, output_features):
        super().__init__()
        self.bn = torch.nn.BatchNorm1d(input_features)
        self.mid = torch.nn.Linear(input_features, output_features)
        self.bn2 = torch.nn.BatchNorm1d(output_features)
        self.fc = torch.nn.Linear(output_features, output_features, bias=False)
        self.act = torch.nn.ELU()

    def embed(self, x):
        x = self.mid(self.bn(x))
        ret = self.fc(self.act(x))
        return ret + x

    def forward(self, data, target=None):
        """Foward operations"""
        return self._forward(data, target, self.fc.weight.device)


def _embed_dataset(model, dataset, record_id_name=None, debug=False, batch_size=32):

    ret = prepare_numpy_dataset(
        model,
        dataset,
        ["y_true", "embed"],
        incl_data_keys=["patient_id"]
        + ([] if record_id_name is None else [record_id_name]),
        forward_kwargs={"embed": True},
        debug=debug,
        batch_size=batch_size,
    )
    return {
        "labels": ret["y_true"],
        "indices": ret.get(record_id_name, None),
        "embed": ret["embed"],
        "group": ret["patient_id"],
    }


class KCal(PostHocCalibrator):
    """Kernel-based Calibration.
    This is a *full* calibration method for *multiclass* classification.
    It tries to calibrate the predicted probabilities for all classes,
    by using KDE classifiers estimated from the calibration set.

    Paper:

        Lin, Zhen, Shubhendu Trivedi, and Jimeng Sun.
        "Taking a Step Back with KCal: Multi-Class Kernel-Based Calibration
        for Deep Neural Networks." ICLR 2023.

    Args:
        model (BaseModel): A trained model.

    Examples:
        >>> from pyhealth.datasets import ISRUCDataset, split_by_patient, get_dataloader
        >>> from pyhealth.models import SparcNet
        >>> from pyhealth.tasks import sleep_staging_isruc_fn
        >>> from pyhealth.calib.calibration import KCal
        >>> sleep_ds = ISRUCDataset("/srv/scratch1/data/ISRUC-I").set_task(sleep_staging_isruc_fn)
        >>> train_data, val_data, test_data = split_by_patient(sleep_ds, [0.6, 0.2, 0.2])
        >>> model = SparcNet(dataset=sleep_ds, feature_keys=["signal"],
        ...     label_key="label", mode="multiclass")
        >>> # ... Train the model here ...
        >>> # Calibrate
        >>> cal_model = KCal(model)
        >>> cal_model.calibrate(cal_dataset=val_data)
        >>> # Alternatively, you could re-fit the reprojection:
        >>> # cal_model.calibrate(cal_dataset=val_data, train_dataset=train_data)
        >>> # Evaluate
        >>> from pyhealth.trainer import Trainer
        >>> test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
        >>> print(Trainer(model=cal_model, metrics=['cwECEt_adapt', 'accuracy']).evaluate(test_dl))
        {'accuracy': 0.7303689172252193, 'cwECEt_adapt': 0.03324275630220515}
    """

    def __init__(self, model: torch.nn.Module, debug=False, **kwargs) -> None:
        super().__init__(model, **kwargs)
        if model.mode != "multiclass":
            raise NotImplementedError()
        self.mode = self.model.mode  # multiclass
        self.model.eval()

        self.device = model.device
        self.debug = debug

        self.proj = Identity()
        self.kern = RBFKernelMean()
        self.record_id_name = None
        self.cal_data = {}
        self.num_classes = None

    def fit(
        self,
        train_dataset,
        val_dataset=None,
        split_by_patient=False,
        dim=32,
        bs_pred=64,
        bs_supp=20,
        epoch_len=5000,
        epochs=10,
        load_best_model_at_last=False,
        **train_kwargs
    ):
        """Fit the reprojection module.
        You don't need to call this function - it is called in :func:`KCal.calibrate`.
        For training details, please refer to the paper.

        Args:
            train_dataset (Dataset):
                The training dataset.
            val_dataset (Dataset, optional):
                The validation dataset. Defaults to None.
            split_by_patient (bool, optional):
                Whether to split the dataset by patient during training. Defaults to False.
            dim (int, optional):
                The dimension of the embedding. Defaults to 32.
            bs_pred (int, optional):
                The batch size for the prediction set. Defaults to 64.
            bs_supp (int, optional):
                The batch size for the support set. Defaults to 20.
            epoch_len (int, optional):
                The number of batches in an epoch. Defaults to 5000.
            epochs (int, optional):
                The number of epochs. Defaults to 10.
            load_best_model_at_last (bool, optional):
                Whether to load the best model (or the last model). Defaults to False.
            **train_kwargs:
                Other keyword arguments for :func:`pyhealth.trainer.Trainer.train`.
        """

        _train_data = _embed_dataset(
            self.model, train_dataset, self.record_id_name, self.debug
        )

        self.num_classes = max(_train_data["labels"]) + 1
        if not split_by_patient:
            # Allow using other samples from the same patient to make the prediction
            _train_data.pop("group")
        _train_data = _EmbedData(
            bs_pred=bs_pred, bs_supp=bs_supp, epoch_len=epoch_len, **_train_data
        )
        train_loader = DataLoader(
            _train_data, batch_size=1, collate_fn=_EmbedData._collate_func
        )

        val_loader = None
        if val_dataset is not None:
            _val_data = _embed_dataset(
                self.model, val_dataset, self.record_id_name, self.debug
            )
            _val_data = _EmbedData(epoch_len=1, **_val_data)
            val_loader = DataLoader(
                _val_data, batch_size=1, collate_fn=_EmbedData._collate_func
            )

        self.proj = SkipELU(len(_train_data.embed[0]), dim).to(self.device)
        trainer = Trainer(model=self.proj)
        trainer.train(
            train_dataloader=train_loader,
            val_dataloader=val_loader,
            epochs=epochs,
            monitor="loss",
            monitor_criterion="min",
            load_best_model_at_last=load_best_model_at_last,
            **train_kwargs
        )
        self.proj.eval()

    def calibrate(
        self,
        cal_dataset: Subset,
        num_fold=20,
        record_id_name=None,
        train_dataset: Subset = None,
        train_split_by_patient=False,
        load_best_model_at_last=True,
        **train_kwargs
    ):
        """Calibrate using a calibration dataset. If ``train_dataset`` is not None,
        it will be used to fit a re-projection from the base model embeddings.
        In either case, the calibration set will be used to construct the KDE classifier.

        Args:
            cal_dataset (Subset): Calibration set.
            record_id_name (str, optional): the key/name of the unique index for records.
                Defaults to None.
            train_dataset (Subset, optional): Dataset to train the reprojection.
                Defaults to None (no training).
            train_split_by_patient (bool, optional):
                Whether to split by patient when training the embeddings.
                That is, do we use samples from the same patient in KDE during *training*.
                Defaults to False.
            load_best_model_at_last (bool, optional):
                Whether to load the best reprojection basing on the calibration set.
                Defaults to True.
            train_kwargs (dict, optional): Additional arguments for training the reprojection.
                Passed to  :func:`KCal.fit`
        """

        self.record_id_name = record_id_name
        if train_dataset is not None:
            self.fit(
                train_dataset,
                val_dataset=cal_dataset,
                split_by_patient=train_split_by_patient,
                load_best_model_at_last=load_best_model_at_last,
                **train_kwargs
            )
        else:
            print(
                "No `train_dataset` - using the raw embeddings from the base classifier."
            )

        _cal_data = _embed_dataset(
            self.model, cal_dataset, self.record_id_name, self.debug
        )
        if self.num_classes is None:
            self.num_classes = max(_cal_data["labels"]) + 1
        assert (
            self.num_classes == max(_cal_data["labels"]) + 1
        ), "Train/Calibration data seem to have different classes"
        self.cal_data["Y"] = torch.tensor(
            _cal_data["labels"], dtype=torch.long, device=self.device
        )
        self.cal_data["Y"] = torch.nn.functional.one_hot(
            self.cal_data["Y"], self.num_classes
        ).float()
        with torch.no_grad():
            self.cal_data["X"] = self.proj.embed(
                torch.tensor(_cal_data["embed"], dtype=torch.float, device=self.device)
            )

        # Choose bandwidth
        self.kern.set_bandwidth(
            fit_bandwidth(group=_cal_data["group"], num_fold=num_fold, **self.cal_data)
        )

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation (just like the original model).

        :param **kwargs: Additional arguments to the base model.

        :return:  A dictionary with all results from the base model, with the following modified:

            ``y_prob``: calibrated predicted probabilities.
            ``loss``: Cross entropy loss  with the new y_prob.
        :rtype: Dict[str, torch.Tensor]
        """
        ret = self.model(embed=True, **kwargs)
        X_pred = self.proj.embed(ret.pop("embed"))
        ret["y_prob"] = KDE_classification(
            kern=self.kern, X_pred=X_pred, **self.cal_data
        )
        ret["loss"] = self.proj.criterion.log_loss(ret["y_prob"], ret["y_true"])
        return ret


if __name__ == "__main__":

    from pyhealth.calib.calibration import KCal
    from pyhealth.datasets import (ISRUCDataset, get_dataloader,
                                   split_by_patient)
    from pyhealth.models import SparcNet
    from pyhealth.tasks import sleep_staging_isruc_fn

    sleep_ds = ISRUCDataset(
        root="/srv/local/data/trash/",
        dev=True,
    ).set_task(sleep_staging_isruc_fn)
    train_data, val_data, test_data = split_by_patient(sleep_ds, [0.6, 0.2, 0.2])
    model = SparcNet(
        dataset=sleep_ds, feature_keys=["signal"], label_key="label", mode="multiclass"
    )
    # ... Train the model here ...
    # Calibrate
    cal_model = KCal(model)
    cal_model.calibrate(cal_dataset=val_data)
    # Evaluate
    from pyhealth.trainer import Trainer

    test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
    print(
        Trainer(model=cal_model, metrics=["cwECEt_adapt", "accuracy"]).evaluate(test_dl)
    )


Here is the code content for bw.py:
"""Bandwidth selection."""
import numpy as np
import torch
import tqdm
from sklearn.model_selection import GroupKFold, KFold

from pyhealth.calib.utils import LogLoss

from .kde import KDE_classification, RBFKernelMean


class GoldenSectionBoundedSearch():
    """An implementation of https://en.wikipedia.org/wiki/Golden-section_search
    """
    gr = (1 + (5 ** 0.5)) * 0.5
    def __init__(self, func, lb, ub, tol=1e-4):
        self.func = func
        self.lb = lb
        self.ub = ub
        self.mem = {}
        self.hist = []
        self.tol = tol

        self.round_digit = -int(np.floor(np.log10(tol/2)))
        self._search(lb, ub)


    def eval(self, x):
        """Cached evaluation of the target function.
        """
        assert x >= self.lb and x <= self.ub
        x = np.round(x, self.round_digit)
        v = self.mem.get(x, None)
        if v is None:
            v = self.func(x)
            self.mem[x] = v
            self.hist.append(x)
        return v

    def _search(self, a, b):
        c = b - (b - a) / self.gr
        d = a + (b - a) / self.gr
        steps = int(np.ceil(np.log((b-a)/self.tol)/np.log(self.gr)))
        with tqdm.tqdm(total=steps) as pbar:
            while abs(b - a) > self.tol:
                lc = self.eval(c)
                ld = self.eval(d)
                if lc < ld:
                    b = d
                    ss_repr = f'h={c:5f} Loss:{lc:3f}'
                else:
                    a = c
                    ss_repr = f'h={d:5f} Loss:{ld:3f}'
                c = b - (b - a) / self.gr
                d = a + (b - a) / self.gr
                pbar.update(1)
                pbar.set_description(ss_repr)
        return (b + a) / 2.

    @classmethod
    def search(cls, func, lb, ub, tol=1e-4):
        """Wrapper of the main search function.
        """
        o = GoldenSectionBoundedSearch(func, lb, ub, tol)
        return o._search(lb, ub), o


def fit_bandwidth(X:torch.Tensor, Y:torch.Tensor, group=None, *,
                 kern:RBFKernelMean=None,
                 num_fold=np.inf, lb:float=1e-1, ub:float=1e1, seed=42) -> float:
    """Use k-fold cross-validation to find the best bandwidth.

    Args:
        X (torch.Tensor): embeddings
        Y (torch.Tensor): one-hot target
        groups (List[str,int,...], optional): groups to observe during the cross-validation.
            Defaults to None.
        kern (RBFKernelMean, optional): kernel object.
            Defaults to None.
        num_fold (int, optional): number of folds for cross-validation.
            Defaults to np.inf, which means leave-one-out cross-validation.
        lb (float, optional): lower-bound of the search range. Defaults to 1e-1.
        ub (float, optional): upper-bound of the search range. Defaults to 1e1.
        seed (int, optional): random seed. Defaults to 42.

    Returns:
        h (float): the optimal bandwidth found.
    """
    loss_func = LogLoss(reduction='mean')
    if kern is None:
        kern = RBFKernelMean()
    base_h = kern.get_bandwidth()

    num_fold = min(num_fold, len(Y))
    if group is not None:
        num_fold = min(num_fold, len(set(group)))
    print(f"Using {num_fold}-folds")

    def eval_loss(h):
        kern.set_bandwidth(h)
        if group is None:
            kf = KFold(n_splits=num_fold, random_state=seed, shuffle=True)
        else:
            kf = GroupKFold(n_splits=num_fold)
        preds, y_true = [], []
        for _supp, _pred in kf.split(Y, Y, group):
            preds.append(KDE_classification(X[_supp], Y[_supp], kern, X[_pred]))
            y_true.append(Y[_pred].argmax(1))
        return loss_func(torch.cat(preds, 0), torch.cat(y_true, 0)).item()
    h, _ = GoldenSectionBoundedSearch.search(eval_loss, lb * base_h, ub * base_h)
    kern.set_bandwidth(base_h)
    return h


Here is the code content for embed_data.py:
import numpy as np
import pandas as pd
import torch
from scipy.stats import multinomial
from torch.utils.data import Dataset


class _IndexSampler:
    def __init__(self, labels, seed=None, group:np.ndarray=None):
        if not isinstance(labels, pd.Series):
            labels = pd.Series(labels, index=np.arange(len(labels)))
        self.labels = labels
        self.index_by_class = {}
        self.class_weights = {}
        for k, tser in self.labels.groupby(self.labels):
            self.index_by_class[k] = tser.index
            self.class_weights[k] = len(tser)
        self.class_weights = pd.Series(self.class_weights) / pd.Series(self.class_weights).sum()
        self.group = group


        self.rs = np.random.RandomState(seed)

    def _sample_class_cnts(self, batch_size):
        
        ret = multinomial.rvs(batch_size, self.class_weights)
        return ret

    def sample(self, batch_size_pred, batch_size):
        weights = 1.
        if self.group is None:
            pred_indices = self.rs.choice(self.labels.index, batch_size_pred, replace=False)
            avoid_supp_indices = pred_indices
        else:
            _pred_group = self.rs.choice(self.group)
            avoid_supp_indices = self.labels.index[self.group == _pred_group]
            pred_indices = self.rs.choice(avoid_supp_indices, batch_size_pred, replace=False)

        # SAMPLE_MODSTRATIFY
        cnt, indices= {}, {}
        weights = []
        for k, all_idx_class_k in self.index_by_class.items():
            all_idx_class_k = all_idx_class_k.difference(avoid_supp_indices)
            indices[k] = _safe_random_choice(all_idx_class_k, batch_size, replace=False, rs=self.rs)
            cnt[k] = (len(indices[k]), len(all_idx_class_k))
            weights.extend([len(all_idx_class_k)/float(len(indices[k]))] * len(indices[k]))
        indices = np.concatenate(list(indices.values()))
        weights = np.asarray(weights)
        pred_indices = pd.Index(pred_indices)
        return pred_indices, indices, cnt, weights

class _EmbedData(Dataset):
    def __init__(self, labels:np.ndarray, embed:np.ndarray, epoch_len=5000, 
                 bs_pred=64, bs_supp=20, group=None, indices=None) -> None:
        self.num_classes = labels.max() + 1

        self.labels, self.indices = labels, indices
        self.embed = embed

        if group is not None:
            group = np.asarray(group)
            if len(set(group)) == 1:
                group = None
        self.group = group
        self.niters_per_epoch = epoch_len
        self.index_sampler = _IndexSampler(self.labels, seed=42, group=self.group)
        self.bs_pred = bs_pred
        self.bs_supp = bs_supp

        self.use_full = epoch_len == 1
        if self.niters_per_epoch == 1 and group is not None:
            self.niters_per_epoch = len(set(group))


    def __len__(self):
        return self.niters_per_epoch

    def __getitem__(self, index):
        if self.use_full:
            if self.group is None or self.niters_per_epoch == 1:
                return {'data': {"supp_embed": torch.tensor(self.embed, dtype=torch.float),
                'supp_target': torch.tensor(self.labels, dtype=torch.long)}}
            unique_groups = sorted(set(self.group))
            full_indices = np.arange(len(self.labels))
            pred_indices = full_indices[self.group == unique_groups[index]]
            indices = full_indices[self.group != unique_groups[index]]
            weights = 1.
        else:
            pred_indices, indices, _, weights = self.index_sampler.sample(self.bs_pred, self.bs_supp)
        data = {
            'weights': torch.tensor(weights, dtype=torch.float) if isinstance(weights, np.ndarray) else weights,
            'pred_embed': torch.tensor(self.embed[pred_indices], dtype=torch.float),
            'supp_embed': torch.tensor(self.embed[indices], dtype=torch.float),
            'supp_target': torch.tensor(self.labels[indices], dtype=torch.long),
            }
        return {'data': data,  'target': torch.tensor(self.labels[pred_indices], dtype=torch.long)}

    @classmethod
    def _collate_func(cls, batch):
        assert len(batch) == 1
        return batch[0]


def _safe_random_choice(a, size, replace=True, p=None, rs=None):
    if size > len(a) and not replace:
        return a
    if rs is None:
        rs = np.random.RandomState()
    return rs.choice(a, size=size, replace=replace, p=p)
    

Here is the code content for kde.py:
from typing import Union

import torch

from pyhealth.calib.utils import LogLoss

_MAX_KERNEL_VALUE = 1.


class RBFKernelMean(torch.nn.Module):
    def __init__(self, h=1.):
        super().__init__()
        self.h = h

    def forward(self, x, x1=None):
        #if x1 is None: return 1
        _, dim = x.shape
        if x1 is None:
            x1 = x
        d = torch.pow(torch.cdist(x,x1,2), 2) / (-dim * self.h)
        return torch.exp(d)
    def set_bandwidth(self, h: Union[int, float]):
        """Set the bandwidth"""
        self.h = h
    def get_bandwidth(self):
        return self.h

def KDE_classification(X: torch.Tensor, Y: torch.Tensor, kern:RBFKernelMean=None, X_pred: torch.Tensor=None,
                       weights: Union[torch.Tensor, float, int]=1., min_eps:float=1e-10, drop_max:bool=False):
    """KDE classifier.
    We will be using X and Y to estimate the density for each of the K classes.
    kern is the kernel. X_pred is the data to predict.

    Args:
        X (torch.Tensor): Data of shape (N, d)
        Y (torch.Tensor): One-hot label of shape (N, K)
        kern (RBFKernelMean, optional): kernel.
            Defaults to None (a RBFKernelMean of bandwidth=1).
        X_pred (torch.Tensor, optional): Data to predict.
            Defaults to None, which means we will perform leave-one-out prediction.
        weights (Union[torch.Tensor, float, int], optional): Weights on each data in X.
            Defaults to 1.
        drop_max (bool, optional): Whether to ignore x where K(x_0, x) = 1.
            This typically means x is just x_0.
            Defaults to False.
            If you know there are overlap between X and X_pred, you could
            set this to True for convenience.
            Note that if X_pred=None (LOO) this is automatically set to True.

    Returns:
        torch.Tensor: Probability predictions for X_pred.
    """
    if kern is None:
        kern = RBFKernelMean(h=1.)
    #Y is a one-hot representation
    if X_pred is None:
        Kijs = kern(X, X)
        drop_max = True
    else:
        if len(X_pred.shape) == 1:
            X_pred = X_pred.unsqueeze(0)
        Kijs = kern(X_pred, X)
    if drop_max:
        Kijs = torch.where(
                Kijs < _MAX_KERNEL_VALUE-min_eps,
                Kijs,
                torch.zeros((), device=Kijs.device, dtype=Kijs.dtype)
                )
    Kijs = Kijs * weights #Kijs[:, j] *= weights[j]
    Kijs = Kijs / torch.sum(Kijs, 1, keepdim=True).clip(min_eps) #K[i,j] = K(x[i], self.X[j])
    pred = torch.matmul(Kijs, Y)
    return pred

def batched_KDE_classification(X: torch.Tensor, Y: torch.Tensor, kern=None, X_pred: torch.Tensor=None,
                       weights: Union[torch.Tensor, float, int]=1., min_eps=1e-10):
    pred = []
    batch_size=32
    drop_max = False
    if X_pred is None:
        drop_max = True
        X_pred = X
    with torch.no_grad():
        for st in range(0, len(X_pred), batch_size):
            ed = min(len(X_pred), st+batch_size)
            pred.append(KDE_classification(
                X, Y, kern, X_pred[st:ed], weights, min_eps=min_eps, drop_max=drop_max))
        return torch.concat(pred)

class KDECrossEntropyLoss(torch.nn.Module):
    reduction: str
    def __init__(self, ignore_index: int = -100,
                 reduction: str = 'mean', h: float = 1.0, nclass=None) -> None:
        super(KDECrossEntropyLoss, self).__init__()
        self.ignore_index = ignore_index
        self.reduction = reduction
        self.kern = RBFKernelMean(h=h)
        self.log_loss = LogLoss(reduction=reduction)
        self.nclass = nclass

    def forward(self, input: dict, target: torch.Tensor, eval_only=False) -> torch.Tensor:
        weights = input['weights'] if 'weights' in input else 1.
        nclass = self.nclass or max(input['supp_target'].max(), target.max()).item() + 1
        supp_Y = torch.nn.functional.one_hot(input['supp_target'], nclass).float()
        if eval_only:
            pred = batched_KDE_classification(
                input['supp_embed'], supp_Y, self.kern, weights=weights, X_pred=input['pred_embed'])
        else:
            pred = KDE_classification(
                input['supp_embed'], supp_Y, self.kern, weights=weights, X_pred=input['pred_embed'])
        ret = {}
        ret['loss'] = self.log_loss(pred, target)
        ret['extra_output'] = {"prediction": pred}
        return ret


Here is the code content for __init__.py:
"""Prediction set construction methods"""
from pyhealth.calib.predictionset.favmac import FavMac
from pyhealth.calib.predictionset.label import LABEL
from pyhealth.calib.predictionset.scrib import SCRIB

__all__ = ['LABEL', 'SCRIB', 'FavMac']


Here is the code content for label.py:
"""
LABEL: Least ambiguous set-valued classifiers with bounded error levels.

Paper:

    Sadinle, Mauricio, Jing Lei, and Larry Wasserman.
    "Least ambiguous set-valued classifiers with bounded error levels."
    Journal of the American Statistical Association 114, no. 525 (2019): 223-234.

"""

from typing import Dict, Union

import numpy as np
import torch
from torch.utils.data import Subset

from pyhealth.calib.base_classes import SetPredictor
from pyhealth.calib.utils import prepare_numpy_dataset
from pyhealth.models import BaseModel

__all__ = ["LABEL"]


def _query_quantile(scores, alpha):
    scores = np.sort(scores)
    N = len(scores)
    loc = int(np.floor(alpha * (N + 1))) - 1
    return -np.inf if loc == -1 else scores[loc]


class LABEL(SetPredictor):
    """LABEL: Least ambiguous set-valued classifiers with bounded error levels.

    This is a prediction-set constructor for multi-class classification problems.
    It controls either :math:`\\mathbb{P}\\{Y \\not \\in C(X) | Y=k\\}\\leq \\alpha_k`
    (when ``alpha`` is an array), or :math:`\\mathbb{P}\\{Y \\not \\in C(X)\\}\\leq \\alpha` (when ``alpha`` is a float).
    Here, :math:`C(X)` denotes the final prediction set.
    This is essentially a split conformal prediction method using the predicted scores.

    Paper:

        Sadinle, Mauricio, Jing Lei, and Larry Wasserman.
        "Least ambiguous set-valued classifiers with bounded error levels."
        Journal of the American Statistical Association 114, no. 525 (2019): 223-234.


    :param model: A trained base model.
    :type model: BaseModel
    :param alpha: Target mis-coverage rate(s).
    :type alpha: Union[float, np.ndarray]

    Examples:
        >>> from pyhealth.datasets import ISRUCDataset, split_by_patient, get_dataloader
        >>> from pyhealth.models import SparcNet
        >>> from pyhealth.tasks import sleep_staging_isruc_fn
        >>> from pyhealth.calib.predictionset import LABEL
        >>> sleep_ds = ISRUCDataset("/srv/scratch1/data/ISRUC-I").set_task(sleep_staging_isruc_fn)
        >>> train_data, val_data, test_data = split_by_patient(sleep_ds, [0.6, 0.2, 0.2])
        >>> model = SparcNet(dataset=sleep_ds, feature_keys=["signal"],
        ...     label_key="label", mode="multiclass")
        >>> # ... Train the model here ...
        >>> # Calibrate the set classifier, with different class-specific mis-coverage rates
        >>> cal_model = LABEL(model, [0.15, 0.3, 0.15, 0.15, 0.15])
        >>> # Note that we used the test set here because ISRUCDataset has relatively few
        >>> # patients, and calibration set should be different from the validation set
        >>> # if the latter is used to pick checkpoint. In general, the calibration set
        >>> # should be something exchangeable with the test set. Please refer to the paper.
        >>> cal_model.calibrate(cal_dataset=test_data)
        >>> # Evaluate
        >>> from pyhealth.trainer import Trainer, get_metrics_fn
        >>> test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
        >>> y_true_all, y_prob_all, _, extra_output = Trainer(model=cal_model).inference(test_dl, additional_outputs=['y_predset'])
        >>> print(get_metrics_fn(cal_model.mode)(
        ... y_true_all, y_prob_all, metrics=['accuracy', 'miscoverage_ps'],
        ... y_predset=extra_output['y_predset'])
        ... )
        {'accuracy': 0.709843241966832, 'miscoverage_ps': array([0.1499847 , 0.29997638, 0.14993964, 0.14994704, 0.14999252])}
    """

    def __init__(
        self, model: BaseModel, alpha: Union[float, np.ndarray], debug=False, **kwargs
    ) -> None:
        super().__init__(model, **kwargs)
        if model.mode != "multiclass":
            raise NotImplementedError()
        self.mode = self.model.mode  # multiclass
        for param in model.parameters():
            param.requires_grad = False
        self.model.eval()
        self.device = model.device
        self.debug = debug

        if not isinstance(alpha, float):
            alpha = np.asarray(alpha)
        self.alpha = alpha

        self.t = None

    def calibrate(self, cal_dataset: Subset):
        """Calibrate the thresholds used to construct the prediction set.

        :param cal_dataset: Calibration set.
        :type cal_dataset: Subset
        """
        cal_dataset = prepare_numpy_dataset(
            self.model, cal_dataset, ["y_prob", "y_true"], debug=self.debug
        )
        y_prob = cal_dataset["y_prob"]
        y_true = cal_dataset["y_true"]

        N, K = cal_dataset["y_prob"].shape
        if isinstance(self.alpha, float):
            t = _query_quantile(y_prob[np.arange(N), y_true], self.alpha)
        else:
            t = [
                _query_quantile(y_prob[y_true == k, k], self.alpha[k]) for k in range(K)
            ]
        self.t = torch.tensor(t, device=self.device)

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation (just like the original model).

        :return: A dictionary with all results from the base model, with the following updates:

                    y_predset: a bool tensor representing the prediction for each class.
        :rtype: Dict[str, torch.Tensor]
        """
        pred = self.model(**kwargs)
        pred["y_predset"] = pred["y_prob"] > self.t
        return pred


if __name__ == "__main__":

    from pyhealth.datasets import ISRUCDataset, split_by_patient, get_dataloader
    from pyhealth.models import SparcNet
    from pyhealth.tasks import sleep_staging_isruc_fn
    from pyhealth.calib.predictionset import LABEL

    sleep_ds = ISRUCDataset("/srv/local/data/trash", dev=True).set_task(
        sleep_staging_isruc_fn
    )
    train_data, val_data, test_data = split_by_patient(sleep_ds, [0.6, 0.2, 0.2])
    model = SparcNet(
        dataset=sleep_ds, feature_keys=["signal"], label_key="label", mode="multiclass"
    )
    # ... Train the model here ...
    # Calibrate the set classifier, with different class-specific mis-coverage rates
    cal_model = LABEL(model, [0.15, 0.3, 0.15, 0.15, 0.15])
    # Note that I used the test set here because ISRUCDataset has relatively few
    # patients, and calibration set should be different from the validation set
    # if the latter is used to pick checkpoint. In general, the calibration set
    # should be something exchangeable with the test set. Please refer to the paper.
    cal_model.calibrate(cal_dataset=test_data)
    # Evaluate
    from pyhealth.trainer import Trainer, get_metrics_fn

    test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
    y_true_all, y_prob_all, _, extra_output = Trainer(model=cal_model).inference(
        test_dl, additional_outputs=["y_predset"]
    )
    print(
        get_metrics_fn(cal_model.mode)(
            y_true_all,
            y_prob_all,
            metrics=["accuracy", "miscoverage_ps"],
            y_predset=extra_output["y_predset"],
        )
    )


Here is the code content for __init__.py:
"""
SCRIB: Set-classifier with Class-specific Risk Bounds

Implementation based on https://github.com/zlin7/scrib

"""

import time
from typing import Dict, Union

import numpy as np
import pandas as pd
import torch

from pyhealth.calib.base_classes import SetPredictor
from pyhealth.calib.utils import prepare_numpy_dataset
from pyhealth.models import BaseModel

from . import quicksearch as qs

OVERALL_LOSSFUNC = "overall"
CLASSPECIFIC_LOSSFUNC = "classspec"

__all__ = ["SCRIB"]


class _CoordDescent:
    def __init__(
        self,
        model_output,
        labels,
        rks,
        loss_func=OVERALL_LOSSFUNC,
        loss_kwargs=None,
        restart_n=1000,
        restart_range=0.1,
        init_range=None,
        verbose=False,
    ):
        self.N, self.K = model_output.shape

        # quantities useful for loss eval
        self.loss_name = loss_func
        if loss_kwargs is None:
            loss_kwargs = {}
        self.loss_kwargs = loss_kwargs
        if self.loss_name == OVERALL_LOSSFUNC:
            assert isinstance(rks, float)
        elif rks is not None:
            rks = np.asarray(rks)
        self.idx2rnk = np.asarray(
            pd.DataFrame(model_output).rank(ascending=True), np.int32
        )
        if np.min(self.idx2rnk) == 1:
            self.idx2rnk -= 1
        self.rnk2idx = np.asarray(np.argsort(model_output, axis=0), np.int32)
        if len(labels.shape) == 2:
            # one-hot -> class indices
            labels = np.argmax(labels, 1)
        self.labels = np.asarray(labels, np.int32)
        self.max_classes = np.argmax(model_output, 1)
        self.rks = rks

        self.model_output = model_output
        self.restart_n = restart_n
        self.restart_range = restart_range
        self.init_range = init_range or (int(np.ceil(self.N / 2)), self.N - 1)
        self.verbose = verbose

    def _search(self, ps):
        _search_fn = {
            CLASSPECIFIC_LOSSFUNC: qs.coord_desc_classspecific,
            OVERALL_LOSSFUNC: qs.coord_desc_overall,
        }[self.loss_name]
        return _search_fn(
            self.idx2rnk,
            self.rnk2idx,
            self.labels,
            self.max_classes,
            ps,
            self.rks,
            **self.loss_kwargs,
        )

    def _loss_eval(self, ps):
        _loss_fn = {
            CLASSPECIFIC_LOSSFUNC: qs.loss_classspecific,
            OVERALL_LOSSFUNC: qs.loss_overall,
        }[self.loss_name]
        return _loss_fn(
            self.idx2rnk,
            self.rnk2idx,
            self.labels,
            self.max_classes,
            ps,
            self.rks,
            **self.loss_kwargs,
        )

    def _p2t(self, p):
        # Translate ranks to thresholds
        return [self.model_output[self.rnk2idx[p[k], k], k] for k in range(self.K)]

    def _sample_new_loc(self, old_p, restart_range=0.1):
        diff = np.random.uniform(-restart_range, restart_range, self.K)
        new_p = old_p.copy()
        for k in range(self.K):
            new_p[k] = max(min(int(new_p[k] + diff[k] * self.N), self.N - 1), 0)
        return new_p

    def search_once(self, seed=7):
        def print_(s):
            if self.verbose:
                print(s)

        np.random.seed(seed)
        best_ps = np.random.randint(*self.init_range, self.K)

        st = time.time()
        best_loss, best_ps, _ = self._search(best_ps)
        ed1 = time.time()
        if self.restart_n > 0:
            keep_going = True
            while keep_going:
                keep_going = False
                curr_restart_best_loss, curr_restart_best_ps = np.inf, None

                for _ in range(self.restart_n):
                    # Restart in neighborhood
                    new_ps_ = self._sample_new_loc(best_ps, self.restart_range)
                    loss_ = self._loss_eval(new_ps_)
                    if loss_ < best_loss:
                        print_(
                            "Neighborhood has a better loc with "
                            f"loss={loss_} < {best_loss}"
                        )
                        best_loss, best_ps, _ = self._search(new_ps_)

                        keep_going = True
                        break
                    elif loss_ < curr_restart_best_loss:
                        curr_restart_best_loss, curr_restart_best_ps = loss_, new_ps_
                if not keep_going:
                    print_(
                        f"Tried {curr_restart_best_ps} vs {best_ps}, "
                        f"loss:{curr_restart_best_loss} > {best_loss}"
                    )
        ed2 = time.time()
        print_(f"{ed1-st:.3f} + {ed2-ed1:.3f} seconds")
        return self._p2t(best_ps), best_loss

    @classmethod
    def search(
        cls,
        prob: np.ndarray,
        label: np.ndarray,
        rks: Union[float, np.ndarray],
        loss_func,
        B: int = 10,
        **kwargs,
    ):
        # label is not one-hot
        best_loss, best_ts = np.inf, None
        searcher = cls(prob, label, rks, loss_func=loss_func, **kwargs)
        for seed in range(B):
            np.random.seed(seed)
            ts, _l = searcher.search_once(seed + 1)
            print(f"{seed}: loss={_l}")
            if _l < best_loss:
                best_loss, best_ts = _l, ts
        return best_ts, best_loss


class SCRIB(SetPredictor):
    """SCRIB: Set-classifier with Class-specific Risk Bounds

    This is a prediction-set constructor for multi-class classification problems.
    SCRIB tries to control class-specific risk while minimizing the ambiguity.
    To to this, it selects class-specific thresholds for the predictions, on a calibration set.


    If ``risk`` is a float (say 0.1), SCRIB controls the overall risk:
    :math:`\\mathbb{P}\\{Y \\not \\in C(X) | |C(X)| = 1\\}\\leq risk`.
    If ``risk`` is an array (say `np.asarray([0.1] * 5)`), SCRIB controls the class specific risks:
    :math:`\\mathbb{P}\\{Y \\not \\in C(X) | Y=k \\land |C(X)| = 1\\}\\leq risk_k`
    Here, :math:`C(X)` denotes the final prediction set.

    Paper:

        Lin, Zhen, Lucas Glass, M. Brandon Westover, Cao Xiao, and Jimeng Sun.
        "SCRIB: Set-classifier with Class-specific Risk Bounds for Blackbox Models."
        AAAI 2022.

    Args:
        model (BaseModel): A trained model.
        risk (Union[float, np.ndarray]): risk targets.
        loss_kwargs (dict, optional): Additional loss parameters (including hyperparameters).
            It could contain the following float/int hyperparameters:
                lk: The coefficient for the loss term associated with risk violation penalty.
                    The higher the lk, the more penalty on risk violation (likely higher ambiguity).
                fill_max: Whether to fill the class with max predicted score
                    when no class exceeds the threshold. In other words, if fill_max,
                    the null region will be filled with max-prediction class.
            Defaults to {'lk': 1e4, 'fill_max': False}
        fill_max (bool, optional): Whether to fill the empty prediction set with the max-predicted class.
            Defaults to True.


    Examples:
        >>> from pyhealth.data import ISRUCDataset, split_by_patient, get_dataloader
        >>> from pyhealth.models import SparcNet
        >>> from pyhealth.tasks import sleep_staging_isruc_fn
        >>> from pyhealth.calib.predictionset import SCRIB
        >>> from pyhealth.trainer import get_metrics_fn
        >>> sleep_ds = ISRUCDataset("/srv/scratch1/data/ISRUC-I").set_task(sleep_staging_isruc_fn)
        >>> train_data, val_data, test_data = split_by_patient(sleep_ds, [0.6, 0.2, 0.2])
        >>> model = SparcNet(dataset=sleep_ds, feature_keys=["signal"],
        ...     label_key="label", mode="multiclass")
        >>> # ... Train the model here ...
        >>> # Calibrate the set classifier, with different class-specific risk targets
        >>> cal_model = SCRIB(model, [0.2, 0.3, 0.1, 0.2, 0.1])
        >>> # Note that we used the test set here because ISRUCDataset has relatively few
        >>> # patients, and calibration set should be different from the validation set
        >>> # if the latter is used to pick checkpoint. In general, the calibration set
        >>> # should be something exchangeable with the test set. Please refer to the paper.
        >>> cal_model.calibrate(cal_dataset=test_data)
        >>> # Evaluate
        >>> from pyhealth.trainer import Trainer
        >>> test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
        >>> y_true_all, y_prob_all, _, extra_output = Trainer(model=cal_model).inference(test_dl, additional_outputs=['y_predset'])
        >>> print(get_metrics_fn(cal_model.mode)(
        ... y_true_all, y_prob_all, metrics=['accuracy', 'error_ps', 'rejection_rate'],
        ... y_predset=extra_output['y_predset'])
        ... )
        {'accuracy': 0.709843241966832, 'rejection_rate': 0.6381305287631919,
        'error_ps': array([0.32161874, 0.36654135, 0.11461734, 0.23728814, 0.14993925])}
    """

    def __init__(
        self,
        model: BaseModel,
        risk: Union[float, np.ndarray],
        loss_kwargs: dict = None,
        debug=False,
        fill_max=True,
        **kwargs,
    ) -> None:
        super().__init__(model, **kwargs)
        if model.mode != "multiclass":
            raise NotImplementedError()
        self.mode = self.model.mode  # multiclass
        for param in model.parameters():
            param.requires_grad = False
        self.model.eval()
        self.device = model.device
        self.debug = debug

        if isinstance(risk, float):
            self.loss_name = OVERALL_LOSSFUNC
        else:
            risk = np.asarray(risk)
            self.loss_name = CLASSPECIFIC_LOSSFUNC
        self.risk = risk
        if loss_kwargs is None:
            loss_kwargs = {"lk": 1e4, "fill_max": fill_max}
        self.loss_kwargs = loss_kwargs

        self.t = None

    def calibrate(self, cal_dataset):
        """Calibrate/Search for the thresholds used to construct the prediction set.

        :param cal_dataset: Calibration set.
        :type cal_dataset: Subset
        """
        cal_dataset = prepare_numpy_dataset(
            self.model, cal_dataset, ["y_prob", "y_true"], debug=self.debug
        )
        if self.loss_name == CLASSPECIFIC_LOSSFUNC:
            assert len(self.risk) == cal_dataset["y_prob"].shape[1]
        best_ts, _ = _CoordDescent.search(
            cal_dataset["y_prob"],
            cal_dataset["y_true"],
            self.risk,
            self.loss_name,
            loss_kwargs=self.loss_kwargs,
            verbose=self.debug,
        )
        self.t = torch.nn.Parameter(torch.tensor(best_ts, device=self.device))

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation (just like the original model).

        :return: A dictionary with all results from the base model, with the following updates:

                    y_predset: a bool tensor representing the prediction for each class.
        :rtype: Dict[str, torch.Tensor]
        """
        ret = self.model(**kwargs)
        ret["y_predset"] = ret["y_prob"] > self.t
        return ret


if __name__ == "__main__":
    from pyhealth.calib.predictionset import SCRIB
    from pyhealth.datasets import (ISRUCDataset, get_dataloader,
                                   split_by_patient)
    from pyhealth.models import SparcNet
    from pyhealth.tasks import sleep_staging_isruc_fn
    from pyhealth.trainer import get_metrics_fn

    sleep_ds = ISRUCDataset("/srv/local/data/trash", dev=True).set_task(
        sleep_staging_isruc_fn
    )
    train_data, val_data, test_data = split_by_patient(sleep_ds, [0.6, 0.2, 0.2])
    model = SparcNet(
        dataset=sleep_ds,
        feature_keys=["signal"],
        label_key="label",
        mode="multiclass",
    )
    # ... Train the model here ...
    # Calibrate the set classifier, with different class-specific risk targets
    cal_model = SCRIB(model, [0.2, 0.3, 0.1, 0.2, 0.1])
    # Note that I used the test set here because ISRUCDataset has relatively few
    # patients, and calibration set should be different from the validation set
    # if the latter is used to pick checkpoint. In general, the calibration set
    # should be something exchangeable with the test set. Please refer to the paper.
    cal_model.calibrate(cal_dataset=test_data)
    # Evaluate
    from pyhealth.trainer import Trainer

    test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
    y_true_all, y_prob_all, _, extra_output = Trainer(model=cal_model).inference(
        test_dl, additional_outputs=["y_predset"]
    )
    print(
        get_metrics_fn(cal_model.mode)(
            y_true_all,
            y_prob_all,
            metrics=["accuracy", "error_ps", "rejection_rate"],
            y_predset=extra_output["y_predset"],
        )
    )


Here is the code content for quicksearch.py:
"""Wrapper for the coordinate descent ("QuickSearch")"""
import numpy as np

from pyhealth.calib.utils import one_hot_np

_CYTHON_ENABLED = False
try:
    import pyximport
    pyximport.install()
    from . import quicksearch_cython as cdc
    _CYTHON_ENABLED = True
except:
    print("This is a warning of potentially slow compute. You could uncomment this line and use the Python implementation instead of Cython.")

__all__ = ['loss_overall', 'loss_classspecific',
           'coord_desc_overall', 'coord_desc_classspecific']

def _thresholding_py(ts, output):
    pred = np.asarray(output > ts, dtype=np.int32)
    return pred

def __loss_overall_helper(total_err, total_sure, alpha, N, la, lc, lcs):
    ambiguity_loss = (1. - total_sure / float(N)) ** 2
    risk = total_err / float(max(total_sure, 1))
    tempf = risk - alpha
    coverage_loss = np.power(max(tempf, 0), 2)
    coverage_loss_sim = np.power(tempf, 2)
    loss = ambiguity_loss * la + coverage_loss * lc + coverage_loss_sim * lcs
    return loss

def __loss_class_specific_complete_helper(err, sure, rs, weights, total_sure, N, la, lc, lcs):
    a_loss = 1- total_sure / N
    if sure.min() == 0: return np.inf
    tempf = err / sure
    if rs is not None:
        tempf -= rs
    if weights is None:
        c_loss = np.power(tempf.clip(0, 1), 2).sum()
        cs_loss = np.power(tempf, 2).sum()
    else:
        c_loss = np.dot(np.power(tempf.clip(0, 1), 2), weights)
        cs_loss = np.dot(np.power(tempf, 2), weights)
    return a_loss * la + c_loss * lc + cs_loss * lcs

def __update_cnts(pred, y, err, sure, inc=1):
    if pred != y:
        err[y] += inc
    sure[y] += inc
    return inc

def loss_overall_py(preds, labels, max_classes, risk, *, lk=1e4, fill_max=False):
    cnt = preds.sum(1)
    cnt1 = np.expand_dims(cnt == 1, 1)
    total_sure = sum(cnt1.squeeze(1))
    risk_indicator = labels * (1 - preds)
    total_err = (risk_indicator * cnt1).sum()
    if fill_max:
        cnt0 = cnt == 0
        total_sure += sum(cnt0)
        new_risk = max_classes != np.argmax(labels, 1)
        total_err += sum(cnt0 & new_risk)
    return __loss_overall_helper(total_err, total_sure, risk, len(labels), la=1, lc=lk, lcs=0)

def loss_class_specific_py(preds, labels, max_classes, risk, *, class_weights=False, lk=1e4, fill_max=False):
    N,K = preds.shape
    cnt = preds.sum(1)
    sure_msk = cnt == 1
    total_sure = np.sum(sure_msk)
    correct_msk = np.sum(preds * labels, 1) == 1
    sure = np.sum(labels[sure_msk], 0)
    err = np.sum(labels[sure_msk & ~correct_msk], 0)
    if fill_max:
        sure_msk = cnt == 0
        total_sure += np.sum(sure_msk)
        correct_msk = max_classes == np.argmax(labels, 1)
        sure += np.sum(labels[sure_msk], 0)
        err += np.sum(labels[sure_msk & ~correct_msk], 0)
    if isinstance(class_weights, bool):
        if class_weights:
            class_weights = np.asarray(
                np.unique(labels, return_counts=True)[1], dtype=np.float64
                ) * K / float(N)
        else:
            class_weights = None
    elif class_weights is not None:
        class_weights = np.asarray(class_weights)
    return __loss_class_specific_complete_helper(
        err, sure, risk, class_weights, total_sure, N, la=1, lc=lk, lcs=0)


def search_full_class_specific_py(mo, rnkscores_or_maxclasses, scores_idx, labels, alphas, class_weights, ps, d, *,
                                  lk=1e4, fill_max=False):
    # d is dimesion to search/descend
    N,K = mo.shape
    ps = ps.copy()
    if len(rnkscores_or_maxclasses.shape) == 2:
        ts = [(rnkscores_or_maxclasses[ps[ki], ki]) for ki in range(K)]
        max_classes = np.argmax(mo, 1)
    else:
        ts = [ps[ki] for ki in range(K)]
        max_classes = rnkscores_or_maxclasses

    preds = _thresholding_py(ts, mo)
    preds[:, d] = 1
    cnt = preds.sum(1)
    labels_onehot = one_hot_np(labels, K)
    cnt1_msk = cnt == 1
    correct_msk = cnt1_msk & (np.sum(preds * labels_onehot, 1) == 1)
    total_sure = np.sum(cnt1_msk)
    sure = np.sum(labels_onehot[cnt1_msk], 0)
    err = np.sum(labels_onehot[cnt1_msk & ~correct_msk], 0)
    if fill_max:
        cnt0_msk = cnt == 0
        cnt0_correct_msk = max_classes == labels
        total_sure += np.sum(cnt0_msk)
        sure += np.sum(labels_onehot[cnt0_msk], 0)
        err += np.sum(labels_onehot[cnt0_msk & ~cnt0_correct_msk], 0)
    _preds = np.ones(N, dtype=int) * K
    _preds[cnt1_msk] = np.argmax(preds[cnt1_msk], 1)
    cnt2_msk = np.asarray((cnt == 2) & (preds[:, d]), dtype=bool) #second mask is trivial
    preds[:, d] = 0 #this order cannot chance
    _preds[cnt2_msk] = np.argmax(preds[cnt2_msk], 1)

    best_i, best_loss = -1, np.inf
    for ijjj in range(N-1):
        i = scores_idx[ijjj, d]
        yi = labels[i]
        tint = _preds[i]
        if cnt[i] == 1 or cnt[i] == 2:
            total_sure += __update_cnts(tint, yi, err, sure, 1 if cnt[i] == 2 else -1)
            if fill_max and cnt[i] == 1:
                total_sure += __update_cnts(max_classes[i], yi, err, sure, 1)
        cnt[i] -= 1
        curr_loss = __loss_class_specific_complete_helper(
            err, sure, alphas, class_weights, total_sure, N, la=1, lc=lk, lcs=0)
        if curr_loss < best_loss:
            best_i, best_loss = ijjj, curr_loss
    return best_i, best_loss


def search_full_overall_py(mo, rnkscores_or_maxclasses, scores_idx, labels, alpha, ps, d, *,
                           lk=1e4, fill_max=False):
    N,K = mo.shape
    ps = ps.copy()
    if len(rnkscores_or_maxclasses.shape) == 2:
        ts = [(rnkscores_or_maxclasses[ps[ki], ki]) for ki in range(K)]
        max_classes = np.argmax(mo, 1)
    else:
        ts = [ps[ki] for ki in range(K)]
        max_classes = rnkscores_or_maxclasses
    preds = _thresholding_py(ts, mo)
    preds[:, d] = 1
    cnt = preds.sum(1)

    #initialize mems for ijjj=0
    total_err, total_sure = 0, 0
    for ii in range(N):
        yii = labels[ii]
        if cnt[ii] == 1:
            total_sure += 1
            total_err += 1 - preds[ii, yii]
        if cnt[ii] == 0 and fill_max:
            total_sure += 1
            if max_classes[ii] != yii:
                total_err += 1
    best_i, best_loss = -1, np.inf

    for ijjj in range(N-1):
        i = scores_idx[ijjj, d]
        yi = labels[i]
        #preds[i, d] will change from 1 to 0
        if cnt[i] == 2:#unsure -> sure
            if d == yi:#we miss this item in this case, so error increases by 1
                total_err += 1
            elif mo[i, yi] <= ts[yi]: #we miss the item
                total_err += 1
            total_sure += 1
        elif cnt[i] == 1: #Sure -> unsure. Also, this cnt has to be class d
            if d != yi:
                total_err -= 1
            total_sure -= 1
            if fill_max:
                total_sure += 1
                if max_classes[i] != yi:
                    total_err += 1
        else: #Do nothing as we have an unsure -> unsure case
            pass
        cnt[i] -= 1

        curr_loss = __loss_overall_helper(total_err, total_sure, alpha, N, la=1, lc=lk, lcs=0)

        if curr_loss < best_loss:
            best_i, best_loss = ijjj, curr_loss
    return best_i, best_loss

def coord_desc_classspecific_py(mo, rnkscores_or_maxclasses, scores_idx, labels, ps, alphas, *,
                                          class_weights=False, lk=1e4, fill_max=False):
    N, K = mo.shape
    best_loss, ps = np.inf, ps.copy()

    keep_going = True
    if isinstance(class_weights, bool):
        if class_weights:
            weights = np.sum(one_hot_np(labels, K), 0) / float(N) * K
        else:
            weights = None
    else:
        weights = class_weights

    while keep_going:
        curr_ps = np.zeros(K)
        best_ki, curr_loss = None, best_loss
        for ki in range(K):
            curr_ps[ki], temp_loss = search_full_class_specific_py(
                mo, rnkscores_or_maxclasses, scores_idx, labels, alphas, weights, ps, ki,
                lk=lk, fill_max=fill_max)
            if temp_loss < curr_loss:
                best_ki, curr_loss = ki, temp_loss
        if curr_loss < best_loss:
            ps[best_ki] = curr_ps[best_ki]
            best_loss = curr_loss
        else:
            keep_going = False
    return best_loss, ps, None


def coord_desc_overall_py(mo, rnkscores_or_maxclasses, scores_idx, labels, ps, alpha, *,
                                   lk=1e4, fill_max=False):
    N, K = mo.shape
    best_loss, ps = np.inf, ps.copy()

    keep_going = True

    while keep_going:
        curr_ps = np.zeros(K)
        best_ki, curr_loss = None, best_loss
        for ki in range(K):
            curr_ps[ki], temp_loss = search_full_overall_py(
                mo, rnkscores_or_maxclasses, scores_idx, labels, alpha, ps, ki,
                lk=lk, fill_max=fill_max)
            if temp_loss < curr_loss:
                best_ki, curr_loss = ki, temp_loss
        if curr_loss < best_loss:
            ps[best_ki] = curr_ps[best_ki]
            best_loss = curr_loss
        else:
            keep_going = False
    return best_loss, ps, None

def loss_overall(idx2rnk, rnk2idx, labels, max_classes, ps, r, *,
                 lk=1e4, fill_max=False):
    if not _CYTHON_ENABLED:
        preds = np.asarray(idx2rnk > ps, np.int32)
        return loss_overall_py(
            preds, one_hot_np(labels, idx2rnk.shape[1]), max_classes, r,
            lk=lk, fill_max=fill_max)
    idx2rnk = np.asarray(idx2rnk, np.int32)
    rnk2idx = np.asarray(rnk2idx, np.int32)
    labels = np.asarray(labels, np.int32)
    max_classes = np.asarray(max_classes, np.int32)
    ps = np.asarray(ps, np.int32)
    return cdc.loss_overall_c(
        idx2rnk, rnk2idx, labels, max_classes, ps, r,
        la=1, lc=lk, lcs=0, fill_max=fill_max)

def loss_classspecific(idx2rnk, rnk2idx, labels, max_classes, ps, rks, *,
                        class_weights=None, lk=1e4, fill_max=False):
    if not _CYTHON_ENABLED:
        preds = np.asarray(idx2rnk > ps, np.int32)
        return loss_class_specific_py(
            preds, one_hot_np(labels, idx2rnk.shape[1]), max_classes, rks,
            class_weights=class_weights, lk=lk, fill_max=fill_max)
    idx2rnk = np.asarray(idx2rnk, np.int32)
    rnk2idx = np.asarray(rnk2idx, np.int32)
    labels = np.asarray(labels, np.int32)
    max_classes = np.asarray(max_classes, np.int32)
    ps = np.asarray(ps, np.int32)
    if class_weights is not None:
        class_weights = np.asarray(class_weights, np.float64)
    if rks is not None:
        rks = np.asarray(rks, np.float64)
    return cdc.loss_class_specific_c(
        idx2rnk, rnk2idx, labels, max_classes, ps, rks,
        class_weights, la=1, lc=lk, lcs=0, fill_max=fill_max)

def coord_desc_overall(idx2rnk, rnk2idx, labels, max_classes, init_ps, r, *,
                               max_step_size=None, lk=1e4, fill_max=False):
    if not _CYTHON_ENABLED:
        assert max_step_size is None
        return coord_desc_overall_py(
            idx2rnk, max_classes, rnk2idx, labels, init_ps, r,
            lk=lk, fill_max=fill_max)
    idx2rnk = np.asarray(idx2rnk, np.int32)
    rnk2idx = np.asarray(rnk2idx, np.int32)
    labels = np.asarray(labels, np.int32)
    max_classes = np.asarray(max_classes, np.int32)
    init_ps = np.asarray(init_ps, np.int32)
    return cdc.coord_desc_overall_c(
        idx2rnk, rnk2idx, labels, max_classes, init_ps, r,
        max_step_size, la=1, lc=lk, lcs=0, fill_max=fill_max)


def coord_desc_classspecific(idx2rnk, rnk2idx, labels, max_classes, init_ps, rks, *,
                                      class_weights=None,
                                      max_step_size=None, lk=1e4, fill_max=False):
    if not _CYTHON_ENABLED:
        assert max_step_size is None
        return coord_desc_classspecific_py(
            idx2rnk, max_classes, rnk2idx, labels, init_ps, rks,
            class_weights=class_weights, lk=lk, fill_max=fill_max)
    idx2rnk = np.asarray(idx2rnk, np.int32)
    rnk2idx = np.asarray(rnk2idx, np.int32)
    labels = np.asarray(labels, np.int32)
    max_classes = np.asarray(max_classes, np.int32)
    init_ps = np.asarray(init_ps, np.int32)
    if class_weights is not None:
        class_weights = np.asarray(class_weights, np.float64)
    if rks is not None:
        rks = np.asarray(rks, np.float64)
    return cdc.coord_desc_classspecific_c(
        idx2rnk, rnk2idx, labels, max_classes, init_ps, rks,
        class_weights, max_step_size, la=1, lc=lk, lcs=0, fill_max=fill_max)


Here is the code content for __init__.py:
"""
Fast Online Value-Maximizing Prediction Sets with Conformal Cost Control (FavMac)

Implementation based on https://github.com/zlin7/FavMac

"""

from importlib import reload
import time
from typing import Dict, Union

import numpy as np
import pandas as pd
import torch

from pyhealth.calib.base_classes import SetPredictor
from pyhealth.calib.predictionset.favmac.core import FavMac_GreedyRatio
from pyhealth.calib.utils import prepare_numpy_dataset
from pyhealth.models import BaseModel

__all__ = ["FavMac"]

INTEGER_SAFE_DELTA = 0.1


class AdditiveSetFunction:
    def __init__(self, values: Union[float, np.ndarray, int], mode=None, name='unknown') -> None:
        self.name = name
        self.values = values
        assert mode is None or mode in {'util', 'cost', 'proxy'}
        self.mode = mode

    def is_additive(self):
        return True

    def __call__(self, S: np.ndarray, Y:np.ndarray=None, pred:np.ndarray=None, sample=100, target_cost=None) -> float:
        if self.mode == 'cost':
            assert pred is None
            return self.cost_call(S, Y)
        if self.mode == 'proxy':
            assert Y is None
            return self.proxy_call(S, pred, target_cost=target_cost)
        assert self.mode == 'util'
        return self.util_call(S, Y, pred, sample=sample)

    def naive_call(self, S: np.ndarray) -> float:
        return np.sum(S * self.values)

    def util_call(self, S: np.ndarray, Y:np.ndarray=None, pred:np.ndarray=None, sample=1000) -> float:
        assert Y is None or pred is None
        if pred is not None:
            return self.naive_call(S * pred) # This is because of additivity.
        if Y is not None: return self.naive_call(S * Y)
        return self.naive_call(S)

    def cost_call(self, S: np.ndarray, Y:np.ndarray) -> float:
        return self.naive_call(S * (1-Y))

    def proxy_call(self, S: np.ndarray, pred: np.ndarray, target_cost: float=None) -> float:
        return self.naive_call(S * (1-pred))

    def greedy_maximize(self, S: np.ndarray, pred: np.ndarray=None, d_proxy:np.ndarray=None, prev_util_and_proxy=None):
        # (prev_u, prev_p) = prev_util_and_proxy
        assert self.mode == 'util', "This is only used for util function"
        if (1-S).sum() == 0: return None
        d_util = self.values
        if pred is not None: d_util = d_util * pred

        objective = d_util / (1 if d_proxy is None else d_proxy.clip(1e-8))
        k = pd.Series((1-S) * objective).dropna().idxmax()
        return k, objective[k]

    def greedy_maximize_seq(self, pred: np.ndarray=None, d_proxy:np.ndarray=None):
        # if cost is also additive, then cost_proxy is fixed: weight * (1-p)
        assert self.mode == 'util', "This is only used for util function"
        d_util = self.values
        if pred is not None: d_util = d_util * pred

        objective = d_util / (1 if d_proxy is None else d_proxy.clip(1e-8))

        assert np.isnan(objective).sum() == 0
        ks = np.argsort(-objective)
        Ss = [np.zeros(len(objective), dtype=int)]
        for k in ks:
            Ss.append(Ss[-1].copy())
            Ss[-1][k] = 1
        return Ss, ks


class FavMac(SetPredictor):
    """Fast Online Value-Maximizing Prediction Sets with Conformal Cost Control (FavMac)

    This is a prediction-set constructor for multi-label classification problems.
    FavMac could control the cost/risk while realizing high value on the prediction set.

    Value and cost functions are functions in the form of :math:`V(S;Y)` or :math:`C(S;Y)`,
    with S being the prediction set and Y being the label.
    For example, a classical cost function would be "numebr of false positives".
    Denote the ``target_cost`` as
    :math:`c`,
    if ``delta=None``, FavMac controls the expected cost in the following sense:

    :math:`\\mathbb{E}[C(S_{N+1};Y_{N+1}] \\leq c`.

    Otherwise, FavMac controls the violation probability in the following sense:

    :math:`\\mathbb{P}\\{C(S_{N+1};Y_{N+1})>c\\}\\leq delta`.

    Right now, this FavMac implementation only supports additive value and cost functions (unlike the
    implementation associated with [1]).
    That is, the value function is specified by the weights ``value_weights`` and the cost function
    is specified by ``cost_weights``.
    With :math:`k` denoting classes, the cost function is then computed as

    :math:`C(S;Y,w) = \\sum_{k} (1-Y_k)S_k w_k`

    Similarly, the value function is computed as

    :math:`V(S;Y,w) = \\sum_{k} Y_k S_k w_k`.

    Papers:

        [1] Lin, Zhen, Shubhendu Trivedi, Cao Xiao, and Jimeng Sun.
        "Fast Online Value-Maximizing Prediction Sets with Conformal Cost Control (FavMac)."
        ICML 2023.

        [2] Fisch, Adam, Tal Schuster, Tommi Jaakkola, and Regina Barzilay.
        "Conformal prediction sets with limited false positives."
        ICML 2022.

    Args:
        model (BaseModel): A trained model.
        value_weights (Union[float, np.ndarray]):
            weights for the value function. See description above.
            Defaults to 1.
        cost_weights (Union[float, np.ndarray]):
            weights for the cost function. See description above.
            Defaults to 1.
        target_cost (float): Target cost.
            When cost_weights is set to 1, this is essentially the number of false positive.
            Defaults to 1.
        delta (float): Violation target (in violation control).
            Defaults to None (which means expectation control instead of violation control).

    Examples:
        >>> from pyhealth.calib.predictionset import FavMac
        >>> from pyhealth.datasets import (MIMIC3Dataset, get_dataloader,split_by_patient)
        >>> from pyhealth.models import Transformer
        >>> from pyhealth.tasks import drug_recommendation_mimic3_fn
        >>> from pyhealth.trainer import get_metrics_fn
        >>> base_dataset = MIMIC3Dataset(
        ...     root="/srv/scratch1/data/physionet.org/files/mimiciii/1.4",
        ...     tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        ...     code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
        ...     refresh_cache=False)
        >>> sample_dataset = base_dataset.set_task(drug_recommendation_mimic3_fn)
        >>> train_data, val_data, test_data = split_by_patient(sample_dataset, [0.6, 0.2, 0.2])
        >>> model = Transformer(dataset=sample_dataset, feature_keys=["conditions", "procedures"],
        ...             label_key="drugs", mode="multilabel")
        >>> # ... Train the model here ...
        >>> # Try to control false positive to <=3
        >>> cal_model = FavMac(model, target_cost=3, delta=None)
        >>> cal_model.calibrate(cal_dataset=val_data)
        >>> # Evaluate
        >>> from pyhealth.trainer import Trainer
        >>> test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
        >>> y_true_all, y_prob_all, _, extra_output = Trainer(model=cal_model).inference(
        ... test_dl, additional_outputs=["y_predset"])
        >>> print(get_metrics_fn(cal_model.mode)(
        ...     y_true_all, y_prob_all, metrics=['tp', 'fp'],
        ...     y_predset=extra_output["y_predset"])) # We get FP~=3
        {'tp': 0.5049893086243763, 'fp': 2.8442622950819674}
    """

    def __init__(
        self,
        model: BaseModel,
        value_weights: Union[float, np.ndarray] = 1.,
        cost_weights: Union[float, np.ndarray] = 1.,
        target_cost: float = 1., delta:float = None,
        debug=False,
        **kwargs,
    ) -> None:
        super().__init__(model, **kwargs)
        if model.mode != "multilabel":
            raise NotImplementedError()
        self.mode = self.model.mode  # multilabel
        for param in model.parameters():
            param.requires_grad = False
        self.model.eval()
        self.device = model.device
        self.debug = debug


        self._cost_weights = cost_weights
        self._value_weights = value_weights
        self.target_cost = target_cost
        self.delta = delta

    def calibrate(self, cal_dataset):
        """Calibrate the cost-control procedure.

        :param cal_dataset: Calibration set.
        :type cal_dataset: Subset
        """
        _cal_data = prepare_numpy_dataset(
            self.model, cal_dataset, ["logit", "y_true"], debug=self.debug
        )

        if isinstance(self._cost_weights, np.ndarray):
            C_max = self._cost_weights.sum()
        else:
            C_max = _cal_data["logit"].shape[1] * self._cost_weights
        self._favmac = FavMac_GreedyRatio(
            cost_fn=AdditiveSetFunction(self._cost_weights / C_max, mode='cost'),
            util_fn=AdditiveSetFunction(self._value_weights, mode='util'),
            proxy_fn=AdditiveSetFunction(self._cost_weights / C_max, mode='proxy'),
            target_cost=self.target_cost/C_max, delta=self.delta, C_max=1.,
        )
        self._favmac.init_calibrate(_cal_data["logit"], _cal_data["y_true"])

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        """Forward propagation (just like the original model).

        :return: A dictionary with all results from the base model, with the following updates:

                y_predset: a bool tensor representing the prediction for each class.
        :rtype: Dict[str, torch.Tensor]
        """
        ret = self.model(**kwargs)
        _logit = ret["logit"].cpu().numpy()
        y_predset = np.asarray([self._favmac(_)[0] for _ in _logit])

        ret["y_predset"] = torch.tensor(y_predset)
        return ret


if __name__ == "__main__":
    from pyhealth.calib.predictionset import FavMac
    from pyhealth.datasets import (MIMIC3Dataset, get_dataloader,
                                   split_by_patient)
    from pyhealth.models import Transformer
    from pyhealth.tasks import drug_recommendation_mimic3_fn
    from pyhealth.trainer import get_metrics_fn

    base_dataset = MIMIC3Dataset(
        root="/srv/scratch1/data/physionet.org/files/mimiciii/1.4",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
        refresh_cache=False,
    )
    sample_dataset = base_dataset.set_task(drug_recommendation_mimic3_fn)
    train_data, val_data, test_data = split_by_patient(sample_dataset, [0.6, 0.2, 0.2])
    model = Transformer(dataset=sample_dataset, feature_keys=["conditions", "procedures"],
                        label_key="drugs", mode="multilabel")
    # ... Train the model here ...
    # calibrate the prediction sets with FavMac
    cal_model = FavMac(model, cost_weights=1., target_cost=3, delta=None)
    cal_model.calibrate(cal_dataset=val_data)
    # Evaluate
    from pyhealth.trainer import Trainer

    test_dl = get_dataloader(test_data, batch_size=32, shuffle=False)
    y_true_all, y_prob_all, _, extra_output = Trainer(model=cal_model).inference(
        test_dl, additional_outputs=["y_predset"]
    )
    print(
        get_metrics_fn(cal_model.mode)(
            y_true_all, y_prob_all, metrics=['tp', 'fp'],
            y_predset=extra_output["y_predset"],
        )
    )


Here is the code content for core.py:
from collections import deque
from importlib import reload

import numpy as np
import pandas as pd
import tqdm
#from data_utils import INTEGER_SAFE_DELTA
from scipy.special import expit

from pyhealth.calib.predictionset.favmac import quantiletree


class FavMac:
    def __init__(self, cost_fn, util_fn, proxy_fn, target_cost, delta=None, C_max=1.) -> None:
        self.target_cost = target_cost
        self.delta = delta

        self.cost_fn = cost_fn # (S, Y)
        self.util_fn = util_fn # (S, Y=None, pred=None)
        self.proxy_fn = proxy_fn # (S, pred)

        #Threshold
        self.t = None

        # reserved parameters to avoid repeating queries
        self._thresholds_mem = {}
        self._cnt = 0

        self.quantiletree = quantiletree.QuantileTree()
        self._queue = deque()
        self.C_max = C_max

    def _add_sample(self, predset, extra_info):
        costs, proxies = extra_info
        _sidx = np.argsort(proxies)
        costs = np.asarray(costs)[_sidx]
        proxies = np.asarray(proxies)[_sidx]
        assert max(costs) <= 1
        if self.delta is not None:
            costs = pd.Series(costs).cummax().values
            _valid_ts = [score for cost, score in zip(costs, proxies) if cost > self.target_cost] # more like invalid ts
            t_k_i = min(_valid_ts) if len(_valid_ts) > 0 else np.inf
            self.quantiletree.insert(t_k_i, 1)
            self._queue.append(t_k_i)
        else:
            curr_cost = 0
            for cost, score in zip(costs, proxies):
                if cost > curr_cost:
                    self.quantiletree.insert(score, cost - curr_cost)
                    curr_cost = cost
            self._queue.append((costs, proxies))

    def _query_threshold(self):
        n = len(self._queue)
        if self.delta is None:
            cutoff = self.target_cost * (n+1) - self.C_max
            return self.quantiletree.query_cumu_weight(cutoff, prev=False)
        else:
            cutoff = self.delta * (n+1) - 1# We should assume a violation for the next point? Should we minus 1??
            return self.quantiletree.query_cumu_weight(cutoff, prev=False)

    def _greedy_sequence(self, pred:np.ndarray):
        raise NotImplementedError()

    def _forward(self, logit, label=None):
        # return predset, (costs, cost_proxies)
        # costs[j] or cost_proxies[j] is for S_j
        # (S_0 \subset S_1 \ldots S_K)
        pred = expit(logit)
        Ss, proxies = self._greedy_sequence(pred)
        costs, predset = None, None
        if label is not None:
            costs = [self.cost_fn(S, label) for S in Ss]
        if self.t is not None:
            candidates = [S for S,v in zip(Ss, proxies) if v < self.t]
            predset = Ss[0] if len(candidates) == 0 else candidates[-1]
        return predset, (costs, proxies)


    def query_threshold(self, target_cost=None):
        if target_cost is not None:
            old_target_cost = self.target_cost
            self.target_cost = target_cost
            t = self._query_threshold()
            self.target_cost = old_target_cost
            return t
        if self._cnt not in self._thresholds_mem:
            self._thresholds_mem[self._cnt] = self._query_threshold()
        return self._thresholds_mem[self._cnt]

    def update(self, logit, label):
        predset, extra_info = self._forward(logit, label)
        self._add_sample(predset, extra_info)
        self._cnt += 1
        self.t = self.query_threshold()
        return predset, extra_info

    def init_calibrate(self, logits, labels):
        n = len(logits)
        for i, (_logit, _y) in tqdm.tqdm(enumerate(zip(logits, labels)), desc='initial calibration...', total=n):
            predset, extra_info = self._forward(_logit, _y)
            self._add_sample(predset, extra_info)
            self._cnt += 1
        self.t = self.query_threshold()

    def __call__(self, logit, label=None, update=True):
        if update and label is not None:
            return self.update(logit, label)
        return self._forward(logit, label)

class FavMac_GreedyRatio(FavMac):
    #In each step, maximize dValue/dProxy.
    def _greedy_sequence(self, pred:np.ndarray):
        proxy_fn = lambda _S: self.proxy_fn(_S, pred=pred, target_cost = None if self.delta is None else self.target_cost)
        try:
            if self.proxy_fn.is_additive():
                Ss, _ = self.util_fn.greedy_maximize_seq(pred=pred, d_proxy = self.proxy_fn.values * (1-pred))
                return Ss, list(map(proxy_fn, Ss))
        except:
            pass

        Ss = [np.zeros(len(pred), dtype=int)]
        proxies = [proxy_fn(Ss[0])]
        while Ss[-1].min() == 0:
            S = Ss[-1].copy()
            curr_d_proxies = [np.nan] * len(S)
            for k in range(len(S)):
                if S[k] == 1: continue
                S[k] = 1
                curr_d_proxies[k] = proxy_fn(S) - proxies[-1]
                S[k] = 0
            k, du_div_dp = self.util_fn.greedy_maximize(S, pred=pred, d_proxy = np.asarray(curr_d_proxies))
            if k is None: break
            S[k] = 1
            Ss.append(S)
            proxies.append(curr_d_proxies[k] + proxies[-1])
        return Ss, proxies

Here is the code content for quantiletree.py:
import math

RED, BLACK, DOUBLEBLACK = 0, 1, 2

class Node:
    def __init__(self, val, parent=None, left=None, right=None) -> None:
        self.parent = parent
        self.val = val
        self.left = left
        self.right = right

class WeightedNode(Node):
    def __init__(self, val, weight=0, parent=None, left=None, right=None) -> None:
        super().__init__(val, parent, left, right)
        self.weight = weight
        self.sum = weight

    def update_sum(self):
        self.sum = self.weight + self.left.sum + self.right.sum

class ColorWeightedNode(WeightedNode):
    def __init__(self, val, color=BLACK, weight=0, parent=None, left=None, right=None) -> None:
        super().__init__(val, weight, parent, left, right)
        self.color = color



class BST:
    def __init__(self, node_cls=Node, debug=False) -> None:
        self.nil = node_cls(None)
        self.root = self.nil
        self.debug = debug

    def _check_properties(self):
        if not self.debug: return True
        def recurse(node: Node):
            if node == self.nil: return True
            assert node.val is not None
            if node.left != self.nil:
                assert node.left.val < node.val, f"left={node.left.val} > parent={node.val}"
            if node.right != self.nil:
                assert node.right.val > node.val, f"right={node.right.val} < parent={node.val}"
            recurse(node.left)
            recurse(node.right)

        recurse(self.root)

        assert self.nil.val is None
        assert self.nil.left is None
        assert self.nil.right is None
        # print("BST Checks")
        return True

    def __search_tree_helper(self, node, key):
        if node == self.nil or key == node.val:
            return node

        if key < node.val:
            return self.__search_tree_helper(node.left, key)
        return self.__search_tree_helper(node.right, key)

    # search the tree for the key k
    # and return the corresponding node
    def searchTree(self, k):
        return self.__search_tree_helper(self.root, k)

    # find the node with the minimum key
    def minimum(self, node):
        while node.left != self.nil:
            node = node.left
        return node

    # find the node with the maximum key
    def maximum(self, node):
        while node.right != self.nil:
            node = node.right
        return node

    # find the successor of a given node
    def successor(self, x):
        # if the right subtree is not None,
        # the successor is the leftmost node in the
        # right subtree
        if x.right != self.nil:
            return self.minimum(x.right)

        # else it is the lowest ancestor of x whose
        # left child is also an ancestor of x.
        y = x.parent
        while y is not None and y != self.nil and x == y.right:
            x = y
            y = y.parent
        if y is None: y = self.nil
        return y

    # find the predecessor of a given node
    def predecessor(self,  x):
        # if the left subtree is not None,
        # the predecessor is the rightmost node in the
        # left subtree
        if (x.left != self.nil):
            return self.maximum(x.left)

        y = x.parent
        while y is not None and y != self.nil and x == y.left:
            x = y
            y = y.parent
        if y is None: y = self.nil
        return y

class WeightedBST(BST):
    def __init__(self, node_cls=WeightedNode, debug=False) -> None:
        super().__init__(node_cls, debug)
        self._eps = 1e-8

    def _check_properties(self):
        if not self.debug: return True
        # weight
        super()._check_properties()
        def check_weight(node: WeightedNode):
            if node == self.nil: return True
            if node.left == self.nil and node.right == self.nil:
                return True

            # assert node.sum == node.left.sum + node.right.sum + node.weight, f"Weight does not sum up: {node.sum} != {node.left.sum} + {node.weight} + {node.right.sum}"
            assert node.sum == node.left.sum + node.right.sum + node.weight, f"Weight does not sum up: {node.sum:.3f} != {node.left.sum:.3f} + {node.weight:.3f} + {node.right.sum:.3f} = {node.left.sum + node.weight + node.right.sum:.3f}"
            check_weight(node.left)
            check_weight(node.right)
        check_weight(self.root)
        assert self.nil.weight == 0 == self.nil.sum, "Nil should have no weight"
        # print("WBST Checks")

    def _update_parent_sum(self, node: WeightedNode):
        while node is not None and node != self.nil:
            node.update_sum()
            node = node.parent

    def query_sum(self, val, inclusive=False):
        def recurse(node):
            if node == self.nil: return 0
            if node.val < val: return node.weight + node.left.sum + recurse(node.right)
            if node.val > val: return recurse(node.left)
            return (node.weight if inclusive else 0) + recurse(node.left)
        return recurse(self.root)

    def query_cumu_weight(self, w, prev=True):
        def recurse(node, w):
            w = max(w, 0.) # adjust for numerical issue
            assert node.sum > w - self._eps
            if node.left.sum <= w and w < node.left.sum + node.weight:
                return node
            if node.right == self.nil and (node.left.sum <= w and w < node.left.sum + node.weight + self._eps):
                return node
            if w < node.left.sum:
                return recurse(node.left, w)
            else: # w >= node.left.sum + node.weight:
                assert node.right != self.nil
                return recurse(node.right, w - node.left.sum - node.weight)
        if w >= self.root.sum:
            node = self.maximum(self.root)
        else:
            node = recurse(self.root, w)
            if prev: node = self.predecessor(node)
        return -math.inf if node == self.nil else node.val

        """
        Assume the threshold to cost is:
            [(0.1, 1), (0.2, 2), (0.4, 3), (0.5, 7)]
        The mass for each threshold is
            [(0.1, 1), (0.2, 1), (0.4, 1), (0.5, 4)]
        Suppose we want to look up is for cost of 3 (i.e. 0.4)
        Suppose we want to look up cost 4 (we would need 0.4 still)
        This means the range for each threshold is
            -inf: [0, 1)
            0.1 : [1, 2)
            0.2 : [2, 3)
            0.4 : [3, 7)
            0.5 : [7, infty)
        We thus need to find the immediate predecesor


        """
# We first assume all entries are different.
# This could be handled by the doubly linked list
class QuantileTree(WeightedBST):
    def __init__(self, node_cls=ColorWeightedNode, debug=False):
        super().__init__(node_cls, debug)

    def _check_properties(self):
        if not self.debug: return True
        super()._check_properties()
        def _check_rb(node: ColorWeightedNode):
            if node == self.nil: return True
            if node.color != RED and node.color != BLACK: return False
            return _check_rb(node.left) and _check_rb(node.right)
        assert _check_rb(self.root), "Not all nodes are red and black."
        assert self.root.color == BLACK, "Root color is wrong"
        assert self.nil.color == BLACK, "Nil color is wrong"
        def _check_rr(node: ColorWeightedNode):
            if node == self.nil: return True
            if node.color == RED:
                if node.left.color == RED or node.right.color == RED: return False
            return _check_rr(node.left) and _check_rr(node.right)
        assert _check_rr(self.root), "Some red nodes have red children"
        def _check_bd(root: ColorWeightedNode):
            def _recurse(curr: ColorWeightedNode, num_black: int):
                if curr == self.nil: return num_black, num_black
                if curr.color == BLACK: num_black += 1
                l_min, l_max = _recurse(curr.left, num_black)
                r_min, r_max = _recurse(curr.right, num_black)
                return min(l_min, r_min), max(l_max, r_max)
            min_cnt, max_cnt = _recurse(root, 0)
            return min_cnt == max_cnt
        assert _check_bd(self.root), "Paths with diff # of blacks"

    def __rb_transplant(self, u, v):
        if u.parent == None:
            self.root = v
        elif u == u.parent.left:
            u.parent.left = v
        else:
            u.parent.right = v
        par = v.parent = u.parent
        return par

    def __fix_delete(self, x):
        while x != self.root and x.color == BLACK:
            if x == x.parent.left:
                s = x.parent.right
                if s.color == RED:
                    # case 3.1
                    s.color = BLACK
                    x.parent.color = RED
                    self.rotate_left(x.parent)
                    s = x.parent.right
                if s.left.color == s.right.color == BLACK:
                    # case 3.2
                    s.color = RED
                    x = x.parent
                else:
                    if s.right.color == BLACK: #left is red
                        # case 3.3
                        s.left.color = BLACK
                        s.color = RED
                        self.rotate_right(s)
                        s = x.parent.right
                    # case 3.4
                    s.color = x.parent.color
                    x.parent.color = BLACK
                    s.right.color = BLACK
                    self.rotate_left(x.parent)
                    x = self.root
            else:
                s = x.parent.left
                if s.color == RED:
                    # case 3.1
                    s.color = BLACK
                    x.parent.color = RED
                    self.rotate_right(x.parent)
                    s = x.parent.left
                if s.left.color == s.right.color == BLACK:
                    # case 3.2
                    s.color = RED
                    x = x.parent
                else:
                    if s.left.color == BLACK: #right is red
                        # case 3.3
                        s.right.color = BLACK
                        s.color = RED
                        self.rotate_left(s)
                        s = x.parent.left
                    # case 3.4
                    s.color = x.parent.color
                    x.parent.color = BLACK
                    s.left.color = BLACK
                    self.rotate_right(x.parent)
                    x = self.root
        x.color = BLACK

    def delete(self, val, weight=1):
        # find the node containing key
        node = self.root
        z = self.nil
        to_del = []
        while node != self.nil:
            to_del.append(node)
            if node.val < val:
                node = node.right
            elif node.val > val:
                node = node.left
            else:
                z = node
                break
            #if node.val == val:
            #    z = node
            ##TODO: change to <?
            #node = node.right if node.val <= val else node.left

        if z == self.nil:
            raise ValueError("Couldn't find key in the tree")
            return
        if z.weight < weight:
            raise ValueError("Too much weight to subtract")
        for _ in to_del: _.sum -= weight
        if z.weight > weight:
            z.weight -= weight
            return
        assert z.weight == weight # remove the whole node

        y = z
        y_original_color = y.color
        if z.left == self.nil:
            x = z.right
            self.__rb_transplant(z, z.right)
        elif z.right == self.nil:
            x = z.left
            self.__rb_transplant(z, z.left)
        else:
            y = self.minimum(z.right)
            y_original_color = y.color
            x = y.right #TODO: What if this is nil???
            if y.parent == z: # y==y.parent.right==z.right
                x.parent = y
            else: # y == y.parent.left
                self.__rb_transplant(y, y.right)
                y.right = z.right
                y.right.parent = y

            self.__rb_transplant(z, y)
            y.left = z.left
            y.left.parent = y
            y.color = z.color

        # update sum
        self._update_parent_sum(x.parent)
        if y_original_color == BLACK:
            self.__fix_delete(x)
        self._check_properties()

    def insert(self, val, weight=1):

        new_node = ColorWeightedNode(val, weight=weight, color=RED, left=self.nil, right=self.nil)

        par = None
        curr = self.root
        while curr != self.nil:
            curr.sum += weight
            par = curr
            if val < curr.val:
                curr = curr.left
            elif val > curr.val:
                curr = curr.right
            else:
                curr.weight += weight
                return

        new_node.parent = par
        if par is None:
            self.root = new_node
        elif val < par.val:
            par.left = new_node
        else:
            par.right = new_node


        self.fix_insert(new_node)
        self._check_properties()

    def rotate_left(self, x):
        #    x    ->    y
        #     y        x
        y = x.right

        new_x_sum = x.left.sum + y.left.sum + x.weight
        new_y_sum = new_x_sum + y.weight + y.right.sum

        x.right = y.left
        if y.left != self.nil:
            y.left.parent = x

        y.parent = x.parent
        if x.parent is None:
            self.root = y
        elif x == x.parent.left:
            x.parent.left = y
        else:
            x.parent.right = y
        y.left = x
        x.parent = y
        x.sum = new_x_sum
        y.sum = new_y_sum

    def rotate_right(self, x):
        #     x    ->   y
        #    y           x
        y = x.left

        new_x_sum = x.right.sum + y.right.sum + x.weight
        new_y_sum = new_x_sum + y.weight + y.left.sum

        x.left = y.right
        if y.right != self.nil:
            y.right.parent = x

        y.parent = x.parent
        if x.parent == None:
            self.root = y
        elif x == x.parent.right:
            x.parent.right = y
        else:
            x.parent.left = y
        y.right = x
        x.parent = y
        x.sum = new_x_sum
        y.sum = new_y_sum


    def _recolor(self, parent):
        # parent is black, need to recolor
        parent.left.color = BLACK
        parent.right.color = BLACK
        parent.color = RED
        return parent

    def fix_insert(self, curr):
        while self.root != curr and curr.parent.color == RED:
            if curr.parent == curr.parent.parent.right:
                u = curr.parent.parent.left # uncle
                if u.color == RED: # recolor
                    curr = self._recolor(u.parent)
                else: # rotate
                    if curr == curr.parent.left:
                        curr = curr.parent
                        self.rotate_right(curr)
                    curr.parent.color = BLACK
                    curr.parent.parent.color = RED
                    self.rotate_left(curr.parent.parent)
                    # The subtree's root is black so we won't need to continue
            else:
                u = curr.parent.parent.right
                if u.color == RED:
                    curr = self._recolor(u.parent)
                else:
                    if curr == curr.parent.right:
                        curr = curr.parent
                        self.rotate_left(curr)
                    curr.parent.color = BLACK
                    curr.parent.parent.color = RED
                    self.rotate_right(curr.parent.parent)
        self.root.color = BLACK



if __name__ == '__main__':
    pass

Here is the code content for medcode.py:
from pyhealth.medcode import CrossMap, InnerMap

ndc = InnerMap.load("NDC")
print("Looking up for NDC code 00597005801")
print(ndc.lookup("00597005801"))

codemap = CrossMap.load("NDC", "ATC")
print("Mapping NDC code 00597005801 to ATC")
print(codemap.map("00597005801"))

atc = InnerMap.load("ATC")
print("Looking up for ATC code G04CA02")
print(atc.lookup("G04CA02"))


Here is the code content for EEG_events_SparcNet.py:
from pyhealth.datasets import split_by_visit, get_dataloader
from pyhealth.trainer import Trainer
from pyhealth.datasets import TUEVDataset
from pyhealth.tasks import EEG_events_fn
from pyhealth.models import SparcNet

# step 1: load signal data
dataset = TUEVDataset(root="/srv/local/data/TUH/tuh_eeg_events/v2.0.0/edf/", 
                            dev=True,
                            refresh_cache=True, 
                            )

# step 2: set task
TUEV_ds = dataset.set_task(EEG_events_fn)
TUEV_ds.stat()

# split dataset
train_dataset, val_dataset, test_dataset = split_by_visit(
    TUEV_ds, [0.6, 0.2, 0.2]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)
print(
    "loader size: train/val/test",
    len(train_dataset),
    len(val_dataset),
    len(test_dataset),
)

# STEP 3: define model
model = SparcNet(
    dataset=TUEV_ds,
    feature_keys=["signal"],
    label_key="label",
    mode="multiclass",
)

# STEP 4: define trainer
trainer = Trainer(model=model, device="cuda:4")
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=10,
    optimizer_params={"lr": 1e-3},
)

# STEP 5: evaluate
print(trainer.evaluate(test_dataloader))


Here is the code content for EEG_isAbnormal_SparcNet.py:
from pyhealth.datasets import split_by_visit, get_dataloader
from pyhealth.trainer import Trainer
from pyhealth.datasets import TUABDataset
from pyhealth.tasks import EEG_isAbnormal_fn
from pyhealth.models import SparcNet

# step 1: load signal data
dataset = TUABDataset(root="/srv/local/data/TUH/tuh_eeg_abnormal/v3.0.0/edf/", 
                            dev=True,
                            refresh_cache=True, 
                            )

# step 2: set task
TUAB_ds = dataset.set_task(EEG_isAbnormal_fn)
TUAB_ds.stat()

# split dataset
train_dataset, val_dataset, test_dataset = split_by_visit(
    TUAB_ds, [0.6, 0.2, 0.2]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)
print(
    "loader size: train/val/test",
    len(train_dataset),
    len(val_dataset),
    len(test_dataset),
)

# STEP 3: define model
model = SparcNet(
    dataset=TUAB_ds,
    feature_keys=["signal"],
    label_key="label",
    mode="binary",
)

# STEP 4: define trainer
trainer = Trainer(model=model, device="cuda:4")
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=10,
    monitor="pr_auc",
    optimizer_params={"lr": 1e-3},
)

# STEP 5: evaluate
print(trainer.evaluate(test_dataloader))


Here is the code content for cardiology_detection_isAR_SparcNet.py:
from pyhealth.datasets import split_by_visit, get_dataloader
from pyhealth.trainer import Trainer
from pyhealth.datasets import CardiologyDataset
from pyhealth.tasks import cardiology_isAR_fn
from pyhealth.models import ContraWR, SparcNet

# step 1: load signal data
dataset = CardiologyDataset(root="/srv/local/data/physionet.org/files/challenge-2020/1.0.2/training", 
                            chosen_dataset=[1,1,1,1,1,1], 
                            refresh_cache=False, 
                            dev=True)

# step 2: set task
cardiology_ds = dataset.set_task(cardiology_isAR_fn)
cardiology_ds.stat()

# split dataset
train_dataset, val_dataset, test_dataset = split_by_visit(
    cardiology_ds, [0.6, 0.2, 0.2]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)
print(
    "loader size: train/val/test",
    len(train_dataset),
    len(val_dataset),
    len(test_dataset),
)

# STEP 3: define model
model = SparcNet(
    dataset=cardiology_ds,
    feature_keys=["signal"],
    label_key="label",
    mode="binary",
)

# STEP 4: define trainer
trainer = Trainer(model=model, device="cuda:4")
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=5,
    monitor="pr_auc",
)

# STEP 5: evaluate
print(trainer.evaluate(test_dataloader))


Here is the code content for drug_recommendation_mimic3_gamenet.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import GAMENet
from pyhealth.tasks import drug_recommendation_mimic3_fn
from pyhealth.trainer import Trainer

# STEP 1: load data
base_dataset = MIMIC3Dataset(
    root="/srv/local/data/physionet.org/files/mimiciii/1.4",
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
    code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
    dev=False,
    refresh_cache=True,
)
base_dataset.stat()

# STEP 2: set task
sample_dataset = base_dataset.set_task(drug_recommendation_mimic3_fn)
sample_dataset.stat()

train_dataset, val_dataset, test_dataset = split_by_patient(
    sample_dataset, [0.8, 0.1, 0.1]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

# STEP 3: define model
model = GAMENet(
    sample_dataset,
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=5,
    monitor="pr_auc_samples",
)

# STEP 5: evaluate
print (trainer.evaluate(test_dataloader))


Here is the code content for drug_recommendation_mimic3_molerec.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import MoleRec
from pyhealth.tasks import drug_recommendation_mimic3_fn
from pyhealth.trainer import Trainer

# STEP 1: load data
base_dataset = MIMIC3Dataset(
    root="/srv/local/data/physionet.org/files/mimiciii/1.4",
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
    code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
    dev=False,
    refresh_cache=False,
)
base_dataset.stat()

# STEP 2: set task
sample_dataset = base_dataset.set_task(drug_recommendation_mimic3_fn)
sample_dataset.stat()

train_dataset, val_dataset, test_dataset = split_by_patient(
    sample_dataset, [0.8, 0.1, 0.1]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

# STEP 3: define model
model = MoleRec(
    sample_dataset,
    feature_keys=["conditions", "procedures"],
    label_key="drugs",
    mode="multilabel",
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=5,
    monitor="pr_auc_samples",
)

# STEP 5: evaluate
print (trainer.evaluate(test_dataloader))


Here is the code content for drug_recommendation_mimic4_gamenet.py:
# import pyhealth
import pyhealth

# import mimic4 dataset and drug recommendaton task
from pyhealth.datasets import MIMIC4Dataset
from pyhealth.tasks import drug_recommendation_mimic4_fn

# import dataloader related functions
from pyhealth.datasets.splitter import split_by_patient
from pyhealth.datasets import split_by_patient, get_dataloader

# import gamenet model
from pyhealth.models import GAMENet

# import trainer
from pyhealth.trainer import Trainer

_DEV = False
_EPOCHS = 20
_LR = 1e-3
_DECAY_WEIGHT = 1e-5


def prepare_drug_task_data():
    mimicvi = MIMIC4Dataset(
        root="/srv/local/data/physionet.org/files/mimiciv/2.0/hosp",
        tables=["diagnoses_icd", "procedures_icd", "prescriptions"],
        code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
        dev=_DEV,
        refresh_cache=False,
    )

    print("stat")
    mimicvi.stat()
    print("info")
    mimicvi.info()

    mimic4_sample = mimicvi.set_task(drug_recommendation_mimic4_fn)
    print(mimic4_sample[0])

    return mimic4_sample


def get_dataloaders(mimic4_sample):
    train_dataset, val_dataset, test_dataset = split_by_patient(
        mimic4_sample, [0.8, 0.1, 0.1]
    )
    train_loader = get_dataloader(train_dataset, batch_size=64, shuffle=True)
    val_loader = get_dataloader(val_dataset, batch_size=64, shuffle=False)
    test_loader = get_dataloader(test_dataset, batch_size=64, shuffle=False)

    return train_loader, val_loader, test_loader


def train_gamenet(mimic4_sample, train_loader, val_loader):
    # gamenet = GAMENet(mimicvi)
    gamenet = GAMENet(mimic4_sample)

    # print(gamenet.generate_ddi_adj())
    trainer = Trainer(
        model=gamenet,
        # metrics = ["jaccard_weighted", "pr_auc_micro", "pr_auc_macro"],
        # metrics = ["jaccard", "pr_auc_micro", "pr_auc_macro"],
        metrics=[
            "jaccard_samples",
            "accuracy",
            "hamming_loss",
            "precision_samples",
            "recall_samples",
            "pr_auc_samples",
            "f1_samples",
        ],
        device="cuda",
        exp_name="drug_recommendation",
    )

    trainer.train(
        train_dataloader=train_loader,
        val_dataloader=val_loader,
        epochs=_EPOCHS,
        # monitor = "jaccard_weighted",
        # monitor = "pr_auc_macro",
        # monitor = "jaccard_samples",
        monitor="accuracy",
        monitor_criterion="max",
        weight_decay=_DECAY_WEIGHT,
        optimizer_params={"lr": _LR},
    )

    return gamenet, trainer


def evaluate_gamenet(trainer, test_loader):
    result = trainer.evaluate(test_loader)
    print(result)
    return result


if __name__ == "__main__":
    data = prepare_drug_task_data()
    train_loader, val_loader, test_loader = get_dataloaders(data)

    model, trainer = train_gamenet(data, train_loader, val_loader)

    result = evaluate_gamenet(trainer, test_loader)


Here is the code content for mortality_mimic3_concare.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import ConCare
from pyhealth.tasks import mortality_prediction_mimic3_fn
from pyhealth.trainer import Trainer

if __name__ == "__main__":
    # STEP 1: load data
    base_dataset = MIMIC3Dataset(
        root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        code_mapping={"ICD9CM": "CCSCM", "ICD9PROC": "CCSPROC", "NDC": "ATC"},
        dev=False,
        refresh_cache=False,
    )
    base_dataset.stat()

    # STEP 2: set task
    sample_dataset = base_dataset.set_task(mortality_prediction_mimic3_fn)
    sample_dataset.stat()

    train_dataset, val_dataset, test_dataset = split_by_patient(
        sample_dataset, [0.8, 0.1, 0.1]
    )
    train_dataloader = get_dataloader(train_dataset, batch_size=256, shuffle=True)
    val_dataloader = get_dataloader(val_dataset, batch_size=256, shuffle=False)
    test_dataloader = get_dataloader(test_dataset, batch_size=256, shuffle=False)

    # STEP 3: define model
    model = ConCare(
        dataset=sample_dataset,
        feature_keys=["conditions", "procedures"],
        label_key="label",
        mode="binary",
        use_embedding=[True, True, True],
        hidden_dim=32,
    )

    # STEP 4: define trainer
    trainer = Trainer(model=model)
    trainer.train(
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        epochs=3,
        monitor="roc_auc",
    )

    # STEP 5: evaluate
    print(trainer.evaluate(test_dataloader))


Here is the code content for mortality_mimic3_stagenet.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import StageNet
from pyhealth.tasks import mortality_prediction_mimic3_fn
from pyhealth.trainer import Trainer

if __name__ == "__main__":
    # STEP 1: load data
    base_dataset = MIMIC3Dataset(
        root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        code_mapping={"ICD9CM": "CCSCM", "ICD9PROC": "CCSPROC", "NDC": "ATC"},
        dev=False,
        refresh_cache=False,
    )
    base_dataset.stat()

    # STEP 2: set task
    sample_dataset = base_dataset.set_task(mortality_prediction_mimic3_fn)
    sample_dataset.stat()

    train_dataset, val_dataset, test_dataset = split_by_patient(
        sample_dataset, [0.8, 0.1, 0.1]
    )
    train_dataloader = get_dataloader(train_dataset, batch_size=256, shuffle=True)
    val_dataloader = get_dataloader(val_dataset, batch_size=256, shuffle=False)
    test_dataloader = get_dataloader(test_dataset, batch_size=256, shuffle=False)

    # STEP 3: define model
    model = StageNet(
        dataset=sample_dataset,
        feature_keys=["conditions", "procedures"],
        label_key="label",
        mode="binary",
        embedding_dim=32,
    )

    # STEP 4: define trainer
    trainer = Trainer(model=model)
    trainer.train(
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        epochs=50,
        monitor="roc_auc",
    )

    # STEP 5: evaluate
    print(trainer.evaluate(test_dataloader))


Here is the code content for readmission_mimic3_rnn.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import RNN
from pyhealth.tasks import readmission_prediction_mimic3_fn
from pyhealth.trainer import Trainer

# STEP 1: load data
base_dataset = MIMIC3Dataset(
    root="/srv/local/data/physionet.org/files/mimiciii/1.4",
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
    code_mapping={"ICD9CM": "CCSCM", "ICD9PROC": "CCSPROC", "NDC": "ATC"},
    dev=False,
    refresh_cache=True,
)
base_dataset.stat()

# STEP 2: set task
sample_dataset = base_dataset.set_task(readmission_prediction_mimic3_fn)
sample_dataset.stat()

train_dataset, val_dataset, test_dataset = split_by_patient(
    sample_dataset, [0.8, 0.1, 0.1]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

# STEP 3: define model
model = RNN(
    dataset=sample_dataset,
    feature_keys=["conditions", "procedures", "drugs"],
    label_key="label",
    mode="binary",
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=50,
    monitor="roc_auc",
)

# STEP 5: evaluate
trainer.evaluate(test_dataloader)


Here is the code content for sleep_staging_shhs_contrawr.py:
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.trainer import Trainer
from pyhealth.datasets import SHHSDataset
from pyhealth.tasks import sleep_staging_shhs_fn
from pyhealth.models import ContraWR, SparcNet

# step 1: load signal data
dataset = SHHSDataset(
    root="/srv/local/data/SHHS/polysomnography",
    dev=True,
    refresh_cache=False,
)

# step 2: set task
sleep_staging_ds = dataset.set_task(sleep_staging_shhs_fn)
sleep_staging_ds.stat()

# split dataset
train_dataset, val_dataset, test_dataset = split_by_patient(
    sleep_staging_ds, [0.6, 0.2, 0.2]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)
print(
    "loader size: train/val/test",
    len(train_dataset),
    len(val_dataset),
    len(test_dataset),
)

# STEP 3: define model
model = SparcNet(
    dataset=sleep_staging_ds,
    feature_keys=["signal"],
    label_key="label",
    mode="multiclass",
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=5,
    monitor="accuracy",
)

# STEP 5: evaluate
print(trainer.evaluate(test_dataloader))


Here is the code content for test.py:
import torch

for i in range(100):
    a = torch.randn((5, 5, 5)) > 0
    print(((torch.sum(a, dim=2) != 0) != (torch.any(a != 0, dim=2))).any())


Here is the code content for test_eICU_addition.py:
from pyhealth.datasets import eICUDataset
from pyhealth.tasks import mortality_prediction_eicu_fn, mortality_prediction_eicu_fn2

base_dataset = eICUDataset(
    root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
    tables=["diagnosis", "admissionDx", "treatment"],
    dev=False,
    refresh_cache=False,
)
sample_dataset = base_dataset.set_task(task_fn=mortality_prediction_eicu_fn2)
sample_dataset.stat()
print(sample_dataset.available_keys)

# base_dataset = eICUDataset(
#     root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
#     tables=["diagnosis", "admissionDx", "treatment"],
#     dev=True,
#     refresh_cache=False,
# )
# sample_dataset = base_dataset.set_task(task_fn=mortality_prediction_eicu_fn2)
# sample_dataset.stat()
# print(sample_dataset.available_keys)


Here is the code content for drug_recommendation_mimic3_safedrug.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import SafeDrug
from pyhealth.tasks import drug_recommendation_mimic3_fn
from pyhealth.trainer import Trainer

# STEP 1: load data
base_dataset = MIMIC3Dataset(
    root="/srv/local/data/physionet.org/files/mimiciii/1.4",
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
    code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
    dev=False,
    refresh_cache=False,
)
base_dataset.stat()

# STEP 2: set task
sample_dataset = base_dataset.set_task(drug_recommendation_mimic3_fn)
sample_dataset.stat()

train_dataset, val_dataset, test_dataset = split_by_patient(
    sample_dataset, [0.8, 0.1, 0.1]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

# STEP 3: define model
model = SafeDrug(
    sample_dataset,
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=5,
    monitor="pr_auc_samples",
)

# STEP 5: evaluate
print (trainer.evaluate(test_dataloader))


Here is the code content for drug_recommendation_mimic3_transformer.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import Transformer
from pyhealth.tasks import drug_recommendation_mimic3_fn
from pyhealth.trainer import Trainer

# STEP 1: load data
base_dataset = MIMIC3Dataset(
    root="/srv/local/data/physionet.org/files/mimiciii/1.4",
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
    code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
    dev=False,
    refresh_cache=True,
)
base_dataset.stat()

# STEP 2: set task
sample_dataset = base_dataset.set_task(drug_recommendation_mimic3_fn)
sample_dataset.stat()

train_dataset, val_dataset, test_dataset = split_by_patient(
    sample_dataset, [0.8, 0.1, 0.1]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

# STEP 3: define model
model = Transformer(
    dataset=sample_dataset,
    feature_keys=["conditions", "procedures"],
    label_key="drugs",
    mode="multilabel",
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=50,
    monitor="pr_auc_samples",
)

# STEP 5: evaluate
print (trainer.evaluate(test_dataloader))


Here is the code content for length_of_stay_mimic3_rnn.py:
from pyhealth.calib import calibration, predictionset
from pyhealth.datasets import MIMIC3Dataset, get_dataloader, split_by_patient
from pyhealth.models import Transformer
from pyhealth.tasks import length_of_stay_prediction_mimic3_fn
from pyhealth.trainer import Trainer, get_metrics_fn

# STEP 1: load data
base_dataset = MIMIC3Dataset(
    root="/srv/local/data/physionet.org/files/mimiciii/1.4",
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
    code_mapping={"ICD9CM": "CCSCM", "ICD9PROC": "CCSPROC", "NDC": "ATC"},
    dev=False,
    refresh_cache=True,
)
base_dataset.stat()

# STEP 2: set task
sample_dataset = base_dataset.set_task(length_of_stay_prediction_mimic3_fn)
sample_dataset.stat()

train_dataset, val_dataset, test_dataset = split_by_patient(
    sample_dataset, [0.8, 0.1, 0.1]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

# STEP 3: define modedl
model = Transformer(
    dataset=sample_dataset,
    feature_keys=["conditions", "procedures", "drugs"],
    label_key="label",
    mode="multiclass",
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=50,
    monitor="accuracy",
)

# STEP 5: evaluate
metrics = ['accuracy', 'f1_macro', 'f1_micro'] + ['ECE_adapt', 'cwECEt_adapt']
y_true_all, y_prob_all = trainer.inference(test_dataloader)[:2]
print(get_metrics_fn(model.mode)(y_true_all, y_prob_all, metrics=metrics))

# STEP 6: calibrate the model
cal_model = calibration.HistogramBinning(model, debug=True)
cal_model.calibrate(cal_dataset=val_dataset)
y_true_all, y_prob_all = Trainer(model=cal_model).inference(test_dataloader)[:2]
print(get_metrics_fn(cal_model.mode)(y_true_all, y_prob_all, metrics=metrics))


# STEP 7: Construct prediction set, controlling overall miscoverage rate (<0.1)
# Note that if you use calibrated model the coverate rate cannot be controlled, because
# with repect to the calibrated model (which was trained on the calibration set), the
# test set and calibration set is not i.i.d
ps_model = predictionset.LABEL(model, 0.1, debug=True)
ps_model.calibrate(cal_dataset=val_dataset)
y_true_all, y_prob_all, _, extra_output = Trainer(model=ps_model).inference(test_dataloader, additional_outputs=['y_predset'])
print(get_metrics_fn(ps_model.mode)(y_true_all, y_prob_all,
                                     metrics=metrics + ['miscoverage_overall_ps', 'rejection_rate'],
                                     y_predset=extra_output['y_predset']))


Here is the code content for mortality_mimic3_agent.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import Agent
from pyhealth.tasks import mortality_prediction_mimic3_fn
from pyhealth.trainer import Trainer

if __name__ == "__main__":
    # STEP 1: load data
    base_dataset = MIMIC3Dataset(
        root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        code_mapping={"ICD9CM": "CCSCM", "ICD9PROC": "CCSPROC", "NDC": "ATC"},
        dev=False,
        refresh_cache=False,
    )
    base_dataset.stat()

    # STEP 2: set task
    sample_dataset = base_dataset.set_task(mortality_prediction_mimic3_fn)
    sample_dataset.stat()

    train_dataset, val_dataset, test_dataset = split_by_patient(
        sample_dataset, [0.8, 0.1, 0.1]
    )
    train_dataloader = get_dataloader(train_dataset, batch_size=256, shuffle=True)
    val_dataloader = get_dataloader(val_dataset, batch_size=256, shuffle=False)
    test_dataloader = get_dataloader(test_dataset, batch_size=256, shuffle=False)

    # STEP 3: define model
    model = Agent(
        dataset=sample_dataset,
        feature_keys=["conditions", "procedures"],
        label_key="label",
        mode="binary",
        embedding_dim=32,
        hidden_dim=32,
    )

    # STEP 4: define trainer
    trainer = Trainer(model=model)
    trainer.train(
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        epochs=50,
        monitor="roc_auc",
    )

    # STEP 5: evaluate
    print(trainer.evaluate(test_dataloader))


Here is the code content for mortality_mimic3_grasp.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import GRASP
from pyhealth.tasks import mortality_prediction_mimic3_fn
from pyhealth.trainer import Trainer

if __name__ == "__main__":
    # STEP 1: load data
    base_dataset = MIMIC3Dataset(
        root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        code_mapping={"ICD9CM": "CCSCM", "ICD9PROC": "CCSPROC", "NDC": "ATC"},
        dev=False,
        refresh_cache=False,
    )
    base_dataset.stat()

    # STEP 2: set task
    sample_dataset = base_dataset.set_task(mortality_prediction_mimic3_fn)
    sample_dataset.stat()

    train_dataset, val_dataset, test_dataset = split_by_patient(
        sample_dataset, [0.8, 0.1, 0.1]
    )
    train_dataloader = get_dataloader(train_dataset, batch_size=256, shuffle=True)
    val_dataloader = get_dataloader(val_dataset, batch_size=256, shuffle=False)
    test_dataloader = get_dataloader(test_dataset, batch_size=256, shuffle=False)

    # STEP 3: define model
    model = GRASP(
        dataset=sample_dataset,
        feature_keys=["conditions", "procedures"],
        label_key="label",
        mode="binary",
        use_embedding=[True, True, True],
        embedding_dim=32,
        hidden_dim=32,
    )

    # STEP 4: define trainer
    trainer = Trainer(model=model)
    trainer.train(
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        epochs=5,
        monitor="roc_auc",
    )

    # STEP 5: evaluate
    print(trainer.evaluate(test_dataloader))


Here is the code content for mortality_mimic3_rnn.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import RNN
from pyhealth.tasks import mortality_prediction_mimic3_fn
from pyhealth.trainer import Trainer

# STEP 1: load data
base_dataset = MIMIC3Dataset(
    root="/srv/local/data/physionet.org/files/mimiciii/1.4",
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
    code_mapping={"ICD9CM": "CCSCM", "ICD9PROC": "CCSPROC", "NDC": "ATC"},
    dev=False,
    refresh_cache=False,
)
base_dataset.stat()

# STEP 2: set task
sample_dataset = base_dataset.set_task(mortality_prediction_mimic3_fn)
sample_dataset.stat()

train_dataset, val_dataset, test_dataset = split_by_patient(
    sample_dataset, [0.8, 0.1, 0.1]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

# STEP 3: define model
model = RNN(
    dataset=sample_dataset,
    feature_keys=["conditions", "procedures", "drugs"],
    label_key="label",
    mode="binary",
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=50,
    monitor="roc_auc",
)

# STEP 5: evaluate
print(trainer.evaluate(test_dataloader))


Here is the code content for readmission_mimic3_fairness.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.tasks import readmission_prediction_mimic3_fn
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.metrics import fairness_metrics_fn
from pyhealth.models import Transformer
from pyhealth.trainer import Trainer
from pyhealth.metrics.fairness_utils.utils import sensitive_attributes_from_patient_ids

# STEP 1: load data
base_dataset = MIMIC3Dataset(
        root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
)
base_dataset.stat()

# STEP 2: set task
sample_dataset = base_dataset.set_task(readmission_prediction_mimic3_fn)
sample_dataset.stat()

train_dataset, val_dataset, test_dataset = split_by_patient(sample_dataset, [0.8, 0.1, 0.1])
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

# STEP 3: define model
model = Transformer(
    dataset=sample_dataset,
    # look up what are available for "feature_keys" and "label_keys" in dataset.samples[0]
    feature_keys=["conditions", "procedures"],
    label_key="label",
    mode="binary",
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=3,
    monitor="pr_auc",
)

# STEP 5: inference, return patient_ids
y_true, y_prob, loss, patient_ids = trainer.inference(test_dataloader, return_patient_ids=True)

# STEP 6: get sensitive attribute array from patient_ids
sensitive_attribute_array = sensitive_attributes_from_patient_ids(base_dataset, patient_ids,
                                                                  'gender', 'F')

# STEP 7: use pyhealth.metrics to evaluate fairness
fairness_metrics = fairness_metrics_fn(y_true, y_prob, sensitive_attribute_array,
                                       favorable_outcome=0)
print(fairness_metrics)

Here is the code content for sleep_staging_sleepEDF_contrawr.py:
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.trainer import Trainer
from pyhealth.datasets import SleepEDFDataset
from pyhealth.tasks import sleep_staging_sleepedf_fn
from pyhealth.models import ContraWR, SparcNet

# step 1: load signal data
dataset = SleepEDFDataset(
    root="/srv/local/data/SLEEPEDF/sleep-edf-database-expanded-1.0.0/sleep-cassette",
    dev=True,
    refresh_cache=False,
)

# step 2: set task
sleep_staging_ds = dataset.set_task(sleep_staging_sleepedf_fn)
sleep_staging_ds.stat()

# split dataset
train_dataset, val_dataset, test_dataset = split_by_patient(
    sleep_staging_ds, [0.6, 0.2, 0.2]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)
print(
    "loader size: train/val/test",
    len(train_dataset),
    len(val_dataset),
    len(test_dataset),
)

# STEP 3: define model
model = SparcNet(
    dataset=sleep_staging_ds,
    feature_keys=["signal"],
    label_key="label",
    mode="multiclass",
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=5,
    monitor="accuracy",
)

# STEP 5: evaluate
print(trainer.evaluate(test_dataloader))


Here is the code content for drug_recommendation_eICU_transformer.py:
from pyhealth.datasets import eICUDataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import Transformer
from pyhealth.tasks import drug_recommendation_eicu_fn
from pyhealth.trainer import Trainer

# STEP 1: load data
base_dataset = eICUDataset(
    root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
    tables=["diagnosis", "medication", "physicalExam"],
    dev=False,
    refresh_cache=True,
)
base_dataset.stat()

# STEP 2: set task

from pyhealth.data import Visit, Patient


def drug_recommendation_eicu_fn(patient: Patient):
    """Processes a single patient for the drug recommendation task.

    Drug recommendation aims at recommending a set of drugs given the patient health
    history  (e.g., conditions and procedures).

    Args:
        patient: a Patient object

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key

    Examples:
        >>> from pyhealth.datasets import eICUDataset
        >>> eicu_base = eICUDataset(
        ...     root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
        ...     tables=["diagnosis", "medication"],
        ...     code_mapping={},
        ...     dev=True
        ... )
        >>> from pyhealth.tasks import drug_recommendation_eicu_fn
        >>> eicu_sample = eicu_base.set_task(drug_recommendation_eicu_fn)
        >>> eicu_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '98', '663', '58', '51']], 'procedures': [['1']], 'label': [['2', '3', '4']]}]
    """
    samples = []
    for i in range(len(patient)):
        visit: Visit = patient[i]
        conditions = visit.get_code_list(table="diagnosis")
        procedures = visit.get_code_list(table="physicalExam")
        drugs = visit.get_code_list(table="medication")
        # exclude: visits without condition, procedure, or drug code
        if len(conditions) * len(procedures) * len(drugs) == 0:
            continue
        # TODO: should also exclude visit with age < 18
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": conditions,
                "procedures": procedures,
                "drugs": drugs,
                "drugs_all": drugs,
            }
        )
    # exclude: patients with less than 2 visit
    if len(samples) < 2:
        return []
    # add history
    samples[0]["conditions"] = [samples[0]["conditions"]]
    samples[0]["procedures"] = [samples[0]["procedures"]]
    samples[0]["drugs_all"] = [samples[0]["drugs_all"]]

    for i in range(1, len(samples)):
        samples[i]["conditions"] = samples[i - 1]["conditions"] + [
            samples[i]["conditions"]
        ]
        samples[i]["procedures"] = samples[i - 1]["procedures"] + [
            samples[i]["procedures"]
        ]
        samples[i]["drugs_all"] = samples[i - 1]["drugs_all"] + [
            samples[i]["drugs_all"]
        ]

    return samples


sample_dataset = base_dataset.set_task(drug_recommendation_eicu_fn)
sample_dataset.stat()

train_dataset, val_dataset, test_dataset = split_by_patient(
    sample_dataset, [0.8, 0.1, 0.1]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

# STEP 3: define model
model = Transformer(
    dataset=sample_dataset,
    feature_keys=["conditions", "procedures"],
    label_key="drugs",
    mode="multilabel",
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=50,
    monitor="pr_auc_samples",
)

# STEP 5: evaluate
print (trainer.evaluate(test_dataloader))


Here is the code content for drug_recommendation_mimic3_micron.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import MICRON
from pyhealth.tasks import drug_recommendation_mimic3_fn
from pyhealth.trainer import Trainer

# STEP 1: load data
base_dataset = MIMIC3Dataset(
    root="/srv/local/data/physionet.org/files/mimiciii/1.4",
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
    code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
    dev=False,
    refresh_cache=True,
)
base_dataset.stat()

# STEP 2: set task
sample_dataset = base_dataset.set_task(drug_recommendation_mimic3_fn)
sample_dataset.stat()

train_dataset, val_dataset, test_dataset = split_by_patient(
    sample_dataset, [0.8, 0.1, 0.1]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

# STEP 3: define model
model = MICRON(
    sample_dataset,
    feature_keys=["conditions", "procedures"],
    label_key="drugs",
    mode="multilabel",
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=50,
    monitor="pr_auc_samples",
)

# STEP 5: evaluate
print (trainer.evaluate(test_dataloader))


Here is the code content for mortality_mimic3_adacare.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import AdaCare
from pyhealth.tasks import mortality_prediction_mimic3_fn
from pyhealth.trainer import Trainer

if __name__ == "__main__":
    # STEP 1: load data
    base_dataset = MIMIC3Dataset(
        root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        code_mapping={"ICD9CM": "CCSCM", "ICD9PROC": "CCSPROC", "NDC": "ATC"},
        dev=False,
        refresh_cache=False,
    )
    base_dataset.stat()

    # STEP 2: set task
    sample_dataset = base_dataset.set_task(mortality_prediction_mimic3_fn)
    sample_dataset.stat()

    train_dataset, val_dataset, test_dataset = split_by_patient(
        sample_dataset, [0.8, 0.1, 0.1]
    )
    train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
    val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
    test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

    # STEP 3: define model
    model = AdaCare(
        dataset=sample_dataset,
        feature_keys=["conditions", "procedures"],
        label_key="label",
        mode="binary",
        use_embedding=[True, True, True],
        embedding_dim=32,
        hidden_dim=32,
        dropout=0.5,
    )

    # STEP 4: define trainer
    trainer = Trainer(model=model)
    trainer.train(
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        epochs=50,
        monitor="roc_auc",
    )

    # STEP 5: evaluate
    print(trainer.evaluate(test_dataloader))


Here is the code content for mortality_mimic3_tcn.py:
from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import TCN
from pyhealth.tasks import mortality_prediction_mimic3_fn
from pyhealth.trainer import Trainer

if __name__ == "__main__":
    # STEP 1: load data
    base_dataset = MIMIC3Dataset(
        root="/srv/local/data/physionet.org/files/mimiciii/1.4",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        code_mapping={"ICD9CM": "CCSCM", "ICD9PROC": "CCSPROC", "NDC": "ATC"},
        dev=False,
        refresh_cache=False,
    )
    base_dataset.stat()

    # STEP 2: set task
    sample_dataset = base_dataset.set_task(mortality_prediction_mimic3_fn)
    sample_dataset.stat()

    train_dataset, val_dataset, test_dataset = split_by_patient(
        sample_dataset, [0.8, 0.1, 0.1]
    )
    train_dataloader = get_dataloader(train_dataset, batch_size=256, shuffle=True)
    val_dataloader = get_dataloader(val_dataset, batch_size=256, shuffle=False)
    test_dataloader = get_dataloader(test_dataset, batch_size=256, shuffle=False)

    # STEP 3: define model
    model = TCN(
        dataset=sample_dataset,
        feature_keys=["conditions", "procedures"],
        label_key="label",
        mode="binary",
        embedding_dim=32,
    )

    # STEP 4: define trainer
    trainer = Trainer(model=model)
    trainer.train(
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        epochs=50,
        monitor="roc_auc",
    )

    # STEP 5: evaluate
    print(trainer.evaluate(test_dataloader))


Here is the code content for sleep_staging_ISRUC_SparcNet.py:
from pyhealth.calib import calibration
from pyhealth.datasets import ISRUCDataset, get_dataloader, split_by_patient
from pyhealth.models import ContraWR, SparcNet
from pyhealth.tasks import sleep_staging_isruc_fn
from pyhealth.trainer import Trainer, get_metrics_fn

# step 1: load signal data
dataset = ISRUCDataset(
    root="/srv/local/data/trash/",
    dev=True,
    refresh_cache=False,
    # download=True,
)

print(dataset.stat())

# step 2: set task
sleep_staging_ds = dataset.set_task(sleep_staging_isruc_fn)
sleep_staging_ds.stat()
print(sleep_staging_ds.samples[0])

# split dataset
train_dataset, val_dataset, test_dataset = split_by_patient(
    sleep_staging_ds, [0.34, 0.33, 0.33]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)
print(
    "loader size: train/val/test",
    len(train_dataset),
    len(val_dataset),
    len(test_dataset),
)

# STEP 3: define model
model = SparcNet(
    dataset=sleep_staging_ds,
    feature_keys=["signal"],
    label_key="label",
    mode="multiclass",
)

# STEP 4: define trainer
trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=5,
    monitor="accuracy",
)

# STEP 5: evaluate
metrics = ['accuracy', 'f1_macro', 'f1_micro'] + ['cwECEt_adapt']
y_true_all, y_prob_all = trainer.inference(test_dataloader)[:2]
print(get_metrics_fn(model.mode)(y_true_all, y_prob_all, metrics=metrics))

# STEP 6: calibrate the model
cal_model = calibration.KCal(model, debug=True, dim=32)
cal_model.calibrate(
    cal_dataset=val_dataset,
    # Uncomment the following line if you want to re-train the embeddings
    # train_dataset=train_dataset,
)
y_true_all, y_prob_all = Trainer(model=cal_model).inference(test_dataloader)[:2]
print(get_metrics_fn(cal_model.mode)(y_true_all, y_prob_all, metrics=metrics))

Here is the code content for kdd'23_(bd)_halo_evaluator_example.py:
# -*- coding: utf-8 -*-
"""KDD'23 (BD) HALO Evaluator Example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YGel2TGDCIKQzgNwBTKZpxvHv_TVRkre
"""

from google.colab import drive
drive.mount('/content/drive')

"""###Install PyHealth"""

!git clone https://github.com/sunlabuiuc/PyHealth.git

cd PyHealth/

!git checkout synthetic_data

!git pull

pip install -e .

"""##Render Evaluation Stats"""

from pyhealth.datasets import eICUDataset
from pyhealth.data import Event
import torch
import numpy as np
from pyhealth.synthetic.halo import HALO, Processor, Trainer, Generator, Evaluator
from datetime import timedelta
import pickle
from typing import List
import os

device = "cuda" if torch.cuda.is_available() else "cpu"

ROOT = "https://storage.googleapis.com/pyhealth/eicu-demo/"
# ROOT = "/home/bdanek2/data/physionet.org/files/eicu-crd/2.0"
dataset = eICUDataset(
    dataset_name="eICU-demo",
    root=ROOT,
    tables=["diagnosis"],
    code_mapping={},
    dev=False,
    refresh_cache=False,
)

basedir = '/'
batch_size = 512

# define a way to make labels from raw data
simple_label_fn_output_size = 1
def simple_label_fn(**kwargs):
    pdata = kwargs['patient_data']
    return (0) if pdata.death_datetime else (1) # 0 for dead, 1 for alive

def handle_diagnosis(event: Event):
    """to reduce the complexity of the model, in this example we will convert granular ICD codes to more broad ones (ie 428.3 --> 428)"""
    split_code = event.code.split('.')
    assert len(split_code) <= 2
    return split_code[0]

LAB_DIGITIZATION_BINS = [0, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]
def handle_lab(event: Event):

    def digitize_lab_result(x):
        return np.digitize(x, LAB_DIGITIZATION_BINS)

    lab_name = event.code
    lab_value = event.attr_dict['lab_result']
    lab_unit = event.attr_dict['lab_measure_name_system']

    return f"{lab_name}_{lab_unit}_{digitize_lab_result(lab_value)}"

# define value handlers
event_handlers = {}
event_handlers['diagnosis'] =  handle_diagnosis
event_handlers['lab'] =  handle_lab

continuous_value_handlers = {}

# handle discretization of time (forward and reverse)
bins = [0.5, 1, 1.5, 2, 2.5, 3, 4] + list(range(5, 85, 5)) # model both readmission and age in the same representation
time_vector_length = len(bins) + 1
def handle_time(t: timedelta):
    year = t.days / 365
    bin = np.digitize(year, bins)
    vect = np.zeros((time_vector_length))
    vect[bin] = 1
    return vect

def handle_digitized_time(time_gap: int):
    """Reverse the digitization of datetime objects."""

    # recover time information
    time_digital = time_gap.nonzero()[0][0] if sum(time_gap) else 0 # use first time index; there may be more if the model is trained poorly
    num_years = bins[time_digital] if time_digital < len(bins) else bins[-1]

    return num_years

# --- define processor ---
processor = Processor(
    dataset=dataset,
    use_tables=None,
    event_handlers=event_handlers,
    continuous_value_handlers=continuous_value_handlers,
    time_handler=handle_time,
    time_vector_length=time_vector_length,
    label_fn=simple_label_fn,
    label_vector_len=simple_label_fn_output_size,
)

print("Processor results in vocab len, max visit num:", processor.total_vocab_size, processor.total_visit_size)

# --- define model ---
model = HALO(
    n_ctx=processor.total_visit_size,
    total_vocab_size=processor.total_vocab_size,
    device=device
)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# --- train model ---
trainer = Trainer(
    dataset=dataset,
    model=model,
    processor=processor,
    optimizer=optimizer,
    checkpoint_dir=f'{basedir}/model_saves',
    model_save_name='eval_developement_test'
)
s = trainer.set_basic_splits(from_save=True, save=False)

# --- define generator ---
generator = Generator(
    model=None,
    processor=processor,
    batch_size=batch_size,
    device=device,
    save_dir=basedir,
    save_name="synthetically_generated_mortality_data", # save at `synthetically_generated_mortality_data.pkl`
    handle_digital_time_gap=handle_digitized_time
)

# --- prerequisites for evaluator ---
def pathfn(plot_type: str, label: List):
    prefix = os.path.join(generator.save_dir, 'plots')

    label = labels[label] if label in labels else 'all_labels'
    label = label.replace('.', '').replace(' ', '').lower()
    path_str = f"{prefix}_{plot_type}_{label}"

    return path_str

labels = {
    (1, ): 'Alive',
    (0, ): 'Expired',
}

evaluator = Evaluator(generator=generator, processor=processor)

# --- plotting ---
stats = pickle.load(open('/content/drive/MyDrive/kdd_tutorial/stats.pkl', 'rb'))
r = evaluator.generate_plots(stats['source_stats'], stats['synthetic_stats'], "Source Data", "Synthetic Data", get_plot_path_fn=pathfn, compare_labels=labels)



Here is the code content for tutorial_5_pyhealth_metrics.py:
# -*- coding: utf-8 -*-
"""Tutorial 5: pyhealth.metrics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mrs77EJ92HwMgDaElJ_CBXbi4iABZBeo

### **Preparation**
- install pyhealth alpha version
"""

!pip install pyhealth

"""### **Instruction on [pyhealth.metrics](https://pyhealth.readthedocs.io/en/latest/api/metrics.html)**
- **[README]**: This module contains the metrics for evaluating
  - [multiclass classification](https://pyhealth.readthedocs.io/en/latest/api/metrics/pyhealth.metrics.multiclass.html)
  - [multilabel classification](https://pyhealth.readthedocs.io/en/latest/api/metrics/pyhealth.metrics.multilabel.html)
  - [binary classification](https://pyhealth.readthedocs.io/en/latest/api/metrics/pyhealth.metrics.binary.html)

### **1. binary classification metrics**
- User specifies the true label list `y_true` and the predicted logits `y_prob`.
- User specifies `metrics`, which is a list of metrics that we want to calculate. Below, we use all the possible metrics for binary classification.
- **Example**: we use `np.random` to generate the `y_true` and `y_prob` below.
"""

import numpy as np

from pyhealth.metrics.binary import binary_metrics_fn

# randomly generated true labels and predicted probability
y_true = np.random.randint(2, size=100000)
y_prob = np.random.random(size=100000)

all_metrics = [
    "pr_auc",
    "roc_auc",
    "accuracy",
    "balanced_accuracy",
    "f1",
    "precision",
    "recall",
    "cohen_kappa",
    "jaccard",
]

binary_metrics_fn(y_true, y_prob, metrics=all_metrics)



"""### **2. multiclass classification metrics**
- User specifies the true label list `y_true` and the predicted logits `y_prob`.
- User specifies `metrics`, which is a list of metrics that we want to calculate. Below, we use all the possible metrics for multiclass classification.
- **Example**: we use `np.random` to generate the `y_true` and `y_prob` below.
"""

from pyhealth.metrics.multiclass import multiclass_metrics_fn

# randomly generated true labels and predicted probability
y_true = np.random.randint(4, size=100000)
y_prob = np.random.randn(100000, 4)
y_prob = np.exp(y_prob) / np.sum(np.exp(y_prob), axis=-1, keepdims=True)

all_metrics = [
    "roc_auc_macro_ovo",
    "roc_auc_macro_ovr",
    "roc_auc_weighted_ovo",
    "roc_auc_weighted_ovr",
    "accuracy",
    "balanced_accuracy",
    "f1_micro",
    "f1_macro",
    "f1_weighted",
    "jaccard_micro",
    "jaccard_macro",
    "jaccard_weighted",
    "cohen_kappa",
]

multiclass_metrics_fn(y_true, y_prob, metrics=all_metrics)



"""### **3. multilabel classification metrics**
- User specifies the true label list `y_true` and the predicted logits `y_prob`.
- User specifies `metrics`, which is a list of metrics that we want to calculate. Below, we use all the possible metrics for multilabel classification.
- **Example**: we use `np.random` to generate the `y_true` and `y_prob` below.
"""

from pyhealth.metrics.multilabel import multilabel_metrics_fn

# randomly generated true labels and predicted probability
y_true = np.random.randint(2, size=(10000, 100))
y_prob = np.random.random(size=(10000, 100))

all_metrics = [
    "roc_auc_micro",
    "roc_auc_macro",
    "roc_auc_weighted",
    "roc_auc_samples",
    "pr_auc_micro",
    "pr_auc_macro",
    "pr_auc_weighted",
    "pr_auc_samples",
    "accuracy",
    "f1_micro",
    "f1_macro",
    "f1_weighted",
    "f1_samples",
    "precision_micro",
    "precision_macro",
    "precision_weighted",
    "precision_samples",
    "recall_micro",
    "recall_macro",
    "recall_weighted",
    "recall_samples",
    "jaccard_micro",
    "jaccard_macro",
    "jaccard_weighted",
    "jaccard_samples",
    "hamming_loss",
]

multilabel_metrics_fn(y_true, y_prob, metrics=all_metrics)



"""If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""

Here is the code content for pipeline_6_[knowledge_graph]_link_prediction.py:
# -*- coding: utf-8 -*-
"""Pipeline 6: [Knowledge Graph] Link Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Refge4j4H1DWf3lopYwyY29WcNGOC_Kq
"""

# ! rm -rf PyHealth

! git clone --branch kg_embedding https://github.com/sunlabuiuc/PyHealth.git
! cd PyHealth; pip install .

"""### **Step 1: Load dataset**
- **[README]:** We call [pyhealth.datasets](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) to process and obtain the dataset.
  - `root` is the arguments directing to the data folder.
  - `dev`: if set `True`, will only load a smaller portion of the knowledge graph.
- **[Next Step]:** This `pyhealth.datasets` object will be used in **Step 2**.
"""

from pyhealth.datasets import UMLSDataset

umls_ds = UMLSDataset(
    root="https://storage.googleapis.com/pyhealth/umls/",
    dev=True,
    refresh_cache=True
)

print(umls_ds.stat())
print("Relations in KG:", umls_ds.relation2id)

"""### **Step 2: Define task**
- **[README]:** This step assigns a **task function** to the dataset for data loading [pyhealth.tasks](https://pyhealth.readthedocs.io/en/latest/api/tasks.html). The **task function** specifics how to process data into a set of samples for the downstream machine learning models.
- **[Next Step]:** This `pyhealth.tasks` object will be used in **Step 3**.
"""

from pyhealth.tasks import link_prediction_fn

umls_ds = umls_ds.set_task(link_prediction_fn, negative_sampling=256, save=False)

umls_ds.stat()

print(umls_ds[0])

from pyhealth.datasets import split, get_dataloader

train_dataset, val_dataset, test_dataset = split(umls_ds, [0.9, 0.05, 0.05])
train_loader = get_dataloader(train_dataset, batch_size=256, shuffle=True)
val_loader = get_dataloader(val_dataset, batch_size=16, shuffle=False)
test_loader = get_dataloader(test_dataset, batch_size=16, shuffle=False)

"""### **Step 3: Select ML Model**
- **[README]:** We initialize an ML model for the healthcare task by calling [pyhealth.models](https://pyhealth.readthedocs.io/en/latest/api/models.html).
- **[Next Step]:** This `pyhealth.models` object will be used in **Step 4**.
- **[Other Use Case]:** Our `pyhealth.models` object is as general as any instance from `torch.nn.Module`. Users may use it separately for supporting any other customized pipeline.
"""

from pyhealth.models.kg import TransE, RotatE, ComplEx, DistMult

model = RotatE(
    dataset=umls_ds,
    e_dim=600,
    r_dim=300,
)

"""### **Step 4: Model Training**
- **[README]:** We call our [pyhealth.train.Trainer](https://pyhealth.readthedocs.io/en/latest/api/trainer.html) to train the model by giving the `train_loader`, the `val_loader`, val_metric, and specify other arguemnts, such as epochs, optimizer, learning rate, etc. The trainer will automatically save the best model and output the path in the end.
- **[Next Step]:** The best model will be used in **Step 5** for evaluation.

"""

from pyhealth.trainer import Trainer

trainer = Trainer(model=model, device='cuda', metrics=['hits@n', 'mean_rank'])
trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=10,
    steps_per_epoch=100,
    evaluation_steps=10,
    optimizer_params={'lr': 1e-3},
    monitor='mean_rank',
    monitor_criterion='min'
)

"""### **Step 5: Evaluation**

"""

trainer.evaluate(test_loader)

"""### **Step 6: Inference (use cases)**"""

from pyhealth.medcode import InnerMap
umls_code_map = InnerMap.load("UMLS")

head = 'C0070122'
relation = "may_be_treated_by"

model.to('cpu')
result_eid = model.inference(
    head=umls_ds.entity2id[head],
    relation=umls_ds.relation2id[relation],
    tail=None,
    top_k=3
    )

print(f"Input Head: {head} - {umls_code_map.lookup(head)}")
print(f"Input Relation: {relation}")

print("Tail Prediction:")
for idx, eid in enumerate(result_eid):
    tail = umls_ds.id2entity[eid]
    print(f"{idx}: {tail} - {umls_code_map.lookup(tail)}")

tail = 'C0677949'
relation = "disease_mapped_to_gene"

result_eid = model.inference(
    head=None,
    relation=umls_ds.relation2id[relation],
    tail=umls_ds.entity2id[tail],
    top_k=3
    )

print(f"Input Tail: {tail} - {umls_code_map.lookup(tail)}")
print(f"Input Relation: {relation}")

print("Head Prediction:")
for idx, eid in enumerate(result_eid):
    head = umls_ds.id2entity[eid]
    print(f"{idx}: {head} - {umls_code_map.lookup(head)}")



Here is the code content for kdd'23_(bd)_synthetic_medical_record_generation_via_halo.py:
# -*- coding: utf-8 -*-
"""KDD'23 (BD) Synthetic Medical Record Generation via HALO

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12fGLkKgGSSHvgaMwrqrCLMKlgRNflqZ0

# Dataset
In the tutorial we will be using the eICU Collaborative Research Database available at: https://physionet.org/content/eicu-crd/2.0/

>The eICU Collaborative Research Database, a freely available multi-center database for critical care research. Pollard TJ, Johnson AEW, Raffa JD, Celi LA, Mark RG and Badawi O. Scientific Data (2018). DOI: http://dx.doi.org/10.1038/sdata.2018.178. Available from: https://www.nature.com/articles/sdata2018178


This dataset represents a real life Electronic Health Record for ICU Patients. It contains data from several different sources, including ICD Diagnosis Codes, Labs, Perscriptions, Treatments, Physical Examinationsm, Respiratory Care.

For the purpose of this tutorial, we will use the Diagnosis and Lab data present in the dataset. In its native form, the eICU dataset contains this information in tables (stored as CSVs). We use the PyHealth parser to convert the raw table form into a first class Python object.

For the purposes of the tutorial, we simply need to place our relevant medical tables as `.csv` files into a single folder, and point PyHealth at the folder for processing the dataset.
"""

!wget -r -N -c -np --user [YOUR USERNAME] --ask-password https://physionet.org/files/eicu-crd/2.0/lab.csv.gz

!wget -r -N -c -np --user [YOUR USERNAME] --ask-password https://physionet.org/files/eicu-crd/2.0/diagnosis.csv.gz

!wget -r -N -c -np --user [YOUR USERNAME] --ask-password https://physionet.org/files/eicu-crd/2.0/hospital.csv.gz

!wget -r -N -c -np --user [YOUR USERNAME] --ask-password https://physionet.org/files/eicu-crd/2.0/patient.csv.gz

from google.colab import drive
drive.mount('/content/drive')

# write the downloaded dataset somewhere where it will persist when we restart the runtime
import shutil

data_folder = '/content/drive/MyDrive/kdd_tutorial/eICU'
download_folder = '/content/physionet.org/files/eicu-crd/2.0'

shutil.copytree(download_folder, data_folder)

!cd /content/drive/MyDrive/kdd_tutorial/eICU; gunzip -dk *.gz

!ls /content/drive/MyDrive/kdd_tutorial/eICU

"""# Download PyHealth"""

# # optional
# from google.colab import drive
# drive.mount('/content/drive')

!git clone https://github.com/sunlabuiuc/PyHealth.git

cd PyHealth/

!git checkout synthetic_data

!git pull

pip install -e .

"""# Synthetic Medical Record Generation via HALO

Adaptation of method:

> Theodorou, Brandon, Cao Xiao, and Jimeng Sun. “Synthesize Extremely High-Dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model.” Preprint. In Review, March 10, 2023. https://doi.org/10.21203/rs.3.rs-2644725/v1.




"""

from pyhealth.synthetic.halo import HALO, Processor, Trainer, Generator, Evaluator

"""###Process raw dataset into PyHealth BaseEHRDataset
`eICUDataset` inherits `BaseEHRDataset` https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.BaseEHRDataset.html
"""

from pyhealth.datasets.eicu import eICUDataset

dataset_source = "https://storage.googleapis.com/pyhealth/eicu-demo/"
# dataset_source ="/content/drive/MyDrive/kdd_tutorial/eICU"

dataset = eICUDataset(
    root=dataset_source,
    tables=["diagnosis", "lab"],
    code_mapping={}
)

dataset.stat()

"""###Processing the PyHealth dataset
The HALO model configuration is split into two components:
* hyper parameters that are conditioned on the provided dataset
* hyper parameters that are user defined

The `halo.Processor` module serves the purpose of computing parameters latent to the dataset. Practically, this module is responsible for allowing the efficient mapping from longitudinal medical events to a vocabulary for the language model. (See section 3.1 of https://pubmed.ncbi.nlm.nih.gov/36945542/)


This module also provides some utility functions such as:
* get_batch
* convert samples from PyHealth `data` objects to a vector representation suitable for training

To support this processing, several user defined arguments must be provided. These arguments are non-trivial, but allow the remainder of the HALO API to be straightforward.

**Event Handlers** server the purpose of unpacking a `pyhealth.data.Event` ([docs](https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Event.html)). The default event handler which is applied to each Event is:
```
default_event_handler = lambda event: event.code
```
For events originating from a table that produces codes, this is a perfectly reasonable handler. In the case that other attributes on the `Event` instance are better suitable for your synthetic data application, the handler serves the purpose of giving user a way to access those values.

example:
```
# the following event handler is applied to every code originating from the `medication` table
def handle_medication_values(event: Event):
  return lower(event.code).replace(' ', '')

event_handlers = {
  'medication': handle_medication_values
}
```
In our particular example we want to convert lab events into a python primitive which serializes lab name, measurement units, and a digital lab value. The digitization process is typically handled in the continuous value handlers (next section), however since the eICU dataset contains several varieties of labs within the same table, we need to record lab metadata here.
"""

import numpy as np
from pyhealth.data import Event

LAB_DIGITIZATION_BINS = list(range(0, 121, 3))

# handle continuous value discretization
def handle_lab(event: Event):

    def digitize_lab_result(x):
      return np.digitize(x, LAB_DIGITIZATION_BINS)

    lab_name = event.code
    lab_value = event.attr_dict['lab_result']
    lab_unit = event.attr_dict['lab_measure_name_system']

    return f"{lab_name}_{lab_unit}_{digitize_lab_result(lab_value)}"

"""When handling diagnosis codes, for the sake of the tutorial speed, we will focus on generating the highest level ICD codes by converting granular ICD codes to more broad ones (ie 428.3 --> 428). For more information on how code mappings, see https://pyhealth.readthedocs.io/en/latest/api/medcode.html"""

def handle_diagnosis(event: Event):
      split_code = event.code.split('.')
      assert len(split_code) <= 2
      return split_code[0]

event_handlers = {
    'diagnosis': handle_diagnosis,
    'lab': handle_lab
}

"""**Continuous Value Handlers** serve the purpose of converting continuous values present in events to categorical values. This interface represents a stage step of user processing for generating a vocabulary, if it is needed. If no continuous value handlers are provided, none will be applied.

The implementation for the example we bundle lab name, binned lab value, and lab measurement units into one value in the event handler already, so there is no need to do so in the continuous value handler.

"""

continuous_value_handlers = {}

"""**Time Gap Handlers** serve the purpose of converting time values into categorical values/bins. In the PyHealth HALO implementation, we are interested in correctly representing the gap between visits, so that the synthetic data generation model can generate visits with reasonable periods of time between them.
```
given visits i, j:
inter_visit_gap = visit_j.admission_time - visit_i.discharge_time
```
This function should expect a datetime.timedelta value, and produce a vector representing the time multihot.
"""

from datetime import timedelta
# handle discretization of time

# model both readmission and patient age in the same representation through the
# nonlinear bucket width vector:
TIME_BINS = [0.5, 1, 1.5, 2, 2.5, 3, 4] + list(range(5, 85, 5))

time_vector_length = len(TIME_BINS) + 1

def handle_time(t: timedelta):
    """Define the digitization of datetime objects."""
    year = t.days / 365
    bin = np.digitize(year, TIME_BINS)
    vect = np.zeros((time_vector_length))
    vect[bin] = 1
    return vect

"""Similar to how digitized continuous event values are reversed, the inter-visit time gap should be reversed. To do this, we use the same vector for digitization used in the processing stage of the halo pipeline, `TIME_BINS`.


"""

from datetime import date, datetime

def handle_digitized_time(time_gap: int):
    """Reverse the digitization of datetime objects."""

    # may occur when a model is not trained completely
    time_digital = 0
    if sum(time_gap):
        time_digital = time_gap.nonzero()[0][0]

    # clip the number of years to the maximum time bucket
    num_years = TIME_BINS[time_digital] if time_digital < len(TIME_BINS) else TIME_BINS[-1]

    return num_years

"""Although it is not necessary to have the vector of time interavls used for digitization (`TIME_BINS`) defined globally here, it is a good practice, since we can reuse the same vector of intervals for processing the syntehtic dataset (shown later in tutorial).

A simple one-hot representation of time is above, but more sophisticated representations can be implemented within this framework.
```
bins_years = list(range(0, 3))
bins_days = list(range(0, 365, 50)) # 7 bins, each bin is 50 days

time_vector_length = len(bins_years) + len(bins_days) + 1
def handle_time_hierarchical(t: timedelta):
    multi_hot_vect = np.zeros((time_vector_length))
    
    years = int(t.years)
    y = np.digitize(years, bins_years)
    multi_hot_vect[y] = 1

    days = int(t.days - 365*y)    
    d = np.digitize(days, bins_days
    multi_hot_vect[len(bins_years) + d] = 1
    
    return multi_hot_vect

v = handle_time_hierarchical(timedelta(days=400))

>>> print(v)
>>> [1, 0, 0, 1, 0, 0, 0, 0, 0, 0]
```

Accompanying the `time_handler` parameter is the `time_vector_length` parameter. This value is typically just the number of categorical values that can be produced by the handle_time value + 1.
In the above code snippets, the values are below (respectivley)
```
time_vector_length = len(bins) + 1
```
```
time_vector_length = len(bins_years) + len(bins_days) + 1
```
These values have already been defined in the handler methods, since we use them in generating the function output.

**Label function** is a function which converts a patient record into a label vector. Defining a "label" is up to the user, but from the perspective of the HALO generative model, the patient visit events, and inter-visit gaps are conditioned on the patient label. Thus the label should have some latent relationship to visit patterns or events.

An example of a patient label may often be a patient phenotype, such as a disease code, a patient state, like their expiry state, or a patient demographic field like the patient gender.

The patient label function is called like:
```
label_fn(patient_data=pdata)
```
and thus when defining a label function, one should expect a `pyhealth.data.Patient` event.
```
def example_label_fn(patient_data: Patient): -> List[int]
  # do something
  return patient_data_as_vector
```
Similar to the time vector, it is necessary to provide the vector output length of the provided `label_fn`
"""

from pyhealth.data import Patient
# label_fn produces a binary label
simple_label_fn_output_size = 1

# patients will have a death_datetime attribute if they are no longer alive
# read label like "is alive?"
def simple_label_fn(**kwargs):
        pdata: Patient = kwargs['patient_data']
        return (0) if pdata.death_datetime else (1)

processor = Processor(
      dataset=dataset,
      use_tables=None,
      event_handlers=event_handlers,
      continuous_value_handlers=continuous_value_handlers,
      time_handler=handle_time,
      time_vector_length=time_vector_length,
      label_fn=simple_label_fn,
      label_vector_len=simple_label_fn_output_size
  )

"""# Define the HALO model
The model parameters have been computed prior using the `halo.Processor` module. For advanced users, a `halo.Configuration` object can simply be passed in, and all settings will be set based on the `halo.Configruation` specificaiton.

```
model = pyhealth.synthetic.halo.HALO(config=halo.Configuration(...))
```
"""

import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
model = HALO(
    n_ctx=processor.total_visit_size,
    total_vocab_size=processor.total_vocab_size,
    device=device
)

# define model optimizer:
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
# the loss function is present in the `HALOModel` class

"""# Train the Model
Using the `halo.HALOTrainer` module, train the HALO model. Pass in the pyhealth.dataset.BaseEHRDataset, halo.HALOProcessor, halo.HALOModel, torch.optim optimizer, and strings used for writing checkpoint files.

`checkpoint_name` and `checkpoint_path` will be used when writing checkpoints. The checkpoint location will be at

```
f"{self.checkpoint_path}.pkl"
```
"""

import os

# used for saving training/test/val splits, as well as model checkpoints.
save_path = '/content/kdd_tutorial/halo_saves'

trainer_checkpoint_path = os.path.join(save_path, 'trainer_checkpoint')

trainer = Trainer(
    dataset=dataset,
    model=model,
    processor=processor,
    optimizer=torch.optim.Adam(model.parameters(), lr=1e-4),
    checkpoint_dir=trainer_checkpoint_path,
    model_save_name='model_checkpoint' # model will be saved at `model_checkpoint.pt`
)

"""##Conduct Model Training
By defining training batch size, and forming the train, test, eval splits.


By calling `trainer.set_basic_splits()` the trainer instance variables representing the train, test, eval split are set.


In this api, each `eval_period`, and at the end of an `epoch` the model is evaluated, and if the validation loss is lower than prior iteration, has a checkpoint generated at `checkpoint_path`. The model training terminates after `epoch` epochs are compelte, or once the loss function produced by the `trainer.eval` routine no longer produces a lower loss. The `patience` parameter configures the number of non-decreasing loss iterations that will be allowed.

"""

batch_size = 512
splits = trainer.set_basic_splits()
print("Train, test val split sizes:", [len(s) for s in splits])

trainer.train(
    batch_size=batch_size,
    epoch=5,
    patience=5,
    eval_period=float('inf')
)

"""## Load The Trained HALO Model
After completing training, load the best model.

"""

state_dict = torch.load(trainer.get_model_checkpoint_path(), map_location=device)
model.load_state_dict(state_dict['model'])
model.to(device)

"""# Generate Synthetic Medical Records

###Define the Generator
To generate samples, define the generator using the trained `model`, the `processor`, a batch size (preferably the one used during training), `device`, and `save_path` which will be used to deposit the saved synthetic data samples.
"""

synthetic_data = os.path.join(save_path, 'synthetic_data')
generator = Generator(
    model=model,
    processor=processor,
    batch_size=batch_size,
    device=device,
    save_dir=save_path,
    save_name='synthetically_generated_mortality_data',
    handle_digital_time_gap=handle_digitized_time
)

"""##Generate Synthetic Samples
By specifying the label vector, and quantity of samples desired for that label vector (defined earlier in the `HALOProcessor` stage), the `HALOGenerator.generate_conditioned` function will use the trained model to produce synthetic patient records.


In the below sample, we want 1000 samples of label `(1)` and label `(0)`.
"""

labels = [((1), 500), ((0), 500)]
synthetic_dataset = generator.generate_conditioned(labels)

import pickle
import pprint

synthetic_dataset = pickle.load(file=open(generator.save_path, 'rb'))
pprint.pprint(synthetic_dataset[2])

"""# Evaluate Synthetic Samples
Determining the value of synthetic samples can be done using the `Evaluator` module. This module generates statistics associated with the training/source dataset, as well as the synthetic dataset, and compares their probability densities. Concretely, the unigram, bigram, and sequential bigrams are computed. Plots demonstrating the model utility are then generated to allow for empirical evaluation of the synthetic samples relative to the model.
"""

evaluator = Evaluator(generator=generator, processor=processor)

"""To conduct evaluation, the a `source` dataset, `synthetic` dataset, `get_plot_path` function, and `compare_label` list must be defined.

`get_plot_path` is an optional callable function responsible for generating plot path strings. The default path generation function uses a formatted label_vector to save plot files.
```
def default_path_fn(self, plot_type, label_vector):
    label_string = str(tuple(label_vector))
    path = f"./pyhealth_halo_eval_{plot_type}_{label_string}"
    return path.replace('.', '').replace(' ', '').lower()
```

The `compare_label` argument specifies the labels for which a plot should be drawn. If none are specified, then the plot for all labels will be generated. The plot for all labels will also be generated, in addition to the provided list, by default.
"""

from typing import List
import os

labels = {
    (1, ): 'alive',
    (0, ): 'expired'
}

def pathfn(plot_type: str, label: List):
    prefix = f"{os.getcwd()}/halo_eval_plots"

    label = labels[label] if label in labels else 'all_labels'
    label = label.replace('.', '').replace(' ', '').lower()
    path_str = f"{prefix}_{plot_type}_{label}"

    return path_str

stats = evaluator.evaluate(
    source=trainer.test_dataset[:10], # evaluate against holdout dataset
    synthetic=synthetic_dataset[:10],
    get_plot_path_fn=pathfn,
    compare_label=list(labels.keys()),
)

"""All statistics are available in `stats`, including the paths where plots are generated."""

stats.keys()

"""# Output to Native Formats
To complete the user workflow for HALO, we will end the tutorial by converting the dataset into types suitable for downstream tasks, such as other PyHealth applications and placing synthetic records into medical systems.
"""

# # import a dataset from a model which has had more training time
# # only contains diagnosis codes
# synthetic_dataset = pickle.load(open('/content/synthetically_generated_mortality_data.pkl', 'rb'))

pprint.pprint(synthetic_dataset[0])

"""The internal format serves the purpose of evaluating the synthetic dataset efficiently, but is not useful from a real-world perspective on its own. We provide an interface for converting this format into PyHealth objects, as well as building CSVs, representative of the original format the dataset came in.

## Translating Labels
It's useful to represent labels in a vector format for training. To translate the vector format into a human readable format, define a mapping to facilitate the translation.

The translation mapping will be applied as such:
```
raw_label = dataset[i]['label']
translated_label = mapping[raw_label]
```

Practically, this function resemble the `pathfn` defined earlier to write plots to the file system.
"""

labels = {
    (1, ): 'Alive',
    (0, ): 'Expired',
}

"""## Translating Digitized Values
The digitization of continuous values and table specific handlig should be reversed in this step.

### Reversing Events (Continuous & Discrete)
Each event type (originating form a particular table) should have it's own handler. Recall that in the Dataset Processing stage, events were handled through a custom event handler, or the default event handler (`lambda event: event.code`).

The return value of each handler should be a `Dict` containing all the `pyhealth.data.Event` attributes which the PyHealth event should be initialized with. The way an event handler is called is below:
```
event_data = event_handlers[event_table](event_data)
event = Event(
    visit_id=unique_visit_id,
    patient_id=patient_id,
    timestamp=current_visit_time,
    **event_data
)
```
At a minimum, the `table` and `code` field must be provided; all additional dictionary attributes on the return type are optional.
"""

def diagnosis_handler(diagnosis_event):
    return {
        'code': diagnosis_event,
        'table': 'diagnosis',
        'vocabulary': 'ICD9/10CM_SYNTHETIC'
    }

"""Recall that lab are serialized through the string
`f"{event.code}_{unit}_{binned_lab_result}`.
"""

def lab_handler(lab_event):

    def reverse_lab_digitization(lab_index):
        # clip to the largest bin if overflow
        return LAB_DIGITIZATION_BINS[lab_index] if lab_index < len(LAB_DIGITIZATION_BINS) else LAB_DIGITIZATION_BINS[-1]

    lab_event_data = lab_event.split('_')
    return {
        'code': lab_event_data[0],
        'lab_measure_name_system': lab_event_data[1],
        'lab_result': reverse_lab_digitization(int(lab_event_data[2])),
        'table': 'lab',
        'vocabulary': 'eICU_LABNAME_SYNTHETIC'
    }

event_handlers = {
    'diagnosis': diagnosis_handler,
    'lab': lab_handler
}

"""## Convert to PyHealth

Call the `halo.Generator` function, `convert_ehr_to_pyhealth` with the appropriate handlers to generate a PyHealth representation of the synthetic dataset
"""

patient_records = Generator.convert_ehr_to_pyhealth(synthetic_dataset, event_handlers, datetime.now(), labels)

print(len(patient_records))
display(patient_records[:10])

visits = patient_records[2].visits
first_visit_key = list(visits.keys())[0]

display(visits[first_visit_key])
display(visits[first_visit_key].encounter_time)
display(visits[first_visit_key].get_event_list('diagnosis')[:10])

"""## Convert to CSV
Call the appropriate `halo.Generator` function to conver the PyHealth representation to the native format used for representing the raw data.
"""

unused_columns = ['death_datetime', 'gender', 'ethnicity']

patient_records_csv = Generator.pyhealth_to_tables(patient_records)
print(patient_records_csv.keys())

patient_records_csv['lab'].groupby('visit_id').first()

patient_records_csv['diagnosis'].groupby('visit_id').first()

patient_records_csv['patients'].rename(columns={'patient_label': 'patient_mortality'}, inplace=True)
patient_records_csv['patients'].drop(columns=unused_columns)

patient_records_csv['patients']['patient_mortality'].value_counts()



Here is the code content for tutorial_0_pyhealth_data.py:
# -*- coding: utf-8 -*-
"""Tutorial 0: pyhealth.data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y9PawgSbyMbSSMw1dpfwtooH7qzOEYdN
"""

!pip install pyhealth

"""### **Instruction on [pyhealth.data](https://pyhealth.readthedocs.io/en/latest/api/data.html)**
- **[README]**: This module defines the basic data structures in PyHealth.
- **[Structures]**:
  - `Event` is the info structure for a single clinical event
  - `Visit` is the info structure for a single clinical visit
  - `Patient` is the info structure for a single patient

### **[pyhealth.data.Event](https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Event.html)**
- Contains information about a single event.

- **[Arguments]**:
  - ``code`` – str, code of the event (e.g., “428.0” for heart failure)
  - ``table`` – str, str, name of the table where the event is recorded. E.g., “DIAGNOSES_ICD”.
  - ``vocabulary`` – str, vocabulary of the code (e.g., ‘ICD9CM’, ‘ICD10CM’, ‘NDC’).
  - ``visit_id`` – str, unique identifier of the visit.
  - ``patient_id`` – str, unique identifier of the patient.
  - ``timestamp`` – Optional[datetime], timestamp of the event. Defaults to None.
  - ``**attr`` – optional attributes of the event. Attributes to add to visit as key=value pairs.
- **[Attributes]**: to show additional attributes
    - ``attr_dict``: show the information stored in ****attr**
"""

from pyhealth.data import Event
from datetime import datetime

# create an event
event1 = Event(
    code="428.0",
    table="DIAGNOSES_ICD",
    vocabulary="ICD9CM",
    visit_id="v001",
    patient_id="p001",
    timestamp=datetime.now(),
)

print(event1)

"""You can also add any additional attributes as key=value pairs."""

# event2 contains an additional attributes
event2 = Event(
    code="00069153041",
    table="PRESCRIPTIONS",
    vocabulary="NDC",
    visit_id="v001",
    patient_id="p001",
    timestamp=datetime.now(),
    active_on_discharge = True,
)

event2.attr_dict

"""**TODO:** create the following event:

- code: 00069153041
- table: PRESCRIPTIONS
- vocabulary: NDC
- visit_id: 130744
- patient_id: 103
- timestamp: 2019-08-12 00:00:00
- dosage: 250 mg

Note that try to create 2 additional attributes dosage and dosage_unit.
"""

event3 = Event(
    code="00069153041",
    table="PRESCRIPTIONS",
    vocabulary="NDC",
    visit_id="130744",
    patient_id="103",
    timestamp=datetime.fromisoformat("2019-08-12 00:00:00"),
    dosage = "250mg",
)

event3.attr_dict

"""### **[pyhealth.data.Visit](https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Visit.html)**
- Contains information about a single visit.

- **[Arguments]**:
  - ``visit_id`` – str, unique identifier of the visit.
  - ``patient_id`` – str, unique identifier of the patient.
  - ``encounter_time`` – Optional[datetime], timestamp of visit’s encounter. Defaults to None.
  - ``discharge_time`` – Optional[datetime], timestamp of visit’s discharge. Defaults to None.
  - ``discharge_status`` - Optional, patient’s status upon discharge. E.g., “Alive”, “Dead”. Defaults to None.
  - ``**attr`` – optional attributes of the visit. Attributes to add to visit as key=value pairs.

- **[Atrributes]**
  - `available_tables`: Returns a list of available tables for the visit.
  - `num_events`: Returns the total number of events in the visit.
- **[Methods]**
  - `add_event()`: Adds an event to the visit.
  - `get_evet_list()`: Returns a list of events from a specific table.
  - `get_code_list()`: Returns a list of codes from a specific table.


"""

from pyhealth.data import Visit
from datetime import datetime, timedelta

# create a visit
visit1 = Visit(
    visit_id="v001",
    patient_id="p001",
    encounter_time=datetime.now() - timedelta(days=2),
    discharge_time=datetime.now() - timedelta(days=1),
    discharge_status='Alive',
)

# add events
visit1.add_event(event1)
visit1.add_event(event2)

# summary
print(visit1)

print(visit1.available_tables)
print(visit1.num_events)

print(visit1.get_event_list('DIAGNOSES_ICD'))
print(visit1.get_code_list('DIAGNOSES_ICD'))

# create another instance
event3 = Event(
    code="585.9",
    table="DIAGNOSES_ICD",
    vocabulary="ICD9CM",
    visit_id="v002",
    patient_id="p001",
    timestamp=datetime.now(),
)

visit2 = Visit(
    visit_id="v002",
    patient_id="p001",
    encounter_time=datetime.now() - timedelta(days=1),
    discharge_time=datetime.now(),
    discharge_status='Dead',
)

# add events
visit2.add_event(event3)

# summary
print (visit2)

"""**TODO:** Gets a list of codes from "DIAGNOSES_ICD" table in visit1."""

visit1.get_code_list(table="DIAGNOSES_ICD")

"""### **[pyhealth.data.Patient](https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Patient.html)**
- Contains information about a single patient.

- **[Arguments]**:
  - ``patient_id`` – str, unique identifier of the patient.
  - ``birth_datetime`` – Optional[datetime], timestamp of patient’s birth. Defaults to None.
  - ``death_datetime`` – Optional[datetime], timestamp of patient’s death. Defaults to None.
  - ``gender`` – Optional, gender of the patient. E.g., “M”, “F”. Defaults to None.
  - ``ethnicity`` – Optional, ethnicity of the patient. E.g., “White”, “Black or African American”, “American Indian or Alaska Native”, “Asian”, “Native Hawaiian or Other Pacific Islander”. Defaults to None.
  - ``**attr`` – optional attributes of the patient. Attributes to add to patient as key=value pairs.

- **[Methods]**
  - `add_visit()`: Adds a visit to the patient.
  - `add_event()`: Adds an event to the patient.
  - `get_visit_by_id()`: Returns a visit by visit id.
  - `get_visit_by_index()`: Returns a visit by its index.

- **[Attributes]**
  - `available_tables`: Returns a list of available tables for the patient.
"""

from pyhealth.data import Patient
from datetime import datetime, timedelta

# patient is a <Patient> instance with many attributes

patient = Patient(
    patient_id="p001",
    birth_datetime=datetime(2012, 9, 16, 0, 0),
    death_datetime=None,
    gender="F",
    ethnicity="White",
)

# add visit
patient.add_visit(visit1)
patient.add_visit(visit2)

print(patient)

print(patient.available_tables)

"""**TODO:** get the second visit from this patient."""

print(patient.get_visit_by_index(1))

patient.get_visit_by_index(1).get_code_list(table="DIAGNOSES_ICD")



"""If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""

Here is the code content for kdd'23_(zl)_calibration_&_uq.py:
# -*- coding: utf-8 -*-
"""KDD'23 (ZL) - Calibration & UQ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yOMoklsP4ciwEvsEYY4zNMAUVh3iufeE

# Install & download the data
"""

!pip install pyhealth

# download three PSG recordings (with their hyponogram annotations)
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4001E0-PSG.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4001EC-Hypnogram.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4002E0-PSG.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4002EC-Hypnogram.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4011E0-PSG.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4011EH-Hypnogram.edf

def set_seed(seed: int = 42) -> None:
    import os
    import numpy as np
    import random
    import torch
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    # When running on the CuDNN backend, two further options must be set
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # Set a fixed value for the hash seed
    os.environ["PYTHONHASHSEED"] = str(seed)
    print(f"Random seed set as {seed}")
set_seed(42)

"""# Train the base sleep staging model (multi-class classification)"""

from pyhealth.datasets import SleepEDFDataset

sleepedf_ds = SleepEDFDataset(
    root="/content/storage.googleapis.com/pyhealth/sleepedf-sample",
)

print (sleepedf_ds.stat())

from pyhealth.tasks import sleep_staging_sleepedf_fn

sleepedf_task_ds = sleepedf_ds.set_task(task_fn=sleep_staging_sleepedf_fn)

print (sleepedf_task_ds.stat())

from pyhealth.datasets.splitter import split_by_visit
from pyhealth.datasets import get_dataloader

# data split
train_dataset, val_dataset, test_dataset = split_by_visit(sleepedf_task_ds, [0.34, 0.33, 0.33])

# create dataloaders (they are <torch.data.DataLoader> object)
train_loader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_loader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_loader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

from pyhealth.models import ContraWR

model = ContraWR(
    dataset=sleepedf_task_ds,
    feature_keys=["signal"],
    label_key="label",
    mode="multiclass",
)

from pyhealth.trainer import Trainer

trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=5,
    monitor="accuracy",
)

"""# Calibration

Define the metrics we care about (accuracy and calibration).
"""

ps_metrics = ['rejection_rate', 'set_size',
              'miscoverage_ps', 'miscoverage_mean_ps', 'miscoverage_overall_ps',
              'error_ps', 'error_mean_ps', 'error_overall_ps']
metrics = ["accuracy", "f1_macro", "f1_micro"] + ['brier_top1', 'ECE', 'ECE_adapt', 'cwECEt', 'cwECEt_adapt'] + ps_metrics

"""Evaluate the un-calibrated model"""

results = {}
from pyhealth.metrics.multiclass import multiclass_metrics_fn
y_true, y_prob, loss = trainer.inference(test_loader)
results['uncal'] = multiclass_metrics_fn(y_true, y_prob, metrics=metrics)

"""Now, let's use Dirichlet calibration and Kernel-based calibration to calibrate this model we just trained"""

from pyhealth.calib.calibration import DirichletCalibration
cal_model = DirichletCalibration(model)
cal_model.calibrate(cal_dataset=val_dataset)

from pyhealth.calib.calibration import KCal
cal_model_kcal = KCal(model)
cal_model_kcal.calibrate(cal_dataset=val_dataset)

"""Evaluate the quality of the two calibrated models"""

results['DirCal'] = Trainer(model=cal_model, metrics=metrics).evaluate(test_loader)
results['KCal'] = Trainer(model=cal_model_kcal, metrics=metrics).evaluate(test_loader)

"""In general, we keep the accuracy but improves the calibration quality (lower the better).
ECE measures confidence calibration quality, and cwECE (class-wise ECE) meansures full calibration quality (to an extent).
"""

import pandas as pd
pd.DataFrame(results).reindex(['accuracy', 'ECE', 'cwECEt']).style.format(precision=3)

_ = get_dataloader(test_dataset, batch_size=1, shuffle=False)
cal_model(**next(iter(_)))

"""# Prediction Set Construction
In this section, we continue on the previous data/model and try to create prediction sets for them. We will focus on [LABEL](https://pyhealth.readthedocs.io/en/latest/api/calib/pyhealth.calib.predictionset.html#pyhealth.calib.predictionset.LABEL).

Note that we use some different evaluation metrics to evaluate prediction sets. In this task, we focus on miscoverage rates (`miscoverage_ps`).
"""

ps_metrics = ['rejection_rate', 'set_size',
              'miscoverage_ps', 'miscoverage_mean_ps', 'miscoverage_overall_ps',
              'error_ps', 'error_mean_ps', 'error_overall_ps']
metrics = ["accuracy", "f1_macro", "f1_micro"] + ['brier_top1', 'ECE', 'ECE_adapt', 'cwECEt', 'cwECEt_adapt'] + ps_metrics

from pyhealth.calib.predictionset import LABEL
cal_model = LABEL(model, [0.05, 0.1, 0.15, 0.2, 0.25, 0.3])
cal_model.calibrate(cal_dataset=val_dataset)

from pyhealth.trainer import Trainer, get_metrics_fn
y_true_all, y_prob_all, _, extra_output = Trainer(model=cal_model).inference(test_loader, additional_outputs=['y_predset'])
_ = get_metrics_fn(cal_model.mode)(
y_true_all, y_prob_all, metrics=['accuracy', 'miscoverage_ps'],
y_predset=extra_output['y_predset'])
print("\n\n", _)

"""We notice that some classes are very rare, which is why the mis-coverage rate is a bit different from our goal. This will not happen if the dataset is relatively large."""

pd.Series(y_true_all).value_counts().sort_index()

"""## Examine a few samples"""

_ = get_dataloader(test_dataset, batch_size=4, shuffle=False)
predictions = cal_model(**next(iter(_)))

# display prediction setr
predictions['y_true']

"""The labels are 1,5,5,5, and our prediction set misses the truth only for the 1-th sample.
The behavior (avearge set size, how often it covers the truth) will depend on 1. quality of the base ML model/classifier, and 2. the miscoverage target we set.
"""

def display_predset(y_predset):
  for _yps in y_predset:
    print(set([i for i,y in enumerate(_yps) if y]))
display_predset(predictions['y_predset'])



Here is the code content for kdd'23_(zw)_medicalnlp.py:
# -*- coding: utf-8 -*-
"""KDD'23 (ZW) - MedicalNLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11tOVJpNUpQbw-rTutCXvtZY12tksEL6Q

# Clinical Text Classification with PyHealth

Welcome to the PyHealth tutorial on clinical text classification. In this notebook, we will explore how to use PyHealth to perform clinical natural language processing.

## Environment Setup

To begin, we need to install PyHealth and a few additional packages to support our analysis.
"""

!pip install mne pandarallel rdkit transformers

!rm -rf PyHealth
!git clone -b zhenbang/f-image_text_support https://github.com/sunlabuiuc/PyHealth.git

import sys


sys.path.append("./PyHealth")

"""## Download Data

Next, we will download the clinical text dataset. Specifically, we will be using the medical transcription data scraped from mtsamples.com. This dataset includes the transcripted medical reports and the corresponding medical category. You can find more information about the dataset [here](https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions).
"""

!wget -N https://storage.googleapis.com/pyhealth/medical_transcriptions_data/MedicalTranscriptions.zip

!unzip -q -o MedicalTranscriptions.zip

!ls -1 MedicalTranscriptions

"""Next, we will proceed with the medical transcriptions classification task using PyHealth, following a five-stage pipeline.

## Step 1. Load Data in PyHealth

The initial step involves loading the data into PyHealth's internal structure. This process is straightforward: import the appropriate dataset class from PyHealth and specify the root directory where the raw dataset is stored. PyHealth will handle the dataset processing automatically.
"""

from pyhealth.datasets import MedicalTranscriptionsDataset


root = "/content/MedicalTranscriptions"
base_dataset = MedicalTranscriptionsDataset(root)

"""Once the data is loaded, we can perform simple queries on the dataset."""

base_dataset.stat()

base_dataset.patients[0]

"""## Step 2. Define the Task

The next step is to define the machine learning task. This step instructs the package to generate a list of samples with the desired features and labels based on the data for each individual patient. Please note that in this dataset, patient identification information is not available. Therefore, we will assume that each medical transcript belongs to a unique patient.

For this dataset, PyHealth offers a default task specifically for transcription classification. This task takes the transcription text as input and aims to predict the medical categories associated with it.
"""

base_dataset.default_task

sample_dataset = base_dataset.set_task()

"""Here is an example of a single sample, represented as a dictionary. The dictionary contains keys for feature names, label names, and other metadata associated with the sample."""

sample_dataset[0]

"""Finally, we will split the entire dataset into training, validation, and test sets using the ratios of 70%, 10%, and 20%, respectively. We will then obtain the corresponding data loaders for each set."""

from pyhealth.datasets import split_by_sample


train_dataset, val_dataset, test_dataset = split_by_sample(
    dataset=sample_dataset,
    ratios=[0.7, 0.1, 0.2]
)

from pyhealth.datasets import get_dataloader


train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

"""## Step 3. Define the Model

Next, we will define the deep learning model we want to use for our task. PyHealth supports all major language models available in the Huggingface's Transformer package. You can load any of these models using the model_name argument.
"""

from pyhealth.models import TransformersModel


model = TransformersModel(
    model_name="emilyalsentzer/Bio_ClinicalBERT",
    dataset=sample_dataset,
    feature_keys=["transcription"],
    label_key="label",
    mode="multiclass",
)

model

"""## Step 4. Training

In this step, we will train the model using PyHealth's Trainer class, which simplifies the training process and provides standard functionalities.
"""

from pyhealth.trainer import Trainer


trainer = Trainer(model=model)

"""Before we begin training, let's first evaluate the initial performance of the model."""

print(trainer.evaluate(test_dataloader))

"""Now, let's start the training process. The trainer will automatically track the best model based on the metric you set to monitor (e.g., accuracy)."""

trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=1,
    monitor="accuracy"
)

"""## Step 5. Evaluation

At the end of training, the trainer will automatically load the best save model weights. So that we can easily eavluate the ResNet model on the test set. This can be done using PyHealth's `Trainer.evaluate()` function.
"""

print(trainer.evaluate(test_dataloader))



Here is the code content for kdd'23_(cq)_eeg_transformer_sleepstaging_sleepedf.py:
# -*- coding: utf-8 -*-
"""KDD'23 (CQ) - EEG-Transformer - SleepStaging - SleepEDF

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pIhfyasmLveCh2QaTz4zhlW_6IKZtYfS
"""

!pip install pyhealth

# download three PSG recordings (with their hyponogram annotations)
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4001E0-PSG.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4001EC-Hypnogram.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4002E0-PSG.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4002EC-Hypnogram.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4011E0-PSG.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4011EH-Hypnogram.edf

"""### **Inspect on the data**"""

!ls /content/storage.googleapis.com/pyhealth/sleepedf-sample

"""### **Step 1: Load dataset**
- **[README]:** We call [pyhealth.datasets](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) to process and obtain the dataset.
  - `root` is the arguments directing to the data folder.
  - `dev`: if set `True`, will only load a smaller set of patients.
- **[Next Step]:** This `pyhealth.datasets` object will be used in **Step 2**.
"""

from pyhealth.datasets import SleepEDFDataset

sleepedf_ds = SleepEDFDataset(
    root="/content/storage.googleapis.com/pyhealth/sleepedf-sample",
)

# check the general-purpose dataset stats
sleepedf_ds.stat()
print ()

sleepedf_ds.patients

"""### **Step 2: Define healthcare task**
- **[README]:** This step assigns a **task function** to the dataset for data loading [pyhealth.tasks](https://pyhealth.readthedocs.io/en/latest/api/tasks.html). The **task function** specifics how to process each pateint's data into a set of samples for the downstream machine learning models.
- **[Next Step]:** This `pyhealth.tasks` object will be used in **Step 3**.
"""

from pyhealth.tasks import sleep_staging_sleepedf_fn

sleepedf_task_ds = sleepedf_ds.set_task(task_fn=sleep_staging_sleepedf_fn)

# check the task-specific structure stats
sleepedf_task_ds.stat()
print ()

from pyhealth.datasets.splitter import split_by_patient
from pyhealth.datasets import split_by_patient, get_dataloader

# data split
train_dataset, val_dataset, test_dataset = split_by_patient(sleepedf_task_ds, [0.34, 0.33, 0.33])

# create dataloaders (they are <torch.data.DataLoader> object)
train_loader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_loader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_loader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

"""### **Step 3: Define ML Model**
- **[README]:** We initialize an ML model for the healthcare task by calling [pyhealth.models](https://pyhealth.readthedocs.io/en/latest/api/models.html).
- **[Next Step]:** This `pyhealth.models` object will be used in **Step 4**.
- **[Other Use Case]:** Our `pyhealth.models` object is as general as any instance from `torch.nn.Module`. Users may use it separately for supporting any other customized pipeline.
"""

from pyhealth.models import ContraWR

model = ContraWR(
    dataset=sleepedf_task_ds,
    feature_keys=["signal"],
    label_key="label",
    mode="multiclass",
)

"""### **Step 4: Model Training**
- **[README]:** We call our [pyhealth.train.Trainer](https://pyhealth.readthedocs.io/en/latest/api/trainer.html) to train the model by giving the `train_loader`, the `val_loader`, val_metric, and specify other arguemnts, such as epochs, optimizer, learning rate, etc. The trainer will automatically save the best model and output the path in the end.
- **[Next Step]:** The best model will be used in **Step 5** for evaluation.

"""

from pyhealth.trainer import Trainer

trainer = Trainer(
    model=model,
    metrics=["cohen_kappa", "accuracy"],
)

trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=2,
    monitor="accuracy",
    monitor_criterion="max",
)

"""### **Step 5: Evaluation**"""

# option 1: use our built-in evaluation metric
score = trainer.evaluate(test_loader)
print (score)

# option 2: use our pyhealth.metrics to evaluate
from pyhealth.metrics.multiclass import multiclass_metrics_fn

y_true, y_prob, loss = trainer.inference(test_loader)
multiclass_metrics_fn(
    y_true,
    y_prob,
    metrics=["f1_weighted"]
)

"""If this helps, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""



Here is the code content for kdd'23_(cq)_retain_lengthofstay_mimic_iii.py:
# -*- coding: utf-8 -*-
"""KDD'23 (CQ) - RETAIN - LengthOfStay - MIMIC-III.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RzNWEb2Y-YWS1S1rjN5z0GndU5RP3y-B

### **Preparation**
- install pyhealth alpha version
"""

!pip install pyhealth

"""### **Step 1: Load dataset**
- **[README]:** We call [pyhealth.datasets](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) to process and obtain the dataset.
  - `root` is the arguments directing to the data folder.
  - `tables` is a list of table names from raw databases, which specifies the information that will be used in building the pipeline. Currently, we provide [MIMIC3Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html), [MIMIC4Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC4Dataset.html), [eICUDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.eICUDataset.html), [OMOPDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.OMOPDataset.html).
  - `code_mapping [default: None]` asks a directionary input, specifying the new coding systems for each data table. For example, `{"NDC": ("ATC", {"target_kwargs": {"level": 3}})}` means that our pyhealth will automatically change the codings from `NDC` into ATC-3 level for tables if any.
  - `dev`: if set `True`, will only load a smaller set of patients.
- **[Next Step]:** This `pyhealth.datasets` object will be used in **Step 2**.
- **[Advanced Use Case]:** Researchers can use the dict-based output alone `dataset.patients` alone for supporting their own tasks.

from pyhealth.datasets import MIMIC3Dataset

mimic3_ds = MIMIC3Dataset(
        root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        dev=True,
)

print (mimic3_ds.stat())

# data format
mimic3_ds.info()

### **Step 2: Define healthcare task**
- **[README]:** This step assigns a **task function** to the dataset for data loading [pyhealth.tasks](https://pyhealth.readthedocs.io/en/latest/api/tasks.html). The **task function** specifics how to process each pateint's data into a set of samples for the downstream machine learning models.
- **[Next Step]:** This `pyhealth.tasks` object will be used in **Step 3**.
"""

from pyhealth.tasks import length_of_stay_prediction_mimic3_fn

mimic3_ds = mimic3_ds.set_task(task_fn=length_of_stay_prediction_mimic3_fn)
# stats info
print (mimic3_ds.stat())


{
    "patient_id": "p001",
    "visit_id": "v001",
    "diagnoses": [...],
    "labs": [...],
    "procedures": [...],
    "label": 1,
}

from pyhealth.datasets.splitter import split_by_patient
from pyhealth.datasets import split_by_patient, get_dataloader

# data split
train_dataset, val_dataset, test_dataset = split_by_patient(mimic3_ds, [0.8, 0.1, 0.1])

# create dataloaders (they are <torch.data.DataLoader> object)
train_loader = get_dataloader(train_dataset, batch_size=64, shuffle=True)
val_loader = get_dataloader(val_dataset, batch_size=64, shuffle=False)
test_loader = get_dataloader(test_dataset, batch_size=64, shuffle=False)

"""### **Step 3: Define ML Model**
- **[README]:** We initialize an ML model for the healthcare task by calling [pyhealth.models](https://pyhealth.readthedocs.io/en/latest/api/models.html).
- **[Next Step]:** This `pyhealth.models` object will be used in **Step 4**.
- **[Other Use Case]:** Our `pyhealth.models` object is as general as any instance from `torch.nn.Module`. Users may use it separately for supporting any other customized pipeline.
"""

mimic3_ds.samples[0].keys()

from pyhealth.models import RETAIN

model = RETAIN(
    dataset=mimic3_ds,
    # look up what are available for "feature_keys" and "label_keys" in dataset.samples[0]
    feature_keys=["conditions", "procedures", "drugs"],
    label_key="label",
    mode="multiclass",
)

"""### **Step 4: Model Training**
- **[README]:** We call our [pyhealth.train.Trainer](https://pyhealth.readthedocs.io/en/latest/api/trainer.html) to train the model by giving the `train_loader`, the `val_loader`, val_metric, and specify other arguemnts, such as epochs, optimizer, learning rate, etc. The trainer will automatically save the best model and output the path in the end.
- **[Next Step]:** The best model will be used in **Step 5** for evaluation.

"""

from pyhealth.trainer import Trainer

trainer = Trainer(
    model=model,
    metrics=["accuracy", "f1_weighted"], # the metrics that we want to log
    )

trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=5,
    monitor="accuracy",
    monitor_criterion="max",
)

"""### **Step 5: Evaluation**"""

# option 1: use our built-in evaluation metric
score = trainer.evaluate(test_loader)
print (score)

# option 2: use our pyhealth.metrics to evaluate
from pyhealth.metrics.multiclass import multiclass_metrics_fn

y_true, y_prob, loss = trainer.inference(test_loader)
multiclass_metrics_fn(
    y_true,
    y_prob,
    metrics=["f1_weighted", "f1_micro", "cohen_kappa"]
)

"""If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""



Here is the code content for advanced_case_4_load_your_samples_datasets_and_try_our_ml_models.py:
# -*- coding: utf-8 -*-
"""Advanced Case 4: Load your samples datasets and try our ML models

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZRnKch2EyJLrI3G5AvDXVpeE2wwgBWfw

### **Preparation**
- install pyhealth
"""

!pip install pyhealth

"""### **Load your own samples and use our ML models**
This advacned tutorials will discuss how to fit your own data (support code inputs and float/int inputs) into our pyhealth and use the downstream models (such as transformer) for prediction.
- **Step 1**: load your own data with pyhealth.datasets.SampleDataset
- **Step 2**: choose one model from our package and training it.

### **Step 1: Fit samples into pyhealth**
> we will simulate the datasets (both code-based features and float-int based features) and use ```pyhealth.datasets.SampleDataset``` to fit into pyhealth
"""

import numpy as np

# simulate N_pat patients
# simulate N_vis visit per patient
# simulate the conditions per visit
# simulate the lab_values per visit
# simualte the binary labels per visit
N_pat, N_vis = 50, 10
condition_space = [f"cond-{i}" for i in range(100)]

samples = []
for pat_i in range(N_pat):
    conditions = []
    lab_values = []
    for visit_j in range(N_vis):
        patient_id = f"patient-{pat_i}"
        visit_id = f"visit-{visit_j}"
        # how many conditions to simulate
        N_cond = np.random.randint(3, 6)
        conditions.append(np.random.choice(condition_space, N_cond, replace=False).tolist())
        # how many lab-values to simulate
        lab_values.append(np.random.random(5).tolist())
        # which binary label
        label = int(np.random.random() > 0.5)

        # make sure to store historical visits information into the current visit as well.
        sample = {
            "patient_id": patient_id,
            "visit_id": visit_id,
            "conditions": conditions.copy(),
            "lab-values": lab_values.copy(),
            "label": label
        }
        samples.append(sample)

"""> **So, the samples is a list of dictionary. Users can process their own dataset into this sample-based list structure and fit into our module below. We provide splitter function to handle patient-wise or visit-wise split.**"""

# let us check the first sample
samples[0]

# let us check the second sample
samples[1]

# load into the dataset
from pyhealth.datasets import SampleDataset
dataset = SampleDataset(samples)

"""##### **get the train/val/test dataset**"""

from pyhealth.datasets.splitter import split_by_patient
from pyhealth.datasets import split_by_patient, get_dataloader

# dataset split by patient id
train_ds, val_ds, test_ds = split_by_patient(dataset, [0.8, 0.1, 0.1])

# obtain train/val/test dataloader, they are <torch.data.DataLoader> object
train_loader = get_dataloader(train_ds, batch_size=64, shuffle=True)
val_loader = get_dataloader(val_ds, batch_size=64, shuffle=False)
test_loader = get_dataloader(test_ds, batch_size=64, shuffle=False)

"""### **Step 2: Select a ML model**
- In this tutorial, we use Transformer as the example.
- please check the [Tutorial 2](https://colab.research.google.com/drive/1LcXZlu7ZUuqepf269X3FhXuhHeRvaJX5?usp=sharing) for more instructions on how to initialize a model.
"""

from pyhealth.models import Transformer

# initialize the Transformer model

device = "cpu"
model = Transformer(
    dataset=dataset,
    feature_keys=["conditions", "lab-values"],
    label_key="label",
    mode="binary",
)
model.to(device)
print ()

from pyhealth.trainer import Trainer

# use our Trainer to train the model

trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=5,
    monitor="roc_auc", # which metric do you want to monitor for selecting the best model, check https://pyhealth.readthedocs.io/en/latest/api/metrics/pyhealth.metrics.multiclass.html
)

# option 1: use our built-in evaluation metric
score = trainer.evaluate(test_loader)
print (score)

# option 2: use our pyhealth.metrics to evaluate
from pyhealth.metrics.binary import binary_metrics_fn
y_true, y_prob, loss = trainer.inference(test_loader)
binary_metrics_fn(y_true, y_prob, metrics=["pr_auc", "roc_auc", "f1"])



"""If this helps, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""



Here is the code content for tutorial_6_pyhealth_tokenizer.py:
# -*- coding: utf-8 -*-
"""Tutorial 6: pyhealth.tokenizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bDOb0A5g0umBjtz8NIp4wqye7taJ03D0

### **Preparation**
- install pyhealth alpha version
"""

!pip install pyhealth

"""### **Instruction on [pyhealth.tokenizer](https://pyhealth.readthedocs.io/en/latest/api/tokenizer.html)**
- **[README]**: tokenizer is used for transformations between string-based tokens and integer-based indices, based on the overall token space. We provide flexible functions to tokenize 1D, 2D and 3D lists. **This module can be used in many other scenarios.**

- **[Arguments]**:
  - `tokens`: List of tokens in the vocabulary.
  - `special_tokens`: List of special tokens to add to the vocabulary. (e.g., `<pad>`, `<unk>`). If not provided, no special tokens are added.

- **[Functionality]**:
  - `get_vocabulary_size`: Return the size of the vocabulary
  - `convert_tokens_to_indices`: 1d conversion from tokens to indices
  - `convert_indices_to_tokens`: 1d conversion from indices to tokens
  - `batch_encode_2d`: 2d conversion from tokens to indices
  - `batch_decode_2d`: 2d conversion from indices to tokens
  - `batch_encode_3d`: 3d conversion from tokens to indices
  - `batch_decode_3d`: 3d conversion from indices to tokens

### **Example 1: 1D tokenization**
- We provide examples for 1d transformation between tokens and indices
- We use `["<pad>", "<unk>"]` as two special tokens, `<pad>` is used for padding in higher dimensional encoding and decoding, and `<unk>` is used for unknown tokens. In 1d tokenization, the `<pad>` token is not useful.
"""

from pyhealth.tokenizer import Tokenizer

# we use a list of ATC3 code as the token
token_space = ['A01A', 'A02A', 'A02B', 'A02X', 'A03A', 'A03B', 'A03C', 'A03D', 'A03E', \
          'A03F', 'A04A', 'A05A', 'A05B', 'A05C', 'A06A', 'A07A', 'A07B', 'A07C', \
          'A07D', 'A07E', 'A07F', 'A07X', 'A08A', 'A09A', 'A10A', 'A10B', 'A10X', \
          'A11A', 'A11B', 'A11C', 'A11D', 'A11E', 'A11G', 'A11H', 'A11J', 'A12A', \
          'A12B', 'A12C', 'A13A', 'A14A', 'A14B', 'A16A']

# initialize the tokenizer
tokenizer = Tokenizer(tokens=token_space, special_tokens=["<pad>", "<unk>"])
print(tokenizer.get_vocabulary_size())

# 1d encode
tokens = ['A03C', 'A03D', 'A03E', 'A03F', 'A04A', 'A05A', 'A05B', 'B035', 'C129']
indices = tokenizer.convert_tokens_to_indices(tokens)
print (indices)

# 1d decode
indices = [0, 1, 2, 3, 4, 5]
tokens = tokenizer.convert_indices_to_tokens(indices)
print (tokens)

"""### **Example 2: 2D tokenization**
- We provide examples for 2d transformation between tokens and indices
- We use `["<pad>", "<unk>"]` as two special tokens, `<pad>` is used for padding in higher dimensional encoding and decoding, and `<unk>` is used for unknown tokens.
"""

# from pyhealth.tokenizer import Tokenizer

# we use a list of ATC3 code as the token
token_space = ['A01A', 'A02A', 'A02B', 'A02X', 'A03A', 'A03B', 'A03C', 'A03D', 'A03E', \
          'A03F', 'A04A', 'A05A', 'A05B', 'A05C', 'A06A', 'A07A', 'A07B', 'A07C', \
          'A07D', 'A07E', 'A07F', 'A07X', 'A08A', 'A09A', 'A10A', 'A10B', 'A10X', \
          'A11A', 'A11B', 'A11C', 'A11D', 'A11E', 'A11G', 'A11H', 'A11J', 'A12A', \
          'A12B', 'A12C', 'A13A', 'A14A', 'A14B', 'A16A']

# initialize the tokenizer
tokenizer = Tokenizer(tokens=token_space, special_tokens=["<pad>", "<unk>"])
print(tokenizer.get_vocabulary_size())

"""
batch: List of lists of tokens to convert to indices.
padding (default: True): whether to pad the tokens to the max number of tokens in the batch (smart padding).
truncation (default: True): whether to truncate the tokens to max_length.
max_length (default: 512): maximum length of the tokens. This argument is ignored if truncation is False.
"""

# 2d encode
tokens = [
    ['A03C', 'A03D', 'A03E', 'A03F'],
    ['A04A', 'B035', 'C129']
]

# case 1: default using padding, truncation and max_length is 512
indices = tokenizer.batch_encode_2d(tokens)
print ('case 1:', indices)

# case 2: no padding
indices = tokenizer.batch_encode_2d(tokens, padding=False)
print ('case 2:', indices)

# case 3: truncation with max_length is 3
indices = tokenizer.batch_encode_2d(tokens, max_length=3)
print ('case 3:', indices)

"""
batch: List of lists of indices to convert to tokens.
padding (default: False): whether to keep the padding tokens from the tokens.
"""

# 2d decode
indices = [
    [8, 9, 10, 11],
    [12, 1, 1, 0]
]

# case 1: default no padding
tokens = tokenizer.batch_decode_2d(indices)
print ('case 1:', tokens)

# case 2: use padding
tokens = tokenizer.batch_decode_2d(indices, padding=True)
print ('case 2:', tokens)

"""### **Example 3: 3D tokenization**
- We provide examples for 3d transformation between tokens and indices
- We use `["<pad>", "<unk>"]` as two special tokens, `<pad>` is used for padding in higher dimensional encoding and decoding, and `<unk>` is used for unknown tokens.
"""

# from pyhealth.tokenizer import Tokenizer

# we use a list of ATC3 code as the token
token_space = ['A01A', 'A02A', 'A02B', 'A02X', 'A03A', 'A03B', 'A03C', 'A03D', 'A03E', \
          'A03F', 'A04A', 'A05A', 'A05B', 'A05C', 'A06A', 'A07A', 'A07B', 'A07C', \
          'A07D', 'A07E', 'A07F', 'A07X', 'A08A', 'A09A', 'A10A', 'A10B', 'A10X', \
          'A11A', 'A11B', 'A11C', 'A11D', 'A11E', 'A11G', 'A11H', 'A11J', 'A12A', \
          'A12B', 'A12C', 'A13A', 'A14A', 'A14B', 'A16A']

# initialize the tokenizer
tokenizer = Tokenizer(tokens=token_space, special_tokens=["<pad>", "<unk>"])
print(tokenizer.get_vocabulary_size())

"""
batch: List of lists of lists of tokens to convert to indices.
padding (default: (True, True)): a tuple of two booleans indicating whether to pad the tokens to the max number of tokens
    and visits (smart padding).
truncation (default: (True, True)): a tuple of two booleans indicating whether to truncate the tokens to the corresponding
    element in max_length
max_length (default: (10, 512)): a tuple of two integers indicating the maximum length of the tokens along the first and
    second dimension. This argument is ignored if truncation is False.
"""

# 3d encode
tokens = [
    [
        ['A03C', 'A03D', 'A03E', 'A03F'],
        ['A08A', 'A09A'],
    ],
    [
        ['A04A', 'B035', 'C129'],
    ]
]

# case 1: default using padding, truncation and max_length is 512
indices = tokenizer.batch_encode_3d(tokens)
print ('case 1:', indices)

# case 2: no padding on the first dimension
indices = tokenizer.batch_encode_3d(tokens, padding=(False, True))
print ('case 2:', indices)

# case 3: no padding on the second dimension
indices = tokenizer.batch_encode_3d(tokens, padding=(True, False))
print ('case 3:', indices)

# case 4: no padding on both dimensions
indices = tokenizer.batch_encode_3d(tokens, padding=(False, False))
print ('case 4:', indices)

# case 5: truncation with max_length is (2,2) on both dimension
indices = tokenizer.batch_encode_3d(tokens, max_length=(2,2))
print ('case 5:', indices)

"""
batch: List of lists of indices to convert to tokens.
padding (default: False): whether to keep the padding tokens from the tokens.
"""

# 3d decode
indices = [
    [
        [8, 9, 10, 11],
        [24, 25, 0, 0]
    ],
    [
        [12, 1, 1, 0],
        [0, 0, 0, 0]
    ]
]


# case 1: default no padding
tokens = tokenizer.batch_decode_3d(indices)
print ('case 1:', tokens)

# case 2: use padding
tokens = tokenizer.batch_decode_3d(indices, padding=True)
print ('case 2:', tokens)



"""If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""

Here is the code content for advanced_case_1_handle_user_unprocessed_datasets.py:
# -*- coding: utf-8 -*-
"""Advanced Case 1: Handle user unprocessed datasets

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UurxwAAov1bL_5OO3gQJ4gAa_paeJwJp

### **Preparation**
- install pyhealth
"""

!pip install pyhealth

"""### **How to use our package for processing your own data?**
- In our pacakge, we have provided modules to process the following datasets
  - [MIMIC-III](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html), [MIMIC-IV](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC4Dataset.html), [eICU](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.eICUDataset.html), [OMOP](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.OMOPDataset.html) (any OMOP-CDM based databases).
  - This modules are all based on [pyhealth.datasets.BaseDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.BaseDataset.html)
- This tutorials give some examples on how to process your own datasets to fit our package. We will inherit [pyhealth.datasets.BaseDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.BaseDataset.html) and create a customized module.

### **Processing simulated MIMIC**
> For demonstration purpose, we build the MIMIC processing module from scratch based on ```pyhealth.datasets.BaseDataset```. In this example, we will implement functions for processing raw MIMIC csv tables, one function for one table.
"""

root = "https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/"

"""#### **Look at the data structure**"""

import pandas as pd
import os

patients_df = pd.read_csv(os.path.join(root, "PATIENTS.csv"), nrows=3)
patients_df

admissions_df = pd.read_csv(os.path.join(root, "ADMISSIONS.csv"), nrows=3)
admissions_df

"""#### **Customized dataset: one function per table**"""

from pyhealth.datasets import BaseDataset
from pyhealth.data import Event, Visit, Patient
from pyhealth.datasets.utils import strptime
from tqdm import tqdm

class CustomMIMIC(BaseDataset):
    def parse_tables(self):
        # patients is a dict of Patient objects indexed by patient_id
        patients = dict()
        # process patients and admissions tables
        patients = self.parse_basic_info(patients)
        # process clinical tables
        for table in self.tables:
            try:
                # use lower case for function name
                patients = getattr(self, f"parse_{table.lower()}")(patients)
            except AttributeError:
                raise NotImplementedError(
                    f"Parser for table {table} is not implemented yet."
                )
        return patients

    """ THE VERY FIRST FUNCTION: clean patient-visit structure

        Note: You will create an empty nested dictionary structrue
          - level 1: patients: patient_id -> Patient
          - level 2: Patient.visit: visit_id -> Visit
    """
    def parse_basic_info(self, patients):

        # load patients info
        patients_df = pd.read_csv(
            os.path.join(self.root, "PATIENTS.csv"),
            dtype={"SUBJECT_ID": str},
            nrows=1000, # set the first 100 rows for example
        )

        # load admission info
        admissions_df = pd.read_csv(
            os.path.join(self.root, "ADMISSIONS.csv"),
            dtype={"SUBJECT_ID": str, "HADM_ID": str},
        )

        # merge patient and admission tables
        df = pd.merge(patients_df, admissions_df, on="SUBJECT_ID", how="inner")

        # sort by admission and discharge time
        df = df.sort_values(["SUBJECT_ID", "ADMITTIME", "DISCHTIME"], ascending=True)

        # load patients
        for p_id, p_info in tqdm(df.groupby("SUBJECT_ID"), desc="Parsing PATIENTS and ADMISSIONS"):
            # check <pyhealth.data.Patient> in https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Patient.html
            patient = Patient(
                patient_id=p_id,
                birth_datetime=strptime(p_info["DOB"].values[0]),
                death_datetime=strptime(p_info["DOD_HOSP"].values[0]),
                gender=p_info["GENDER"].values[0],
                ethnicity=p_info["ETHNICITY"].values[0],
            )
            # load visits
            for v_id, v_info in p_info.groupby("HADM_ID"):
                # check <pyhealth.data.Patient> in https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Visit.html
                visit = Visit(
                    visit_id=v_id,
                    patient_id=p_id,
                    encounter_time=strptime(v_info["ADMITTIME"].values[0]),
                    discharge_time=strptime(v_info["DISCHTIME"].values[0]),
                    discharge_status=v_info["HOSPITAL_EXPIRE_FLAG"].values[0],
                )
                # add visit
                patient.add_visit(visit)
            # add patient
            patients[p_id] = patient
        return patients


    """ DIAGNOSES_ICD Process Function

        Note: You will insert the diagnosis info into each visit
            - the function name is '_parse_' + lowercase(csv_table_name)
    """
    def parse_diagnoses_icd(self, patients):

        # the table name
        table = "DIAGNOSES_ICD"

        # load csv table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"SUBJECT_ID": str, "HADM_ID": str, "ICD9_CODE": str},
        )

        # drop rows with missing values
        df = df.dropna(subset=["SUBJECT_ID", "HADM_ID", "ICD9_CODE"])

        # sort by sequence number (i.e., priority)
        df = df.sort_values(["SUBJECT_ID", "HADM_ID", "SEQ_NUM"], ascending=True)

        # iterate over each patient and visit
        for (p_id, v_id), v_info in tqdm(df.groupby(["SUBJECT_ID", "HADM_ID"]), desc=f"Parsing {table}"):
            for code in v_info["ICD9_CODE"]:
                event = Event(
                    code=code,
                    table=table,
                    vocabulary="ICD9CM",
                    visit_id=v_id,
                    patient_id=p_id,
                )
                # update patient-visit structure
                patients = self._add_event_to_patient_dict(patients, event)
        return patients

    """ PROCEDURES_ICD Process Function

        Note: You will insert the diagnosis info into each visit
            - the function name is '_parse_' + lowercase(csv_table_name)
    """
    def parse_procedures_icd(self, patients):

        # the table name
        table = "PROCEDURES_ICD"

        # read table
        df = pd.read_csv(
            os.path.join(self.root, f"{table}.csv"),
            dtype={"SUBJECT_ID": str, "HADM_ID": str, "ICD9_CODE": str},
        )

        # drop rows with missing values
        df = df.dropna(subset=["SUBJECT_ID", "HADM_ID", "SEQ_NUM", "ICD9_CODE"])

        # sort by sequence number (i.e., priority)
        df = df.sort_values(["SUBJECT_ID", "HADM_ID", "SEQ_NUM"], ascending=True)

        # iterate over each patient and visit
        for (p_id, v_id), v_info in tqdm(df.groupby(["SUBJECT_ID", "HADM_ID"]), desc=f"Parsing {table}"):
            for code in v_info["ICD9_CODE"]:
                event = Event(
                    code=code,
                    table=table,
                    vocabulary="ICD9PROC",
                    visit_id=v_id,
                    patient_id=p_id,
                )
                # update patient-visit structure
                patients = self._add_event_to_patient_dict(patients, event)
        return patients

"""#### **Use our customized dataset**"""

dataset = CustomMIMIC(
        root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD"],
)

dataset.stat()



"""If this helps, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""



Here is the code content for pipeline_5_[biosignal]_sleepstaging.py:
# -*- coding: utf-8 -*-
"""Pipeline 5: [biosignal] SleepStaging.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mpSeNCAthXG3cqROkdUcUdozIPIMTCuo
"""

!git clone https://github.com/sunlabuiuc/PyHealth.git
!cd PyHealth; pip install .

# download three PSG recordings (with their hyponogram annotations)
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4001E0-PSG.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4001EC-Hypnogram.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4002E0-PSG.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4002EC-Hypnogram.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4011E0-PSG.edf
!wget -r https://storage.googleapis.com/pyhealth/sleepedf-sample/SC4011EH-Hypnogram.edf

"""### **Inspect on the data**"""

!ls /content/storage.googleapis.com/pyhealth/sleepedf-sample

# look at the first sample
import mne
import os

signal = mne.io.read_raw_edf(os.path.join("/content/storage.googleapis.com/pyhealth/sleepedf-sample", "SC4001E0-PSG.edf")).get_data()

# the long recording
signal.shape

# look at the corresponding labels
ann = mne.read_annotations(os.path.join("/content/storage.googleapis.com/pyhealth/sleepedf-sample", "SC4001EC-Hypnogram.edf"))

ann.description

"""Info for the first sample:
- signal.shape `(7, 7950000)`
- ann.description `array(['Sleep stage W', 'Sleep stage 1', ...])`

These three samples are from the SleepEDF dataset (https://www.physionet.org/content/sleep-edfx/1.0.0/) which has 100 Hz as the sampling frequency. The dataset task is collected for the **EEG Sleep Staging** task.

For **data processing**, the long recording should be segment into 30-second windows (it is a matrix with size (7, 3000), 7 is the channel size and 3000 points = 30s * 100Hz) and the labels `{awake 'W', rapid eye movenent (REM) 'R', non-REM '1', non-REM '2', non-REM '3', non-REM '4'}` can be found from the annotation files following the order. Usually, people merge the  `non-REM '3'`, `non-REM '4'` classes.

So, the first segment is `signal[:, :3000]` and the first label is `'W'` according to the annotation. The second segment is `signal[:, 3000:6000]` and the first label is `'1'`.

For **model builing**, we are going to build a signal classification model that takes a (7, 3000)-shape signal and can output a probability distribution over 5 classes (`'3'` and `'4'` are merged)


Below, we show that pyhealth can handle this sleep staging tasks in 5 stages (including data processing and model building).

### **Step 1: Load dataset**
- **[README]:** We call [pyhealth.datasets](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) to process and obtain the dataset.
  - `root` is the arguments directing to the data folder.
  - `dev`: if set `True`, will only load a smaller set of patients.
- **[Next Step]:** This `pyhealth.datasets` object will be used in **Step 2**.
"""

from pyhealth.datasets import SleepEDFDataset

sleepedf_ds = SleepEDFDataset(
    root="/content/storage.googleapis.com/pyhealth/sleepedf-sample",
)

print (sleepedf_ds.stat())

sleepedf_ds.patients

"""### **Step 2: Define healthcare task**
- **[README]:** This step assigns a **task function** to the dataset for data loading [pyhealth.tasks](https://pyhealth.readthedocs.io/en/latest/api/tasks.html). The **task function** specifics how to process each pateint's data into a set of samples for the downstream machine learning models.
- **[Next Step]:** This `pyhealth.tasks` object will be used in **Step 3**.
"""

from pyhealth.tasks import sleep_staging_sleepedf_fn

sleepedf_task_ds = sleepedf_ds.set_task(task_fn=sleep_staging_sleepedf_fn)

print (sleepedf_task_ds.stat())

from pyhealth.datasets.splitter import split_by_patient
from pyhealth.datasets import split_by_patient, get_dataloader

# data split
train_dataset, val_dataset, test_dataset = split_by_patient(sleepedf_task_ds, [0.34, 0.33, 0.33])

# create dataloaders (they are <torch.data.DataLoader> object)
train_loader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_loader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_loader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

"""### **Step 3: Define ML Model**
- **[README]:** We initialize an ML model for the healthcare task by calling [pyhealth.models](https://pyhealth.readthedocs.io/en/latest/api/models.html).
- **[Next Step]:** This `pyhealth.models` object will be used in **Step 4**.
- **[Other Use Case]:** Our `pyhealth.models` object is as general as any instance from `torch.nn.Module`. Users may use it separately for supporting any other customized pipeline.
"""

from pyhealth.models import ContraWR

model = ContraWR(
    dataset=sleepedf_task_ds,
    feature_keys=["signal"],
    label_key="label",
    mode="multiclass",
)

"""### **Step 4: Model Training**
- **[README]:** We call our [pyhealth.train.Trainer](https://pyhealth.readthedocs.io/en/latest/api/trainer.html) to train the model by giving the `train_loader`, the `val_loader`, val_metric, and specify other arguemnts, such as epochs, optimizer, learning rate, etc. The trainer will automatically save the best model and output the path in the end.
- **[Next Step]:** The best model will be used in **Step 5** for evaluation.

"""

from pyhealth.trainer import Trainer

trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=5,
    monitor="accuracy",
)

"""### **Step 5: Evaluation**"""

# option 1: use our built-in evaluation metric
score = trainer.evaluate(test_loader)
print (score)

# option 2: use our pyhealth.metrics to evaluate
from pyhealth.metrics.multiclass import multiclass_metrics_fn
y_true, y_prob, loss = trainer.inference(test_loader)
multiclass_metrics_fn(y_true, y_prob, metrics=["f1_weighted"])

"""If this helps, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""



Here is the code content for tutorial_7_pyhealth_medcode.py:
# -*- coding: utf-8 -*-
"""Tutorial 7: pyhealth.medcode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xrp_ACM2_Hg5Wxzj0SKKKgZfMY0WwEj3

### **Preparation**
- install pyhealth alpha version
"""

!pip install pyhealth

"""### **Instruction on [pyhealth.medcode](https://pyhealth.readthedocs.io/en/latest/api/medcode.html)**
- **[README]**: medcode provides two core functionalities:
    - (1) looking up information for a given medical code (e.g., name, category, sub-concept);
    - (2) mapping codes across coding systems (e.g., ICD9CM to CCSCM). **This module can be easily applied to your research.**

### 1. Code Lookup

`class pyhealth.medcode.InnerMap`

- **[Functionality]**:
    - lookup(code): looks up code in a coding system
    - contains(code): checks whether a code belongs to a specific coding system
    - get_ancestors(code): returns the ancestors for a given code

Currently, we support the following coding systems:

- Diagnosis codes:
    - ICD9CM
    - ICD10CM
    - CCSCM
- Procedure codes:
    - ICD9PROC
    - ICD10PROC
    - CCSPROC
- Medication codes:
    - NDC
    - RxNorm
    - ATC

### Example 1: Look up ICD9CM code

Let's first try to look up the ICD9 CM code 428.0, which stands for "Congestive heart failure, unspecified".
"""

from pyhealth.medcode import InnerMap

# initialize an InnerMap
icd9cm = InnerMap.load("ICD9CM")

# let's first check if the code is in ICD9CM
"428.0" in icd9cm

# next let's look up this code
icd9cm.lookup("428.0")

# we can also get the ancestors of this code
icd9cm.get_ancestors("428.0")

icd9cm.get_descendants("428")

"""Note that if the code is not in standard format (e.g., "4280" instead of "428.0"), medcode will automatically normalize it."""

# non-standard format
icd9cm.lookup("4280")

"""**TODO**: look up the following ICD9CM codes: 480.1, 280, 394"""

icd9cm.lookup("480.1")

icd9cm.lookup("280")

icd9cm.lookup("394")

"""**TODO:** look up the following CCSPROC codes: 2, 5, 10"""

ccsproc = InnerMap.load("CCSPROC")

ccsproc.lookup("2")

ccsproc.lookup("5")

ccsproc.lookup("10")

"""### Example 2: Look up ATC code

For the medication code ATC, medcode provides additional information from DrugBank,.
"""

atc = InnerMap.load("ATC")

# let's search for M01AE51
atc.lookup("M01AE51")

#  DrugBank ID
print(atc.lookup("M01AE51", "drugbank_id"))

#  Drug description from DrugBank
print(atc.lookup("M01AE51", "description"))

#  Drug indication from DrugBank
print(atc.lookup("M01AE51", "indication"))

#  Drug SMILES string from DrugBank
print(atc.lookup("M01AE51", "smiles"))

"""**TODO**: look up other ATC codes from https://go.drugbank.com/atc

E.g., B01AC06 for Aspirin.
"""

print(atc.lookup("B01AC06", "indication"))

# print(atc.lookup("A12CE02", "drugbank_id"))
atc.convert("A12CE02", level=3)

"""### 2. Code Mapping

`class pyhealth.medcode.CodeMap`
- **[Args]**:
    - source: source code vocabulary to map from
    - target: target code vocabulary to map to

- **[Functionality]**:
- map(source_code): maps source_code to the target vocabulary

Currently, we support the following mapping:

- With in diagnosis codes:
    - ICD9CM <-> CCSCM
    - ICD10CM <-> CCSCM
- With in procedure codes:
    - ICD9PROC <-> CCSPROC
    - ICD10PROC <-> CCSPROC
- With in medication codes:
    - NDC <-> RxNorm
    - NDC <-> ATC
    - RxNorm <-> ATC
- Between diagnosis and medication codes:
    - ATC <-> ICD9CM

### Example 3: Map ICD9CM code to CCSCM code

Let's try to map the ICD9 CM code 428.0, which stands for "Congestive heart failure, unspecified", to CCS CM code.
"""

from pyhealth.medcode import CrossMap

mapping = CrossMap.load(source_vocabulary="ICD9CM", target_vocabulary="CCSCM")
mapping.map("428.0")

"""Note that the returned variable is a list of codes, due to the possible one-to-many mapping."""

# let's check if the mapping is correct
ccscm = InnerMap.load("CCSCM")
print(ccscm.lookup("108"))

"""chf;nonhp: Congestive heart failure, nonhypertensive

### Example 4: Map NDC code to ATC code

Let's try to map the NDC code 5058060001, which is acetaminophen 325 MG Oral Tablet [Tylenol].

See https://fda.report/NDC/50580-496.
"""

from pyhealth.medcode import CrossMap

# (it may take up to 5 minutes)
mapping = CrossMap.load("NDC", "RxNorm")
mapping.map("50580049698")

# (it may take up to 5 minutes)
mapping = CrossMap.load("RxNorm", "NDC")
mapping.map("209387")

# let's check if the mapping is correct
ccscm = InnerMap.load("RxNorm")
ccscm.lookup("209387")

"""**TODO:** Map NDC code 50090539100 to ATC.

See https://ndclist.com/ndc/50090-5391/package/50090-5391-0.
"""

mapping = CrossMap.load("NDC", "ATC")
mapping.map("50090539100")



"""If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""

Here is the code content for pipeline_4_mortality_prediction.py:
# -*- coding: utf-8 -*-
"""Pipeline 4: Mortality Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qblpcv4NWjrnADT66TjBcNwOe8x6wU4c
"""

!pip install pyhealth

"""### **Step 1: Load dataset**
- **[README]:** We call [pyhealth.datasets](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) to process and obtain the dataset.
  - `root` is the arguments directing to the data folder.
  - `tables` is a list of table names from raw databases, which specifies the information that will be used in building the pipeline. Currently, we provide [MIMIC3Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html), [MIMIC4Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC4Dataset.html), [eICUDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.eICUDataset.html), [OMOPDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.OMOPDataset.html).
  - `code_mapping [default: None]` asks a directionary input, specifying the new coding systems for each data table. For example, `{"NDC": ("ATC", {"target_kwargs": {"level": 3}})}` means that our pyhealth will automatically change the codings from `NDC` into ATC-3 level for tables if any.
  - `dev`: if set `True`, will only load a smaller set of patients.
- **[Next Step]:** This `pyhealth.datasets` object will be used in **Step 2**.
- **[Advanced Use Case]:** Researchers can use the dict-based output alone `dataset.patients` alone for supporting their own tasks.
"""

from pyhealth.datasets import MIMIC3Dataset

mimic3_ds = MIMIC3Dataset(
        root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
)

mimic3_ds.stat()

# data format
mimic3_ds.info()

### **Step 2: Define healthcare task**
- **[README]:** This step assigns a **task function** to the dataset for data loading [pyhealth.tasks](https://pyhealth.readthedocs.io/en/latest/api/tasks.html). The **task function** specifics how to process each pateint's data into a set of samples for the downstream machine learning models.
- **[Next Step]:** This `pyhealth.tasks` object will be used in **Step 3**.
"""

from pyhealth.tasks import mortality_prediction_mimic3_fn

mimic3_ds = mimic3_ds.set_task(task_fn=mortality_prediction_mimic3_fn)
# stats info
mimic3_ds.stat()

mimic3_ds[0]

from pyhealth.datasets.splitter import split_by_patient
from pyhealth.datasets import split_by_patient, get_dataloader

# data split
train_dataset, val_dataset, test_dataset = split_by_patient(mimic3_ds, [0.8, 0.1, 0.1])

# create dataloaders (they are <torch.data.DataLoader> object)
train_loader = get_dataloader(train_dataset, batch_size=64, shuffle=True)
val_loader = get_dataloader(val_dataset, batch_size=64, shuffle=False)
test_loader = get_dataloader(test_dataset, batch_size=64, shuffle=False)

"""### **Step 3: Define ML Model**
- **[README]:** We initialize an ML model for the healthcare task by calling [pyhealth.models](https://pyhealth.readthedocs.io/en/latest/api/models.html).
- **[Next Step]:** This `pyhealth.models` object will be used in **Step 4**.
- **[Other Use Case]:** Our `pyhealth.models` object is as general as any instance from `torch.nn.Module`. Users may use it separately for supporting any other customized pipeline.
"""

from pyhealth.models import Transformer

model = Transformer(
    dataset=mimic3_ds,
    # look up what are available for "feature_keys" and "label_keys" in dataset.samples[0]
    feature_keys=["conditions", "procedures"],
    label_key="label",
    mode="binary",
)

model

from pyhealth.models import RNN

model = RNN(
    dataset=mimic3_ds,
    # look up what are available for "feature_keys" and "label_keys" in dataset.samples[0]
    feature_keys=["conditions", "procedures"],
    label_key="label",
    mode="binary",
)

model

from pyhealth.models import RNN

model = RNN(
    dataset=mimic3_ds,
    # look up what are available for "feature_keys" and "label_keys" in dataset.samples[0]
    feature_keys=["conditions", "procedures", "drugs"],
    label_key="label",
    mode="binary",
)

model

"""### **Step 4: Model Training**
- **[README]:** We call our [pyhealth.train.Trainer](https://pyhealth.readthedocs.io/en/latest/api/trainer.html) to train the model by giving the `train_loader`, the `val_loader`, val_metric, and specify other arguemnts, such as epochs, optimizer, learning rate, etc. The trainer will automatically save the best model and output the path in the end.
- **[Next Step]:** The best model will be used in **Step 5** for evaluation.

"""

model

from pyhealth.trainer import Trainer

trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=3,
    monitor="pr_auc",
)

"""### **Step 5: Evaluation**"""

# option 1: use our built-in evaluation metric
score = trainer.evaluate(test_loader)
print (score)

# option 2: use our pyhealth.metrics to evaluate
from pyhealth.metrics.binary import binary_metrics_fn
y_true, y_prob, loss = trainer.inference(test_loader)
binary_metrics_fn(y_true, y_prob, metrics=["pr_auc", "roc_auc", "f1"])



"""If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""

Here is the code content for advanced_case_3_adopt_customized_models_to_pyhealth.py:
# -*- coding: utf-8 -*-
"""Advanced Case 3: Adopt customized models to PyHealth

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F_NJ90GC8_Eq-vKTf7Tyziew4gWjjKoH

### **Preparation**
- install pyhealth
"""

!pip install pyhealth

"""### **How to adopt your own ML models into PyHealth**
- In our [five-stage pipeline](https://pyhealth.readthedocs.io/en/latest/) (load dataset, define task functions, initialize ML models, model training, model inference), users can adopt their own models into the third stage.
- We use the readmission prediction task on MIMIC-III as an example

#### **Preparation: readmission prediction on MIMIC-III**
- we use [pyhealth.datasets.MIMIC3Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html) to process a simulated MIMIC-III dataset.
    - Please check [Tutorial 1](https://colab.research.google.com/drive/18kbzEQAj1FMs_J9rTGX8eCoxnWdx4Ltn?usp=sharing) for some examples
- we use [pyhealth.tasks.readmission_prediction_mimic3_fn](https://pyhealth.readthedocs.io/en/latest/api/tasks/pyhealth.tasks.readmission_prediction.html) to obtain the train/val/test data loaders.
    - Please check [Tutorial 2](https://colab.research.google.com/drive/1r7MYQR_5yCJGpK_9I9-A10HmpupZuIN-?usp=sharing) for some examples
"""

from pyhealth.datasets import MIMIC3Dataset
from pyhealth.tasks import readmission_prediction_mimic3_fn

# Stage 1: load dataset
dataset = MIMIC3Dataset(
        root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
)

# Stage 2: define task function
dataset = dataset.set_task(readmission_prediction_mimic3_fn)

# check dataset statistics
print (dataset.stat())

# check the format of each sample
dataset.samples[0]

from pyhealth.tasks import drug_recommendation_mimic3_fn
from pyhealth.datasets import split_by_patient, get_dataloader

# dataset split by patient id
train_ds, val_ds, test_ds = split_by_patient(dataset, [0.8, 0.1, 0.1])

# obtain train/val/test dataloader, they are <torch.data.DataLoader> object
train_loader = get_dataloader(train_ds, batch_size=32, shuffle=True)
val_loader = get_dataloader(val_ds, batch_size=32, shuffle=False)
test_loader = get_dataloader(test_ds, batch_size=32, shuffle=False)

"""### **Example 1: Deepr model**
> In this example, we build a Deepr model [1] (using one Deepr encoder for each feature).

[1] P. Nguyen, T. Tran, N. Wickramasinghe and S. Venkatesh, " Deepr : A Convolutional Net for Medical Records," in IEEE Journal of Biomedical and Health Informatics, vol. 21, no. 1, pp. 22-30
"""

from typing import Tuple, List, Dict, Optional
import functools

import torch
import torch.nn as nn

from pyhealth.datasets import BaseDataset
from pyhealth.models import BaseModel
class MyPyTorchDeepr(nn.Module):
    def __init__(self, feature_size: int = 100, window: int = 1, hidden_size: int = 3) -> None:
        super().__init__()
        self.conv = torch.nn.Conv1d(feature_size, hidden_size, kernel_size=2 * window + 1)

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        if mask is not None: x = x * mask.unsqueeze(-1)
        x = x.permute(0, 2, 1)  # [batch size, input size, sequence len]
        x = torch.relu(self.conv(x))
        return x.max(-1)[0]

class MyPyHealthDeepr(BaseModel):
    def __init__(self, dataset: BaseDataset, feature_keys: List[str], label_key: str, mode: str,
                 embedding_dim=128, hidden_dim=32, **kwargs):
        super().__init__(dataset, feature_keys, label_key, mode)

        # Any BaseModel should have these attributes, as functions like add_feature_transform_layer uses them
        self.feat_tokenizers = {}
        self.embeddings = nn.ModuleDict()
        self.linear_layers = nn.ModuleDict()
        self.label_tokenizer = self.get_label_tokenizer()
        self.embedding_dim = embedding_dim

        # self.add_feature_transform_layer will create a transformation layer for each feature
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]
            self.add_feature_transform_layer(
                feature_key, input_info, special_tokens=["<pad>", "<unk>", "<gap>"]
            )

        # We create one MyPyTorchDeepr for each feature (e.g. drug, diagnosis, etc.)
        self.deepr_modules = nn.ModuleDict()
        for feature_key in feature_keys:
            self.deepr_modules[feature_key] = MyPyTorchDeepr(feature_size=embedding_dim, hidden_size=hidden_dim, **kwargs)

        # final output layer
        output_size = self.get_output_size(self.label_tokenizer)
        self.fc = nn.Linear(len(self.feature_keys) * hidden_dim, output_size)

    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
        patient_emb = []
        for feature_key in self.feature_keys:
            input_info = self.dataset.input_info[feature_key]

            # each patient's feature is represented by [[code1, code2],[code3]]
            assert input_info["dim"] == 3 and input_info["type"] == str

            # Aggregating features according to deepr's requirement
            feature_vals = [
                    functools.reduce(lambda a, b: a + ["<gap>"] + b, _)
                    for _ in kwargs[feature_key]
                ]

            x = self.feat_tokenizers[feature_key].batch_encode_2d(feature_vals, padding=True, truncation=False)
            x = torch.tensor(x, dtype=torch.long, device=self.device)
            pad_idx = self.feat_tokenizers[feature_key].vocabulary("<pad>")
            #create the mask
            mask = (x != pad_idx).long()
            embeds = self.embeddings[feature_key](x)
            feature_embed = self.deepr_modules[feature_key](embeds, mask)
            patient_emb.append(feature_embed)

        # (patient, features * hidden_dim)
        patient_emb = torch.cat(patient_emb, dim=1)
        # (patient, label_size)
        logits = self.fc(patient_emb)
        # obtain y_true, loss, y_prob
        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)
        loss = self.get_loss_function()(logits, y_true)
        y_prob = self.prepare_y_prob(logits)
        return {"loss": loss, "y_prob": y_prob, "y_true": y_true}

"""#### **model training and inference**
We use this above model for training and inference with our pyhealth package.
"""

# Stage 3: define model
device = "cpu"

model = MyPyHealthDeepr(
    dataset=dataset,
    feature_keys=["conditions", "drugs", "procedures"],
    label_key="label",
    mode="binary",
    embedding_dim=128,
    hidden_dim=64,
)
model.to(device)

# Stage 4: model training
from pyhealth.trainer import Trainer

trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=3,
    monitor="pr_auc",
)

# evaluation option 1: use our built-in evaluation metric
result = trainer.evaluate(test_loader)
print ('\n', result)

# evaluation option 2: use pyhealth.metrics
from pyhealth.metrics.binary import binary_metrics_fn
y_true, y_prob, loss = trainer.inference(test_loader)
result = binary_metrics_fn(y_true, y_prob, metrics=["pr_auc", "roc_auc"])
print ('\n', result)

# evaluation option 3: use sklearn.metrics
from sklearn.metrics import average_precision_score, roc_auc_score
y_pred = (y_prob > 0.5).astype('int')
print (
    '\n',
    'roc_auc', roc_auc_score(y_true, y_prob),
    'pr_auc:', average_precision_score(y_true, y_prob)
)



"""If this helps, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""



Here is the code content for tutorial_3_pyhealth_models.py:
# -*- coding: utf-8 -*-
"""Tutorial 3: pyhealth.models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LcXZlu7ZUuqepf269X3FhXuhHeRvaJX5

### **Preparation**
- install pyhealth alpha version
"""

!pip install pyhealth

"""### **Instruction on [pyhealth.models](https://pyhealth.readthedocs.io/en/latest/api/models.html)**
- **[README]**: In this package, we provide common deep learning models (e.g., RNN, CNN, Transformer) and special healthcare deep learning models, such as (e.g., RETAIN, SafeDrug, GAMENet). All except some special models (e.g., GAMENet, SafeDrug, MICRON are designed only for drug recommendation task) can be applied to all healthcare prediction tasks. Note that, we have provided two callable methods of each deep learning model:
  - Model, such as RNN, CNN, Transformer, RETAIN, **initialized by our dataset object**
  - ModelLayer (alternatively), such as RNNLayer, CNNLayer, TransformerLayer, RETAINLayer. Alternatively, **initialized by auxiliary information (specified for each layer)**.

- **[Arguments for Model]**:
  The arguments for each DL Model follows the arguments below.
    - `dataset`: this is the [pyhealth.dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) object. All model specific processing is based on this and is handled within the Model class.
    - `feature_keys`: a list of string-based table names, indicating that these tables will be used in the pipeline.
    - `label_key`: currently, we only support `label`, defined in task function.
    - `mode`: `multiclass`, `multilabel`, or `binary`.
    
- **[Arguments for the ModelLayer]**:
Alternatively, if users do not want to use the [pyhealth.dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) object for initializing Model, then they can choose to call the ModelLayer by preparing inputs following the requirements. The inputs of each ModelLayer can be different (refer to [pyhealth.models](https://pyhealth.readthedocs.io/en/latest/api/models.html). For example, we list the arguments of a RNNLayer below:
  - `input_size`: input size of rnn
  - `hidden_size`: hidden size of rnn
  - `rnn_type`: type of rnn, e.g. GRU, LSTM
  - `num_layers`: number of rnn layers
  - `dropout`: dropout rate
  - `bidirectional`: whether to use bidirectional rnn

### **Step 1 & 2: Prepare datasets and task function**
- We use **OMOP dataset** for **readmission prediction** task. More details can refer to [Tutorial 1](https://colab.research.google.com/drive/18kbzEQAj1FMs_J9rTGX8eCoxnWdx4Ltn?usp=sharing) and [Tutorial 2](https://colab.research.google.com/drive/1r7MYQR_5yCJGpK_9I9-A10HmpupZuIN-?usp=sharing)
"""

from pyhealth.datasets import MIMIC3Dataset
from pyhealth.tasks import readmission_prediction_mimic3_fn

dataset = MIMIC3Dataset(
        root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        code_mapping={"NDC": "ATC"},
        # develop mode is True enables small data load
        dev=True,
)

# specify which task
dataset = dataset.set_task(readmission_prediction_mimic3_fn)

# check the format of the first sample
dataset.samples[2]

"""### **Step 3 (Example): Using RETAIN or RETAIN Layer**
- Option 1: we choose to initialize the **pyhealth.models.RETAIN** model.
- Option 2: we choose to customize a new model with our **pyhealth.models.RETAINLayer**.

"""

# option 1

from pyhealth.models import RETAIN

device = "cpu"

model = RETAIN(
    # argument 1: call the dataset
    dataset=dataset,
    # argument 2: use a subset of keys in the data sample format for features
    # look up what are available for "feature_keys" and "label_keys" in dataset.samples[0]
    feature_keys=["conditions", "procedures"],
    # argument 3: use `label` for indicating the prediction label_key
    label_key="label",
    # argument 4: set the embedding dimension
    embedding_dim=128,
    # argument 5: what type of tasks, multiclass, multilabel, or binary?
    mode="binary",
)
model.to(device)

# option 2

from pyhealth.models import RETAINLayer
import torch.nn as nn

class NewModel(nn.Module):
    def __init__(
        self,
        input_size: int = 64,
        hidden_size: int = 128,
        num_layers: int = 1,
        dropout: float = 0.5,
    ):
        super(NewModel, self).__init__()

        # TODO: implement other module 1
        self.module1 = None

        # initilize the RNNLayer
        self.rnn = RETAINLayer(input_size, dropout)

        # TODO: implement other module 2
        self.module2 = None


    def forward(self, x):
        x = self.module1(x)
        # call the RNNLayer
        x = self.rnn(x)
        x = self.module2(x)
        return x

model = NewModel()

"""### **Step 4 (Example): Using Transformer model**
- Option 1: we choose to initialize the **pyhealth.models.Transformer** model.
- Option 2: we choose to customize a new model with our **pyhealth.models.TransformerLayer**.

"""

# option 1

from pyhealth.models import Transformer

device = "cpu"

model = Transformer(
    # argument 1: call the dataset
    dataset=dataset,
    # argument 2: use a subset of keys in the data sample format for features
    # look up what are available for "feature_keys" and "label_keys" in dataset.samples[0]
    feature_keys=["conditions", "procedures"],
    # argument 3: use `label` for indicating the prediction label_key
    label_key="label",
    # argument 4: set the embedding dimension
    embedding_dim=128,
    # argument 5: what type of tasks, multiclass, multilabel, or binary?
    mode="binary",
)
model.to(device)

# option 2: build your new model

from pyhealth.models import TransformerLayer
import torch.nn as nn

class NewModel(nn.Module):
    def __init__(
        self,
        input_size: int = 64,
        hidden_size: int = 128,
        num_layers: int = 1,
        dropout: float = 0.5,
    ):
        super(NewModel, self).__init__()

        # you can implement other modules here
        self.module1 = None

        # initilize the RNNLayer
        self.transformer = TransformerLayer(input_size, dropout)

        # you can implement other modules here
        self.module2 = None


    def forward(self, x):
        x = self.module1(x)
        # call the RNNLayer
        x = self.transformer(x)
        x = self.module2(x)
        return x

model = NewModel()



"""If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""

Here is the code content for pipeline_1_drug_recommendation.py:
# -*- coding: utf-8 -*-
"""Pipeline 1: Drug Recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10CSb4F4llYJvv42yTUiRmvSZdoEsbmFF
"""

!pip install pyhealth

"""### **Step 1: Load dataset**
- **[README]:** We call [pyhealth.datasets](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) to process and obtain the dataset.
  - `root` is the arguments directing to the data folder.
  - `tables` is a list of table names from raw databases, which specifies the information that will be used in building the pipeline. Currently, we support [MIMIC3Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html), [MIMIC4Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC4Dataset.html), [eICUDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.eICUDataset.html), [OMOPDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.OMOPDataset.html).
  - `code_mapping [default: None]` asks a directionary input, specifying the new coding systems for each data table. For example, `{"NDC": ("ATC", {"target_kwargs": {"level": 3}})}` means that our pyhealth will automatically change the codings from `NDC` into ATC-3 level for tables if any.
  - `dev`: if set `True`, will only load a smaller set of patients.
- **[Next Step]:** This `pyhealth.datasets` object will be used in **Step 2**.
"""

from pyhealth.datasets import MIMIC3Dataset

mimic3_ds = MIMIC3Dataset(
        root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        code_mapping={"NDC": ("ATC", {"target_kwargs": {"level": 3}})},
        dev=True
)

mimic3_ds.stat()

# data format
mimic3_ds.info()

""" check the information """
# mimic3_ds.patients
# mimic3_ds.patients['947']
# mimic3_ds.patients['947'].visits
# mimic3_ds.patients['947'].visits['100999']
# mimic3_ds.patients['947'].visits['100999'].get_code_list('DIAGNOSES_ICD')

"""### **Step 2: Define healthcare task**
- **[README]:** This step assigns a **task function** to the dataset for data loading [pyhealth.tasks](https://pyhealth.readthedocs.io/en/latest/api/tasks.html). The **task function** specifics how to process each pateint's data into a set of samples for the downstream machine learning models.
- **[Next Step]:** This `pyhealth.tasks` object will be used in **Step 3**.
"""

from pyhealth.tasks import drug_recommendation_mimic3_fn

mimic3_ds = mimic3_ds.set_task(task_fn=drug_recommendation_mimic3_fn)
# stats info
mimic3_ds.stat()

# mimic3_ds.samples[0]

from pyhealth.datasets.splitter import split_by_patient
from pyhealth.datasets import split_by_patient, get_dataloader

# data split
train_dataset, val_dataset, test_dataset = split_by_patient(mimic3_ds, [0.8, 0.1, 0.1])

# create dataloaders (they are <torch.data.DataLoader> object)
train_loader = get_dataloader(train_dataset, batch_size=64, shuffle=True)
val_loader = get_dataloader(val_dataset, batch_size=64, shuffle=False)
test_loader = get_dataloader(test_dataset, batch_size=64, shuffle=False)

"""### **Step 3: Define ML Model**
- **[README]:** We initialize an ML model for the healthcare task by calling [pyhealth.models](https://pyhealth.readthedocs.io/en/latest/api/models.html).
- **[Next Step]:** This `pyhealth.models` object will be used in **Step 4**.
- **[Other Use Case]:** Our `pyhealth.models` object is as general as any instance from `torch.nn.Module`. Users may use it separately for supporting any other customized pipeline.
"""

from pyhealth.models import Transformer

model = Transformer(
    dataset=mimic3_ds,
    feature_keys=["conditions", "procedures"],
    label_key="drugs",
    mode="multilabel",
)

"""### **Step 4: Model Training**
- **[README]:** We call our [pyhealth.train.Trainer](https://pyhealth.readthedocs.io/en/latest/api/trainer.html) to train the model by giving the `train_loader`, the `val_loader`, val_metric, and specify other arguemnts, such as epochs, optimizer, learning rate, etc. The trainer will automatically save the best model and output the path in the end.
- **[Next Step]:** The best model will be used in **Step 5** for evaluation.

"""

from pyhealth.trainer import Trainer

trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=3,
    monitor="pr_auc_samples",
)

"""### **Step 5: Evaluation**"""

# option 1: use our built-in evaluation metric
score = trainer.evaluate(test_loader)
print (score)

# option 2: use our pyhealth.metrics to evaluate
from pyhealth.metrics.multilabel import multilabel_metrics_fn
y_true, y_prob, loss = trainer.inference(test_loader)
multilabel_metrics_fn(y_true, y_prob, metrics=["pr_auc_samples"])



"""If this helps, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""



Here is the code content for tutorial_2_pyhealth_tasks.py:
# -*- coding: utf-8 -*-
"""Tutorial 2: pyhealth.tasks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r7MYQR_5yCJGpK_9I9-A10HmpupZuIN-
"""

!pip install pyhealth

"""### **Instruction on [pyhealth.tasks](https://pyhealth.readthedocs.io/en/latest/api/tasks.html)**
- **[README]**: This module is used to define the healthcare AI task by **task function**. The **task function** specifics how to process each pateint's data (the structured dataset) into a set of samples for the downstream machine learning models.
  - The current [pyhealth.tasks](https://pyhealth.readthedocs.io/en/latest/api/tasks.html) module contains several common healthcare tasks AS EXAMPLES, such as drug recommendation, length of stay prediction. Users can use our functions to work with existing tasks or define customized healthcare task functions.

- **[Arguments]**:
  - `patient`: A [Patient](https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Patient.html) object.

- **[Returns]**:
    - `samples`: a list of samples, each sample is a dict with patient_id, visit_id, and other task-specific attributes as key

- **[Functionality]**: currently, we provide [the following tasks](https://pyhealth.readthedocs.io/en/latest/api/tasks.html) on the datasets:
  - drug_recommendation_mimic3_fn
  - readmission_prediction_mimic3_fn
  - mortality_prediction_mimic3_fn
  - length_of_stay_prediction_mimic3_fn
  - We provide similar functions for MIMIC-IV, eICU, and OMOP dataset.

### **Example 1: Mortality Prediction on MIMIC-III**
- step 1: we use the DIAGNOSES_ICD, PROCEDURES_ICD table information.
- step 2: we remove visit that has empty conditions and procedures
- step 3: we create a dict-based sample and assign `mortality status` as the prediction target: `label`

#### Notes
- To lookup [visit](https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Visit.html) attributes and functions
- To lookup [patient](https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Patient.html) attributes and functions
"""

def mortality_prediction_mimic3_fn(patient):
    """
    Mortality prediction aims at predicting whether the patient will decease in the
        next hospital visit based on the clinical information from current visit
        (e.g., conditions and procedures).

    """
    samples = []
    for visit_idx in range(len(patient) - 1):
        visit = patient[visit_idx]
        next_visit = patient[visit_idx + 1]

        # obtain the label
        if next_visit.discharge_status not in [0, 1]:
            mortality_label = 0
        else:
            mortality_label = int(next_visit.discharge_status)

        # step 1: obtain features
        conditions = visit.get_code_list(table="DIAGNOSES_ICD")
        procedures = visit.get_code_list(table="PROCEDURES_ICD")

        # step 2: exclusion criteria
        if len(conditions) + len(procedures) == 0: continue

        # step 3: assemble the sample
        # REMEMBER: the key here will be the "feature_keys" and "label_key" for initializing the downstream model
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": conditions, # feature key 1
                "procedures": procedures, # feature key 2
                "label": mortality_label, # label key
            }
        )

    return samples

"""#### We apply this task function on MIIMC-III dataset"""

from pyhealth.datasets import MIMIC3Dataset

dataset = MIMIC3Dataset(
    root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD"],
    dev=True,
)

"""In practice, we will automatically iterate through all patients for you using the set_task() function."""

# use `.set_task()`, which returns a task dataset
task_ds = dataset.set_task(task_fn=mortality_prediction_mimic3_fn)

# we show the statistic of the task
task_ds.stat()

# what many keys are available?
task_ds.available_keys

# check the first sample
task_ds.samples[0]

"""##### TODO: Implement a task function to predict the mortality of the current visit"""

def new_mortality_prediction_mimic3_fn(patient):
    """
    Mortality prediction aims at predicting whether the patient will decease in this
        hospital visit based on the clinical information from current visit
        (e.g., conditions and procedures).

    """
    samples = []
    for visit in patient:

        # obtain the label
        mortality_label = int(visit.discharge_status)

        # step 1: obtain features
        conditions = visit.get_code_list(table="DIAGNOSES_ICD")
        procedures = visit.get_code_list(table="PROCEDURES_ICD")

        # step 2: exclusion criteria
        if len(conditions) + len(procedures) == 0: continue

        # step 3: assemble the sample
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": conditions,
                "procedures": procedures,
                "label": mortality_label,
            }
        )

    return samples

"""### **Example 2: Readmission Prediction on MIMIC-IV data**
- step 1: we use condition, procedures and drugs information from tables `"diagnoses_icd"`, `"procedures_icd"`, `"prescriptions"`
- step 2: we remove visit that has empty conditions, procedures, and drugs
- step 3: we create a dict-based sample and assign `readmission label` as the prediction target: `label`
"""

def readmission_prediction_mimic4_fn(patient, time_window=15):
    """
    Readmission prediction aims at predicting whether the patient will be readmitted
        into hospital within time_window days based on the clinical information from
        current visit (e.g., conditions and procedures).
    """
    samples = []

    # we will drop the last visit
    for i in range(len(patient) - 1):
        visit = patient[i]
        next_visit = patient[i + 1]

        # obtain label: get time difference between current visit and next visit
        time_diff = (next_visit.encounter_time - visit.encounter_time).days
        readmission_label = 1 if time_diff < time_window else 0

        # step 1: obtain features
        conditions = visit.get_code_list(table="diagnoses_icd")
        procedures = visit.get_code_list(table="procedures_icd")
        drugs = visit.get_code_list(table="prescriptions")

        # step 2: exclusion criteria
        # exclude: visits without condition, procedure, and drug code
        if len(conditions) + len(procedures) + len(drugs) == 0: continue

        # step 3: assemble the sample
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": conditions,
                "procedures": procedures,
                "drugs": drugs,
                "label": readmission_label,
            }
        )

    return samples

"""
>>> from pyhealth.datasets import MIMIC4Dataset
>>> mimic4_base = MIMIC4Dataset(
...     root="/srv/local/data/physionet.org/files/mimiciv/2.0/hosp",
...     tables=["diagnoses_icd", "procedures_icd", "prescriptions"],
...     code_mapping={"ICD10PROC": "CCSPROC"},
... )

>>> from pyhealth.tasks import readmission_prediction_mimic4_fn
>>> mimic4_sample = mimic4_base.set_task(readmission_prediction_mimic4_fn)

>>> mimic4_sample.samples[0]
{
    'visit_id': '130744',
    'patient_id': '103',
    'diagnoses_icd': [5],
    'conditions': [['42', '109', '19', '122', '98', '663', '58', '51']],
    'procedures': [['1']],
    'label': 0
}
"""

"""### **Example 3: Drug Recommendation on eICU**
- step 1: we use condition, procedures and drugs information from tables `"diagnosis"`, `"physicalExam"`, `"medication"`
- step 2: we remove visit that has empty conditions, procedures or drugs
- step 3: we create a dict-based sample and assigne `"drugs"` as the prediction target: `label`
- step 4: we remove patients with fewer than 2 visits and recursively add historical information for conditions, procedures and drugs (we remove the current set of drugs to prevent label leaking).
"""

def drug_recommendation_eicu_fn(patient):
    samples = []
    for visit in patient:

        # step 1: obtain visit-level information, conditions, procedures, and drugs
        conditions = visit.get_code_list(table="diagnosis")
        procedures = visit.get_code_list(table="physicalExam")
        drugs = visit.get_code_list(table="medication")

        # step 2: exclusion criteria: cohort selection
        if len(conditions) * len(procedures) * len(drugs) == 0: continue

        # step 3: assemble the sample
        # REMEMBER: the key here will be the "feature_keys" and "label_key" for initializing the downstream model
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                "conditions": conditions,
                "procedures": procedures,
                "drugs": drugs,
                "label": drugs,
            }
        )

    # step 4: patient-level cohort selection and other process
    # exclude patients with less than 2 visit
    if len(samples) < 2:
        return []

    """
    Add historical visit (use "conditions" key as an example)
        before this step:
            samples[0]["conditions"]: 1st visit
            samples[1]["conditions"]: 2nd visit
            samples[2]["conditions"]: 3rd visit
            ...

        after this step:
            samples[0]["conditions"]: [1st visit]
            samples[1]["conditions"]: [1st visit, 2nd visit]
            samples[2]["conditions"]: [1st visit, 2nd visit, 3rd visit]
            ...
    """
    samples[0]["conditions"] = [samples[0]["conditions"]]
    samples[0]["procedures"] = [samples[0]["procedures"]]
    samples[0]["drugs"] = [samples[0]["drugs"]]

    for i in range(1, len(samples)):
        samples[i]["conditions"] = samples[i - 1]["conditions"] + [
            samples[i]["conditions"]
        ]
        samples[i]["procedures"] = samples[i - 1]["procedures"] + [
            samples[i]["procedures"]
        ]
        samples[i]["drugs"] = samples[i - 1]["drugs"] + [
            samples[i]["drugs"]
        ]
    for i in range(len(samples)):
        samples[i]["drugs"][i] = []

    return samples


"""
>>> from pyhealth.datasets import eICUDataset
>>> eicu_base = eICUDataset(
...     root="/srv/local/data/physionet.org/files/eicu-crd/2.0",
...     tables=["diagnosis", "medication"],
... )

>>> from pyhealth.tasks import drug_recommendation_eicu_fn
>>> eicu_sample = eicu_base.set_task(drug_recommendation_eicu_fn)

>>> eicu_sample.samples[0]
{
    'visit_id': '130744',
    'patient_id': '103',
    'conditions': [['42', '109', '98', '663', '58', '51']],
    'procedures': [['1']],
    'label': [['2', '3', '4']]
}
"""



"""If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""

Here is the code content for kdd'23_(pj)_pre_trained_embedding.py:
# -*- coding: utf-8 -*-
"""KDD'23 (PJ) - Pre-trained Embedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_wHjYq91Rqa_7c8uIZnwT_KOvQLZrzyg

## **Install PyHealth and Load Packages**
"""

! git clone --branch pretrain_embedding https://github.com/sunlabuiuc/PyHealth.git
! pip install ./PyHealth/.

from pyhealth.datasets import MIMIC3Dataset
from pyhealth.datasets import split_by_patient, get_dataloader
from pyhealth.models import Transformer, RNN, RETAIN, MLP
from pyhealth.tasks import mortality_prediction_mimic3_fn, readmission_prediction_mimic3_fn, drug_recommendation_mimic3_fn, length_of_stay_prediction_mimic3_fn
from pyhealth.trainer import Trainer

from google.colab.output import eval_js
eval_js('google.colab.output.setIframeHeight("300")')

"""### **Step 1: Load dataset**
- **[README]:** We call [pyhealth.datasets](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) to process and obtain the dataset.
  - `root` is the arguments directing to the data folder.
  - `tables` is a list of table names from raw databases, which specifies the information that will be used in building the pipeline. Currently, we support [MIMIC3Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html), [MIMIC4Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC4Dataset.html), [eICUDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.eICUDataset.html), [OMOPDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.OMOPDataset.html).
  - `code_mapping [default: None]` asks a directionary input, specifying the new coding systems for each data table. For example, `{"NDC": ("ATC", {"target_kwargs": {"level": 3}})}` means that our pyhealth will automatically change the codings from `NDC` into ATC-3 level for tables if any.
  - `dev`: if set `True`, will only load a smaller set of patients.
- **[Next Step]:** This `pyhealth.datasets` object will be used in **Step 2**.


"""

dataset = MIMIC3Dataset(
    root='https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/',
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
    code_mapping={
        # "ICD9CM": "CCSCM",
        # "ICD9PROC": "CCSPROC",
        "NDC": ("ATC", {"target_kwargs": {"level": 3}})
        },
    dev=False,
    refresh_cache=False
)

"""### **Step 2: Define healthcare task**
- **[README]:** This step assigns a **task function** to the dataset for data loading [pyhealth.tasks](https://pyhealth.readthedocs.io/en/latest/api/tasks.html). The **task function** specifics how to process each pateint's data into a set of samples for the downstream machine learning models.
- **[Next Step]:** This `pyhealth.tasks` object will be used in **Step 3**.
"""

mimic3_ds = dataset.set_task(drug_recommendation_mimic3_fn)

train_dataset, val_dataset, test_dataset = split_by_patient(
    mimic3_ds, [0.8, 0.1, 0.1]
)
train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

"""### **Step 3: Define ML Model & Step 4: Model Training**

#### **Without Pre-trained Embedding**
"""

# define model
model_no_pre = Transformer(
    dataset=mimic3_ds,
    feature_keys=["conditions", "procedures"],
    label_key="drugs",
    mode="multilabel",
    embedding_dim=256,
)

# define trainer
trainer = Trainer(model=model_no_pre)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=15,
    optimizer_params = {"lr": 1e-4},
    monitor="pr_auc_samples",
)

# evaluate
raw_perf = trainer.evaluate(test_dataloader)
print(raw_perf)

"""### **Language Model's Pre-trained Embedding**

#### **With ClinicalBERT's Pre-trained Embedding**
"""

# define model
model_w_pre_cb = Transformer(
    dataset=mimic3_ds,
    feature_keys=["conditions", "procedures"],
    label_key="drugs",
    mode="multilabel",
    pretrained_emb="LM/clinicalbert",
    embedding_dim=256,
)

# define trainer
trainer = Trainer(model=model_w_pre_cb)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=15,
    optimizer_params = {"lr": 1e-4},
    monitor="pr_auc_samples",
)

# evaluate
clinicalbert_perf = trainer.evaluate(test_dataloader)
print(clinicalbert_perf)

"""#### **With SapBERT's Pre-trained Embedding**"""

# define model
model_w_pre_sp = Transformer(
    dataset=mimic3_ds,
    feature_keys=["conditions", "procedures"],
    label_key="drugs",
    mode="multilabel",
    pretrained_emb="LM/sapbert",
    embedding_dim=256,
)

# define trainer
trainer = Trainer(model=model_w_pre_sp)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=15,
    optimizer_params = {"lr": 1e-4},
    monitor="pr_auc_samples",
)

# evaluate
sapbert_perf = trainer.evaluate(test_dataloader)
print(sapbert_perf)

"""#### **With TransE's Pre-trained Embedding** (trained on full-set of UMLS*)
*https://www.nlm.nih.gov/research/umls/index.html
"""

# define model
model_w_pre_umls_ = Transformer(
    dataset=mimic3_ds,
    feature_keys=["conditions", "procedures"],
    label_key="drugs",
    mode="multilabel",
    pretrained_emb="KG/transe",
    embedding_dim=256,
)

# define trainer
trainer = Trainer(model=model_w_pre_sp)
trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=15,
    optimizer_params = {"lr": 1e-4},
    monitor="pr_auc_samples",
)

# evaluate
umls_transe_perf = trainer.evaluate(test_dataloader)
print(umls_transe_perf)

"""### **Performance Comparison**"""

print("Without Pre-trained Embedding: ")
print(raw_perf)

print("With ClinicalBERT's Embedding: ")
print(clinicalbert_perf)

print("With SapBERT's Embedding: ")
print(sapbert_perf)

print("With TransE-UMLS's Embedding: ")
print(umls_transe_perf)

while True:pass



Here is the code content for tutorial_4_pyhealth_trainer.py:
# -*- coding: utf-8 -*-
"""Tutorial 4: pyhealth.trainer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L1Nz76cRNB7wTp5Pz_4Vp4N2eRZ9R6xl

### **Preparation**
- install pyhealth alpha version
"""

!pip install pyhealth

"""### **Instruction on [pyhealth.trainer.Trainer](https://pyhealth.readthedocs.io/en/latest/api/trainer.html)**
- **[README]**: The Trainer class is the training handler (similar to [pytorch-lightning.Trainer](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html)) in the package. We use it to train the ML and DL model. It has the following arguments and functionality

- **[Arguments]**:
To initialize a trainer instance, the following environments should be specified.
  - `model`: the pyhealth.models object
  - `checkpoint_path`: path to intermediate checkpoint
  - `metrics`: which metrics to record during training. For example, we can record the pr_auc and auc_roc metrics.
  - `device`: device to use
  - `enable_logging`: enable logging
  - `output_path`: output path
  - `exp_name`: experiment/task name

- **[Functionality]**:
  - `Trainer.train()`: simply call the `.train()` function will start to train the DL or ML model.
    - `train_dataloader`: train data loader
    - `val_dataloader`: validation data loader
    - `epochs`: number of epochs to train the model
    - `optimizer_class`: optimizer, such as `torch.optim.Adam`
    - `optimizer_params`: optimizer parameters, including
      - `lr`: learning rate
      - `weight_decay`: weight decay
    - `max_grad_norm`: max gradient norm
    - `monitor`: metric name to monitor, default is None
    - `monitor_criterion`: criterion to monitor, default is "max"
    - `load_best_model_at_last`: whether to load the best model during the last iteration.

### **Step 1 & 2 & 3: Prepare datasets, task, and model**
- Example: We use **MIMIC-III dataset** and **RETAIN** model for **readmission prediction** task. Refer to [Tutorial 1](https://colab.research.google.com/drive/18kbzEQAj1FMs_J9rTGX8eCoxnWdx4Ltn?usp=sharing), [Tutorial 2](https://colab.research.google.com/drive/1r7MYQR_5yCJGpK_9I9-A10HmpupZuIN-?usp=sharing), and [Tutorial 3](https://colab.research.google.com/drive/1LcXZlu7ZUuqepf269X3FhXuhHeRvaJX5?usp=sharing).
"""

# load dataset
from pyhealth.datasets import MIMIC3Dataset
mimic3dataset = MIMIC3Dataset(
    root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
    dev=False,
)

from pyhealth.tasks import readmission_prediction_mimic3_fn
from pyhealth.datasets import split_by_patient, get_dataloader

# set task
dataset = mimic3dataset.set_task(task_fn=readmission_prediction_mimic3_fn)

# dataset split
train_ds, val_ds, test_ds = split_by_patient(dataset, [0.8, 0.1, 0.1])

# obtain train/val/test dataloader, they are <torch.data.DataLoader> object
train_loader = get_dataloader(train_ds, batch_size=32, shuffle=True)
val_loader = get_dataloader(val_ds, batch_size=32, shuffle=False)
test_loader = get_dataloader(test_ds, batch_size=32, shuffle=False)

# use RETAIN model
from pyhealth.models import RETAIN

model = RETAIN(
    dataset=dataset,
    # look up what are available for "feature_keys" and "label_keys" in dataset.samples[0]
    feature_keys=["conditions", "procedures"],
    label_key="label",
    mode="binary",
)

"""### **How to use the Trainer**
- We first initialize the trainer by config the environments, setting `enable_logging` to be True, assign the output_path as `"../output"`, and specify the device.
- We use the trainer for training the `model`. In this step, we need the `train_loader` for model training, `val_loader` to do hold-out validation and use `average_precision_score` as the monitoring metric to select the best model. Training epoch is set 5.
"""

from pyhealth.trainer import Trainer

trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=50,
    monitor="roc_auc",
)

"""### **Evaluation**"""

# evaluation option 1: use our built-in evaluation metric
result = trainer.evaluate(test_loader)
print ('\n', result)

# evaluation option 2: use pyhealth.metrics
from pyhealth.metrics.binary import binary_metrics_fn
y_true, y_prob, loss = trainer.inference(test_loader)
result = binary_metrics_fn(y_true, y_prob, metrics=["pr_auc", "roc_auc"])
print ('\n', result)

# evaluation option 3: use sklearn.metrics
from sklearn.metrics import average_precision_score, roc_auc_score
y_pred = (y_prob > 0.5).astype('int')
print (
    '\n',
    'roc_auc', roc_auc_score(y_true, y_prob),
    'pr_auc:', average_precision_score(y_true, y_prob)
)



"""If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""

Here is the code content for pipeline_3_readmission_prediction.py:
# -*- coding: utf-8 -*-
"""Pipeline 3: Readmission Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bhCwbXce1YFtVaQLsOt4FcyZJ1_my7Cs

### **Preparation**
- install pyhealth alpha version
"""

!pip install pyhealth

"""### **Step 1: Load dataset**
- **[README]:** We call [pyhealth.datasets](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) to process and obtain the dataset.
  - `root` is the arguments directing to the data folder.
  - `tables` is a list of table names from raw databases, which specifies the information that will be used in building the pipeline. Currently, we provide [MIMIC3Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html), [MIMIC4Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC4Dataset.html), [eICUDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.eICUDataset.html), [OMOPDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.OMOPDataset.html).
  - `code_mapping [default: None]` asks a directionary input, specifying the new coding systems for each data table. For example, `{"NDC": ("ATC", {"target_kwargs": {"level": 3}})}` means that our pyhealth will automatically change the codings from `NDC` into ATC-3 level for tables if any.
  - `dev`: if set `True`, will only load a smaller set of patients.
- **[Next Step]:** This `pyhealth.datasets` object will be used in **Step 2**.
- **[Advanced Use Case]:** Researchers can use the dict-based output alone `dataset.patients` alone for supporting their own tasks.
"""

from pyhealth.datasets import MIMIC3Dataset

mimic3_ds = MIMIC3Dataset(
        root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
)

mimic3_ds.stat()

# data format
mimic3_ds.info()

### **Step 2: Define healthcare task**
- **[README]:** This step assigns a **task function** to the dataset for data loading [pyhealth.tasks](https://pyhealth.readthedocs.io/en/latest/api/tasks.html). The **task function** specifics how to process each pateint's data into a set of samples for the downstream machine learning models.
- **[Next Step]:** This `pyhealth.tasks` object will be used in **Step 3**.
"""

from pyhealth.tasks import readmission_prediction_mimic3_fn

mimic3_ds = mimic3_ds.set_task(task_fn=readmission_prediction_mimic3_fn)
# stats info
mimic3_ds.stat()

from pyhealth.datasets.splitter import split_by_patient
from pyhealth.datasets import split_by_patient, get_dataloader

# data split
train_dataset, val_dataset, test_dataset = split_by_patient(mimic3_ds, [0.8, 0.1, 0.1])

# create dataloaders (they are <torch.data.DataLoader> object)
train_loader = get_dataloader(train_dataset, batch_size=64, shuffle=True)
val_loader = get_dataloader(val_dataset, batch_size=64, shuffle=False)
test_loader = get_dataloader(test_dataset, batch_size=64, shuffle=False)

"""### **Step 3: Define ML Model**
- **[README]:** We initialize an ML model for the healthcare task by calling [pyhealth.models](https://pyhealth.readthedocs.io/en/latest/api/models.html).
- **[Next Step]:** This `pyhealth.models` object will be used in **Step 4**.
- **[Other Use Case]:** Our `pyhealth.models` object is as general as any instance from `torch.nn.Module`. Users may use it separately for supporting any other customized pipeline.
"""

from pyhealth.models import Transformer

model = Transformer(
    dataset=mimic3_ds,
    # look up what are available for "feature_keys" and "label_keys" in dataset.samples[0]
    feature_keys=["conditions", "procedures"],
    label_key="label",
    mode="binary",
)

"""### **Step 4: Model Training**
- **[README]:** We call our [pyhealth.train.Trainer](https://pyhealth.readthedocs.io/en/latest/api/trainer.html) to train the model by giving the `train_loader`, the `val_loader`, val_metric, and specify other arguemnts, such as epochs, optimizer, learning rate, etc. The trainer will automatically save the best model and output the path in the end.
- **[Next Step]:** The best model will be used in **Step 5** for evaluation.

"""

from pyhealth.trainer import Trainer

trainer = Trainer(model=model)
trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=3,
    monitor="pr_auc",
)

"""### **Step 5: Evaluation**"""

# option 1: use our built-in evaluation metric
score = trainer.evaluate(test_loader)
print (score)

# option 2: use our pyhealth.metrics to evaluate
from pyhealth.metrics.binary import binary_metrics_fn
y_true, y_prob, loss = trainer.inference(test_loader)
binary_metrics_fn(y_true, y_prob, metrics=["pr_auc"])



"""If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""



Here is the code content for tutorial_1_pyhealth_datasets.py:
# -*- coding: utf-8 -*-
"""Tutorial 1: pyhealth.datasets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18kbzEQAj1FMs_J9rTGX8eCoxnWdx4Ltn
"""

!pip install pyhealth

"""### **Instruction on [pyhealth.datasets](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) (e.g., MIMIC-III, MIMIC-IV, eICU, OMOP-CDM)**
- **[README]**: This module is used to process the unstructured raw data into a structured dataset object. The process will not drop any information.
- **[Arguments]**:
  - `root` is the arguments directing to the data folder, e.g., "mimiciii/1.4/".
  - `tables` is a list of table names from raw databases, which specifies the information that will be used in building your dataset.
  - `code_mapping [default: None]` asks a directionary input, specifying the new coding systems for each data table. For example, `{"NDC": ("ATC", {"target_kwargs": {"level": 3}})},` means that our pyhealth will automatically change the codings from `NDC` into `ATC-3` level for tables if any. Look up [here](https://pyhealth.readthedocs.io/en/latest/api/medcode.html#diagnosis-codes) and check which code transforms are supported.
  - ``dev``: whether to enable dev mode (only use a small subset of the data)
  Default is False.
  - ``refresh_cache``: whether to refresh the cache; if true, the dataset will be processed from scratch and the cache will be updated. Default is False.      

- **[Functionality]**: currently, we provide the api for:
  - [MIMIC3Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html)
  - [MIMIC4Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC4Dataset.html)
  - [eICUDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.eICUDataset.html)
  - [OMOPDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.OMOPDataset.html): any OMOP-CDM based databases.

### **Example 1: MIMIC-III**
- **[Initialize]:** In this example, we load the MIMIC-III data by [pyhealth.datasets.MIMIC3Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html).

  - The root of this datasets is in (we use a synthetic MIMIC-III for demo)
    - `https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/`.
  - For each patient's ICU stay, user wants to obtain the `DIAGNOSES_ICD`, and `PROCEDURES_ICD` tables. **Note that, different databases have different raw table names**.
  - Further, user wants to `map the ICD9CM codes into CCSCM codes` for ease of downstream prediction applications.
"""

from pyhealth.datasets import MIMIC3Dataset

mimic3_ds = MIMIC3Dataset(
    # Argument 1: It specifies the data folder root.
    root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",

    # Argument 2: The users need to input a list of raw table names (e.g., DIAGNOSES_ICD.csv, PROCEDURES_ICD.csv).
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD"],

    # Argument 3: This argument input a dictionary (key is the source code
    # vocabulary and value is the target code vocabulary .
    # Default is empty dict, which means the original code will be used.
    code_mapping={"ICD9CM": "CCSCM"},
)

"""- **[Output]:** user can check the output by calling the `.info()` function. The output `mimics_ds.patients` is a dict structure, key is the `patient_id` and the value is [Patient](https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Patient.html) object."""

mimic3_ds.info()

# You can find the list of all available tables in this dataset as
mimic3_ds.available_tables

# You can also print the statistics of the entire dataset.
mimic3_ds.stat()

"""### Let us do some explorations


"""

# get patient dictionary
patient_dict = mimic3_ds.patients
print(list(patient_dict.keys())[:10])

# get the "10005" patient
patient = patient_dict["1005"]
patient.gender, patient.birth_datetime

# get the visit list of this patient
visit_dict = patient.visits
print (list(visit_dict.keys()))

# get the first visit
visit = visit_dict['101058']
visit.encounter_time, visit.available_tables

# get the diagnosis information in the visit
visit.get_event_list('DIAGNOSES_ICD')

"""### **Example 2: OMOP-CDM**
- **[Initialize]:** In this example, we load one OMOP-CDM formatted dataset by [pyhealth.datasets.OMOPDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.OMOPDataset.html). The initialization structure is very similar to previous examples, but with standard OMOP-CDM table names.
"""

from pyhealth.datasets import OMOPDataset
omop_ds = OMOPDataset(
    root="https://storage.googleapis.com/pyhealth/synpuf1k_omop_cdm_5.2.2",
    tables=["condition_occurrence", "procedure_occurrence", "drug_exposure"],
    code_mapping={},
)

"""### We can explore the information in the same way"""

# check the info
omop_ds.info()

# show the statistics
omop_ds.stat()

# get patient dictionary (show the first 10 patients)
patient_dict = omop_ds.patients
print(list(patient_dict.keys())[:10])

# get the "10005" patient
patient = patient_dict["1007"]
patient.gender, patient.birth_datetime

# get the visit list of this patient
visit_dict = patient.visits
print (list(visit_dict.keys()))

# get the first visit
visit = visit_dict['50405']
visit.encounter_time, visit.available_tables

# get the diagnosis information in the visit
print (visit.get_event_list('condition_occurrence'))
print (visit.get_event_list('procedure_occurrence'))

"""### **MIMIC-III, MIMIC-IV, eICU datasets are not allowed to distribute**
- We do not provide more examples for them
- However, the procedures are exactly the same

### **Instruction on [pyhealth.datasets.SampleDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) (users have their own processed dataset)**
- **[README]**: This module is used to load the dataset that users have processed.
- **[Args]**:
  - ``samples``: a list of samples, each sample is a dict with patient_id, visit_id, and other task-specific attributes as key.
  - ``dataset_name``: the name of the dataset. Default is None.
  - ``task_name``: the name of the task. Default is None.

- **[Attributes]**:
  - ``input_info``: Dict, a dict whose keys are the same as the keys in the samples, and values are the corresponding input information:
    - **type**: the element type of each key attribute, one of float, int, str.
    - **dim**: the list dimension of each key attribute, one of 0, 1, 2, 3.
    - **len**: the length of the vector, only valid for vector-based attributes.
  - ``patient_to_index``: Dict[str, List[int]], a dict mapping patient_id to a list of sample indices.
  - ``visit_to_index``: Dict[str, List[int]], a dict mapping visit_id to a list of sample indices.
"""

from pyhealth.datasets import SampleDataset

samples = [
    {
        "patient_id": "patient-0",
        "visit_id": "visit-0",
        "single_vector": [1, 2, 3],
        "list_codes": ['505800458', '50580045810', '50580045811'], # NDC
        "list_vectors": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],
        "list_list_codes": [['A05B', 'A05C', 'A06A'], ['A11D', 'A11E']], # ATC-4
        "list_list_vectors": [[[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]], [[7.7, 8.5, 9.4]]],
        "label": 1,
    },
    {
        "patient_id": "patient-0",
        "visit_id": "visit-1",
        "single_vector": [1, 5, 8],
        "list_codes": ['55154191800', '551541928', '55154192800', '705182798', '70518279800'],
        "list_vectors": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7]],
        "list_list_codes": [['A04A', 'B035', 'C129'], ['A07B', 'A07C']],
        "list_list_vectors": [[[1.0, 2.8, 3.3], [4.9, 5.0, 6.6]], [[7.7, 8.4, 1.3]]],
        "label": 0,
    },
]

dataset = SampleDataset(samples=samples)

# check the information of each attributes
dataset.input_info

# check the patient index collection
dataset.patient_to_index

# check the visit index collection
dataset.visit_to_index

# get the first sample
dataset.samples[0]



"""If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""

Here is the code content for advanced_case_2_work_on_customized_healthcare_task.py:
# -*- coding: utf-8 -*-
"""Advanced Case 2: Work on customized healthcare task

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gK6zPXvfFGBM1uNaLP32BOKrnnJdqRq2

### **Preparation**
- install pyhealth
"""

!pip install pyhealth

"""### **How to build your own task function?**
- **Prerequisites**: ```pyhealth.datasets``` object. Refer to [the doc](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) for more information
- **Existing examples**: Find them in [task functions](https://pyhealth.readthedocs.io/en/latest/api/tasks.html), such as:
  - drug_recommendation_mimic3_fn
  - readmission_prediction_mimic3_fn
- New tasks can be defined in same procedures. Let us see some examples.

**[Prerequisites]** We load a synthetic MIMIC dataset for designing some example tasks.
"""

from pyhealth.datasets import MIMIC3Dataset

dataset = MIMIC3Dataset(
    root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",
    tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
)

dataset.stat()

"""### **What is available, what are the input and output?**
- **What is the input?** The input of the task function is a ```pyhealth.data.Patient``` object, refer to [the doc](https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Patient.html).
- **What is available?**
  - (i) Patient level information, such as death date, birth date, gender, ethnicity, etc. Refer to [pateint level information](https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Patient.html)
  - (ii) Visit level information, such as visit encounter time, visit duration, etc. Refer to [visit level information](https://pyhealth.readthedocs.io/en/latest/api/data/pyhealth.data.Visit.html)
  - (iii) Clinical Event information. They are specified in ``tables`` arguments, when loading the dataset.
- **What is the output?** The output is a list of samples. Each sample is structured as a distionary, which gives "patient_id", "visit_id", "label", and other keys for indicating features (user can specify that).
  - If it is a patient level prediction task, then the output is [sample].
  - If it is a visit level prediction task, then the output is [sample1, sample2, ...], one sample per visit.

### **Task 1: Mortality Prediction**
> Mortality prediction aims at predicting whether the patient will decease in the next hospital visit based on the clinical information from current visit (e.g., conditions and procedures).
"""

def mortality_prediction(patient):
    """
    patient is a <pyhealth.data.Patient> object
    """
    samples = []

    # loop over all visits but the last one
    for i in range(len(patient) - 1):

        # visit and next_visit are both <pyhealth.data.Visit> objects
        visit = patient[i]
        next_visit = patient[i + 1]

        # step 1: define the mortality_label
        if next_visit.discharge_status not in [0, 1]:
            mortality_label = 0
        else:
            mortality_label = int(next_visit.discharge_status)

        # step 2: get code-based feature information
        conditions = visit.get_code_list(table="DIAGNOSES_ICD")
        procedures = visit.get_code_list(table="PROCEDURES_ICD")
        drugs = visit.get_code_list(table="PRESCRIPTIONS")

        # step 3: exclusion criteria: visits without condition, procedure, or drug
        if len(conditions) * len(procedures) * len(drugs) == 0: continue

        # step 4: assemble the samples
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                # the following keys can be the "feature_keys" or "label_key" for initializing downstream ML model
                "conditions": conditions,
                "procedures": procedures,
                "drugs": drugs,
                "label": mortality_label,
            }
        )
    return samples

mor_dataset = dataset.set_task(mortality_prediction)
mor_dataset.stat()

# let us look at the first sample
mor_dataset.samples[0]

"""### **Task 2: Drug Recommendation**
> The current drug recommendation aims at predicting the drug set of the current visit given the diagnosis and procedure information from the current visit
"""

def drug_recommendation(patient):
    samples = []

    for visit in patient:
        conditions = visit.get_code_list(table="DIAGNOSES_ICD")
        procedures = visit.get_code_list(table="PROCEDURES_ICD")
        drugs = visit.get_code_list(table="PRESCRIPTIONS")

        # step 1: exclusion criteria: visits without drug
        if len(drugs) == 0: continue

        # step 2: assemble the samples
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                # the following keys can be the "feature_keys" or "label_key" for initializing downstream ML model
                "conditions": conditions,
                "procedures": procedures,
                "label": drugs,
            }
        )
    return samples

drug_dataset = dataset.set_task(drug_recommendation)
drug_dataset.stat()

# let us look at the first sample
drug_dataset.samples[0]

"""### **Task 3: Sequential Drug Recommendation**
> Sequential drug recommendation aims at predicting the drug set of the current visit given the history of diagnosis and procedure information as well as past drug information
"""

def sequential_drug_recommendation(patient):
    samples = []

    sequential_conditions = []
    sequential_procedures = []
    sequential_drugs = [] # not include the drugs now
    for visit in patient:

        # step 1: obtain feature information
        conditions = visit.get_code_list(table="DIAGNOSES_ICD")
        procedures = visit.get_code_list(table="PROCEDURES_ICD")
        drugs = visit.get_code_list(table="PRESCRIPTIONS")

        sequential_conditions.append(conditions)
        sequential_procedures.append(procedures)
        sequential_drugs.append([])

        # step 2: exclusion criteria: visits without drug
        if len(drugs) == 0:
            sequential_drugs[-1] = drugs
            continue

        # step 3: assemble the samples
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                # the following keys can be the "feature_keys" or "label_key" for initializing downstream ML model
                "sequential_conditions": sequential_drugs.copy(),
                "sequential_procedures": sequential_procedures.copy(),
                "sequential_drugs": sequential_drugs.copy(),
                "label": drugs,
            }
        )
        sequential_drugs[-1] = drugs

    return samples

seq_dataset = dataset.set_task(sequential_drug_recommendation)
seq_dataset.stat()

# let us look at the first sample
seq_dataset.samples[0]

"""### **Task 4: Diagnosis Prediction**
> Diagnosis Prediction aims at predicting the diagnosis set of the next visit given the diagnosis, procedure, and drug information from the current visit.
"""

def diagnosis_prediction(patient):
    samples = []

    for idx, visit in enumerate(patient):
        if idx == len(patient) - 1: break
        next_visit = patient[idx+1]

        # step 1: diganosis as label
        diagnosis_label = next_visit.get_code_list(table="DIAGNOSES_ICD")

        # step 2: define features
        conditions = visit.get_code_list(table="DIAGNOSES_ICD")
        procedures = visit.get_code_list(table="PROCEDURES_ICD")
        drugs = visit.get_code_list(table="PRESCRIPTIONS")

        # step 3: exclusion criteria: visits without drug
        if len(diagnosis_label) == 0: continue

        # step 4: assemble the samples
        samples.append(
            {
                "visit_id": visit.visit_id,
                "patient_id": patient.patient_id,
                # the following keys can be the "feature_keys" or "label_key" for initializing downstream ML model
                "conditions": [conditions],
                "procedures": [procedures],
                "drugs": [drugs],
                "label": diagnosis_label,
            }
        )
    return samples

diag_dataset = dataset.set_task(diagnosis_prediction)
diag_dataset.stat()

# let us look at the first sample
diag_dataset.samples[0]



"""### **Task 5: Patient level mortality prediction**
> Patient level mortality prediction aims at using all patient hospital visits information (by the end of (N-1)th hospital visit) to predict whether the patient will desease in the Nth observed visit.
"""

def patient_level_mortality_prediction(patient):
    """
    patient is a <pyhealth.data.Patient> object
    """
    samples = []

    # if the patient only has one visit, we drop it
    if len(patient) == 1:
        return []

    # step 1: define label
    idx_last_visit = len(patient)-1
    if patient[idx_last_visit].discharge_status not in [0, 1]:
        mortality_label = 0
    else:
        mortality_label = int(patient[idx_last_visit].discharge_status)

    # step 2: obtain features
    conditions_merged = []
    procedures_merged = []
    drugs_merged = []
    for idx, visit in enumerate(patient):
        if idx == len(patient) - 1: break
        conditions = visit.get_code_list(table="DIAGNOSES_ICD")
        procedures = visit.get_code_list(table="PROCEDURES_ICD")
        drugs = visit.get_code_list(table="PRESCRIPTIONS")

        conditions_merged += conditions
        procedures_merged += procedures
        drugs_merged += drugs

    uniq_conditions = list(set(conditions_merged))
    uniq_procedures = list(set(procedures_merged))
    uniq_drugs = list(set(drugs_merged))

    # step 3: exclusion criteria
    if len(uniq_conditions) * len(uniq_procedures) * len(uniq_drugs) == 0:
        return []

    # step 4: assemble the sample
    samples.append(
        {
            "visit_id": visit.visit_id,
            "patient_id": patient.patient_id,
            "conditions": uniq_conditions,
            "procedures": uniq_procedures,
            "drugs": uniq_drugs,
            "label": mortality_label,
        }
    )
    return samples

pat_dataset = dataset.set_task(patient_level_mortality_prediction)
pat_dataset.stat()

# let us look at the first sample
pat_dataset.samples[0]

"""### **Remark**
- Other customized healthcare tasks can be defined in the same way. Most of the tasks need to define: what is the "label key"? and what is the "feature keys", such as conditions and procedures.
- The dataset.samples are the list of samples. User can take and use it to fit your own pipeline. Users may also use our follow-up models for finishing model building.
"""



"""If this helps, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""



Here is the code content for kdd'23_(zw)_chestxrayclassification.py:
# -*- coding: utf-8 -*-
"""KDD'23 (ZW)-ChestXrayClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dNh2wGM-gVMciUpJQihxFg4hGV1xzLI9

# Medical Image Classification with PyHealth

Welcome to the PyHealth tutorial on image classification. In this notebook, we will explore how to use PyHealth to analyze chest X-ray images and classify them into various chest diseases.

## Environment Setup

To begin, we need to install PyHealth and a few additional packages to support our analysis.
"""

!pip install mne pandarallel rdkit transformers

!rm -rf PyHealth
!git clone -b zhenbang/f-image_text_support https://github.com/sunlabuiuc/PyHealth.git

import sys


sys.path.append("./PyHealth")

"""## Download Data

Next, we will download the dataset containing COVID-19 data. This dataset includes chest X-ray images of normal cases, lung opacity, viral pneumonia, and COVID-19 patients. You can find more information about the dataset [here](https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database).

This dataset is hosted on Google Cloud, so the download speed should be relatively fast, taking approximately 10 seconds to complete.
"""

!wget -N https://storage.googleapis.com/pyhealth/covid19_cxr_data/archive.zip

!unzip -q -o archive.zip

!ls -1 COVID-19_Radiography_Dataset

"""Next, we will proceed with the chest X-ray classification task using PyHealth, following a five-stage pipeline.

## Step 1. Load Data in PyHealth

The initial step involves loading the data into PyHealth's internal structure. This process is straightforward: import the appropriate dataset class from PyHealth and specify the root directory where the raw dataset is stored. PyHealth will handle the dataset processing automatically.
"""

from pyhealth.datasets import COVID19CXRDataset


root = "/content/COVID-19_Radiography_Dataset"
base_dataset = COVID19CXRDataset(root)

"""Once the data is loaded, we can perform simple queries on the dataset."""

base_dataset.stat()

base_dataset.patients[0]

"""## Step 2. Define the Task

The next step is to define the machine learning task. This step instructs the package to generate a list of samples with the desired features and labels based on the data for each individual patient. Please note that in this dataset, patient identification information is not available. Therefore, we will assume that each chest X-ray belongs to a unique patient.

For this dataset, PyHealth offers a default task specifically for chest X-ray classification. This task takes the image as input and aims to predict the chest diseases associated with it.
"""

base_dataset.default_task

sample_dataset = base_dataset.set_task()

"""Additionally, let's define a transformation function for the X-ray image. This function will resize the image to 224x224 pixels, convert it to grayscale, and normalize the pixel values."""

from torchvision import transforms


transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.Grayscale(),
    transforms.Normalize(mean=[0.5862785803043838], std=[0.27950088968644304])
])


def encode(sample):
    sample["path"] = transform(sample["path"])
    return sample


sample_dataset.set_transform(encode)

"""Here is an example of a single sample, represented as a dictionary. The dictionary contains keys for feature names, label names, and other metadata associated with the sample."""

sample_dataset[0]

"""We can also check the input and output schemas, which specify the data types of the features and labels."""

sample_dataset.input_schema

sample_dataset.output_schema

"""Below, we plot the number of samples per classes, and visualize some samples."""

from collections import defaultdict
import matplotlib.pyplot as plt


label_counts = defaultdict(int)
for sample in sample_dataset.samples:
    label_counts[sample ["label"]] += 1
print(label_counts)
plt.bar(label_counts.keys(), label_counts.values())

import random


label_to_idxs = defaultdict(list)
for idx, sample in enumerate(sample_dataset.samples):
    label_to_idxs[sample["label"]].append(idx)

fig, axs = plt.subplots(1, 4, figsize=(15, 3))
for ax, label in zip(axs, label_to_idxs.keys()):
    ax.set_title(label, fontsize=15)
    idx = random.choice(label_to_idxs[label])
    sample = sample_dataset[idx]
    image = sample["path"][0]
    ax.imshow(image, cmap="gray")

"""Finally, we will split the entire dataset into training, validation, and test sets using the ratios of 70%, 10%, and 20%, respectively. We will then obtain the corresponding data loaders for each set."""

from pyhealth.datasets import split_by_sample


train_dataset, val_dataset, test_dataset = split_by_sample(
    dataset=sample_dataset,
    ratios=[0.7, 0.1, 0.2]
)

from pyhealth.datasets import get_dataloader


train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)
test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)

"""## Step 3. Define the Model

Next, we will define the deep learning model we want to use for our task. PyHealth supports all major vision models available in the Torchvision package. You can load any of these models using the model_name argument.
"""

from pyhealth.models import TorchvisionModel


resnet = TorchvisionModel(
    dataset=sample_dataset,
    feature_keys=["path"],
    label_key="label",
    mode="multiclass",
    model_name="resnet18",
    model_config={"weights": "DEFAULT"}
)

resnet

from pyhealth.models import TorchvisionModel


vit = TorchvisionModel(
    dataset=sample_dataset,
    model_name="vit_b_16",
    feature_keys=["path"],
    label_key="label",
    mode="multiclass",
    model_config={"weights": "DEFAULT"}
)

vit

"""## Step 4. Training


In this step, we will train the model using PyHealth's Trainer class, which simplifies the training process and provides standard functionalities.

Let us first train the ResNet model.
"""

from pyhealth.trainer import Trainer


resnet_trainer = Trainer(model=resnet)

"""Before we begin training, let's first evaluate the initial performance of the model."""

print(resnet_trainer.evaluate(test_dataloader))

"""Now, let's start the training process. Due to computational constraints, we will train the model for only one epoch."""

resnet_trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=1,
    monitor="accuracy"
)

"""After training the model, we can compare its performance before and after. We should expect to see an increase in the accuracy score as the model learns from the training data.

## Step 5. Evaluation

Lastly, we can eavluate the ResNet model on the test set. This can be done using PyHealth's `Trainer.evaluate()` function.
"""

print(resnet_trainer.evaluate(test_dataloader))

"""Additionally, you can perform inference using the `Trainer.inference()` function."""

y_true, y_prob, loss = resnet_trainer.inference(test_dataloader)
y_pred = y_prob.argmax(axis=1)

"""Below we show a confusion matrix of the trained ResNet model."""

from sklearn.metrics import confusion_matrix
import seaborn as sns


cf_matrix = confusion_matrix(y_true, y_pred)
ax = sns.heatmap(cf_matrix, linewidths=1, annot=True, fmt='g')
ax.set_xticklabels(resnet.label_tokenizer.convert_indices_to_tokens(list(range(4))))
ax.set_yticklabels(resnet.label_tokenizer.convert_indices_to_tokens(list(range(4))))
ax.set_xlabel("Pred")
ax.set_ylabel("True")

"""Further, let us also train the ViT model and compare it with ResNet."""

vit_trainer = Trainer(model=vit)
vit_trainer.train(
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    epochs=1,
    monitor="accuracy"
)
print(vit_trainer.evaluate(test_dataloader))



Here is the code content for pipeline_2_length_of_stay_prediction.py:
# -*- coding: utf-8 -*-
"""Pipeline 2: Length of Stay Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JoPpXqqB1_lGF1XscBOsDHMLtgvlOYI1
"""

!pip install pyhealth

"""### **Step 1: Load dataset**
- **[README]:** We call [pyhealth.datasets](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) to process and obtain the dataset.
  - `root` is the arguments directing to the data folder.
  - `tables` is a list of table names from raw databases, which specifies the information that will be used in building the pipeline. Currently, we provide [MIMIC3Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html), [MIMIC4Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC4Dataset.html), [eICUDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.eICUDataset.html), [OMOPDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.OMOPDataset.html).
  - `code_mapping [default: None]` asks a directionary input, specifying the new coding systems for each data table. For example, `{"NDC": ("ATC", {"target_kwargs": {"level": 3}})}` means that our pyhealth will automatically change the codings from `NDC` into ATC-3 level for tables if any.
  - `dev`: if set `True`, will only load a smaller set of patients.
- **[Next Step]:** This `pyhealth.datasets` object will be used in **Step 2**.
"""

from pyhealth.datasets import MIMIC3Dataset

mimic3_ds = MIMIC3Dataset(
        root="https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/",
        tables=["DIAGNOSES_ICD", "PROCEDURES_ICD", "PRESCRIPTIONS"],
        code_mapping={},
        dev=True,
)

mimic3_ds.stat()

# data format
mimic3_ds.info()

# mimic3_ds.patients
# mimic3_ds.patients['947']
# mimic3_ds.patients['947'].visits
# mimic3_ds.patients['947'].visits['100999']
# mimic3_ds.patients['947'].visits['100999'].get_code_list('DIAGNOSES_ICD')

"""### **Step 2: Define healthcare task**
- **[README]:** This step assigns a **task function** to the dataset for data loading [pyhealth.tasks](https://pyhealth.readthedocs.io/en/latest/api/tasks.html). The **task function** specifics how to process each pateint's data into a set of samples for the downstream machine learning models.
- **[Next Step]:** This `pyhealth.tasks` object will be used in **Step 3**.
"""

from pyhealth.tasks import length_of_stay_prediction_mimic3_fn

task_mimic3_ds = mimic3_ds.set_task(task_fn=length_of_stay_prediction_mimic3_fn)
# stats info
task_mimic3_ds.stat()

# show a data sample
task_mimic3_ds.samples[0]

from pyhealth.datasets.splitter import split_by_patient
from pyhealth.datasets import split_by_patient, get_dataloader

# data split
train_dataset, val_dataset, test_dataset = split_by_patient(task_mimic3_ds, [0.8, 0.1, 0.1])

# create dataloaders (they are <torch.data.DataLoader> object)
train_loader = get_dataloader(train_dataset, batch_size=64, shuffle=True)
val_loader = get_dataloader(val_dataset, batch_size=64, shuffle=False)
test_loader = get_dataloader(test_dataset, batch_size=64, shuffle=False)

"""### **Step 3: Define ML Model**
- **[README]:** We initialize an ML model for the healthcare task by calling [pyhealth.models](https://pyhealth.readthedocs.io/en/latest/api/models.html).
- **[Next Step]:** This `pyhealth.models` object will be used in **Step 4**.
- **[Other Use Case]:** Our `pyhealth.models` object is as general as any instance from `torch.nn.Module`. Users may use it separately for supporting any other customized pipeline.
"""

from pyhealth.models import Transformer

model = Transformer(
    dataset=task_mimic3_ds,
    # look up what are available for "feature_keys" and "label_keys" in dataset.samples[0]
    feature_keys=["conditions", "procedures"],
    label_key="label",
    mode="multiclass",
    num_layers=2,
)

"""### **Step 4: Model Training**
- **[README]:** We call our [pyhealth.train.Trainer](https://pyhealth.readthedocs.io/en/latest/api/trainer.html) to train the model by giving the `train_loader`, the `val_loader`, val_metric, and specify other arguemnts, such as epochs, optimizer, learning rate, etc. The trainer will automatically save the best model and output the path in the end.
- **[Next Step]:** The best model will be used in **Step 5** for evaluation.
"""

from pyhealth.trainer import Trainer

trainer = Trainer(model=model, metrics=["accuracy", "jaccard_weighted"])

trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=5,
    monitor="jaccard_weighted",
)

"""### **Step 5: Evaluation**"""

# option 1: use our built-in evaluation metric
score = trainer.evaluate(test_loader)
print (score)

# option 2: use our pyhealth.metrics to evaluate
from pyhealth.metrics.multiclass import multiclass_metrics_fn

y_true, y_prob, loss = trainer.inference(test_loader)
multiclass_metrics_fn(y_true, y_prob, metrics=["accuracy", "f1_macro", "f1_micro", "cohen_kappa"])



"""If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.

Thanks very much for your support!
"""



