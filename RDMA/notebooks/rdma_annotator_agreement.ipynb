{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94930d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "# Set the parent directory as the current directory\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789de916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Human Corrections File ------\n",
      "Dictionary:\n",
      "  metadata (dict): \n",
      "  Dictionary:\n",
      "    timestamp (str): \n",
      "    total_entities_in_file (int): \n",
      "    reviewed_entities (int): \n",
      "  corrected_annotations (list): \n",
      "  List: (333 items)\n",
      "    Item 0 (dict): \n",
      "    Dictionary:\n",
      "      entity (str): \n",
      "      document_id (str): \n",
      "      orpha_code (str): \n",
      "      category (str): \n",
      "      is_rare_disease (bool): \n",
      "      ... and 2 more items\n",
      "    Item 1 (dict): \n",
      "    Dictionary:\n",
      "      entity (str): \n",
      "      document_id (str): \n",
      "      orpha_code (str): \n",
      "      category (str): \n",
      "      is_rare_disease (bool): \n",
      "      ... and 2 more items\n",
      "    Item 2 (dict): \n",
      "    Dictionary:\n",
      "      entity (str): \n",
      "      document_id (str): \n",
      "      orpha_code (str): \n",
      "      category (str): \n",
      "      is_rare_disease (bool): \n",
      "      ... and 2 more items\n",
      "    Item 3 (dict): \n",
      "    Dictionary:\n",
      "      entity (str): \n",
      "      document_id (str): \n",
      "      orpha_code (str): \n",
      "      category (str): \n",
      "      is_rare_disease (bool): \n",
      "      ... and 2 more items\n",
      "    Item 4 (dict): \n",
      "    Dictionary:\n",
      "      entity (str): \n",
      "      document_id (str): \n",
      "      orpha_code (str): \n",
      "      category (str): \n",
      "      is_rare_disease (bool): \n",
      "      ... and 2 more items\n",
      "    ... and 328 more items\n",
      "------- Supervisor Corrections File -------\n",
      "Dictionary:\n",
      "  metadata (dict): \n",
      "  Dictionary:\n",
      "    timestamp (str): \n",
      "    predictions_file (str): \n",
      "    ground_truth_file (str): \n",
      "    evaluation_file (str): \n",
      "    model_info (dict): \n",
      "    Dictionary:\n",
      "      llm_type (str): \n",
      "      model_type (str): \n",
      "      temperature (float): \n",
      "      retriever (str): \n",
      "      retriever_model (str): \n",
      "  summary (dict): \n",
      "  Dictionary:\n",
      "    timestamp (str): \n",
      "    categories (dict): \n",
      "    Dictionary:\n",
      "      false_negatives (dict): \n",
      "      Dictionary:\n",
      "        total (int): \n",
      "        confirmed_rare_disease_count (int): \n",
      "        confirmed_rare_disease_percentage (float): \n",
      "        flagged_for_review_count (int): \n",
      "        flagged_for_review_percentage (float): \n",
      "        ... and 1 more items\n",
      "      false_positives (dict): \n",
      "      Dictionary:\n",
      "        total (int): \n",
      "        confirmed_rare_disease_count (int): \n",
      "        confirmed_rare_disease_percentage (float): \n",
      "        flagged_for_review_count (int): \n",
      "        flagged_for_review_percentage (float): \n",
      "        ... and 1 more items\n",
      "      true_positives (dict): \n",
      "      Dictionary:\n",
      "        total (int): \n",
      "        confirmed_rare_disease_count (int): \n",
      "        confirmed_rare_disease_percentage (float): \n",
      "        flagged_for_review_count (int): \n",
      "        flagged_for_review_percentage (float): \n",
      "        ... and 1 more items\n",
      "    total_entities (int): \n",
      "    total_flagged_for_review (int): \n",
      "    flagged_for_review_percentage (float): \n",
      "    ... and 1 more items\n",
      "  results (dict): \n",
      "  Dictionary:\n",
      "    false_negatives (list): \n",
      "    List: (128 items)\n",
      "      Item 0 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      Item 1 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 6 more items\n",
      "      Item 2 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      Item 3 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      Item 4 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      ... and 123 more items\n",
      "    false_positives (list): \n",
      "    List: (24 items)\n",
      "      Item 0 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      Item 1 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      Item 2 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      Item 3 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      Item 4 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      ... and 19 more items\n",
      "    true_positives (list): \n",
      "    List: (49 items)\n",
      "      Item 0 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      Item 1 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      Item 2 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      Item 3 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      Item 4 (dict): \n",
      "      Dictionary:\n",
      "        entity (str): \n",
      "        context (str): \n",
      "        is_rare_disease (bool): \n",
      "        flag_for_review (bool): \n",
      "        explanation (str): \n",
      "        ... and 5 more items\n",
      "      ... and 44 more items\n"
     ]
    }
   ],
   "source": [
    "# load fully corrected datasets\n",
    "from rdma.utils.data import read_json_file, print_json_structure\n",
    "\n",
    "\n",
    "# all human labels\n",
    "human_corrections_full = read_json_file(\"data/dataset/rare_disease_corrections_john.json\")\n",
    "print(\"-------- Human Corrections File ------\")\n",
    "print_json_structure(human_corrections_full)\n",
    "# human + supervisor labels\n",
    "human_rdma_corrections = read_json_file(\"data/dataset/rare_disease_annotations_rdma_john_comprehensive.json\")\n",
    "# supervisor labels\n",
    "print(\"------- Supervisor Corrections File -------\")\n",
    "rdma_corrections = read_json_file(\"data/results/supervisor/multistage_no_min.json\")\n",
    "print_json_structure(rdma_corrections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01851fe0",
   "metadata": {},
   "source": [
    "# Comparing RDMA (only) and my annotations, because they go through the entire set of possible annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa875957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Improved Rare Disease Annotator Agreement Analysis with ORPHA Code Priority\n",
    "\n",
    "This script computes the inter-annotator agreement between human corrections\n",
    "and RDMA supervisor corrections for rare disease entity recognition,\n",
    "using ORPHA codes as the primary matching criterion and falling back to\n",
    "hierarchical clustering for entity variants without ORPHA codes.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Dict, List, Set, Tuple, Any, Optional\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import scipy.stats as stats\n",
    "from collections import defaultdict, Counter\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_json_file(filename: str) -> dict:\n",
    "    \"\"\"Read a JSON file and return its contents.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def normalize_entity(entity: str, abbreviations: Dict[str, str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Normalize entity text by lowercasing, removing extra spaces, \n",
    "    and expanding known abbreviations.\n",
    "    \n",
    "    Args:\n",
    "        entity: The entity text to normalize\n",
    "        abbreviations: Dictionary of abbreviations to expand\n",
    "        \n",
    "    Returns:\n",
    "        Normalized entity text\n",
    "    \"\"\"\n",
    "    if not entity:\n",
    "        return \"\"\n",
    "        \n",
    "    # Convert to lowercase and strip\n",
    "    normalized = entity.lower().strip()\n",
    "    \n",
    "    # Remove multiple spaces and replace with single space\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized)\n",
    "    \n",
    "    # Remove hyphens between words (convert \"heparin-induced\" to \"heparin induced\")\n",
    "    normalized = re.sub(r'(\\w)-(\\w)', r'\\1 \\2', normalized)\n",
    "    \n",
    "    # Expand abbreviation if it exists in the dictionary\n",
    "    if abbreviations and normalized in abbreviations:\n",
    "        return abbreviations[normalized]\n",
    "        \n",
    "    return normalized\n",
    "\n",
    "def normalize_orpha_code(orpha_code: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize ORPHA code to a standard format.\n",
    "    \n",
    "    Args:\n",
    "        orpha_code: Raw ORPHA code string\n",
    "        \n",
    "    Returns:\n",
    "        Normalized ORPHA code (e.g., \"ORPHA:12345\")\n",
    "    \"\"\"\n",
    "    if not orpha_code or not isinstance(orpha_code, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove whitespace and convert to uppercase\n",
    "    code = orpha_code.strip().upper()\n",
    "    \n",
    "    # Extract just the number part if it exists\n",
    "    match = re.search(r'(\\d+)', code)\n",
    "    if match:\n",
    "        number = match.group(1)\n",
    "        return f\"ORPHA:{number}\"\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def build_entity_similarity_graph(entities: List[str], threshold: int = 90) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Build a graph where nodes are entities and edges exist if similarity exceeds threshold.\n",
    "    Improved to better handle substring relationships.\n",
    "    \n",
    "    Args:\n",
    "        entities: List of entity strings\n",
    "        threshold: Minimum similarity score to create an edge\n",
    "        \n",
    "    Returns:\n",
    "        NetworkX graph with entities as nodes and similarities as edge weights\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add all entities as nodes\n",
    "    for entity in entities:\n",
    "        G.add_node(entity)\n",
    "    \n",
    "    # Add edges for similar entities\n",
    "    n = len(entities)\n",
    "    print(f\"Building similarity graph for {n} entities without ORPHA codes...\")\n",
    "    \n",
    "    # Use tqdm for progress tracking in the nested loop\n",
    "    with tqdm(total=n*(n-1)//2) as pbar:\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                # Update progress\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Skip comparison if entities are identical\n",
    "                entity1 = entities[i]\n",
    "                entity2 = entities[j]\n",
    "                if entity1 == entity2:\n",
    "                    continue\n",
    "                \n",
    "                # Use improved substring checking\n",
    "                should_connect, similarity_score, match_type = improved_substring_check(entity1, entity2, threshold)\n",
    "                \n",
    "                if should_connect:\n",
    "                    G.add_edge(entity1, entity2, weight=similarity_score)\n",
    "    \n",
    "    print(f\"Graph built with {len(G.nodes)} nodes and {len(G.edges)} edges\")\n",
    "    return G\n",
    "\n",
    "def improved_substring_check(entity1: str, entity2: str, threshold: int = 90) -> Tuple[bool, int, str]:\n",
    "    \"\"\"\n",
    "    Improved substring checking that should catch more cases.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (should_connect, similarity_score, match_type)\n",
    "    \"\"\"\n",
    "    # Normalize entities\n",
    "    norm1 = ' '.join(entity1.lower().split())\n",
    "    norm2 = ' '.join(entity2.lower().split())\n",
    "    \n",
    "    # 1. Exact match\n",
    "    if norm1 == norm2:\n",
    "        return True, 100, \"exact\"\n",
    "    \n",
    "    # 2. Direct substring (case insensitive)\n",
    "    if norm1 in norm2 or norm2 in norm1:\n",
    "        return True, 95, \"substring\"\n",
    "    \n",
    "    # 3. Word-level subset check\n",
    "    words1 = set(norm1.split())\n",
    "    words2 = set(norm2.split())\n",
    "    \n",
    "    if len(words1) != len(words2):  # Only check if different word counts\n",
    "        if len(words1) < len(words2) and words1.issubset(words2):\n",
    "            return True, 92, \"word_subset\"\n",
    "        elif len(words2) < len(words1) and words2.issubset(words1):\n",
    "            return True, 92, \"word_subset\"\n",
    "    \n",
    "    # 4. Check for partial word matches (e.g., \"sarcoid\" vs \"sarcoidosis\")\n",
    "    # This handles cases where one entity is a shortened form of another\n",
    "    if len(words1) == 1 and len(words2) == 1:\n",
    "        word1, word2 = list(words1)[0], list(words2)[0]\n",
    "        if len(word1) >= 4 and len(word2) >= 4:  # Only for words of reasonable length\n",
    "            if word1 in word2 or word2 in word1:\n",
    "                return True, 90, \"partial_word\"\n",
    "    \n",
    "    # 5. Fuzzy matching for typos and slight variations\n",
    "    fuzzy_score = fuzz.token_sort_ratio(entity1, entity2)\n",
    "    if fuzzy_score >= threshold:\n",
    "        return True, fuzzy_score, \"fuzzy\"\n",
    "    \n",
    "    return False, 0, \"no_match\"\n",
    "\n",
    "def cluster_entities(entities: List[str], threshold: int = 90) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Cluster entities using connected components in a similarity graph.\n",
    "    Improved to better handle canonical form selection.\n",
    "    \n",
    "    Args:\n",
    "        entities: List of entity strings\n",
    "        threshold: Minimum similarity score to consider entities similar\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping each entity to its canonical form\n",
    "    \"\"\"\n",
    "    if not entities:\n",
    "        return {}\n",
    "        \n",
    "    # Build similarity graph\n",
    "    G = build_entity_similarity_graph(entities, threshold)\n",
    "    \n",
    "    # Find connected components (clusters)\n",
    "    clusters = list(nx.connected_components(G))\n",
    "    print(f\"Found {len(clusters)} entity clusters for entities without ORPHA codes\")\n",
    "    \n",
    "    # Create entity mapping\n",
    "    entity_mapping = {}\n",
    "    \n",
    "    # Process each cluster\n",
    "    for cluster in clusters:\n",
    "        # Choose canonical form more intelligently\n",
    "        canonical = choose_canonical_entity(list(cluster))\n",
    "        \n",
    "        # Map all entities in this cluster to the canonical form\n",
    "        for entity in cluster:\n",
    "            entity_mapping[entity] = canonical\n",
    "    \n",
    "    # Add identity mappings for entities not in any cluster\n",
    "    for entity in entities:\n",
    "        if entity not in entity_mapping:\n",
    "            entity_mapping[entity] = entity\n",
    "    \n",
    "    return entity_mapping\n",
    "\n",
    "def choose_canonical_entity(cluster_entities: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Choose the best canonical entity from a cluster of similar entities.\n",
    "    Prioritizes longer, more complete forms while avoiding overly complex ones.\n",
    "    \n",
    "    Args:\n",
    "        cluster_entities: List of entities in the same cluster\n",
    "        \n",
    "    Returns:\n",
    "        The canonical entity string\n",
    "    \"\"\"\n",
    "    if len(cluster_entities) == 1:\n",
    "        return cluster_entities[0]\n",
    "    \n",
    "    # Normalize entities for comparison\n",
    "    normalized_entities = [(entity, ' '.join(entity.lower().split())) for entity in cluster_entities]\n",
    "    \n",
    "    # Sort by various criteria to find the best canonical form\n",
    "    def entity_score(entity_tuple):\n",
    "        entity, normalized = entity_tuple\n",
    "        \n",
    "        # Prefer entities that are not abbreviations (length > 3 or contain spaces)\n",
    "        is_likely_abbreviation = len(normalized) <= 3 and ' ' not in normalized\n",
    "        abbreviation_penalty = 50 if is_likely_abbreviation else 0\n",
    "        \n",
    "        # Prefer longer entities (more complete names)\n",
    "        length_score = len(normalized)\n",
    "        \n",
    "        # Prefer entities with more words (more descriptive)\n",
    "        word_count_score = len(normalized.split()) * 10\n",
    "        \n",
    "        # Penalize entities that are too long (might be overly specific)\n",
    "        length_penalty = max(0, len(normalized) - 100) * 0.5\n",
    "        \n",
    "        return length_score + word_count_score - abbreviation_penalty - length_penalty\n",
    "    \n",
    "    # Sort by score and return the best one\n",
    "    scored_entities = [(entity, entity_score((entity, normalized))) \n",
    "                      for entity, normalized in normalized_entities]\n",
    "    scored_entities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    canonical = scored_entities[0][0]\n",
    "    \n",
    "    # Debug output for clusters with multiple entities\n",
    "    if len(cluster_entities) > 1:\n",
    "        print(f\"  Cluster: {cluster_entities}\")\n",
    "        print(f\"  Chosen canonical: '{canonical}'\")\n",
    "    \n",
    "    return canonical\n",
    "\n",
    "def create_entity_canonical_mapping(\n",
    "    all_entities_with_orpha: List[Tuple[str, str]], \n",
    "    entities_without_orpha: List[str],\n",
    "    similarity_threshold: int = 90\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create a mapping from entities to their canonical forms, prioritizing ORPHA codes.\n",
    "    Improved to handle substring relationships better.\n",
    "    \n",
    "    Args:\n",
    "        all_entities_with_orpha: List of (entity, orpha_code) tuples\n",
    "        entities_without_orpha: List of entity strings without ORPHA codes\n",
    "        similarity_threshold: Minimum similarity for entity clustering\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping each entity to its canonical identifier\n",
    "    \"\"\"\n",
    "    entity_mapping = {}\n",
    "    \n",
    "    # Step 1: Group entities by ORPHA code\n",
    "    orpha_to_entities = defaultdict(list)\n",
    "    for entity, orpha_code in all_entities_with_orpha:\n",
    "        normalized_orpha = normalize_orpha_code(orpha_code)\n",
    "        if normalized_orpha:\n",
    "            orpha_to_entities[normalized_orpha].append(entity)\n",
    "    \n",
    "    print(f\"Found {len(orpha_to_entities)} unique ORPHA codes\")\n",
    "    \n",
    "    # Step 2: For each ORPHA code group, choose a canonical entity\n",
    "    orpha_canonical_entities = {}\n",
    "    for orpha_code, entities in orpha_to_entities.items():\n",
    "        # Use the improved canonical selection for ORPHA groups too\n",
    "        canonical_entity = choose_canonical_entity(entities)\n",
    "        orpha_canonical_entities[orpha_code] = canonical_entity\n",
    "        \n",
    "        # Map all entities with this ORPHA code to the canonical entity\n",
    "        for entity in entities:\n",
    "            entity_mapping[entity] = canonical_entity\n",
    "    \n",
    "    print(f\"Mapped {sum(len(entities) for entities in orpha_to_entities.values())} entities using ORPHA codes\")\n",
    "    \n",
    "    # Step 3: Cluster entities without ORPHA codes using improved clustering\n",
    "    if entities_without_orpha:\n",
    "        print(f\"Clustering {len(entities_without_orpha)} entities without ORPHA codes\")\n",
    "        string_clustering = cluster_entities(entities_without_orpha, similarity_threshold)\n",
    "        entity_mapping.update(string_clustering)\n",
    "    \n",
    "    return entity_mapping\n",
    "\n",
    "def create_entity_canonical_mapping_fixed(\n",
    "    all_entities_with_orpha: List[Tuple[str, str]], \n",
    "    entities_without_orpha: List[str],\n",
    "    similarity_threshold: int = 85  # Lower threshold to catch more similarities\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create entity mapping with more aggressive clustering to catch cases like 'budd chiari'.\n",
    "    \"\"\"\n",
    "    entity_mapping = {}\n",
    "    \n",
    "    # Step 1: Group entities by ORPHA code\n",
    "    orpha_to_entities = defaultdict(list)\n",
    "    for entity, orpha_code in all_entities_with_orpha:\n",
    "        normalized_orpha = normalize_orpha_code(orpha_code)\n",
    "        if normalized_orpha:\n",
    "            orpha_to_entities[normalized_orpha].append(entity)\n",
    "    \n",
    "    print(f\"Found {len(orpha_to_entities)} unique ORPHA codes\")\n",
    "    \n",
    "    # Step 2: For each ORPHA code group, choose a canonical entity\n",
    "    for orpha_code, entities in orpha_to_entities.items():\n",
    "        canonical_entity = choose_canonical_entity(entities)\n",
    "        \n",
    "        # Map all entities with this ORPHA code to the canonical entity\n",
    "        for entity in entities:\n",
    "            entity_mapping[entity] = canonical_entity\n",
    "    # Step 2.5: Cross-reference ORPHA entities with non-ORPHA entities\n",
    "    for entity_without_orpha in entities_without_orpha:\n",
    "        for orpha_code, orpha_entities in orpha_to_entities.items():\n",
    "            for orpha_entity in orpha_entities:\n",
    "                should_connect, _, _ = improved_substring_check_aggressive(\n",
    "                    entity_without_orpha, orpha_entity, similarity_threshold\n",
    "                )\n",
    "                print(\"DEBUG:\", orpha_entity, entity_without_orpha, should_connect)\n",
    "                if should_connect:\n",
    "                    # Map the non-ORPHA entity to the ORPHA canonical form\n",
    "                    entity_mapping[entity_without_orpha] = choose_canonical_entity(orpha_entities)\n",
    "                    break\n",
    "    \n",
    "    print(f\"Mapped {sum(len(entities) for entities in orpha_to_entities.values())} entities using ORPHA codes\")\n",
    "    \n",
    "    # Step 3: Cluster entities without ORPHA codes with more aggressive matching\n",
    "    if entities_without_orpha:\n",
    "        print(f\"Clustering {len(entities_without_orpha)} entities without ORPHA codes\")\n",
    "        \n",
    "        # Use more aggressive clustering\n",
    "        string_clustering = cluster_entities_aggressive(entities_without_orpha, similarity_threshold)\n",
    "        entity_mapping.update(string_clustering)\n",
    "    \n",
    "    return entity_mapping\n",
    "\n",
    "def cluster_entities_aggressive(entities: List[str], threshold: int = 85) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    More aggressive clustering that should definitely catch 'budd chiari' variants.\n",
    "    \"\"\"\n",
    "    if not entities:\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Using aggressive clustering with threshold {threshold}\")\n",
    "    \n",
    "    # Build similarity graph with more aggressive matching\n",
    "    G = build_entity_similarity_graph_aggressive(entities, threshold)\n",
    "    \n",
    "    # Find connected components\n",
    "    clusters = list(nx.connected_components(G))\n",
    "    print(f\"Found {len(clusters)} entity clusters for entities without ORPHA codes\")\n",
    "    \n",
    "    # Create entity mapping\n",
    "    entity_mapping = {}\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        canonical = choose_canonical_entity(list(cluster))\n",
    "        \n",
    "        # Debug output for interesting clusters\n",
    "        if len(cluster) > 1:\n",
    "            print(f\"  Cluster: {list(cluster)} -> '{canonical}'\")\n",
    "        \n",
    "        for entity in cluster:\n",
    "            entity_mapping[entity] = canonical\n",
    "    \n",
    "    # Add identity mappings for entities not in any cluster\n",
    "    for entity in entities:\n",
    "        if entity not in entity_mapping:\n",
    "            entity_mapping[entity] = entity\n",
    "    \n",
    "    return entity_mapping\n",
    "\n",
    "def build_entity_similarity_graph_aggressive(entities: List[str], threshold: int = 85) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    More aggressive similarity graph building that should catch more substring relationships.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add all entities as nodes\n",
    "    for entity in entities:\n",
    "        G.add_node(entity)\n",
    "    \n",
    "    # Track edges for debugging\n",
    "    edges_added = []\n",
    "    \n",
    "    n = len(entities)\n",
    "    print(f\"Building aggressive similarity graph for {n} entities...\")\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            entity1 = entities[i]\n",
    "            entity2 = entities[j]\n",
    "            if \"budd\" in entity1.lower() or \"budd\" in entity2.lower():\n",
    "                print(f\"  Checking '{entity1}' <-> '{entity2}'\")\n",
    "            if entity1 == entity2:\n",
    "                continue\n",
    "            \n",
    "            # Use# Use improved substring check with lower threshold\n",
    "            should_connect, similarity_score, match_type = improved_substring_check_aggressive(\n",
    "                entity1, entity2, threshold\n",
    "            )\n",
    "            \n",
    "            if should_connect:\n",
    "                G.add_edge(entity1, entity2, weight=similarity_score)\n",
    "                edges_added.append((entity1, entity2, similarity_score, match_type))\n",
    "                \n",
    "                # Debug output for interesting connections\n",
    "                if 'budd' in entity1.lower() or 'budd' in entity2.lower():\n",
    "                    print(f\"  BUDD EDGE: '{entity1}' <-> '{entity2}' (score={similarity_score}, type={match_type})\")\n",
    "    \n",
    "    print(f\"Graph built with {len(G.nodes)} nodes and {len(G.edges)} edges\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "def improved_substring_check_aggressive(entity1: str, entity2: str, threshold: int = 85) -> Tuple[bool, int, str]:\n",
    "    \"\"\"\n",
    "    More aggressive substring checking that should definitely catch 'budd chiari' cases.\n",
    "    \"\"\"\n",
    "    # Normalize entities\n",
    "    norm1 = ' '.join(entity1.lower().split())\n",
    "    norm2 = ' '.join(entity2.lower().split())\n",
    "    \n",
    "    # 1. Exact match\n",
    "    if norm1 == norm2:\n",
    "        return True, 100, \"exact\"\n",
    "    \n",
    "    # 2. Direct substring (case insensitive) - this should catch \"budd chiari\" in \"budd chiari syndrome\"\n",
    "    if norm1 in norm2 or norm2 in norm1:\n",
    "        return True, 95, \"substring\"\n",
    "    \n",
    "    # 3. Word-level subset check - more aggressive\n",
    "    words1 = set(norm1.split())\n",
    "    words2 = set(norm2.split())\n",
    "    \n",
    "    # Check if one is a subset of the other (regardless of length difference)\n",
    "    if words1.issubset(words2) and words1 != words2:\n",
    "        return True, 92, \"word_subset\"\n",
    "    elif words2.issubset(words1) and words1 != words2:\n",
    "        return True, 92, \"word_subset\"\n",
    "    \n",
    "    # 4. Check for partial word matches with lower requirements\n",
    "    if len(words1) == 1 and len(words2) == 1:\n",
    "        word1, word2 = list(words1)[0], list(words2)[0]\n",
    "        if len(word1) >= 3 and len(word2) >= 3:  # Lower requirement\n",
    "            if word1 in word2 or word2 in word1:\n",
    "                return True, 88, \"partial_word\"\n",
    "    \n",
    "    # 5. More aggressive fuzzy matching\n",
    "    fuzzy_score = fuzz.token_sort_ratio(entity1, entity2)\n",
    "    if fuzzy_score >= threshold:\n",
    "        return True, fuzzy_score, \"fuzzy\"\n",
    "    \n",
    "    # 6. Additional check: ratio similarity for very close matches\n",
    "    ratio_score = fuzz.ratio(norm1, norm2)\n",
    "    if ratio_score >= threshold:\n",
    "        return True, ratio_score, \"ratio\"\n",
    "    \n",
    "    return False, 0, \"no_match\"\n",
    "\n",
    "def apply_or_operation_to_similar_entities(doc_entities: Dict[str, bool], \n",
    "                                         entity_mapping: Dict[str, str]) -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Apply OR operation to entities that map to the same canonical form.\n",
    "    \n",
    "    Args:\n",
    "        doc_entities: Dictionary of {entity: is_rare_disease} for a single document\n",
    "        entity_mapping: Mapping from entities to their canonical forms\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with unified entities using OR operation\n",
    "    \"\"\"\n",
    "    # Group entities by their canonical form\n",
    "    canonical_groups = defaultdict(list)\n",
    "    \n",
    "    for entity, is_rare in doc_entities.items():\n",
    "        canonical = entity_mapping.get(entity, entity)  # Use entity itself if not in mapping\n",
    "        canonical_groups[canonical].append((entity, is_rare))\n",
    "    \n",
    "    # Apply OR operation for each canonical group\n",
    "    unified_entities = {}\n",
    "    \n",
    "    for canonical, entity_list in canonical_groups.items():\n",
    "        # OR operation: True if ANY entity in the group is True\n",
    "        unified_is_rare = any(is_rare for _, is_rare in entity_list)\n",
    "        unified_entities[canonical] = unified_is_rare\n",
    "        \n",
    "        # Debug output for groups with multiple entities\n",
    "        if len(entity_list) > 1:\n",
    "            print(f\"  Unified '{canonical}': {entity_list} -> {unified_is_rare}\")\n",
    "    \n",
    "    return unified_entities\n",
    "\n",
    "def fix_document_entities_with_or_operation(\n",
    "    human_doc_entities: Dict[str, Dict[str, bool]], \n",
    "    supervisor_doc_entities: Dict[str, Dict[str, bool]],\n",
    "    entity_mapping: Dict[str, str]\n",
    ") -> Tuple[Dict[str, Dict[str, bool]], Dict[str, Dict[str, bool]]]:\n",
    "    \"\"\"\n",
    "    Apply OR operation to fix documents where similar entities are separate.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (fixed_human_doc_entities, fixed_supervisor_doc_entities)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== APPLYING OR OPERATION TO SIMILAR ENTITIES ===\")\n",
    "    \n",
    "    fixed_human = {}\n",
    "    fixed_supervisor = {}\n",
    "    \n",
    "    # Process human documents\n",
    "    for doc_id, entities in human_doc_entities.items():\n",
    "        unified = apply_or_operation_to_similar_entities(entities, entity_mapping)\n",
    "        fixed_human[doc_id] = unified\n",
    "        \n",
    "        if unified != entities:\n",
    "            print(f\"  Document {doc_id} - Human entities unified\")\n",
    "    \n",
    "    # Process supervisor documents\n",
    "    for doc_id, entities in supervisor_doc_entities.items():\n",
    "        unified = apply_or_operation_to_similar_entities(entities, entity_mapping)\n",
    "        fixed_supervisor[doc_id] = unified\n",
    "        \n",
    "        if unified != entities:\n",
    "            print(f\"  Document {doc_id} - Supervisor entities unified\")\n",
    "    \n",
    "    return fixed_human, fixed_supervisor\n",
    "\n",
    "def create_entity_canonical_mapping_with_cross_orpha(\n",
    "    all_entities_with_orpha: List[Tuple[str, str]], \n",
    "    entities_without_orpha: List[str],\n",
    "    similarity_threshold: int = 85\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create entity mapping with cross-ORPHA matching to unify entities like 'budd chiari' -> 'budd chiari syndrome'.\n",
    "    \"\"\"\n",
    "    entity_mapping = {}\n",
    "    \n",
    "    # Step 1: Group entities by ORPHA code\n",
    "    orpha_to_entities = defaultdict(list)\n",
    "    for entity, orpha_code in all_entities_with_orpha:\n",
    "        normalized_orpha = normalize_orpha_code(orpha_code)\n",
    "        if normalized_orpha:\n",
    "            orpha_to_entities[normalized_orpha].append(entity)\n",
    "    \n",
    "    print(f\"Found {len(orpha_to_entities)} unique ORPHA codes\")\n",
    "    \n",
    "    # Step 2: For each ORPHA code group, choose a canonical entity\n",
    "    orpha_canonical_entities = {}\n",
    "    for orpha_code, entities in orpha_to_entities.items():\n",
    "        canonical_entity = choose_canonical_entity(entities)\n",
    "        orpha_canonical_entities[orpha_code] = canonical_entity\n",
    "        \n",
    "        # Map all entities with this ORPHA code to the canonical entity\n",
    "        for entity in entities:\n",
    "            entity_mapping[entity] = canonical_entity\n",
    "    \n",
    "    print(f\"Mapped {sum(len(entities) for entities in orpha_to_entities.values())} entities using ORPHA codes\")\n",
    "    \n",
    "    # Step 3: Cross-reference non-ORPHA entities with ORPHA entities\n",
    "    remaining_entities_without_orpha = []\n",
    "    \n",
    "    for entity_without_orpha in entities_without_orpha:\n",
    "        matched_to_orpha = False\n",
    "        \n",
    "        # Check against all ORPHA canonical entities\n",
    "        for orpha_code, canonical_entity in orpha_canonical_entities.items():\n",
    "            # Check against the canonical entity\n",
    "            should_connect, score, match_type = improved_substring_check_aggressive(\n",
    "                entity_without_orpha, canonical_entity, similarity_threshold\n",
    "            )\n",
    "            \n",
    "            if should_connect:\n",
    "                entity_mapping[entity_without_orpha] = canonical_entity\n",
    "                matched_to_orpha = True\n",
    "                print(f\"  Cross-ORPHA match: '{entity_without_orpha}' -> '{canonical_entity}' (ORPHA entity, {match_type}, score={score})\")\n",
    "                break\n",
    "            \n",
    "            # Also check against all entities in this ORPHA group\n",
    "            if not matched_to_orpha:\n",
    "                for orpha_entity in orpha_to_entities[orpha_code]:\n",
    "                    should_connect, score, match_type = improved_substring_check_aggressive(\n",
    "                        entity_without_orpha, orpha_entity, similarity_threshold\n",
    "                    )\n",
    "                    \n",
    "                    if should_connect:\n",
    "                        entity_mapping[entity_without_orpha] = canonical_entity\n",
    "                        matched_to_orpha = True\n",
    "                        print(f\"  Cross-ORPHA match: '{entity_without_orpha}' -> '{canonical_entity}' (via '{orpha_entity}', {match_type}, score={score})\")\n",
    "                        break\n",
    "            \n",
    "            if matched_to_orpha:\n",
    "                break\n",
    "        \n",
    "        # If not matched to any ORPHA entity, keep for string clustering\n",
    "        if not matched_to_orpha:\n",
    "            remaining_entities_without_orpha.append(entity_without_orpha)\n",
    "    \n",
    "    print(f\"Cross-ORPHA matching: {len(entities_without_orpha) - len(remaining_entities_without_orpha)} entities matched to ORPHA entities\")\n",
    "    print(f\"Remaining for string clustering: {len(remaining_entities_without_orpha)} entities\")\n",
    "    \n",
    "    # Step 4: Cluster remaining entities without ORPHA codes\n",
    "    if remaining_entities_without_orpha:\n",
    "        print(f\"Clustering {len(remaining_entities_without_orpha)} remaining entities without ORPHA codes\")\n",
    "        string_clustering = cluster_entities_aggressive(remaining_entities_without_orpha, similarity_threshold)\n",
    "        entity_mapping.update(string_clustering)\n",
    "    \n",
    "    return entity_mapping\n",
    "\n",
    "def extract_document_entity_sets_with_orpha_priority_fixed(\n",
    "    human_corrections: dict, \n",
    "    supervisor_corrections: dict, \n",
    "    similarity_threshold: int = 85\n",
    ") -> Tuple[Dict[str, Dict[str, bool]], Dict[str, Dict[str, bool]], Dict[str, str], Dict[str, Dict]]:\n",
    "    \"\"\"\n",
    "    Extract document-level entity sets with proper OR logic applied during processing.\n",
    "    \"\"\"\n",
    "    # Dictionary for abbreviation expansion\n",
    "    abbreviations = {\n",
    "        \"hit\": \"heparin induced thrombocytopenia\",\n",
    "        \"pah\": \"pulmonary arterial hypertension\",\n",
    "        \"pfo\": \"patent foramen ovale\",\n",
    "        \"pcd\": \"primary ciliary dyskinesia\",\n",
    "        \"hids\": \"hyper-igd syndrome\",\n",
    "        \"ald\": \"adrenoleukodystrophy\",\n",
    "    }\n",
    "    \n",
    "    def is_valid_annotation(entity, context):\n",
    "        \"\"\"Check if an annotation is valid.\"\"\"\n",
    "        excluded_terms = [\"high altitude pulmonary edema\"]\n",
    "        if any(term.lower() in entity.lower() for term in excluded_terms):\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    # Step 1: Collect all entities and create mapping (same as before)\n",
    "    all_entities_with_orpha = []\n",
    "    entities_without_orpha = []\n",
    "    raw_entity_to_original = {}\n",
    "    \n",
    "    def process_annotations_for_mapping(annotations, source_name):\n",
    "        \"\"\"First pass: collect entities for mapping creation.\"\"\"\n",
    "        for annotation in annotations:\n",
    "            if 'entity' in annotation and 'document_id' in annotation and 'is_rare_disease' in annotation:\n",
    "                entity = annotation['entity']\n",
    "                context = annotation.get('context', '')\n",
    "                \n",
    "                if not is_valid_annotation(entity, context):\n",
    "                    continue\n",
    "                \n",
    "                normalized = normalize_entity(entity, abbreviations)\n",
    "                raw_entity_to_original[normalized] = entity\n",
    "                \n",
    "                orpha_code = annotation.get('orpha_code', '') or annotation.get('orpha_id', '')\n",
    "                normalized_orpha = normalize_orpha_code(orpha_code)\n",
    "                \n",
    "                if normalized_orpha:\n",
    "                    all_entities_with_orpha.append((normalized, normalized_orpha))\n",
    "                else:\n",
    "                    entities_without_orpha.append(normalized)\n",
    "    \n",
    "    # Process both annotation sets to get all entities\n",
    "    if human_corrections and 'corrected_annotations' in human_corrections:\n",
    "        process_annotations_for_mapping(human_corrections['corrected_annotations'], \"human\")\n",
    "    \n",
    "    if supervisor_corrections and 'results' in supervisor_corrections:\n",
    "        all_supervisor_annotations = []\n",
    "        for category in ['true_positives', 'false_negatives']:\n",
    "            if category in supervisor_corrections['results']:\n",
    "                all_supervisor_annotations.extend(supervisor_corrections['results'][category])\n",
    "        process_annotations_for_mapping(all_supervisor_annotations, \"supervisor\")\n",
    "    \n",
    "    # Create entity mapping\n",
    "    unique_entities_with_orpha = list(set(all_entities_with_orpha))\n",
    "    unique_entities_without_orpha = list(set(entities_without_orpha))\n",
    "    \n",
    "    print(f\"Found {len(unique_entities_with_orpha)} unique entities with ORPHA codes\")\n",
    "    print(f\"Found {len(unique_entities_without_orpha)} unique entities without ORPHA codes\")\n",
    "    \n",
    "    entity_mapping = create_entity_canonical_mapping_with_cross_orpha(\n",
    "        unique_entities_with_orpha, \n",
    "        unique_entities_without_orpha,\n",
    "        similarity_threshold\n",
    "    )\n",
    "    \n",
    "    # Step 2: Process annotations with OR logic applied immediately\n",
    "    human_doc_entities = defaultdict(dict)\n",
    "    supervisor_doc_entities = defaultdict(dict)\n",
    "    context_mapping = defaultdict(dict)\n",
    "    \n",
    "    def process_annotations_with_or_logic(annotations, target_doc_entities, source_name):\n",
    "        \"\"\"Process annotations and apply OR logic immediately for same canonical entities.\"\"\"\n",
    "        for annotation in annotations:\n",
    "            if 'entity' in annotation and 'document_id' in annotation and 'is_rare_disease' in annotation:\n",
    "                entity = annotation['entity']\n",
    "                context = annotation.get('context', '')\n",
    "                doc_id = annotation['document_id']\n",
    "                is_rare = annotation['is_rare_disease']\n",
    "                orpha_code = annotation.get('orpha_code', '') or annotation.get('orpha_id', '')\n",
    "                \n",
    "                if not is_valid_annotation(entity, context):\n",
    "                    continue\n",
    "                \n",
    "                # Get canonical form\n",
    "                normalized = normalize_entity(entity, abbreviations)\n",
    "                canonical_entity = entity_mapping.get(normalized, normalized)\n",
    "                \n",
    "                # Apply OR logic immediately - if canonical entity already exists, OR with existing value\n",
    "                if canonical_entity in target_doc_entities[doc_id]:\n",
    "                    # OR logic: True if current OR existing is True\n",
    "                    target_doc_entities[doc_id][canonical_entity] = (\n",
    "                        target_doc_entities[doc_id][canonical_entity] or is_rare\n",
    "                    )\n",
    "                    print(f\"  OR operation: '{canonical_entity}' in doc {doc_id} - {source_name}: \"\n",
    "                          f\"existing={target_doc_entities[doc_id][canonical_entity]} OR new={is_rare} = \"\n",
    "                          f\"{target_doc_entities[doc_id][canonical_entity] or is_rare}\")\n",
    "                else:\n",
    "                    target_doc_entities[doc_id][canonical_entity] = is_rare\n",
    "                \n",
    "                # Store context information (keep the most recent or merge as needed)\n",
    "                context_key = (doc_id, canonical_entity)\n",
    "                if context_key not in context_mapping:\n",
    "                    context_mapping[context_key] = {}\n",
    "                context_mapping[context_key][source_name] = {\n",
    "                    'context': context,\n",
    "                    'original_entity': entity,\n",
    "                    'orpha_code': orpha_code,\n",
    "                    'is_rare': is_rare,\n",
    "                    'canonical_entity': canonical_entity\n",
    "                }\n",
    "    \n",
    "    # Process human annotations\n",
    "    if human_corrections and 'corrected_annotations' in human_corrections:\n",
    "        process_annotations_with_or_logic(\n",
    "            human_corrections['corrected_annotations'], \n",
    "            human_doc_entities, \n",
    "            'human'\n",
    "        )\n",
    "    \n",
    "    # Process supervisor annotations\n",
    "    if supervisor_corrections and 'results' in supervisor_corrections:\n",
    "        all_supervisor_annotations = []\n",
    "        for category in ['true_positives', 'false_negatives']:\n",
    "            if category in supervisor_corrections['results']:\n",
    "                all_supervisor_annotations.extend(supervisor_corrections['results'][category])\n",
    "        \n",
    "        process_annotations_with_or_logic(\n",
    "            all_supervisor_annotations, \n",
    "            supervisor_doc_entities, \n",
    "            'supervisor'\n",
    "        )\n",
    "    \n",
    "    # Ensure all documents are represented in both dictionaries\n",
    "    all_docs = set(human_doc_entities.keys()) | set(supervisor_doc_entities.keys())\n",
    "    for doc_id in all_docs:\n",
    "        if doc_id not in human_doc_entities:\n",
    "            human_doc_entities[doc_id] = {}\n",
    "        if doc_id not in supervisor_doc_entities:\n",
    "            supervisor_doc_entities[doc_id] = {}\n",
    "    \n",
    "    print(f\"\\nProcessed {len(all_docs)} documents with OR logic applied during processing\")\n",
    "    \n",
    "    return dict(human_doc_entities), dict(supervisor_doc_entities), entity_mapping, dict(context_mapping)\n",
    "\n",
    "\n",
    "def compute_agreement_metrics(human_doc_entities: Dict[str, Dict[str, bool]], \n",
    "                             supervisor_doc_entities: Dict[str, Dict[str, bool]],\n",
    "                             excluded_docs: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute agreement metrics between human and supervisor annotations without filtering by rarity.\n",
    "    \n",
    "    Args:\n",
    "        human_doc_entities: Dictionary mapping document_id to a dict of {entity: is_rare_disease}\n",
    "        supervisor_doc_entities: Dictionary mapping document_id to a dict of {entity: is_rare_disease}\n",
    "        excluded_docs: Optional list of document IDs to exclude from evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with agreement metrics\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    all_judgments = []  # List of (human, supervisor) tuples for each entity judgment\n",
    "    \n",
    "    # Get all document IDs\n",
    "    all_doc_ids = set(human_doc_entities.keys()) | set(supervisor_doc_entities.keys())\n",
    "    \n",
    "    # Remove excluded documents if specified\n",
    "    if excluded_docs:\n",
    "        all_doc_ids = all_doc_ids - set(excluded_docs)\n",
    "    \n",
    "    # Collect all unique entities across all documents\n",
    "    all_unique_entities_set = set()\n",
    "    all_doc_entity_pairs = []  # List of (doc_id, entity) pairs\n",
    "    \n",
    "    for doc_id in all_doc_ids:\n",
    "        # Combine both sets of entities for this document\n",
    "        human_entities = human_doc_entities[doc_id]\n",
    "        supervisor_entities = supervisor_doc_entities[doc_id]\n",
    "        all_entities = set(human_entities.keys()) | set(supervisor_entities.keys())\n",
    "        \n",
    "        for entity in all_entities:\n",
    "            # Track unique entities\n",
    "            all_unique_entities_set.add(entity)\n",
    "            # Track all document-entity pairs\n",
    "            all_doc_entity_pairs.append((doc_id, entity))\n",
    "    \n",
    "    # Lists for Cohen's Kappa and other metrics\n",
    "    human_judgments = []\n",
    "    supervisor_judgments = []\n",
    "    \n",
    "    # Count metrics for confusion matrix\n",
    "    tp = 0  # Both say it's rare\n",
    "    tn = 0  # Both say it's not rare\n",
    "    fp = 0  # Supervisor says rare, Human says not rare\n",
    "    fn = 0  # Human says rare, Supervisor says not rare\n",
    "    \n",
    "    # Process each document-entity pair\n",
    "    for doc_id, entity in all_doc_entity_pairs:\n",
    "        # Get human and supervisor judgments (default to False if entity is not present)\n",
    "        human_judgment = human_doc_entities[doc_id].get(entity, False)\n",
    "        supervisor_judgment = supervisor_doc_entities[doc_id].get(entity, False)\n",
    "        \n",
    "        # Add to lists for metrics calculation\n",
    "        human_judgments.append(1 if human_judgment else 0)\n",
    "        supervisor_judgments.append(1 if supervisor_judgment else 0)\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        if human_judgment and supervisor_judgment:\n",
    "            tp += 1\n",
    "        elif not human_judgment and not supervisor_judgment:\n",
    "            tn += 1\n",
    "        elif supervisor_judgment and not human_judgment:\n",
    "            fp += 1\n",
    "        elif human_judgment and not supervisor_judgment:\n",
    "            fn += 1\n",
    "        \n",
    "        # Save the pair of judgments\n",
    "        all_judgments.append((human_judgment, supervisor_judgment))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_judgments = len(all_judgments)\n",
    "    agreements = tp + tn\n",
    "    disagreements = fp + fn\n",
    "    \n",
    "    # Agreement rates\n",
    "    if total_judgments > 0:\n",
    "        percent_agreement = agreements / total_judgments\n",
    "    else:\n",
    "        percent_agreement = 0\n",
    "    \n",
    "    # Precision, recall, F1\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate Cohen's Kappa\n",
    "    if human_judgments and supervisor_judgments:\n",
    "        kappa = cohen_kappa_score(human_judgments, supervisor_judgments)\n",
    "    else:\n",
    "        kappa = 0\n",
    "    \n",
    "    # Calculate Pearson correlation\n",
    "    if human_judgments and supervisor_judgments and len(human_judgments) > 1:\n",
    "        pearson_corr, p_value = stats.pearsonr(human_judgments, supervisor_judgments)\n",
    "    else:\n",
    "        pearson_corr = 0\n",
    "        p_value = 1\n",
    "    \n",
    "    # Compile entity statistics (with rarity classifications)\n",
    "    human_rare_entities = sum(1 for doc_id in all_doc_ids for entity, is_rare in human_doc_entities[doc_id].items() if is_rare)\n",
    "    supervisor_rare_entities = sum(1 for doc_id in all_doc_ids for entity, is_rare in supervisor_doc_entities[doc_id].items() if is_rare)\n",
    "    \n",
    "    human_nonrare_entities = sum(1 for doc_id in all_doc_ids for entity, is_rare in human_doc_entities[doc_id].items() if not is_rare)\n",
    "    supervisor_nonrare_entities = sum(1 for doc_id in all_doc_ids for entity, is_rare in supervisor_doc_entities[doc_id].items() if not is_rare)\n",
    "    \n",
    "    # Count unique entities\n",
    "    unique_human_entities = set()\n",
    "    unique_supervisor_entities = set()\n",
    "    unique_human_rare = set()\n",
    "    unique_supervisor_rare = set()\n",
    "    \n",
    "    for doc_id in all_doc_ids:\n",
    "        for entity, is_rare in human_doc_entities[doc_id].items():\n",
    "            unique_human_entities.add(entity)\n",
    "            if is_rare:\n",
    "                unique_human_rare.add(entity)\n",
    "                \n",
    "        for entity, is_rare in supervisor_doc_entities[doc_id].items():\n",
    "            unique_supervisor_entities.add(entity)\n",
    "            if is_rare:\n",
    "                unique_supervisor_rare.add(entity)\n",
    "    \n",
    "    return {\n",
    "        'total_documents': len(all_doc_ids),\n",
    "        'total_entity_judgments': total_judgments,\n",
    "        'total_agreements': agreements,\n",
    "        'total_disagreements': disagreements,\n",
    "        \n",
    "        # Entity counts with rarity classification\n",
    "        'human_rare_entities': human_rare_entities,\n",
    "        'human_nonrare_entities': human_nonrare_entities,\n",
    "        'supervisor_rare_entities': supervisor_rare_entities,\n",
    "        'supervisor_nonrare_entities': supervisor_nonrare_entities,\n",
    "        \n",
    "        # Unique entity counts\n",
    "        'unique_entities_total': len(all_unique_entities_set),\n",
    "        'unique_human_entities': len(unique_human_entities),\n",
    "        'unique_supervisor_entities': len(unique_supervisor_entities),\n",
    "        'unique_human_rare': len(unique_human_rare),\n",
    "        'unique_supervisor_rare': len(unique_supervisor_rare),\n",
    "        \n",
    "        # Confusion matrix\n",
    "        'true_positives': tp,  # Both say it's rare\n",
    "        'true_negatives': tn,  # Both say it's not rare\n",
    "        'false_positives': fp,  # Supervisor says rare, Human says not rare  \n",
    "        'false_negatives': fn,  # Human says rare, Supervisor says not rare\n",
    "        \n",
    "        # Agreement metrics\n",
    "        'percent_agreement': percent_agreement,\n",
    "        'cohen_kappa': kappa,\n",
    "        'pearson_correlation': pearson_corr,\n",
    "        'pearson_p_value': p_value,\n",
    "        \n",
    "        # Classification metrics\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def analyze_disagreements(human_doc_entities: Dict[str, Dict[str, bool]], \n",
    "                         supervisor_doc_entities: Dict[str, Dict[str, bool]],\n",
    "                         context_mapping: Dict[Tuple[str, str], Dict] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze disagreements in entity classification with context information.\n",
    "    \n",
    "    Args:\n",
    "        human_doc_entities: Dictionary mapping document_id to entity judgments\n",
    "        supervisor_doc_entities: Dictionary mapping document_id to entity judgments  \n",
    "        context_mapping: Optional dictionary with context information for each entity\n",
    "    \"\"\"\n",
    "    # Get all document IDs\n",
    "    all_doc_ids = set(human_doc_entities.keys()) | set(supervisor_doc_entities.keys())\n",
    "    \n",
    "    # Initialize disagreement lists\n",
    "    human_rare_supervisor_not = []  # Human says rare, Supervisor says not rare\n",
    "    supervisor_rare_human_not = []  # Supervisor says rare, Human says not rare\n",
    "    \n",
    "    # To track contradictory classifications\n",
    "    entity_classifications = defaultdict(lambda: {\"human_rare\": 0, \"human_not_rare\": 0, \n",
    "                                                 \"supervisor_rare\": 0, \"supervisor_not_rare\": 0,\n",
    "                                                 \"documents\": set()})\n",
    "    \n",
    "    # Process each document\n",
    "    for doc_id in all_doc_ids:\n",
    "        human_entities = human_doc_entities.get(doc_id, {})\n",
    "        supervisor_entities = supervisor_doc_entities.get(doc_id, {})\n",
    "        \n",
    "        # Get all entities from both sets\n",
    "        all_entities = set(human_entities.keys()) | set(supervisor_entities.keys())\n",
    "        \n",
    "        for entity in all_entities:\n",
    "            # Get judgments (default to False if not present)\n",
    "            human_judgment = human_entities.get(entity, False)\n",
    "            supervisor_judgment = supervisor_entities.get(entity, False)\n",
    "            \n",
    "            # Track all classifications for this entity\n",
    "            entity_classifications[entity][\"documents\"].add(doc_id)\n",
    "            if human_judgment:\n",
    "                entity_classifications[entity][\"human_rare\"] += 1\n",
    "            else:\n",
    "                entity_classifications[entity][\"human_not_rare\"] += 1\n",
    "            \n",
    "            if supervisor_judgment:\n",
    "                entity_classifications[entity][\"supervisor_rare\"] += 1\n",
    "            else:\n",
    "                entity_classifications[entity][\"supervisor_not_rare\"] += 1\n",
    "            \n",
    "            # Check for disagreements\n",
    "            if human_judgment and not supervisor_judgment:\n",
    "                # Human says rare, Supervisor says not rare\n",
    "                disagreement_entry = {\n",
    "                    'entity': entity,\n",
    "                    'document_id': doc_id,\n",
    "                    'human_judgment': True,\n",
    "                    'supervisor_judgment': False\n",
    "                }\n",
    "                \n",
    "                # Add context information if available\n",
    "                if context_mapping:\n",
    "                    context_key = (doc_id, entity)\n",
    "                    context_info = context_mapping.get(context_key, {})\n",
    "                    \n",
    "                    human_info = context_info.get('human', {})\n",
    "                    supervisor_info = context_info.get('supervisor', {})\n",
    "                    \n",
    "                    disagreement_entry.update({\n",
    "                        'human_context': human_info.get('context', 'No context available'),\n",
    "                        'human_original_entity': human_info.get('original_entity', entity),\n",
    "                        'human_orpha_code': human_info.get('orpha_code', ''),\n",
    "                        'supervisor_context': supervisor_info.get('context', 'No context available'),\n",
    "                        'supervisor_original_entity': supervisor_info.get('original_entity', 'Entity not found in supervisor'),\n",
    "                        'supervisor_orpha_code': supervisor_info.get('orpha_code', '')\n",
    "                    })\n",
    "                \n",
    "                human_rare_supervisor_not.append(disagreement_entry)\n",
    "                \n",
    "            elif supervisor_judgment and not human_judgment:\n",
    "                # Supervisor says rare, Human says not rare\n",
    "                disagreement_entry = {\n",
    "                    'entity': entity,\n",
    "                    'document_id': doc_id,\n",
    "                    'human_judgment': False,\n",
    "                    'supervisor_judgment': True\n",
    "                }\n",
    "                \n",
    "                # Add context information if available\n",
    "                if context_mapping:\n",
    "                    context_key = (doc_id, entity)\n",
    "                    context_info = context_mapping.get(context_key, {})\n",
    "                    \n",
    "                    human_info = context_info.get('human', {})\n",
    "                    supervisor_info = context_info.get('supervisor', {})\n",
    "                    \n",
    "                    disagreement_entry.update({\n",
    "                        'human_context': human_info.get('context', 'No context available'),\n",
    "                        'human_original_entity': human_info.get('original_entity', 'Entity not found in human'),\n",
    "                        'human_orpha_code': human_info.get('orpha_code', ''),\n",
    "                        'supervisor_context': supervisor_info.get('context', 'No context available'),\n",
    "                        'supervisor_original_entity': supervisor_info.get('original_entity', entity),\n",
    "                        'supervisor_orpha_code': supervisor_info.get('orpha_code', '')\n",
    "                    })\n",
    "                \n",
    "                supervisor_rare_human_not.append(disagreement_entry)\n",
    "    \n",
    "    # Count frequencies\n",
    "    human_rare_freq = {}\n",
    "    for item in human_rare_supervisor_not:\n",
    "        entity = item['entity']\n",
    "        if entity not in human_rare_freq:\n",
    "            human_rare_freq[entity] = 0\n",
    "        human_rare_freq[entity] += 1\n",
    "    \n",
    "    supervisor_rare_freq = {}\n",
    "    for item in supervisor_rare_human_not:\n",
    "        entity = item['entity']\n",
    "        if entity not in supervisor_rare_freq:\n",
    "            supervisor_rare_freq[entity] = 0\n",
    "        supervisor_rare_freq[entity] += 1\n",
    "    \n",
    "    # Sort by frequency\n",
    "    human_rare_sorted = sorted(human_rare_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    supervisor_rare_sorted = sorted(supervisor_rare_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Identify problematic entities with mixed classifications\n",
    "    contradictory_entities = {}\n",
    "    for entity, stats in entity_classifications.items():\n",
    "        if stats[\"human_rare\"] > 0 and stats[\"human_not_rare\"] > 0:\n",
    "            contradictory_entities[entity] = {\n",
    "                \"human_contradictory\": True,\n",
    "                \"human_rare_count\": stats[\"human_rare\"],\n",
    "                \"human_not_rare_count\": stats[\"human_not_rare\"],\n",
    "                \"documents\": list(stats[\"documents\"])\n",
    "            }\n",
    "        if stats[\"supervisor_rare\"] > 0 and stats[\"supervisor_not_rare\"] > 0:\n",
    "            if entity not in contradictory_entities:\n",
    "                contradictory_entities[entity] = {\n",
    "                    \"human_contradictory\": False,\n",
    "                    \"documents\": list(stats[\"documents\"])\n",
    "                }\n",
    "            contradictory_entities[entity][\"supervisor_contradictory\"] = True\n",
    "            contradictory_entities[entity][\"supervisor_rare_count\"] = stats[\"supervisor_rare\"]\n",
    "            contradictory_entities[entity][\"supervisor_not_rare_count\"] = stats[\"supervisor_not_rare\"]\n",
    "    \n",
    "    # For specific entities of interest\n",
    "    entities_of_interest = [\"heparin induced thrombocytopenia\", \"portal vein thrombosis\", \n",
    "                          \"tracheobronchomalacia\", \"rheumatic fever\", \"sarcoid\"]\n",
    "    entities_detail = {}\n",
    "    \n",
    "    # Find the closest match for each entity of interest\n",
    "    for target_entity in entities_of_interest:\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        # Find the best matching entity in our classification data\n",
    "        for entity in entity_classifications:\n",
    "            # Try exact match first\n",
    "            if entity.lower() == target_entity.lower():\n",
    "                best_match = entity\n",
    "                break\n",
    "                \n",
    "            # Otherwise use fuzzy matching\n",
    "            score = fuzz.token_sort_ratio(entity.lower(), target_entity.lower())\n",
    "            if score > best_score and score >= 85:  # At least 85% similarity\n",
    "                best_score = score\n",
    "                best_match = entity\n",
    "        \n",
    "        # If we found a match, get its details\n",
    "        if best_match:\n",
    "            stats = entity_classifications[best_match]\n",
    "            entities_detail[target_entity] = {\n",
    "                \"matched_entity\": best_match,\n",
    "                \"match_score\": best_score if best_match.lower() != target_entity.lower() else 100,\n",
    "                \"stats\": stats,\n",
    "                \"human_rare_documents\": [doc_id for doc_id in stats[\"documents\"] \n",
    "                                        if human_doc_entities.get(doc_id, {}).get(best_match, False)],\n",
    "                \"supervisor_rare_documents\": [doc_id for doc_id in stats[\"documents\"] \n",
    "                                            if supervisor_doc_entities.get(doc_id, {}).get(best_match, False)]\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'human_rare_supervisor_not': human_rare_supervisor_not,\n",
    "        'supervisor_rare_human_not': supervisor_rare_human_not,\n",
    "        'human_rare_freq': human_rare_sorted,\n",
    "        'supervisor_rare_freq': supervisor_rare_sorted,\n",
    "        'total_human_rare_disagreements': len(human_rare_supervisor_not),\n",
    "        'total_supervisor_rare_disagreements': len(supervisor_rare_human_not),\n",
    "        'unique_human_rare_disagreements': len(human_rare_freq),\n",
    "        'unique_supervisor_rare_disagreements': len(supervisor_rare_freq),\n",
    "        'contradictory_entities': contradictory_entities,\n",
    "        'entities_detail': entities_detail\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a83fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validate_or_logic_application(human_doc_entities, supervisor_doc_entities, entity_mapping):\n",
    "    \"\"\"\n",
    "    Validate that OR logic has been properly applied by checking for any potential missed groupings.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== VALIDATING OR LOGIC APPLICATION ===\")\n",
    "    \n",
    "    # Check if there are any entities in the same document that should be grouped but aren't\n",
    "    issues_found = 0\n",
    "    \n",
    "    for doc_id in set(human_doc_entities.keys()) | set(supervisor_doc_entities.keys()):\n",
    "        human_entities = human_doc_entities.get(doc_id, {})\n",
    "        supervisor_entities = supervisor_doc_entities.get(doc_id, {})\n",
    "        \n",
    "        # Check if any entities in this document map to the same canonical form\n",
    "        # This shouldn't happen if OR logic was applied correctly\n",
    "        human_canonical_counts = defaultdict(int)\n",
    "        supervisor_canonical_counts = defaultdict(int)\n",
    "        \n",
    "        for entity in human_entities:\n",
    "            canonical = entity_mapping.get(entity, entity)\n",
    "            human_canonical_counts[canonical] += 1\n",
    "        \n",
    "        for entity in supervisor_entities:\n",
    "            canonical = entity_mapping.get(entity, entity)\n",
    "            supervisor_canonical_counts[canonical] += 1\n",
    "        \n",
    "        # Check for multiple entities mapping to same canonical (should not happen)\n",
    "        for canonical, count in human_canonical_counts.items():\n",
    "            if count > 1:\n",
    "                print(f\"  WARNING: Doc {doc_id} has {count} human entities mapping to '{canonical}'\")\n",
    "                issues_found += 1\n",
    "        \n",
    "        for canonical, count in supervisor_canonical_counts.items():\n",
    "            if count > 1:\n",
    "                print(f\"  WARNING: Doc {doc_id} has {count} supervisor entities mapping to '{canonical}'\")\n",
    "                issues_found += 1\n",
    "    \n",
    "    if issues_found == 0:\n",
    "        print(\"✓ OR logic validation passed - no duplicate canonical entities found\")\n",
    "    else:\n",
    "        print(f\"✗ OR logic validation failed - found {issues_found} potential issues\")\n",
    "    \n",
    "    return issues_found == 0\n",
    "\n",
    "\n",
    "def analyze_disagreements_with_clustering_info(\n",
    "    human_doc_entities: Dict[str, Dict[str, bool]], \n",
    "    supervisor_doc_entities: Dict[str, Dict[str, bool]],\n",
    "    entity_mapping: Dict[str, str],\n",
    "    context_mapping: Dict[Tuple[str, str], Dict] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Enhanced disagreement analysis that includes information about entity clustering and OR operations.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== ANALYZING DISAGREEMENTS WITH CLUSTERING INFO ===\")\n",
    "    \n",
    "    # Standard disagreement analysis\n",
    "    disagreements = analyze_disagreements(human_doc_entities, supervisor_doc_entities, context_mapping)\n",
    "    \n",
    "    # Additional analysis: identify disagreements involving clustered entities\n",
    "    clustered_disagreements = {\n",
    "        'human_rare_supervisor_not_clustered': [],\n",
    "        'supervisor_rare_human_not_clustered': []\n",
    "    }\n",
    "    \n",
    "    # Find reverse mapping: canonical -> list of original entities\n",
    "    canonical_to_entities = defaultdict(list)\n",
    "    for entity, canonical in entity_mapping.items():\n",
    "        if entity != canonical:  # Only include actual mappings\n",
    "            canonical_to_entities[canonical].append(entity)\n",
    "    \n",
    "    # Analyze disagreements involving clustered entities\n",
    "    for disagreement in disagreements['human_rare_supervisor_not']:\n",
    "        entity = disagreement['entity']\n",
    "        if entity in canonical_to_entities and len(canonical_to_entities[entity]) > 0:\n",
    "            disagreement['is_clustered_entity'] = True\n",
    "            disagreement['clustered_variants'] = canonical_to_entities[entity]\n",
    "            clustered_disagreements['human_rare_supervisor_not_clustered'].append(disagreement)\n",
    "        else:\n",
    "            disagreement['is_clustered_entity'] = False\n",
    "    \n",
    "    for disagreement in disagreements['supervisor_rare_human_not']:\n",
    "        entity = disagreement['entity']\n",
    "        if entity in canonical_to_entities and len(canonical_to_entities[entity]) > 0:\n",
    "            disagreement['is_clustered_entity'] = True\n",
    "            disagreement['clustered_variants'] = canonical_to_entities[entity]\n",
    "            clustered_disagreements['supervisor_rare_human_not_clustered'].append(disagreement)\n",
    "        else:\n",
    "            disagreement['is_clustered_entity'] = False\n",
    "    \n",
    "    # Add clustering info to the disagreements\n",
    "    disagreements.update(clustered_disagreements)\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_human_rare_disagree = len(disagreements['human_rare_supervisor_not'])\n",
    "    clustered_human_rare_disagree = len(clustered_disagreements['human_rare_supervisor_not_clustered'])\n",
    "    \n",
    "    total_supervisor_rare_disagree = len(disagreements['supervisor_rare_human_not'])\n",
    "    clustered_supervisor_rare_disagree = len(clustered_disagreements['supervisor_rare_human_not_clustered'])\n",
    "    \n",
    "    print(f\"Disagreements involving clustered entities:\")\n",
    "    print(f\"  Human rare, supervisor not: {clustered_human_rare_disagree}/{total_human_rare_disagree}\")\n",
    "    print(f\"  Supervisor rare, human not: {clustered_supervisor_rare_disagree}/{total_supervisor_rare_disagree}\")\n",
    "    \n",
    "    return disagreements\n",
    "\n",
    "\n",
    "def print_clustering_summary(entity_mapping: Dict[str, str]):\n",
    "    \"\"\"Print a summary of the entity clustering results.\"\"\"\n",
    "    print(\"\\n=== ENTITY CLUSTERING SUMMARY ===\")\n",
    "    \n",
    "    # Count mappings\n",
    "    identity_mappings = sum(1 for entity, canonical in entity_mapping.items() if entity == canonical)\n",
    "    actual_mappings = len(entity_mapping) - identity_mappings\n",
    "    \n",
    "    # Group by canonical entity\n",
    "    canonical_groups = defaultdict(list)\n",
    "    for entity, canonical in entity_mapping.items():\n",
    "        canonical_groups[canonical].append(entity)\n",
    "    \n",
    "    # Count cluster sizes\n",
    "    cluster_sizes = defaultdict(int)\n",
    "    for canonical, entities in canonical_groups.items():\n",
    "        cluster_size = len(entities)\n",
    "        cluster_sizes[cluster_size] += 1\n",
    "    \n",
    "    print(f\"Total entities: {len(entity_mapping)}\")\n",
    "    print(f\"Identity mappings (no clustering): {identity_mappings}\")\n",
    "    print(f\"Actual mappings (clustered): {actual_mappings}\")\n",
    "    print(f\"Unique canonical entities: {len(canonical_groups)}\")\n",
    "    \n",
    "    print(\"\\nCluster size distribution:\")\n",
    "    for size in sorted(cluster_sizes.keys()):\n",
    "        count = cluster_sizes[size]\n",
    "        print(f\"  Size {size}: {count} clusters\")\n",
    "    \n",
    "    # Show largest clusters\n",
    "    largest_clusters = sorted(canonical_groups.items(), key=lambda x: len(x[1]), reverse=True)[:10]\n",
    "    print(f\"\\nLargest clusters:\")\n",
    "    for canonical, entities in largest_clusters:\n",
    "        if len(entities) > 1:  # Only show actual clusters\n",
    "            print(f\"  '{canonical}': {len(entities)} entities\")\n",
    "            for entity in entities[:5]:  # Show first 5\n",
    "                print(f\"    - '{entity}'\")\n",
    "            if len(entities) > 5:\n",
    "                print(f\"    - ... and {len(entities)-5} more\")\n",
    "\n",
    "\n",
    "# Updated main analysis function\n",
    "def run_complete_disagreement_analysis(human_corrections, supervisor_corrections, similarity_threshold=85):\n",
    "    \"\"\"\n",
    "    Run the complete disagreement analysis with proper OR logic and validation.\n",
    "    \"\"\"\n",
    "    print(\"=== RUNNING COMPLETE DISAGREEMENT ANALYSIS ===\")\n",
    "    \n",
    "    # Step 1: Extract entities with proper OR logic\n",
    "    human_doc_entities, supervisor_doc_entities, entity_mapping, context_mapping = (\n",
    "        extract_document_entity_sets_with_orpha_priority_fixed(\n",
    "            human_corrections, supervisor_corrections, similarity_threshold\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Step 2: Print clustering summary\n",
    "    print_clustering_summary(entity_mapping)\n",
    "    \n",
    "    # Step 3: Validate OR logic application\n",
    "    validation_passed = validate_or_logic_application(\n",
    "        human_doc_entities, supervisor_doc_entities, entity_mapping\n",
    "    )\n",
    "    \n",
    "    # Step 4: Compute agreement metrics\n",
    "    metrics = compute_agreement_metrics(human_doc_entities, supervisor_doc_entities)\n",
    "    \n",
    "    # Step 5: Analyze disagreements with clustering info\n",
    "    disagreements = analyze_disagreements_with_clustering_info(\n",
    "        human_doc_entities, supervisor_doc_entities, entity_mapping, context_mapping\n",
    "    )\n",
    "    \n",
    "    # Step 6: Print comprehensive report\n",
    "    print_report(metrics, disagreements)\n",
    "    \n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'disagreements': disagreements,\n",
    "        'entity_mapping': entity_mapping,\n",
    "        'validation_passed': validation_passed,\n",
    "        'human_doc_entities': human_doc_entities,\n",
    "        'supervisor_doc_entities': supervisor_doc_entities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb291a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing bracket filtering:\n",
      "'[Entity 'retinopathy of prematurity' occurrence #2 (index 1) not found by string search]' -> Bracketed: True\n",
      "'Patient has diabetes mellitus' -> Bracketed: False\n",
      "'[This is a bracketed context]' -> Bracketed: True\n",
      "'Normal clinical context without brackets' -> Bracketed: False\n",
      "'   [  Bracketed with spaces  ]   ' -> Bracketed: True\n",
      "'' -> Bracketed: False\n",
      "'None' -> Bracketed: False\n",
      "\n",
      "Filtering test: 3 original, 1 filtered, 2 excluded\n",
      "Remaining disagreements: ['diabetes']\n"
     ]
    }
   ],
   "source": [
    "def is_bracketed_context(context: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a context string is bracketed (starts with '[' and ends with ']').\n",
    "    \n",
    "    Args:\n",
    "        context: The context string to check\n",
    "        \n",
    "    Returns:\n",
    "        True if the context is bracketed, False otherwise\n",
    "    \"\"\"\n",
    "    if not context or not isinstance(context, str):\n",
    "        return False\n",
    "    \n",
    "    stripped_context = context.strip()\n",
    "    return stripped_context.startswith('[') and stripped_context.endswith(']')\n",
    "\n",
    "\n",
    "def filter_bracketed_disagreements(disagreements_list: List[Dict]) -> Tuple[List[Dict], int]:\n",
    "    \"\"\"\n",
    "    Filter out disagreements where either human or supervisor context is bracketed.\n",
    "    \n",
    "    Args:\n",
    "        disagreements_list: List of disagreement dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (filtered_disagreements, excluded_count)\n",
    "    \"\"\"\n",
    "    filtered_disagreements = []\n",
    "    excluded_count = 0\n",
    "    \n",
    "    for disagreement in disagreements_list:\n",
    "        human_context = disagreement.get('human_context', '')\n",
    "        supervisor_context = disagreement.get('supervisor_context', '')\n",
    "        \n",
    "        # Check if either context is bracketed\n",
    "        human_bracketed = is_bracketed_context(human_context)\n",
    "        supervisor_bracketed = is_bracketed_context(supervisor_context)\n",
    "        \n",
    "        if human_bracketed or supervisor_bracketed:\n",
    "            excluded_count += 1\n",
    "            # Optional: Add exclusion reason to the disagreement for debugging\n",
    "            disagreement['excluded_reason'] = f\"Bracketed context - Human: {human_bracketed}, Supervisor: {supervisor_bracketed}\"\n",
    "        else:\n",
    "            filtered_disagreements.append(disagreement)\n",
    "    \n",
    "    return filtered_disagreements, excluded_count\n",
    "\n",
    "\n",
    "def print_detailed_disagreements(disagreements: Dict[str, Any], max_examples: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Print concise disagreement analysis with contexts, excluding bracketed contexts.\n",
    "    \n",
    "    Args:\n",
    "        disagreements: Dictionary containing disagreement analysis results\n",
    "        max_examples: Maximum number of examples to show for each type\n",
    "    \"\"\"\n",
    "    print(\"\\n=== DETAILED DISAGREEMENT ANALYSIS (EXCLUDING BRACKETED CONTEXTS) ===\")\n",
    "    \n",
    "    # Human says rare, supervisor says not rare\n",
    "    if disagreements['human_rare_supervisor_not']:\n",
    "        # Filter out bracketed contexts\n",
    "        filtered_human_rare, human_excluded = filter_bracketed_disagreements(\n",
    "            disagreements['human_rare_supervisor_not']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n--- HUMAN RARE, SUPERVISOR NOT RARE ---\")\n",
    "        print(f\"Total disagreements: {len(disagreements['human_rare_supervisor_not'])}\")\n",
    "        print(f\"Excluded (bracketed contexts): {human_excluded}\")\n",
    "        print(f\"Valid disagreements: {len(filtered_human_rare)}\")\n",
    "        \n",
    "        if filtered_human_rare:\n",
    "            # Group by entity and show top examples\n",
    "            human_rare_freq_with_context = {}\n",
    "            for item in filtered_human_rare:\n",
    "                entity = item['entity']\n",
    "                if entity not in human_rare_freq_with_context:\n",
    "                    human_rare_freq_with_context[entity] = []\n",
    "                human_rare_freq_with_context[entity].append(item)\n",
    "            \n",
    "            sorted_entities = sorted(human_rare_freq_with_context.items(), \n",
    "                                   key=lambda x: len(x[1]), reverse=True)\n",
    "            \n",
    "            for i, (entity, examples) in enumerate(sorted_entities[:max_examples]):\n",
    "                print(f\"\\n{i+1}. '{entity}' ({len(examples)}x)\")\n",
    "                example = examples[0]  # Show just the first example\n",
    "                print(f\"   Doc {example['document_id']} | H: '{example.get('human_original_entity', 'N/A')}' | S: '{example.get('supervisor_original_entity', 'N/A')}'\")\n",
    "                print(f\"   H-Context: \\\"{example.get('human_context', 'N/A')[:100]}{'...' if len(example.get('human_context', '')) > 100 else ''}\\\"\")\n",
    "                print(f\"   S-Context: \\\"{example.get('supervisor_context', 'N/A')[:100]}{'...' if len(example.get('supervisor_context', '')) > 100 else ''}\\\"\")\n",
    "        else:\n",
    "            print(\"   No valid disagreements after filtering bracketed contexts.\")\n",
    "    \n",
    "    # Supervisor says rare, human says not rare\n",
    "    if disagreements['supervisor_rare_human_not']:\n",
    "        # Filter out bracketed contexts\n",
    "        filtered_supervisor_rare, supervisor_excluded = filter_bracketed_disagreements(\n",
    "            disagreements['supervisor_rare_human_not']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n--- SUPERVISOR RARE, HUMAN NOT RARE ---\")\n",
    "        print(f\"Total disagreements: {len(disagreements['supervisor_rare_human_not'])}\")\n",
    "        print(f\"Excluded (bracketed contexts): {supervisor_excluded}\")\n",
    "        print(f\"Valid disagreements: {len(filtered_supervisor_rare)}\")\n",
    "        \n",
    "        if filtered_supervisor_rare:\n",
    "            supervisor_rare_freq_with_context = {}\n",
    "            for item in filtered_supervisor_rare:\n",
    "                entity = item['entity']\n",
    "                if entity not in supervisor_rare_freq_with_context:\n",
    "                    supervisor_rare_freq_with_context[entity] = []\n",
    "                supervisor_rare_freq_with_context[entity].append(item)\n",
    "            \n",
    "            sorted_entities = sorted(supervisor_rare_freq_with_context.items(), \n",
    "                                   key=lambda x: len(x[1]), reverse=True)\n",
    "            \n",
    "            for i, (entity, examples) in enumerate(sorted_entities[:max_examples]):\n",
    "                print(f\"\\n{i+1}. '{entity}' ({len(examples)}x)\")\n",
    "                example = examples[0]  # Show just the first example\n",
    "                print(f\"   Doc {example['document_id']} | H: '{example.get('human_original_entity', 'N/A')}' | S: '{example.get('supervisor_original_entity', 'N/A')}'\")\n",
    "                print(f\"   H-Context: \\\"{example.get('human_context', 'N/A')[:100]}{'...' if len(example.get('human_context', '')) > 100 else ''}\\\"\")\n",
    "                print(f\"   S-Context: \\\"{example.get('supervisor_context', 'N/A')[:100]}{'...' if len(example.get('supervisor_context', '')) > 100 else ''}\\\"\")\n",
    "        else:\n",
    "            print(\"   No valid disagreements after filtering bracketed contexts.\")\n",
    "\n",
    "\n",
    "def compute_filtered_disagreement_stats(disagreements: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute disagreement statistics after filtering out bracketed contexts.\n",
    "    \n",
    "    Args:\n",
    "        disagreements: Dictionary containing disagreement analysis results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with filtered statistics\n",
    "    \"\"\"\n",
    "    # Filter human rare disagreements\n",
    "    filtered_human_rare, human_excluded = filter_bracketed_disagreements(\n",
    "        disagreements.get('human_rare_supervisor_not', [])\n",
    "    )\n",
    "    \n",
    "    # Filter supervisor rare disagreements\n",
    "    filtered_supervisor_rare, supervisor_excluded = filter_bracketed_disagreements(\n",
    "        disagreements.get('supervisor_rare_human_not', [])\n",
    "    )\n",
    "    \n",
    "    # Compute frequencies for filtered disagreements\n",
    "    human_rare_freq = {}\n",
    "    for item in filtered_human_rare:\n",
    "        entity = item['entity']\n",
    "        human_rare_freq[entity] = human_rare_freq.get(entity, 0) + 1\n",
    "    \n",
    "    supervisor_rare_freq = {}\n",
    "    for item in filtered_supervisor_rare:\n",
    "        entity = item['entity']\n",
    "        supervisor_rare_freq[entity] = supervisor_rare_freq.get(entity, 0) + 1\n",
    "    \n",
    "    # Sort by frequency\n",
    "    human_rare_sorted = sorted(human_rare_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    supervisor_rare_sorted = sorted(supervisor_rare_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'filtered_human_rare_supervisor_not': filtered_human_rare,\n",
    "        'filtered_supervisor_rare_human_not': filtered_supervisor_rare,\n",
    "        'human_excluded_count': human_excluded,\n",
    "        'supervisor_excluded_count': supervisor_excluded,\n",
    "        'filtered_human_rare_freq': human_rare_sorted,\n",
    "        'filtered_supervisor_rare_freq': supervisor_rare_sorted,\n",
    "        'filtered_total_human_rare_disagreements': len(filtered_human_rare),\n",
    "        'filtered_total_supervisor_rare_disagreements': len(filtered_supervisor_rare),\n",
    "        'filtered_unique_human_rare_disagreements': len(human_rare_freq),\n",
    "        'filtered_unique_supervisor_rare_disagreements': len(supervisor_rare_freq),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_report(metrics: Dict[str, Any], disagreements: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Print a report with the agreement metrics, excluding bracketed contexts from disagreement analysis.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary containing agreement metrics\n",
    "        disagreements: Dictionary containing disagreement analysis results\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RARE DISEASE ANNOTATOR AGREEMENT REPORT WITH ORPHA CODE PRIORITY ===\")\n",
    "    \n",
    "    print(\"\\n--- ENTITY STATISTICS ---\")\n",
    "    print(f\"Documents analyzed: {metrics['total_documents']}\")\n",
    "    print(f\"Total entity judgments: {metrics['total_entity_judgments']}\")\n",
    "    \n",
    "    print(f\"\\nHuman annotations:\")\n",
    "    print(f\"  Total entities: {metrics['human_rare_entities'] + metrics['human_nonrare_entities']}\")\n",
    "    print(f\"  Rare disease entities: {metrics['human_rare_entities']}\")\n",
    "    print(f\"  Non-rare entities: {metrics['human_nonrare_entities']}\")\n",
    "    \n",
    "    print(f\"\\nSupervisor annotations:\")\n",
    "    print(f\"  Total entities: {metrics['supervisor_rare_entities'] + metrics['supervisor_nonrare_entities']}\")\n",
    "    print(f\"  Rare disease entities: {metrics['supervisor_rare_entities']}\")\n",
    "    print(f\"  Non-rare entities: {metrics['supervisor_nonrare_entities']}\")\n",
    "    \n",
    "    print(f\"\\nUnique entities after ORPHA-priority mapping:\")\n",
    "    print(f\"  Total unique entities: {metrics['unique_entities_total']}\")\n",
    "    print(f\"  Unique in human annotations: {metrics['unique_human_entities']}\")\n",
    "    print(f\"  Unique in supervisor annotations: {metrics['unique_supervisor_entities']}\")\n",
    "    print(f\"  Unique rare in human: {metrics['unique_human_rare']}\")\n",
    "    print(f\"  Unique rare in supervisor: {metrics['unique_supervisor_rare']}\")\n",
    "    \n",
    "    print(\"\\n--- AGREEMENT METRICS ---\")\n",
    "    print(f\"Agreements: {metrics['total_agreements']} of {metrics['total_entity_judgments']} judgments\")\n",
    "    print(f\"Percentage Agreement (Accuracy): {metrics['percent_agreement']:.4f}\")\n",
    "    print(f\"Cohen's Kappa: {metrics['cohen_kappa']:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- CONFUSION MATRIX ---\")\n",
    "    print(f\"True Positives (both rare): {metrics['true_positives']}\")\n",
    "    print(f\"True Negatives (both non-rare): {metrics['true_negatives']}\")\n",
    "    print(f\"False Positives (supervisor rare, human non-rare): {metrics['false_positives']}\")\n",
    "    print(f\"False Negatives (human rare, supervisor non-rare): {metrics['false_negatives']}\")\n",
    "    \n",
    "    print(\"\\n--- CLASSIFICATION METRICS ---\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Compute filtered disagreement statistics\n",
    "    filtered_stats = compute_filtered_disagreement_stats(disagreements)\n",
    "    \n",
    "    print(\"\\n--- DISAGREEMENT SUMMARY (EXCLUDING BRACKETED CONTEXTS) ---\")\n",
    "    print(f\"Original disagreements:\")\n",
    "    print(f\"  Human says rare, supervisor says not: {disagreements['total_human_rare_disagreements']} total\")\n",
    "    print(f\"  Supervisor says rare, human says not: {disagreements['total_supervisor_rare_disagreements']} total\")\n",
    "    \n",
    "    print(f\"\\nFiltered disagreements (excluding bracketed contexts):\")\n",
    "    print(f\"  Human says rare, supervisor says not: {filtered_stats['filtered_total_human_rare_disagreements']} valid ({filtered_stats['human_excluded_count']} excluded)\")\n",
    "    print(f\"  Supervisor says rare, human says not: {filtered_stats['filtered_total_supervisor_rare_disagreements']} valid ({filtered_stats['supervisor_excluded_count']} excluded)\")\n",
    "    print(f\"  Unique entities in human rare disagreements: {filtered_stats['filtered_unique_human_rare_disagreements']}\")\n",
    "    print(f\"  Unique entities in supervisor rare disagreements: {filtered_stats['filtered_unique_supervisor_rare_disagreements']}\")\n",
    "    \n",
    "    if filtered_stats['filtered_human_rare_freq']:\n",
    "        print(\"\\nTop disagreements (human says rare, supervisor says not) - after filtering:\")\n",
    "        for entity, count in filtered_stats['filtered_human_rare_freq'][:5]:\n",
    "            print(f\"  {entity}: {count} occurrences\")\n",
    "    \n",
    "    if filtered_stats['filtered_supervisor_rare_freq']:\n",
    "        print(\"\\nTop disagreements (supervisor says rare, human says not) - after filtering:\")\n",
    "        for entity, count in filtered_stats['filtered_supervisor_rare_freq'][:5]:\n",
    "            print(f\"  {entity}: {count} occurrences\")\n",
    "    \n",
    "    # Print contradictory entities (unchanged)\n",
    "    if disagreements.get('contradictory_entities'):\n",
    "        print(\"\\nEntities with contradictory classifications:\")\n",
    "        for entity, details in sorted(disagreements['contradictory_entities'].items(), \n",
    "                                    key=lambda x: x[1].get('human_rare_count', 0) + \n",
    "                                                 x[1].get('supervisor_rare_count', 0), \n",
    "                                    reverse=True)[:10]:\n",
    "            print(f\"\\n  {entity}:\")\n",
    "            if details.get('human_contradictory', False):\n",
    "                print(f\"    Human classified as rare: {details['human_rare_count']} times\")\n",
    "                print(f\"    Human classified as not rare: {details['human_not_rare_count']} times\")\n",
    "            if details.get('supervisor_contradictory', False):\n",
    "                print(f\"    Supervisor classified as rare: {details['supervisor_rare_count']} times\")\n",
    "                print(f\"    Supervisor classified as not rare: {details['supervisor_not_rare_count']} times\")\n",
    "            print(f\"    Appears in {len(details['documents'])} documents\")\n",
    "    \n",
    "    # Print details for entities of interest (unchanged)\n",
    "    if disagreements.get('entities_detail'):\n",
    "        print(\"\\nSpecific entities of interest:\")\n",
    "        for target_entity, details in disagreements['entities_detail'].items():\n",
    "            matched_entity = details.get(\"matched_entity\", \"\")\n",
    "            match_info = \"\"\n",
    "            if matched_entity != target_entity and details.get(\"match_score\", 0) < 100:\n",
    "                match_info = f\" (matched to '{matched_entity}' with {details['match_score']}% similarity)\"\n",
    "                \n",
    "            stats = details['stats']\n",
    "            print(f\"\\n  {target_entity}{match_info}:\")\n",
    "            print(f\"    Human classified as rare: {stats['human_rare']} times\")\n",
    "            print(f\"    Human classified as not rare: {stats['human_not_rare']} times\")\n",
    "            print(f\"    Supervisor classified as rare: {stats['supervisor_rare']} times\")\n",
    "            print(f\"    Supervisor classified as not rare: {stats['supervisor_not_rare']} times\")\n",
    "            print(f\"    Human rare documents: {details['human_rare_documents']}\")\n",
    "            print(f\"    Supervisor rare documents: {details['supervisor_rare_documents']}\")\n",
    "    \n",
    "    # Validation assertions (unchanged)\n",
    "    print(\"\\n--- VALIDATION ---\")\n",
    "    print(\"Assertion checks:\")\n",
    "    print(\"human rare entities:\", metrics['human_rare_entities'])\n",
    "    print(\"supervisor rare entities:\", metrics['supervisor_rare_entities'])\n",
    "    print(\"true positives:\", metrics['true_positives'])\n",
    "    print(\"false negatives:\", metrics['false_negatives'])\n",
    "    print(\"false positives:\", metrics['false_positives'])\n",
    "\n",
    "    try:\n",
    "        assert metrics['human_rare_entities'] == metrics['true_positives'] + metrics['false_negatives']\n",
    "        print(\"✓ Human rare entities = TP + FN\")\n",
    "    except AssertionError:\n",
    "        print(\"✗ Human rare entities ≠ TP + FN\")\n",
    "    \n",
    "    try:\n",
    "        assert metrics['supervisor_rare_entities'] == metrics['true_positives'] + metrics['false_positives']\n",
    "        print(\"✓ Supervisor rare entities = TP + FP\")\n",
    "    except AssertionError:\n",
    "        print(\"✗ Supervisor rare entities ≠ TP + FP\")\n",
    "    \n",
    "    # Print detailed disagreement analysis with filtering\n",
    "    print_detailed_disagreements(disagreements)\n",
    "\n",
    "\n",
    "# Example usage and test cases\n",
    "def test_bracket_filtering():\n",
    "    \"\"\"Test the bracket filtering functionality.\"\"\"\n",
    "    test_contexts = [\n",
    "        \"[Entity 'retinopathy of prematurity' occurrence #2 (index 1) not found by string search]\",\n",
    "        \"Patient has diabetes mellitus\",\n",
    "        \"[This is a bracketed context]\",\n",
    "        \"Normal clinical context without brackets\",\n",
    "        \"   [  Bracketed with spaces  ]   \",\n",
    "        \"\",\n",
    "        None\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing bracket filtering:\")\n",
    "    for context in test_contexts:\n",
    "        is_bracketed = is_bracketed_context(context)\n",
    "        print(f\"'{context}' -> Bracketed: {is_bracketed}\")\n",
    "    \n",
    "    # Test disagreement filtering\n",
    "    test_disagreements = [\n",
    "        {\n",
    "            'entity': 'diabetes',\n",
    "            'human_context': 'Patient has diabetes mellitus',\n",
    "            'supervisor_context': 'Diabetes noted in history'\n",
    "        },\n",
    "        {\n",
    "            'entity': 'retinopathy',\n",
    "            'human_context': \"[Entity 'retinopathy of prematurity' occurrence #2 not found]\",\n",
    "            'supervisor_context': 'Retinopathy of prematurity diagnosed'\n",
    "        },\n",
    "        {\n",
    "            'entity': 'hypertension',\n",
    "            'human_context': 'Blood pressure elevated',\n",
    "            'supervisor_context': '[Hypertension context bracketed]'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    filtered, excluded_count = filter_bracketed_disagreements(test_disagreements)\n",
    "    print(f\"\\nFiltering test: {len(test_disagreements)} original, {len(filtered)} filtered, {excluded_count} excluded\")\n",
    "    print(\"Remaining disagreements:\", [d['entity'] for d in filtered])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_bracket_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19a4fe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 98 unique entities with ORPHA codes\n",
      "Found 31 unique entities without ORPHA codes\n",
      "Found 84 unique ORPHA codes\n",
      "  Cluster: ['postpolio syndrome', 'post polio syndrome']\n",
      "  Chosen canonical: 'post polio syndrome'\n",
      "  Cluster: ['lyme disease', \"lyme's disease\"]\n",
      "  Chosen canonical: 'lyme's disease'\n",
      "  Cluster: ['hemochromatosis', 'iron storage disease']\n",
      "  Chosen canonical: 'iron storage disease'\n",
      "  Cluster: [\"alport's syndrome\", 'alport syndrome']\n",
      "  Chosen canonical: 'alport's syndrome'\n",
      "  Cluster: ['als', 'amyotrophic lateral sclerosis']\n",
      "  Chosen canonical: 'amyotrophic lateral sclerosis'\n",
      "  Cluster: ['fulminant liver failure', 'acute hepatic failure', 'fulminant hepatic failure']\n",
      "  Chosen canonical: 'fulminant hepatic failure'\n",
      "  Cluster: ['essential thrombocythemia', 'essential thrombocytosis']\n",
      "  Chosen canonical: 'essential thrombocythemia'\n",
      "  Cluster: ['epileptic seizures', 'epileptic', 'seizure disorder', 'epilepsy', 'epileptic seizure']\n",
      "  Chosen canonical: 'epileptic seizures'\n",
      "  Cluster: ['sarcoidosis', 'sarcoid']\n",
      "  Chosen canonical: 'sarcoidosis'\n",
      "  Cluster: ['thrombotic thrombocytopenic purpura', 'ttp']\n",
      "  Chosen canonical: 'thrombotic thrombocytopenic purpura'\n",
      "Mapped 98 entities using ORPHA codes\n",
      "  Cross-ORPHA match: 'protein s' -> 'protein s' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'methemoglobinemia' -> 'methemoglobinemia' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'papillary carcinoma' -> 'papillary carcinoma' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'transitional cell carcinoma' -> 'transitional cell carcinoma' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'mesenteric vein thrombosis' -> 'mesenteric vein thrombosis' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'pyoderma gangrenosum' -> 'pyoderma gangrenosum' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'glioblastoma multiforme' -> 'glioblastoma multiforme' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'protein c' -> 'protein c deficiency' (ORPHA entity, substring, score=95)\n",
      "  Cross-ORPHA match: 'bronchiectasis' -> 'bronchiectasis' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'neovascular glaucoma' -> 'neovascular glaucoma' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'alopecia' -> 'alopecia' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'intraductal papillary mucinous tumor' -> 'intraductal papillary mucinous tumor' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'budd chiari' -> 'budd chiari syndrome' (ORPHA entity, substring, score=95)\n",
      "  Cross-ORPHA match: 'meningocele' -> 'meningocele' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'complex tracheal stenosis' -> 'tracheal stenosis' (ORPHA entity, substring, score=95)\n",
      "  Cross-ORPHA match: 'hepatocellular carcinoma' -> 'hepatocellular carcinoma' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'central nervous system and systemic lymphoma' -> 'central nervous system and systemic lymphoma' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'multiple myeloma' -> 'multiple myeloma' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'anaplastic thyroid carcinoma' -> 'anaplastic thyroid carcinoma' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'primary cns lymphoma' -> 'primary cns lymphoma' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'bechet's disease' -> 'bechet's disease' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'antiphospholipid antibody syndrome' -> 'antiphospholipid syndrome' (ORPHA entity, word_subset, score=92)\n",
      "  Cross-ORPHA match: 'thymoma' -> 'thymoma' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'neovascular glaucoma angle closure' -> 'neovascular glaucoma' (ORPHA entity, substring, score=95)\n",
      "  Cross-ORPHA match: 'cervical stenosis' -> 'cervical stenosis' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'mediastinitis' -> 'mediastinitis' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'sclerosis cholangitis' -> 'sclerosis cholangitis' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'tracheal stenosis' -> 'tracheal stenosis' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'congenital bleeding disorder' -> 'congenital bleeding disorder' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'medullary sponge kidney' -> 'medullary sponge kidney' (ORPHA entity, exact, score=100)\n",
      "  Cross-ORPHA match: 'acute myelogenous leukemia' -> 'acute myelogenous leukemia' (ORPHA entity, exact, score=100)\n",
      "Cross-ORPHA matching: 31 entities matched to ORPHA entities\n",
      "Remaining for string clustering: 0 entities\n",
      "  OR operation: 'nocardiosis' in doc 1208 - human: existing=True OR new=True = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 1552 - human: existing=True OR new=True = True\n",
      "  OR operation: 'tracheobronchomalacia' in doc 1790 - human: existing=True OR new=True = True\n",
      "  OR operation: 'sarcoidosis' in doc 2452 - human: existing=True OR new=True = True\n",
      "  OR operation: 'sarcoidosis' in doc 2452 - human: existing=True OR new=True = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 3390 - human: existing=True OR new=False = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 3390 - human: existing=True OR new=True = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 3390 - human: existing=True OR new=True = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 3390 - human: existing=True OR new=True = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 3390 - human: existing=True OR new=True = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 3390 - human: existing=True OR new=True = True\n",
      "  OR operation: 'epileptic seizures' in doc 4043 - human: existing=False OR new=False = False\n",
      "  OR operation: 'epileptic seizures' in doc 4043 - human: existing=False OR new=False = False\n",
      "  OR operation: 'nph' in doc 7363 - human: existing=False OR new=False = False\n",
      "  OR operation: 'nph' in doc 7363 - human: existing=False OR new=False = False\n",
      "  OR operation: 'nph' in doc 13231 - human: existing=False OR new=False = False\n",
      "  OR operation: 'nph' in doc 13231 - human: existing=False OR new=False = False\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 7688 - human: existing=True OR new=False = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 7688 - human: existing=True OR new=False = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 7688 - human: existing=True OR new=False = True\n",
      "  OR operation: 'pml' in doc 6188 - human: existing=True OR new=True = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 8979 - human: existing=True OR new=True = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 8979 - human: existing=True OR new=True = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 8979 - human: existing=True OR new=False = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 8979 - human: existing=True OR new=True = True\n",
      "  OR operation: 'nph' in doc 6206 - human: existing=False OR new=False = False\n",
      "  OR operation: 'nph' in doc 6206 - human: existing=False OR new=False = False\n",
      "  OR operation: 'dilated cardiomyopathy' in doc 10004 - human: existing=True OR new=True = True\n",
      "  OR operation: 'asbestosis' in doc 10004 - human: existing=True OR new=True = True\n",
      "  OR operation: 'epileptic seizures' in doc 13600 - human: existing=False OR new=False = False\n",
      "  OR operation: 'iron storage disease' in doc 11604 - human: existing=True OR new=True = True\n",
      "  OR operation: 'iron storage disease' in doc 11604 - human: existing=True OR new=True = True\n",
      "  OR operation: 'iron storage disease' in doc 11604 - human: existing=True OR new=True = True\n",
      "  OR operation: 'iron storage disease' in doc 11604 - human: existing=True OR new=True = True\n",
      "  OR operation: 'iron storage disease' in doc 11604 - human: existing=True OR new=True = True\n",
      "  OR operation: 'protein c deficiency' in doc 11604 - human: existing=True OR new=True = True\n",
      "  OR operation: 'protein s' in doc 11604 - human: existing=True OR new=True = True\n",
      "  OR operation: 'protein s' in doc 11604 - human: existing=True OR new=True = True\n",
      "  OR operation: 'protein c deficiency' in doc 11604 - human: existing=True OR new=True = True\n",
      "  OR operation: 'protein c deficiency' in doc 11604 - human: existing=True OR new=False = True\n",
      "  OR operation: 'protein c deficiency' in doc 11604 - human: existing=True OR new=True = True\n",
      "  OR operation: 'protein c deficiency' in doc 11604 - human: existing=True OR new=False = True\n",
      "  OR operation: 'protein c deficiency' in doc 11604 - human: existing=True OR new=False = True\n",
      "  OR operation: 'protein c deficiency' in doc 11604 - human: existing=True OR new=True = True\n",
      "  OR operation: 'protein c deficiency' in doc 11604 - human: existing=True OR new=False = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 22297 - human: existing=True OR new=True = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 22297 - human: existing=True OR new=False = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 22297 - human: existing=True OR new=True = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 17850 - human: existing=True OR new=True = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 17850 - human: existing=True OR new=True = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 17850 - human: existing=True OR new=False = True\n",
      "  OR operation: 'nph' in doc 15908 - human: existing=False OR new=False = False\n",
      "  OR operation: 'portal vein thrombosis' in doc 20208 - human: existing=True OR new=True = True\n",
      "  OR operation: 'portal vein thrombosis' in doc 20208 - human: existing=True OR new=True = True\n",
      "  OR operation: 'portal vein thrombosis' in doc 20208 - human: existing=True OR new=True = True\n",
      "  OR operation: 'portal vein thrombosis' in doc 20208 - human: existing=True OR new=True = True\n",
      "  OR operation: 'portal vein thrombosis' in doc 20208 - human: existing=True OR new=True = True\n",
      "  OR operation: 'portal vein thrombosis' in doc 20208 - human: existing=True OR new=True = True\n",
      "  OR operation: 'portal vein thrombosis' in doc 20208 - human: existing=True OR new=False = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 20208 - human: existing=False OR new=False = False\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 20208 - human: existing=False OR new=False = False\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 20208 - human: existing=True OR new=True = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 20208 - human: existing=True OR new=True = True\n",
      "  OR operation: 'familial mediterranean fever' in doc 31210 - human: existing=True OR new=True = True\n",
      "  OR operation: 'necrotizing enterocolitis' in doc 20162 - human: existing=True OR new=True = True\n",
      "  OR operation: 'necrotizing enterocolitis' in doc 20162 - human: existing=True OR new=True = True\n",
      "  OR operation: 'nph' in doc 20617 - human: existing=False OR new=False = False\n",
      "  OR operation: 'nph' in doc 20617 - human: existing=False OR new=False = False\n",
      "  OR operation: 'nph' in doc 20617 - human: existing=False OR new=False = False\n",
      "  OR operation: 'mesenteric vein thrombosis' in doc 20387 - human: existing=True OR new=True = True\n",
      "  OR operation: 'sarcoidosis' in doc 40196 - human: existing=True OR new=True = True\n",
      "  OR operation: 'necrotizing enterocolitis' in doc 26976 - human: existing=True OR new=True = True\n",
      "  OR operation: 'necrotizing enterocolitis' in doc 26976 - human: existing=True OR new=True = True\n",
      "  OR operation: 'necrotizing enterocolitis' in doc 26976 - human: existing=True OR new=True = True\n",
      "  OR operation: 'necrotizing enterocolitis' in doc 26976 - human: existing=True OR new=True = True\n",
      "  OR operation: 'necrotizing enterocolitis' in doc 26976 - human: existing=True OR new=True = True\n",
      "  OR operation: 'necrotizing enterocolitis' in doc 26976 - human: existing=True OR new=False = True\n",
      "  OR operation: 'retinopathy of prematurity' in doc 26976 - human: existing=True OR new=False = True\n",
      "  OR operation: 'anaplastic thyroid carcinoma' in doc 26825 - human: existing=True OR new=True = True\n",
      "  OR operation: 'retinopathy of prematurity' in doc 32261 - human: existing=True OR new=False = True\n",
      "  OR operation: 'primary cns lymphoma' in doc 47026 - human: existing=True OR new=True = True\n",
      "  OR operation: 'neovascular glaucoma' in doc 47026 - human: existing=True OR new=True = True\n",
      "  OR operation: 'neovascular glaucoma' in doc 47026 - human: existing=True OR new=True = True\n",
      "  OR operation: 'neovascular glaucoma' in doc 47026 - human: existing=True OR new=False = True\n",
      "  OR operation: 'neovascular glaucoma' in doc 47026 - human: existing=True OR new=False = True\n",
      "  OR operation: 'hepatocellular carcinoma' in doc 47035 - human: existing=True OR new=True = True\n",
      "  OR operation: 'hepatocellular carcinoma' in doc 47035 - human: existing=True OR new=True = True\n",
      "  OR operation: 'beta thalassemia' in doc 45138 - human: existing=True OR new=True = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 44132 - human: existing=True OR new=False = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 44132 - human: existing=True OR new=True = True\n",
      "  OR operation: 'amyotrophic lateral sclerosis' in doc 44132 - human: existing=True OR new=False = True\n",
      "  OR operation: 'epileptic seizures' in doc 45078 - human: existing=False OR new=False = False\n",
      "  OR operation: 'epileptic seizures' in doc 32946 - human: existing=False OR new=False = False\n",
      "  OR operation: 'tracheal stenosis' in doc 35397 - human: existing=True OR new=True = True\n",
      "  OR operation: 'tracheal stenosis' in doc 35397 - human: existing=True OR new=True = True\n",
      "  OR operation: 'tracheal stenosis' in doc 35397 - human: existing=True OR new=True = True\n",
      "  OR operation: 'tracheal stenosis' in doc 35397 - human: existing=True OR new=True = True\n",
      "  OR operation: 'tracheal stenosis' in doc 35397 - human: existing=True OR new=False = True\n",
      "  OR operation: 'tracheal stenosis' in doc 35397 - human: existing=True OR new=False = True\n",
      "  OR operation: 'tracheal stenosis' in doc 35397 - human: existing=True OR new=False = True\n",
      "  OR operation: 'tracheal stenosis' in doc 35397 - human: existing=True OR new=False = True\n",
      "  OR operation: 'dilated cardiomyopathy' in doc 34416 - human: existing=True OR new=True = True\n",
      "  OR operation: 'dilated cardiomyopathy' in doc 34416 - human: existing=True OR new=True = True\n",
      "  OR operation: 'dilated cardiomyopathy' in doc 34416 - human: existing=True OR new=True = True\n",
      "  OR operation: 'dilated cardiomyopathy' in doc 34416 - human: existing=True OR new=True = True\n",
      "  OR operation: 'multiple myeloma' in doc 35579 - human: existing=True OR new=True = True\n",
      "  OR operation: 'multiple myeloma' in doc 35579 - human: existing=True OR new=True = True\n",
      "  OR operation: 'multiple myeloma' in doc 35579 - human: existing=True OR new=True = True\n",
      "  OR operation: 'pmr' in doc 35658 - human: existing=True OR new=True = True\n",
      "  OR operation: 'transitional cell carcinoma' in doc 35228 - human: existing=False OR new=False = False\n",
      "  OR operation: 'bronchiectasis' in doc 33893 - human: existing=True OR new=True = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 33909 - human: existing=True OR new=True = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 33909 - human: existing=True OR new=True = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 33909 - human: existing=True OR new=False = True\n",
      "  OR operation: 'calciphylaxis' in doc 34305 - human: existing=True OR new=True = True\n",
      "  OR operation: 'calciphylaxis' in doc 34305 - human: existing=True OR new=True = True\n",
      "  OR operation: 'tracheobronchomalacia' in doc 34316 - human: existing=True OR new=True = True\n",
      "  OR operation: 'mediastinitis' in doc 36547 - human: existing=True OR new=True = True\n",
      "  OR operation: 'mediastinitis' in doc 36547 - human: existing=True OR new=True = True\n",
      "  OR operation: 'mediastinitis' in doc 36547 - human: existing=True OR new=True = True\n",
      "  OR operation: 'mediastinitis' in doc 36547 - human: existing=True OR new=True = True\n",
      "  OR operation: 'mediastinitis' in doc 36547 - human: existing=True OR new=True = True\n",
      "  OR operation: 'heparin induced thrombocytopenia' in doc 52501 - human: existing=True OR new=True = True\n",
      "  OR operation: 'fulminant hepatic failure' in doc 49795 - human: existing=True OR new=True = True\n",
      "  OR operation: 'fulminant hepatic failure' in doc 49795 - human: existing=True OR new=True = True\n",
      "  OR operation: 'fulminant hepatic failure' in doc 49795 - human: existing=True OR new=False = True\n",
      "  OR operation: 'fulminant hepatic failure' in doc 49795 - human: existing=True OR new=False = True\n",
      "  OR operation: 'fulminant hepatic failure' in doc 49795 - human: existing=True OR new=False = True\n",
      "  OR operation: 'alport's syndrome' in doc 49021 - human: existing=True OR new=True = True\n",
      "  OR operation: 'epileptic seizures' in doc 41073 - human: existing=False OR new=False = False\n",
      "  OR operation: 'epileptic seizures' in doc 41073 - human: existing=False OR new=False = False\n",
      "  OR operation: 'epileptic seizures' in doc 41073 - human: existing=False OR new=False = False\n",
      "  OR operation: 'epileptic seizures' in doc 41073 - human: existing=False OR new=False = False\n",
      "  OR operation: 'epileptic seizures' in doc 41300 - human: existing=False OR new=False = False\n",
      "  OR operation: 'nph' in doc 41466 - human: existing=False OR new=False = False\n",
      "  OR operation: 'thrombotic thrombocytopenic purpura' in doc 54937 - human: existing=True OR new=True = True\n",
      "  OR operation: 'primary sclerosing cholangitis' in doc 49532 - human: existing=True OR new=True = True\n",
      "  OR operation: 'primary sclerosing cholangitis' in doc 49532 - human: existing=True OR new=True = True\n",
      "  OR operation: 'hyperparathyroidism' in doc 47713 - human: existing=False OR new=False = False\n",
      "  OR operation: 'crest syndrome' in doc 49482 - human: existing=True OR new=True = True\n",
      "  OR operation: 'crest syndrome' in doc 49482 - human: existing=True OR new=True = True\n",
      "  OR operation: 'legionella' in doc 48652 - human: existing=True OR new=False = True\n",
      "  OR operation: 'polymyalgia rheumatica' in doc 49514 - human: existing=False OR new=False = False\n",
      "  OR operation: 'essential thrombocythemia' in doc 53382 - human: existing=True OR new=True = True\n",
      "  OR operation: 'essential thrombocythemia' in doc 53382 - human: existing=True OR new=True = True\n",
      "  OR operation: 'essential thrombocythemia' in doc 53382 - human: existing=True OR new=True = True\n",
      "  OR operation: 'essential thrombocythemia' in doc 53382 - human: existing=True OR new=True = True\n",
      "  OR operation: 'essential thrombocythemia' in doc 53382 - human: existing=True OR new=True = True\n",
      "  OR operation: 'budd chiari syndrome' in doc 53382 - human: existing=True OR new=True = True\n",
      "  OR operation: 'budd chiari syndrome' in doc 53382 - human: existing=True OR new=True = True\n",
      "  OR operation: 'budd chiari syndrome' in doc 53382 - human: existing=True OR new=True = True\n",
      "  OR operation: 'budd chiari syndrome' in doc 53382 - human: existing=True OR new=True = True\n",
      "  OR operation: 'budd chiari syndrome' in doc 53382 - human: existing=True OR new=True = True\n",
      "  OR operation: 'budd chiari syndrome' in doc 53382 - human: existing=True OR new=False = True\n",
      "  OR operation: 'epileptic seizures' in doc 53406 - human: existing=False OR new=False = False\n",
      "  OR operation: 'epileptic seizures' in doc 53406 - human: existing=False OR new=False = False\n",
      "  OR operation: 'epileptic seizures' in doc 53406 - human: existing=False OR new=False = False\n",
      "  OR operation: 'multifocal atrial tachycardia' in doc 59366 - human: existing=True OR new=True = True\n",
      "  OR operation: 'protein c deficiency' in doc 11604 - supervisor: existing=True OR new=True = True\n",
      "  OR operation: 'sarcoidosis' in doc 40196 - supervisor: existing=True OR new=True = True\n",
      "\n",
      "Processed 117 documents with OR logic applied during processing\n",
      "\n",
      "=== RARE DISEASE ANNOTATOR AGREEMENT REPORT WITH ORPHA CODE PRIORITY ===\n",
      "\n",
      "--- ENTITY STATISTICS ---\n",
      "Documents analyzed: 117\n",
      "Total entity judgments: 175\n",
      "\n",
      "Human annotations:\n",
      "  Total entities: 175\n",
      "  Rare disease entities: 113\n",
      "  Non-rare entities: 62\n",
      "\n",
      "Supervisor annotations:\n",
      "  Total entities: 175\n",
      "  Rare disease entities: 76\n",
      "  Non-rare entities: 99\n",
      "\n",
      "Unique entities after ORPHA-priority mapping:\n",
      "  Total unique entities: 82\n",
      "  Unique in human annotations: 82\n",
      "  Unique in supervisor annotations: 82\n",
      "  Unique rare in human: 72\n",
      "  Unique rare in supervisor: 44\n",
      "\n",
      "--- AGREEMENT METRICS ---\n",
      "Agreements: 126 of 175 judgments\n",
      "Percentage Agreement (Accuracy): 0.7200\n",
      "Cohen's Kappa: 0.4607\n",
      "\n",
      "--- CONFUSION MATRIX ---\n",
      "True Positives (both rare): 70\n",
      "True Negatives (both non-rare): 56\n",
      "False Positives (supervisor rare, human non-rare): 6\n",
      "False Negatives (human rare, supervisor non-rare): 43\n",
      "\n",
      "--- CLASSIFICATION METRICS ---\n",
      "Precision: 0.9211\n",
      "Recall: 0.6195\n",
      "F1 Score: 0.7407\n",
      "\n",
      "--- DISAGREEMENT SUMMARY (EXCLUDING BRACKETED CONTEXTS) ---\n",
      "Original disagreements:\n",
      "  Human says rare, supervisor says not: 43 total\n",
      "  Supervisor says rare, human says not: 6 total\n",
      "\n",
      "Filtered disagreements (excluding bracketed contexts):\n",
      "  Human says rare, supervisor says not: 41 valid (2 excluded)\n",
      "  Supervisor says rare, human says not: 2 valid (4 excluded)\n",
      "  Unique entities in human rare disagreements: 28\n",
      "  Unique entities in supervisor rare disagreements: 1\n",
      "\n",
      "Top disagreements (human says rare, supervisor says not) - after filtering:\n",
      "  tracheobronchomalacia: 4 occurrences\n",
      "  portal vein thrombosis: 3 occurrences\n",
      "  hepatocellular carcinoma: 3 occurrences\n",
      "  lyme's disease: 2 occurrences\n",
      "  hyperthyroidism: 2 occurrences\n",
      "\n",
      "Top disagreements (supervisor says rare, human says not) - after filtering:\n",
      "  heparin induced thrombocytopenia: 2 occurrences\n",
      "\n",
      "Entities with contradictory classifications:\n",
      "\n",
      "  heparin induced thrombocytopenia:\n",
      "    Human classified as rare: 10 times\n",
      "    Human classified as not rare: 3 times\n",
      "    Appears in 13 documents\n",
      "\n",
      "  hepatocellular carcinoma:\n",
      "    Human classified as rare: 3 times\n",
      "    Human classified as not rare: 1 times\n",
      "    Appears in 4 documents\n",
      "\n",
      "  pulmonary arterial hypertension:\n",
      "    Human classified as rare: 2 times\n",
      "    Human classified as not rare: 2 times\n",
      "    Appears in 4 documents\n",
      "\n",
      "  hyperthyroidism:\n",
      "    Human classified as rare: 2 times\n",
      "    Human classified as not rare: 4 times\n",
      "    Appears in 6 documents\n",
      "\n",
      "  iron storage disease:\n",
      "    Human classified as rare: 2 times\n",
      "    Human classified as not rare: 3 times\n",
      "    Appears in 5 documents\n",
      "\n",
      "  polymyalgia rheumatica:\n",
      "    Human classified as rare: 1 times\n",
      "    Human classified as not rare: 2 times\n",
      "    Appears in 3 documents\n",
      "\n",
      "  sick sinus syndrome:\n",
      "    Human classified as rare: 1 times\n",
      "    Human classified as not rare: 4 times\n",
      "    Appears in 5 documents\n",
      "\n",
      "  legionella:\n",
      "    Human classified as rare: 1 times\n",
      "    Human classified as not rare: 4 times\n",
      "    Appears in 5 documents\n",
      "\n",
      "  thrombotic thrombocytopenic purpura:\n",
      "    Human classified as rare: 1 times\n",
      "    Human classified as not rare: 1 times\n",
      "    Appears in 2 documents\n",
      "\n",
      "  multifocal atrial tachycardia:\n",
      "    Supervisor classified as rare: 1 times\n",
      "    Supervisor classified as not rare: 1 times\n",
      "    Appears in 2 documents\n",
      "\n",
      "Specific entities of interest:\n",
      "\n",
      "  heparin induced thrombocytopenia:\n",
      "    Human classified as rare: 10 times\n",
      "    Human classified as not rare: 3 times\n",
      "    Supervisor classified as rare: 13 times\n",
      "    Supervisor classified as not rare: 0 times\n",
      "    Human rare documents: ['20208', '33909', '16334', '17640', '10715', '1552', '22297', '8979', '52501', '6465']\n",
      "    Supervisor rare documents: ['20208', '52528', '10406', '33909', '16334', '17640', '10715', '1552', '2541', '22297', '8979', '52501', '6465']\n",
      "\n",
      "  portal vein thrombosis:\n",
      "    Human classified as rare: 4 times\n",
      "    Human classified as not rare: 0 times\n",
      "    Supervisor classified as rare: 0 times\n",
      "    Supervisor classified as not rare: 4 times\n",
      "    Human rare documents: ['20208', '27335', '20387', '47035']\n",
      "    Supervisor rare documents: []\n",
      "\n",
      "  tracheobronchomalacia:\n",
      "    Human classified as rare: 4 times\n",
      "    Human classified as not rare: 0 times\n",
      "    Supervisor classified as rare: 0 times\n",
      "    Supervisor classified as not rare: 4 times\n",
      "    Human rare documents: ['46361', '1790', '35397', '34316']\n",
      "    Supervisor rare documents: []\n",
      "\n",
      "  rheumatic fever:\n",
      "    Human classified as rare: 3 times\n",
      "    Human classified as not rare: 0 times\n",
      "    Supervisor classified as rare: 3 times\n",
      "    Supervisor classified as not rare: 0 times\n",
      "    Human rare documents: ['20589', '4806', '977']\n",
      "    Supervisor rare documents: ['20589', '4806', '977']\n",
      "\n",
      "--- VALIDATION ---\n",
      "Assertion checks:\n",
      "human rare entities: 113\n",
      "supervisor rare entities: 76\n",
      "true positives: 70\n",
      "false negatives: 43\n",
      "false positives: 6\n",
      "✓ Human rare entities = TP + FN\n",
      "✓ Supervisor rare entities = TP + FP\n",
      "\n",
      "=== DETAILED DISAGREEMENT ANALYSIS (EXCLUDING BRACKETED CONTEXTS) ===\n",
      "\n",
      "--- HUMAN RARE, SUPERVISOR NOT RARE ---\n",
      "Total disagreements: 43\n",
      "Excluded (bracketed contexts): 2\n",
      "Valid disagreements: 41\n",
      "\n",
      "1. 'tracheobronchomalacia' (4x)\n",
      "   Doc 1790 | H: 'tracheobronchomalacia' | S: 'tracheobronchomalacia'\n",
      "   H-Context: \"his may be followed up with chest radiographs or a CT as per\n",
      "clinical need to assess resolution.\n",
      "3. ...\"\n",
      "   S-Context: \"osa - on bipap at home but has not been using it. 8. tracheobronchomalacia s/p tracheal bronchoplast...\"\n",
      "\n",
      "2. 'portal vein thrombosis' (3x)\n",
      "   Doc 20387 | H: 'portal vein thrombosis' | S: 'portal vein thrombosis'\n",
      "   H-Context: \"tal with hypoxemia, right pleural\n",
      "effusion, and mesenteric ischemia with mesenteric vein\n",
      "thrombosis/...\"\n",
      "   S-Context: \"from an outside hospital with hypoxemia, right pleural effusion, and mesenteric ischemia with mesent...\"\n",
      "\n",
      "3. 'hepatocellular carcinoma' (3x)\n",
      "   Doc 45138 | H: 'Hepatocellular Carcinoma' | S: 'Hepatocellular Carcinoma'\n",
      "   H-Context: \"tal6 85**] - [**Location (un) 86**]\n",
      "\n",
      "Discharge Diagnosis:\n",
      "Left Subdural Hematoma s/p fall\n",
      "Cirrhosis\n",
      "...\"\n",
      "   S-Context: \"left subdural hematoma s/p fall cirrhosis hepatocellular carcinoma chf thalassemia\"\n",
      "\n",
      "4. 'lyme's disease' (2x)\n",
      "   Doc 38775 | H: 'lyme disease' | S: 'lyme disease'\n",
      "   H-Context: \"orne diseases should also be\n",
      "considered given his possible exposure history (consider\n",
      "babesiosis or ...\"\n",
      "   S-Context: \"urinary tract infection, tick-borne diseases should also be considered given his possible exposure h...\"\n",
      "\n",
      "5. 'hyperthyroidism' (2x)\n",
      "   Doc 13231 | H: 'hyperthyroidism' | S: 'hyperthyroidism'\n",
      "   H-Context: \"w, pt had\n",
      "denied chest pain.\n",
      "\n",
      "Past Medical History:\n",
      "Epilepsy\n",
      "HTN\n",
      "DM\n",
      "CAD\n",
      "AFib\n",
      "CVA-?right side stroke\n",
      "...\"\n",
      "   S-Context: \"cva-? right side stroke hyperthyroidism asthma pe\"\n",
      "\n",
      "--- SUPERVISOR RARE, HUMAN NOT RARE ---\n",
      "Total disagreements: 6\n",
      "Excluded (bracketed contexts): 4\n",
      "Valid disagreements: 2\n",
      "\n",
      "1. 'heparin induced thrombocytopenia' (2x)\n",
      "   Doc 10406 | H: 'HIT' | S: 'HIT'\n",
      "   H-Context: \"ent had transient thrombocytopenia\n",
      "while in the ICU that resolved prior to transfer to the floor.\n",
      "A ...\"\n",
      "   S-Context: \"thrombocytopenia: patient had transient thrombocytopenia while in the icu that resolved prior to tra...\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load human and supervisor corrections\n",
    "human_corrections = read_json_file(\"data/dataset/rare_disease_corrections_john.json\")\n",
    "rdma_corrections = read_json_file(\"data/results/supervisor/multistage_no_min.json\")\n",
    "\n",
    "# Extract document-level entity sets with ORPHA code priority\n",
    "human_doc_entities, supervisor_doc_entities, entity_mapping, context_mapping = extract_document_entity_sets_with_orpha_priority_fixed(\n",
    "    human_corrections, \n",
    "    rdma_corrections,\n",
    "    similarity_threshold=90  # Using 90% as the clustering threshold for entities without ORPHA codes\n",
    ")\n",
    "\n",
    "# Compute agreement metrics\n",
    "metrics = compute_agreement_metrics(\n",
    "    human_doc_entities, \n",
    "    supervisor_doc_entities\n",
    ")\n",
    "\n",
    "# Analyze disagreements (now with context information)\n",
    "disagreements = analyze_disagreements(\n",
    "    human_doc_entities, \n",
    "    supervisor_doc_entities,\n",
    "    context_mapping\n",
    ")\n",
    "\n",
    "# Print report\n",
    "print_report(metrics, disagreements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa37e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb37ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sorted(supervisor_doc_entities.keys()))\n",
    "for doc_id, entities in human_doc_entities.items():\n",
    "    \n",
    "    if doc_id in supervisor_doc_entities:\n",
    "        if len(supervisor_doc_entities[doc_id]) != len(human_doc_entities[doc_id]):\n",
    "            print(f\"Document ID: {doc_id}\")\n",
    "            print(f\"Entities: {entities}\")\n",
    "            print(\"Supervisor Entities:\")\n",
    "            print(supervisor_doc_entities[doc_id])\n",
    "    else:\n",
    "        print(\"No supervisor entities found for this document.\")\n",
    "        print(human_doc_entities[doc_id])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5c0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from circuitsvis.tokens import colored_tokens\n",
    "\n",
    "class SimpleDisagreementVisualizer:\n",
    "    \"\"\"Simple visualizer for rare disease entity extraction disagreements.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def is_bracketed_context(self, context: str) -> bool:\n",
    "        \"\"\"Check if context is bracketed (starts with '[' and ends with ']').\"\"\"\n",
    "        if not context or not isinstance(context, str):\n",
    "            return False\n",
    "        stripped = context.strip()\n",
    "        return stripped.startswith('[') and stripped.endswith(']')\n",
    "    \n",
    "    def tokenize_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenization that preserves spaces and handles punctuation.\"\"\"\n",
    "        if not text or self.is_bracketed_context(text):\n",
    "            return []\n",
    "        \n",
    "        # Split text into tokens including words, punctuation, and spaces\n",
    "        # This handles cases like \"thrombosis/portal\" by keeping \"/\" as separate token\n",
    "        tokens = []\n",
    "        current_pos = 0\n",
    "        \n",
    "        # Find all word boundaries, punctuation, and spaces\n",
    "        import re\n",
    "        for match in re.finditer(r'\\S+', text):\n",
    "            start, end = match.span()\n",
    "            \n",
    "            # Add any spaces before this token\n",
    "            if start > current_pos:\n",
    "                space_text = text[current_pos:start]\n",
    "                if space_text:\n",
    "                    tokens.append(space_text)\n",
    "            \n",
    "            # Get the word/punctuation chunk\n",
    "            chunk = text[start:end]\n",
    "            \n",
    "            # Split chunk further if it contains punctuation mixed with words\n",
    "            # This handles \"thrombosis/portal\" -> [\"thrombosis\", \"/\", \"portal\"]\n",
    "            sub_tokens = re.findall(r'\\w+|[^\\w\\s]', chunk)\n",
    "            \n",
    "            for i, sub_token in enumerate(sub_tokens):\n",
    "                tokens.append(sub_token)\n",
    "                # Don't add space after the last sub-token of this chunk\n",
    "                if i < len(sub_tokens) - 1:\n",
    "                    # Add minimal space between sub-tokens within same chunk\n",
    "                    pass  # Let the natural spacing handle this\n",
    "            \n",
    "            current_pos = end\n",
    "        \n",
    "        # Add any trailing spaces\n",
    "        if current_pos < len(text):\n",
    "            trailing_space = text[current_pos:]\n",
    "            if trailing_space:\n",
    "                tokens.append(trailing_space)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def find_entity_positions(self, tokens: List[str], entity: str) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Find positions of entity in tokenized text, including partial matches within chunks.\"\"\"\n",
    "        if not tokens or not entity:\n",
    "            return []\n",
    "        \n",
    "        entity_words = entity.lower().split()\n",
    "        if not entity_words:\n",
    "            return []\n",
    "        \n",
    "        positions = []\n",
    "        \n",
    "        # Method 1: Find complete word sequence matches\n",
    "        word_tokens = []\n",
    "        token_indices = []\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.strip() and re.match(r'\\w', token):  # Only actual words, not spaces or pure punctuation\n",
    "                word_tokens.append(token.lower())\n",
    "                token_indices.append(i)\n",
    "        \n",
    "        # Find complete entity matches in word sequence\n",
    "        for i in range(len(word_tokens) - len(entity_words) + 1):\n",
    "            if word_tokens[i:i + len(entity_words)] == entity_words:\n",
    "                start_idx = token_indices[i]\n",
    "                if i + len(entity_words) - 1 < len(token_indices):\n",
    "                    # Include tokens up to and including the last entity word\n",
    "                    end_idx = token_indices[i + len(entity_words) - 1] + 1\n",
    "                else:\n",
    "                    end_idx = len(tokens)\n",
    "                positions.append((start_idx, end_idx))\n",
    "        \n",
    "        # Method 2: Find partial matches within individual tokens\n",
    "        # This handles cases like \"thrombosis/portal vein thrombosis\" where entity spans across punctuation\n",
    "        entity_text = entity.lower()\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.strip() and re.match(r'\\w', token):  # Only check word tokens\n",
    "                token_lower = token.lower()\n",
    "                \n",
    "                # Check if this token contains any part of our entity\n",
    "                if any(word in token_lower for word in entity_words):\n",
    "                    # Check if we can find the full entity starting from this position\n",
    "                    remaining_text = ' '.join([t for t in tokens[i:] if t.strip() and re.match(r'\\w', t)])\n",
    "                    \n",
    "                    if entity_text in remaining_text.lower():\n",
    "                        # Find how many tokens we need to include to get the full entity\n",
    "                        current_text = \"\"\n",
    "                        end_idx = i\n",
    "                        \n",
    "                        for j in range(i, len(tokens)):\n",
    "                            if tokens[j].strip() and re.match(r'\\w', tokens[j]):\n",
    "                                if current_text:\n",
    "                                    current_text += \" \"\n",
    "                                current_text += tokens[j].lower()\n",
    "                                \n",
    "                                if entity_text in current_text:\n",
    "                                    end_idx = j + 1\n",
    "                                    # Avoid duplicates by checking if we already found this position\n",
    "                                    if (i, end_idx) not in positions:\n",
    "                                        positions.append((i, end_idx))\n",
    "                                    break\n",
    "        \n",
    "        # Remove duplicate positions and sort\n",
    "        positions = list(set(positions))\n",
    "        positions.sort()\n",
    "        \n",
    "        return positions\n",
    "    \n",
    "    def create_highlight_scores(self, tokens: List[str], entity: str) -> List[float]:\n",
    "        \"\"\"Create highlight scores for tokens (1.0 for entity, 0.0 for others).\"\"\"\n",
    "        if not tokens:\n",
    "            return []\n",
    "        \n",
    "        scores = [0.0] * len(tokens)\n",
    "        positions = self.find_entity_positions(tokens, entity)\n",
    "        \n",
    "        for start_idx, end_idx in positions:\n",
    "            for i in range(start_idx, end_idx):\n",
    "                scores[i] = 1.0\n",
    "                \n",
    "        return scores\n",
    "    \n",
    "    def visualize_single_disagreement(self, disagreement: Dict, title: str = \"\"):\n",
    "        \"\"\"Visualize a single disagreement with both contexts highlighted.\"\"\"\n",
    "        entity = disagreement.get('entity', '')\n",
    "        human_context = disagreement.get('human_context', '')\n",
    "        supervisor_context = disagreement.get('supervisor_context', '')\n",
    "        doc_id = disagreement.get('document_id', 'Unknown')\n",
    "        \n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        if title:\n",
    "            print(f\"📊 {title}\")\n",
    "        print(f\"Entity: '{entity}'\")\n",
    "        print(f\"Document: {doc_id}\")\n",
    "        \n",
    "        # Skip if either context is bracketed\n",
    "        if self.is_bracketed_context(human_context) or self.is_bracketed_context(supervisor_context):\n",
    "            print(\"⚠️  SKIPPED: Bracketed context detected\")\n",
    "            return\n",
    "        \n",
    "        # Show human context\n",
    "        if human_context:\n",
    "            print(f\"\\n👤 Context:\")\n",
    "            tokens = self.tokenize_text(human_context)\n",
    "            if tokens:\n",
    "                scores = self.create_highlight_scores(tokens, entity)\n",
    "                colored_tokens(tokens, scores, positive_color=\"#ff6b6b\")\n",
    "    \n",
    "    def filter_valid_disagreements(self, disagreements_list: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Filter out disagreements with bracketed contexts.\"\"\"\n",
    "        return [d for d in disagreements_list \n",
    "                if not (self.is_bracketed_context(d.get('human_context', '')) or \n",
    "                       self.is_bracketed_context(d.get('supervisor_context', '')))]\n",
    "    \n",
    "    def visualize_category(self, disagreements: Dict, category: str, max_examples: int = 5):\n",
    "        \"\"\"Visualize disagreements for a specific category.\"\"\"\n",
    "        if category not in disagreements:\n",
    "            print(f\"Category '{category}' not found\")\n",
    "            return\n",
    "        \n",
    "        category_data = disagreements[category]\n",
    "        valid_disagreements = self.filter_valid_disagreements(category_data)\n",
    "        \n",
    "        # Category titles\n",
    "        titles = {\n",
    "            'human_rare_supervisor_not': '👤 SAYS RARE, 🤖 SAYS NOT RARE',\n",
    "            'supervisor_rare_human_not': '🤖 SAYS RARE, 👤 SAYS NOT RARE'\n",
    "        }\n",
    "        \n",
    "        title = titles.get(category, category.upper())\n",
    "        \n",
    "        print(f\"\\n{'='*20} {title} {'='*20}\")\n",
    "        print(f\"Total: {len(category_data)} | Valid: {len(valid_disagreements)} | Showing: {min(max_examples, len(valid_disagreements))}\")\n",
    "        \n",
    "        if not valid_disagreements:\n",
    "            print(\"No valid disagreements to show.\")\n",
    "            return\n",
    "        \n",
    "        # Show examples\n",
    "        for i, disagreement in enumerate(valid_disagreements[:max_examples]):\n",
    "            self.visualize_single_disagreement(\n",
    "                disagreement, \n",
    "                f\"Example {i+1}/{min(max_examples, len(valid_disagreements))}\"\n",
    "            )\n",
    "    \n",
    "    def visualize_all(self, disagreements: Dict, max_examples_per_category: int = 3):\n",
    "        \"\"\"Visualize both disagreement categories.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🔍 DISAGREEMENT VISUALIZATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Human rare, supervisor not rare\n",
    "        self.visualize_category(disagreements, 'human_rare_supervisor_not', max_examples_per_category)\n",
    "        \n",
    "        # Supervisor rare, human not rare  \n",
    "        self.visualize_category(disagreements, 'supervisor_rare_human_not', max_examples_per_category)\n",
    "    \n",
    "    def save_to_html(self, disagreements: Dict, max_examples: int = 10, output_dir: str = \"figs\", \n",
    "                     show_human: bool = True):\n",
    "        \"\"\"Save disagreement visualizations to HTML files using CircuitsVis.\"\"\"\n",
    "        # Create output directory\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save each category to separate HTML files\n",
    "        categories = {\n",
    "            'human_rare_supervisor_not': {\n",
    "                'title': '👤 Says Rare, 🤖 Says Not Rare',\n",
    "                'filename_human': 'human_rare_supervisor_not_human_context.html',\n",
    "                'filename_supervisor': 'human_rare_supervisor_not_supervisor_context.html'\n",
    "            },\n",
    "            'supervisor_rare_human_not': {\n",
    "                'title': '🤖 Says Rare, 👤 Says Not Rare', \n",
    "                'filename_human': 'supervisor_rare_human_not_human_context.html',\n",
    "                'filename_supervisor': 'supervisor_rare_human_not_supervisor_context.html'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for category, info in categories.items():\n",
    "            if category in disagreements:\n",
    "                # Save human context version\n",
    "                html_file_human = os.path.join(output_dir, info['filename_human'])\n",
    "                self._create_html_file(\n",
    "                    disagreements[category], \n",
    "                    info['title'] + \" (👤 Context)\",\n",
    "                    html_file_human,\n",
    "                    max_examples,\n",
    "                    show_context='human'\n",
    "                )\n",
    "                print(f\"Saved {category} (human context) to: {html_file_human}\")\n",
    "                \n",
    "                # Save supervisor context version\n",
    "                html_file_supervisor = os.path.join(output_dir, info['filename_supervisor'])\n",
    "                self._create_html_file(\n",
    "                    disagreements[category], \n",
    "                    info['title'] + \" (🤖 Context)\",\n",
    "                    html_file_supervisor,\n",
    "                    max_examples,\n",
    "                    show_context='supervisor'\n",
    "                )\n",
    "                print(f\"Saved {category} (supervisor context) to: {html_file_supervisor}\")\n",
    "    \n",
    "    def _create_html_file(self, category_data: List[Dict], title: str, \n",
    "                         output_file: str, max_examples: int, show_context: str = 'human'):\n",
    "        \"\"\"Create simple HTML file showing just highlighted tokens using table layout.\"\"\"\n",
    "        # Filter valid disagreements\n",
    "        valid_disagreements = self.filter_valid_disagreements(category_data)\n",
    "        \n",
    "        if not valid_disagreements:\n",
    "            html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html><head><title>{title}</title></head>\n",
    "<body><h1>{title}</h1><p>No valid disagreements found.</p></body></html>\"\"\"\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(html_content)\n",
    "            return\n",
    "        \n",
    "        # Group by entity and take top examples\n",
    "        entity_groups = {}\n",
    "        for item in valid_disagreements:\n",
    "            entity = item['entity']\n",
    "            if entity not in entity_groups:\n",
    "                entity_groups[entity] = []\n",
    "            entity_groups[entity].append(item)\n",
    "        \n",
    "        sorted_entities = sorted(entity_groups.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "        \n",
    "        # Create HTML with table-based layout to prevent CSS cascading\n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>{title}</title>\n",
    "    <style>\n",
    "        body {{ \n",
    "            font-family: Arial, sans-serif; \n",
    "            margin: 15px; \n",
    "            padding: 0;\n",
    "        }}\n",
    "        \n",
    "        .disagreement-table {{\n",
    "            width: 100%;\n",
    "            border-collapse: separate;\n",
    "            border-spacing: 0;\n",
    "            margin: 8px 0;  /* Reduced from 20px */\n",
    "        }}\n",
    "        \n",
    "        .disagreement-row {{\n",
    "            border: 1px solid #ddd;  /* Thinner border */\n",
    "        }}\n",
    "        \n",
    "        .entity-cell {{\n",
    "            background-color: #f5f5f5;\n",
    "            padding: 8px 12px;  /* Reduced from 15px */\n",
    "            border-bottom: 1px solid #ddd;\n",
    "            font-weight: bold;\n",
    "            font-size: 15px;  /* Slightly smaller */\n",
    "        }}\n",
    "        \n",
    "        .context-cell {{\n",
    "            padding: 10px 12px;  /* Reduced from 15px */\n",
    "            background-color: white;\n",
    "            vertical-align: top;\n",
    "        }}\n",
    "        \n",
    "        .context-label {{\n",
    "            font-weight: bold;\n",
    "            margin-bottom: 5px;  /* Reduced from 8px */\n",
    "            color: #555;\n",
    "            font-size: 14px;\n",
    "        }}\n",
    "        \n",
    "        .tokens-wrapper {{\n",
    "            background-color: #f9f9f9;\n",
    "            padding: 6px 8px;  /* Reduced from 10px */\n",
    "            border-radius: 3px;\n",
    "            border: 1px solid #e0e0e0;\n",
    "            margin: 2px 0;  /* Reduced from 5px */\n",
    "            line-height: 1.3;  /* Tighter line spacing */\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1 style=\"margin-bottom: 15px;\">{title}</h1>\n",
    "\"\"\"\n",
    "        \n",
    "        # Generate visualizations and create table rows\n",
    "        for i, (entity, examples) in enumerate(sorted_entities[:max_examples]):\n",
    "            example = examples[0]\n",
    "            human_context = example.get('human_context', '')\n",
    "            supervisor_context = example.get('supervisor_context', '')\n",
    "            human_original = example.get('human_original_entity', entity)\n",
    "            supervisor_original = example.get('supervisor_original_entity', entity)\n",
    "            doc_id = example.get('document_id', 'Unknown')\n",
    "            \n",
    "            # Generate appropriate context visualization based on show_context parameter\n",
    "            context_viz_html = \"\"\n",
    "            context_icon = \"\"\n",
    "            original_entity = entity\n",
    "            \n",
    "            if show_context == 'human':\n",
    "                context_icon = \"👤\"\n",
    "                original_entity = human_original\n",
    "                if human_context and not self.is_bracketed_context(human_context):\n",
    "                    tokens = self.tokenize_text(human_context)\n",
    "                    if tokens:\n",
    "                        scores = self.create_highlight_scores(tokens, human_original)\n",
    "                        context_viz = colored_tokens(\n",
    "                            tokens, scores, \n",
    "                            min_value=0.0, max_value=1.0,\n",
    "                            positive_color=\"#ff6b6b\"\n",
    "                        )\n",
    "                        context_viz_html = context_viz._repr_html_()\n",
    "            else:  # supervisor\n",
    "                context_icon = \"🤖\"\n",
    "                original_entity = supervisor_original\n",
    "                if supervisor_context and not self.is_bracketed_context(supervisor_context):\n",
    "                    tokens = self.tokenize_text(supervisor_context)\n",
    "                    if tokens:\n",
    "                        scores = self.create_highlight_scores(tokens, supervisor_original)\n",
    "                        context_viz = colored_tokens(\n",
    "                            tokens, scores, \n",
    "                            min_value=0.0, max_value=1.0,\n",
    "                            positive_color=\"#ff6b6b\"\n",
    "                        )\n",
    "                        context_viz_html = context_viz._repr_html_()\n",
    "            \n",
    "            # Create table row for this disagreement\n",
    "            html_content += f\"\"\"\n",
    "<table class=\"disagreement-table\">\n",
    "    <tr class=\"disagreement-row\">\n",
    "        <td class=\"entity-cell\">\n",
    "            #{i+1}: \"{entity}\" ({len(examples)} occurrences) - Document: {doc_id}\n",
    "        </td>\n",
    "    </tr>\n",
    "\"\"\"\n",
    "            \n",
    "            if context_viz_html:\n",
    "                html_content += f\"\"\"\n",
    "    <tr class=\"disagreement-row\">\n",
    "        <td class=\"context-cell\">\n",
    "            <div class=\"context-label\">{context_icon} Context:</div>\n",
    "            <div class=\"tokens-wrapper\">{context_viz_html}</div>\n",
    "        </td>\n",
    "    </tr>\n",
    "\"\"\"\n",
    "            else:\n",
    "                # Show message if no valid context\n",
    "                html_content += f\"\"\"\n",
    "    <tr class=\"disagreement-row\">\n",
    "        <td class=\"context-cell\">\n",
    "            <div class=\"context-label\">{context_icon} Context:</div>\n",
    "            <div class=\"tokens-wrapper\" style=\"color: #999; font-style: italic;\">No valid context available</div>\n",
    "        </td>\n",
    "    </tr>\n",
    "\"\"\"\n",
    "            \n",
    "            html_content += \"</table>\\n\\n\"\n",
    "        \n",
    "        html_content += \"</body></html>\"\n",
    "        \n",
    "        # Write to file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "\n",
    "# Simple usage functions\n",
    "def visualize_disagreements(disagreements_dict: Dict, max_examples: int = 5, save_html: bool = False):\n",
    "    \"\"\"\n",
    "    Main function to visualize disagreements.\n",
    "    \n",
    "    Args:\n",
    "        disagreements_dict: Dictionary with 'human_rare_supervisor_not' and 'supervisor_rare_human_not' keys\n",
    "        max_examples: Maximum examples to show per category\n",
    "        save_html: Whether to save HTML files to figs/ directory\n",
    "    \"\"\"\n",
    "    visualizer = SimpleDisagreementVisualizer()\n",
    "    \n",
    "    # Console visualization\n",
    "    visualizer.visualize_all(disagreements_dict, max_examples)\n",
    "    \n",
    "    # Save HTML files if requested\n",
    "    if save_html:\n",
    "        print(f\"\\n💾 Saving HTML files...\")\n",
    "        visualizer.save_to_html(disagreements_dict, max_examples * 2, \"figs\")\n",
    "        print(f\"HTML files saved to figs/ directory\")\n",
    "\n",
    "def save_disagreements_html(disagreements_dict: Dict, max_examples: int = 10, output_dir: str = \"figs\"):\n",
    "    \"\"\"\n",
    "    Save disagreement visualizations to HTML files.\n",
    "    \n",
    "    Args:\n",
    "        disagreements_dict: Dictionary with disagreement data\n",
    "        max_examples: Maximum examples per category\n",
    "        output_dir: Output directory (default: \"figs\")\n",
    "    \"\"\"\n",
    "    visualizer = SimpleDisagreementVisualizer()\n",
    "    visualizer.save_to_html(disagreements_dict, max_examples, output_dir)\n",
    "    \n",
    "    print(f\"✅ HTML files created:\")\n",
    "    print(f\"   👤 Human context versions:\")\n",
    "    print(f\"     - {output_dir}/human_rare_supervisor_not_human_context.html\")\n",
    "    print(f\"     - {output_dir}/supervisor_rare_human_not_human_context.html\")\n",
    "    print(f\"   🤖 Supervisor context versions:\")\n",
    "    print(f\"     - {output_dir}/human_rare_supervisor_not_supervisor_context.html\")\n",
    "    print(f\"     - {output_dir}/supervisor_rare_human_not_supervisor_context.html\")\n",
    "\n",
    "# Test with sample data\n",
    "def test_visualization():\n",
    "    \"\"\"Test the visualizer with sample data.\"\"\"\n",
    "    sample_data = {\n",
    "        'human_rare_supervisor_not': [\n",
    "            {\n",
    "                'entity': 'diabetes',\n",
    "                'human_context': 'Patient diagnosed with diabetes mellitus type 1 last year',\n",
    "                'supervisor_context': 'History of diabetes noted in chart',\n",
    "                'document_id': 'DOC001'\n",
    "            }\n",
    "        ],\n",
    "        'supervisor_rare_human_not': [\n",
    "            {\n",
    "                'entity': 'hypertension', \n",
    "                'human_context': 'Blood pressure consistently elevated above normal range',\n",
    "                'supervisor_context': 'Patient has essential hypertension requiring medication',\n",
    "                'document_id': 'DOC002'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    visualize_disagreements(sample_data, max_examples=2)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef1a2ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🔍 DISAGREEMENT VISUALIZATION\n",
      "================================================================================\n",
      "\n",
      "==================== 👤 SAYS RARE, 🤖 SAYS NOT RARE ====================\n",
      "Total: 43 | Valid: 41 | Showing: 2\n",
      "\n",
      "------------------------------------------------------------\n",
      "📊 Example 1/2\n",
      "Entity: 'methemoglobinemia'\n",
      "Document: 14936\n",
      "\n",
      "👤 Context:\n",
      "\n",
      "------------------------------------------------------------\n",
      "📊 Example 2/2\n",
      "Entity: 'hypogammaglobulinemia'\n",
      "Document: 33893\n",
      "\n",
      "👤 Context:\n",
      "\n",
      "==================== 🤖 SAYS RARE, 👤 SAYS NOT RARE ====================\n",
      "Total: 6 | Valid: 2 | Showing: 2\n",
      "\n",
      "------------------------------------------------------------\n",
      "📊 Example 1/2\n",
      "Entity: 'heparin induced thrombocytopenia'\n",
      "Document: 10406\n",
      "\n",
      "👤 Context:\n",
      "\n",
      "------------------------------------------------------------\n",
      "📊 Example 2/2\n",
      "Entity: 'heparin induced thrombocytopenia'\n",
      "Document: 52528\n",
      "\n",
      "👤 Context:\n",
      "\n",
      "💾 Saving HTML files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved human_rare_supervisor_not (human context) to: figs/human_rare_supervisor_not_human_context.html\n",
      "Saved human_rare_supervisor_not (supervisor context) to: figs/human_rare_supervisor_not_supervisor_context.html\n",
      "Saved supervisor_rare_human_not (human context) to: figs/supervisor_rare_human_not_human_context.html\n",
      "Saved supervisor_rare_human_not (supervisor context) to: figs/supervisor_rare_human_not_supervisor_context.html\n",
      "HTML files saved to figs/ directory\n"
     ]
    }
   ],
   "source": [
    "visualize_disagreements(disagreements, max_examples=2, save_html=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f3ed6",
   "metadata": {},
   "source": [
    "# All Human vs. RDMA + Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d5db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb5ac2ac",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hporag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
