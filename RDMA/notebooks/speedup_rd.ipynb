{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "# Set the parent directory as the current directory\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset \n",
    "from utils.data import read_json_file, print_json_structure\n",
    "\n",
    "dataset_path = \"data/dataset/filtered_rd_annos_updated_adam.json\"\n",
    "dataset = read_json_file(dataset_path)\n",
    "print_json_structure(dataset)\n",
    "text = dataset[\"287\"][\"note_details\"][\"text\"]\n",
    "# print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mistral_llm_client():\n",
    "    \"\"\"\n",
    "    Load a Mistral 24B LLM client configured with default cache directories\n",
    "    and assigned to cuda:0 device.\n",
    "    \n",
    "    Returns:\n",
    "        LocalLLMClient: Initialized LLM client for Mistral 24B\n",
    "    \"\"\"\n",
    "    from utils.llm_client import LocalLLMClient\n",
    "    \n",
    "    # Default cache directory from mine_hpo.py\n",
    "    default_cache_dir = \"/u/zelalae2/scratch/rdma_cache\"\n",
    "    \n",
    "    # Initialize and return the client with specific configuration\n",
    "    llm_client = LocalLLMClient(\n",
    "        model_type=\"mistral_24b\",  # Explicitly request mistral_24b model\n",
    "        device=\"cuda:0\",           # Assign to first GPU (cuda:0)\n",
    "        cache_dir=default_cache_dir,\n",
    "        temperature=0.1           # Default temperature from mine_hpo.py\n",
    "    )\n",
    "    \n",
    "    return llm_client\n",
    "\n",
    "llm_client = load_mistral_llm_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from fuzzywuzzy import fuzz\n",
    "from hporag.context import ContextExtractor\n",
    "from rdrag.entity import BaseRDExtractor\n",
    "\n",
    "class RetrievalEnhancedRDExtractor(BaseRDExtractor):\n",
    "    \"\"\"\n",
    "    Entity extractor that uses embedding retrieval to enhance LLM-based extraction.\n",
    "    \n",
    "    For each sentence:\n",
    "    1. Retrieves relevant rare disease terms to provide context\n",
    "    2. Uses this context to prompt the LLM for more accurate disease extraction\n",
    "    \n",
    "    This approach helps the LLM with domain knowledge before extraction begins.\n",
    "    \n",
    "    Features:\n",
    "    - Optionally merges small sentences to ensure minimum chunk size for processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, embedding_manager, embedded_documents, \n",
    "                 system_message: str, top_k: int = 10, min_sentence_size: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize the retrieval-enhanced rare disease extractor.\n",
    "        \n",
    "        Args:\n",
    "            llm_client: Client for querying the language model\n",
    "            embedding_manager: Manager for vector embeddings and search\n",
    "            embedded_documents: Rare disease terms with embeddings\n",
    "            system_message: System message for LLM extraction\n",
    "            top_k: Number of top candidates to retrieve per sentence\n",
    "            min_sentence_size: Minimum character length for sentences (smaller ones will be merged)\n",
    "        \"\"\"\n",
    "        self.llm_client = llm_client\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.embedded_documents = embedded_documents\n",
    "        self.index = None\n",
    "        self.system_message = system_message\n",
    "        self.top_k = top_k\n",
    "        self.min_sentence_size = min_sentence_size\n",
    "        self.context_extractor = ContextExtractor()\n",
    "        \n",
    "    def prepare_index(self):\n",
    "        \"\"\"Prepare FAISS index from embedded documents if not already prepared.\"\"\"\n",
    "        if self.index is None:\n",
    "            embeddings_array = self.embedding_manager.prepare_embeddings(self.embedded_documents)\n",
    "            self.index = self.embedding_manager.create_index(embeddings_array)\n",
    "    \n",
    "    def _retrieve_candidates(self, sentence: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant rare disease candidates for a sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Clinical sentence to find relevant rare disease terms for\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with rare disease information and similarity scores\n",
    "        \"\"\"\n",
    "        self.prepare_index()\n",
    "        \n",
    "        # Embed the query\n",
    "        query_vector = self.embedding_manager.query_text(sentence).reshape(1, -1)\n",
    "        \n",
    "        # Search for similar items\n",
    "        distances, indices = self.embedding_manager.search(query_vector, self.index, k=min(800, len(self.embedded_documents)))\n",
    "        \n",
    "        # Extract unique metadata\n",
    "        candidates = []\n",
    "        seen_metadata = set()\n",
    "        \n",
    "        for idx, distance in zip(indices[0], distances[0]):\n",
    "            try:\n",
    "                # Access document directly since it already has name, id, definition\n",
    "                document = self.embedded_documents[idx]\n",
    "                \n",
    "                # Create an identifier for deduplication\n",
    "                metadata_id = f\"{document.get('name', '')}-{document.get('id', '')}\"\n",
    "                \n",
    "                if metadata_id not in seen_metadata:\n",
    "                    seen_metadata.add(metadata_id)\n",
    "                    candidates.append({\n",
    "                        'name': document.get('name', ''),\n",
    "                        'id': document.get('id', ''),\n",
    "                        'definition': document.get('definition', ''),\n",
    "                        'similarity_score': 1.0 / (1.0 + distance)  # Convert distance to similarity\n",
    "                    })\n",
    "                    \n",
    "                    if len(candidates) >= self.top_k:\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing metadata at index {idx}: {e}\")\n",
    "                continue\n",
    "                    \n",
    "        return candidates\n",
    "    def _create_enhanced_prompt(self, sentence: str, candidates: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Create a prompt enhanced with retrieved candidates.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Clinical sentence to extract rare diseases from\n",
    "            candidates: Retrieved rare disease candidates to use as context\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt for LLM\n",
    "        \"\"\"\n",
    "        # Format candidates as context\n",
    "        context_items = []\n",
    "        for candidate in candidates:\n",
    "            context_items.append(f\"- {candidate['name']} (ID: {candidate['id']})\")\n",
    "        \n",
    "        context_text = \"\\n\".join(context_items)\n",
    "        \n",
    "        # Create the enhanced prompt\n",
    "        prompt = (\n",
    "            f\"I have CLINICAL TEXT: \\\"{sentence}\\\"\\n\\n\"\n",
    "            f\"Here are some relevant rare disease terms for reference that may help you find rare disease mentions in the sentence:\\n\\n\"\n",
    "            f\"{context_text}\\n\\n\"\n",
    "            f\"Based on this sentence and the provided rare disease terms as reference, extract all potential disease mentions \"\n",
    "            f\"that are NOT negated (i.e., NOT preceded by 'no', 'not', 'without', 'ruled out', etc.). \"\n",
    "            f\"Please also include any potential abbreviations that might be referring to rare diseases in the CLINICAL TEXT.\"\n",
    "            f\"\\n\\nReturn only a Python list of strings, with each disease exactly as it appears in the CLINICAL TEXT. \"\n",
    "            f\"Ensure the output is concise without any additional notes, commentary, or meta explanations.\"\n",
    "        )\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def _merge_small_sentences(self, sentences: List[str], min_size: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Merge sentences smaller than the minimum size with subsequent sentences.\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of extracted sentences\n",
    "            min_size: Minimum character length for a sentence\n",
    "            \n",
    "        Returns:\n",
    "            List of merged sentences meeting the minimum size requirement\n",
    "        \"\"\"\n",
    "        if not sentences:\n",
    "            return []\n",
    "        \n",
    "        if min_size is None or min_size <= 0:\n",
    "            return sentences\n",
    "            \n",
    "        merged_sentences = []\n",
    "        current_idx = 0\n",
    "        \n",
    "        while current_idx < len(sentences):\n",
    "            current_sentence = sentences[current_idx]\n",
    "            \n",
    "            # If the current sentence is already large enough, add it directly\n",
    "            if len(current_sentence) >= min_size:\n",
    "                merged_sentences.append(current_sentence)\n",
    "                current_idx += 1\n",
    "                continue\n",
    "            \n",
    "            # Start merging with next sentences until we reach min_size\n",
    "            merged_chunk = current_sentence\n",
    "            next_idx = current_idx + 1\n",
    "            \n",
    "            while next_idx < len(sentences) and len(merged_chunk) < min_size:\n",
    "                # Add the next sentence to our chunk with a space\n",
    "                if merged_chunk and sentences[next_idx]:\n",
    "                    merged_chunk += \" \" + sentences[next_idx]\n",
    "                else:\n",
    "                    merged_chunk += sentences[next_idx]\n",
    "                next_idx += 1\n",
    "            \n",
    "            # Add the merged chunk to our results\n",
    "            merged_sentences.append(merged_chunk)\n",
    "            \n",
    "            # Update the index to continue after the merged sentences\n",
    "            current_idx = next_idx\n",
    "        \n",
    "        return merged_sentences\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract rare disease mentions from text using retrieval-enhanced prompting.\n",
    "        With sentence merging for efficiency when min_sentence_size is set.\n",
    "        \n",
    "        Args:\n",
    "            text: Clinical text to extract rare disease mentions from\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted rare disease mentions\n",
    "        \"\"\"\n",
    "        # Split text into sentences\n",
    "        original_sentences = self.context_extractor.extract_sentences(text)\n",
    "        \n",
    "        # Merge small sentences if min_sentence_size is set\n",
    "        if self.min_sentence_size:\n",
    "            sentences = self._merge_small_sentences(original_sentences, self.min_sentence_size)\n",
    "            print(f\"After merging: Processing {len(sentences)} chunks instead of {len(original_sentences)} raw sentences\")\n",
    "        else:\n",
    "            sentences = original_sentences\n",
    "        \n",
    "        all_entities = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Skip empty or very short sentences\n",
    "            if not sentence or len(sentence) < 5:\n",
    "                continue\n",
    "                \n",
    "            # Retrieve candidates for this sentence/chunk\n",
    "            candidates = self._retrieve_candidates(sentence)\n",
    "            \n",
    "            # Create enhanced prompt\n",
    "            prompt = self._create_enhanced_prompt(sentence, candidates)\n",
    "            \n",
    "            # Query LLM\n",
    "            findings_text = self.llm_client.query(prompt, self.system_message)\n",
    "            # Extract entities from response\n",
    "\n",
    "            print(\"--------- DEBUG ---------\")\n",
    "            print(prompt)\n",
    "            print()\n",
    "            print(findings_text)\n",
    "            print(\"------------------------\")\n",
    "            entities = self._extract_findings_from_response(findings_text)\n",
    "            # Add to results\n",
    "            all_entities.extend(entities)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_entities = []\n",
    "        seen = set()\n",
    "        for entity in all_entities:\n",
    "            entity_lower = entity.lower()\n",
    "            if entity_lower not in seen and entity:\n",
    "                seen.add(entity_lower)\n",
    "                unique_entities.append(entity)\n",
    "        \n",
    "        return unique_entities\n",
    "    \n",
    "    def _extract_findings_from_response(self, response: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Parse LLM response to extract findings.\n",
    "        \n",
    "        Args:\n",
    "            response: Raw LLM response text\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted entities\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract content between square brackets if present\n",
    "            if '[' in response and ']' in response:\n",
    "                response = response[response.find('[') + 1:response.rfind(']')]\n",
    "            \n",
    "            # Split on commas and clean up each term\n",
    "            findings = []\n",
    "            for term in response.split(','):\n",
    "                cleaned_term = term.strip().strip('\"\\'')\n",
    "                if cleaned_term:\n",
    "                    findings.append(cleaned_term)\n",
    "                    \n",
    "            return findings\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing LLM response: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def process_batch(self, texts: List[str]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Process a batch of texts for rare disease extraction.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of clinical texts to process\n",
    "            \n",
    "        Returns:\n",
    "            List of lists containing extracted rare disease mentions for each text\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            entities = self.extract_entities(text)\n",
    "            results.append(entities)\n",
    "        return results\n",
    "    \n",
    "from utils.embedding import EmbeddingsManager\n",
    "embedding_manager = EmbeddingsManager(\n",
    "    model_type=\"sentence_transformer\",\n",
    "    model_name=\"abhinand/MedEmbed-small-v0.1\",  # Medical-domain sentence transformer model\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "embedded_documents = np.load('/home/johnwu3/projects/rare_disease/workspace/repos/RDMA/data/vector_stores/rd_orpha_medembed.npy', allow_pickle=True)\n",
    "extractor = RetrievalEnhancedRDExtractor(llm_client, \n",
    "                                         embedding_manager, \n",
    "                                         embedded_documents, \n",
    "                                         system_message=\"You are a rare disease expert. Please extract disease mentions from the text.\", \n",
    "                                         min_sentence_size=500,\n",
    "                                         top_k=10)\n",
    "extracted = extractor.extract_entities(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = ['congestive heart failure',\n",
    " \"Parkinson's\",\n",
    " 'cardiogenic shock',\n",
    " 'atrial fibrillation',\n",
    " 'solar urticaria',\n",
    " 'insufficiency',\n",
    " 'mitral regurgitation',\n",
    " 'mib e3 ubiquitin protein ligase 2',\n",
    " 'Hypercholesterolemia',\n",
    " 'Peripheral vascular disease',\n",
    " 'peripheral neuropathy',\n",
    " 'glaucoma',\n",
    " \"Parkinson's disease\",\n",
    " 'Osteoarthritis',\n",
    " 'NPH insulin',\n",
    " 'epilepsy and/or ataxia with myoclonus as a major feature',\n",
    " 'diffuse cerebral and cerebellar atrophy-intractable seizures-progressive microcephaly syndrome',\n",
    " 'infantile cerebral and cerebellar atrophy with postnatal progressive microcephaly',\n",
    " 'hypomyelination with atrophy of basal ganglia and cerebellum',\n",
    " 'porencephaly',\n",
    " 'progressive myoclonic epilepsy type 9',\n",
    " 'focal epilepsy-intellectual disability-cerebro-cerebellar malformation',\n",
    " 'neuronal tumor',\n",
    " 'spastic paraplegia 29',\n",
    " 'phosphatase 80',\n",
    " 'albumin',\n",
    " 'serum',\n",
    " 'spastic paraplegia 36',\n",
    " 'Type 2 diabetes mellitus',\n",
    " 'sick sinus syndrome',\n",
    " 'cardiomegaly',\n",
    " 'hypertrophy',\n",
    " 'restrictive lung disease',\n",
    " 'multifocal opacities',\n",
    " 'adenoma',\n",
    " 'pneumonia',\n",
    " 'insulin']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hporag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
