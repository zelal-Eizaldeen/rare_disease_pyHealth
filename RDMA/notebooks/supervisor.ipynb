{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "# Set the parent directory as the current directory\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_flagged_entities(json_file):\n",
    "    \"\"\"Extract flagged entities with context from supervisor output JSON file.\"\"\"\n",
    "    \n",
    "    # Load the JSON file\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Get the flagged entities from the summary\n",
    "    flagged_entities = data.get('summary', {}).get('flagged_entities', [])\n",
    "    print(f\"Found {len(flagged_entities)} flagged entities\")\n",
    "    \n",
    "    # Prepare detailed data for review\n",
    "    detailed_entities = []\n",
    "    \n",
    "    # Process each flagged entity\n",
    "    for entity in flagged_entities:\n",
    "        entity_info = {\n",
    "            'entity': entity.get('entity', ''),\n",
    "            'document_id': entity.get('document_id', ''),\n",
    "            'orpha_code': entity.get('orpha_code', ''),\n",
    "            'category': entity.get('category', ''),\n",
    "            'explanation': entity.get('explanation', '')\n",
    "        }\n",
    "        \n",
    "        # Get detailed information from the results section\n",
    "        category = entity.get('category', '')\n",
    "        doc_id = entity.get('document_id', '')\n",
    "        \n",
    "        if doc_id and category and category in data.get('results', {}):\n",
    "            # Find the detailed entity data\n",
    "            for result in data['results'][category]:\n",
    "                if (result.get('document_id') == doc_id and \n",
    "                    result.get('entity') == entity_info['entity']):\n",
    "                    # Add detailed info\n",
    "                    entity_info['context'] = result.get('context', '')\n",
    "                    entity_info['is_rare_disease'] = result.get('is_rare_disease', False)\n",
    "                    \n",
    "                    # Add top candidate\n",
    "                    candidates = result.get('orpha_candidates', [])\n",
    "                    if candidates:\n",
    "                        top_candidate = candidates[0]\n",
    "                        entity_info['top_candidate_name'] = top_candidate.get('name', '')\n",
    "                        entity_info['top_candidate_id'] = top_candidate.get('id', '')\n",
    "                        entity_info['top_candidate_similarity'] = top_candidate.get('similarity', 0.0)\n",
    "                    \n",
    "                    break\n",
    "        \n",
    "        detailed_entities.append(entity_info)\n",
    "    \n",
    "    return detailed_entities\n",
    "\n",
    "def main():\n",
    "    # parser = argparse.ArgumentParser(description=\"Extract flagged entities for review\")\n",
    "    # parser.add_argument(\"json_file\", help=\"Path to supervisor output JSON file\")\n",
    "    # parser.add_argument(\"--output\", help=\"Output CSV file for review (optional)\")\n",
    "    # parser.add_argument(\"--category\", choices=[\"false_positives\", \"false_negatives\", \"true_positives\"],\n",
    "    #                   help=\"Filter by category (optional)\")\n",
    "    \n",
    "    # args = parser.parse_args()\n",
    "    \n",
    "    # Extract entities\n",
    "    json_file = \"data/results/supervisor/multistage_no_min.json\"\n",
    "    category = \"false_positives\"\n",
    "    entities = extract_flagged_entities(json_file)\n",
    "    print(entities[0])\n",
    "    # Filter by category if requested\n",
    "    if category:\n",
    "        entities = [e for e in entities if e.get('category') == category]\n",
    "        print(f\"Filtered to {len(entities)} {category}\")\n",
    "    \n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame(entities)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new set of annotations frome existing ones and human ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "from typing import Dict, List, Any, Set, Tuple\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class AnnotationCorrector:\n",
    "    \"\"\"\n",
    "    A class to handle the correction and enhancement of clinical annotations,\n",
    "    including removing inappropriate annotations and adding valid ones.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 existing_annotations: Dict[str, Any], \n",
    "                 corrections: Dict[str, Any],\n",
    "                 debug: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the AnnotationCorrector with existing annotations and corrections.\n",
    "        \n",
    "        Args:\n",
    "            existing_annotations: Original annotations dictionary\n",
    "            corrections: Corrections dictionary from verification\n",
    "            debug: Enable debug output\n",
    "        \"\"\"\n",
    "        self.existing_annotations = copy.deepcopy(existing_annotations)\n",
    "        self.corrections = corrections\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Initialize statistics tracking\n",
    "        self.stats = {\n",
    "            'total_corrections': 0,\n",
    "            'added_annotations': 0,\n",
    "            'updated_annotations': 0,\n",
    "            'removed_annotations': 0,\n",
    "            'skipped_corrections': 0,\n",
    "            'documents_modified': set()\n",
    "        }\n",
    "        \n",
    "        # Track entities to be removed\n",
    "        self.entities_to_remove = {}  # {document_id: [entity1, entity2, ...]}\n",
    "\n",
    "    def _identify_removals(self) -> None:\n",
    "        \"\"\"\n",
    "        Identify annotations that should be removed based on correction data.\n",
    "        These are entities marked as false negatives but verified not to be rare diseases.\n",
    "        \"\"\"\n",
    "        # Process all corrections to identify entries to remove\n",
    "        for correction in self.corrections.get('corrected_annotations', []):\n",
    "            document_id = str(correction.get('document_id', ''))\n",
    "            entity = correction.get('entity', '').lower() if correction.get('entity') else None\n",
    "            category = correction.get('category', '')\n",
    "            is_rare_disease = correction.get('is_rare_disease', False)\n",
    "            \n",
    "            # Skip if missing critical information\n",
    "            if not document_id or not entity:\n",
    "                continue\n",
    "                \n",
    "            # If it's a false negative but NOT a rare disease, it should be removed\n",
    "            if category == 'false_negatives' and not is_rare_disease:\n",
    "                if document_id not in self.entities_to_remove:\n",
    "                    self.entities_to_remove[document_id] = []\n",
    "                \n",
    "                self.entities_to_remove[document_id].append(entity)\n",
    "                \n",
    "                if self.debug:\n",
    "                    logger.info(f\"Marked for removal: Entity '{entity}' in document {document_id}\")\n",
    "\n",
    "    def _process_updates_and_additions(self) -> None:\n",
    "        \"\"\"\n",
    "        Process corrections to update existing annotations or add new ones.\n",
    "        \"\"\"\n",
    "        # Process each correction\n",
    "        for correction in self.corrections.get('corrected_annotations', []):\n",
    "            # Increment total corrections\n",
    "            self.stats['total_corrections'] += 1\n",
    "            \n",
    "            # Extract key information\n",
    "            document_id = str(correction.get('document_id', ''))\n",
    "            entity = correction.get('entity', '')\n",
    "            entity_lower = entity.lower() if entity else ''\n",
    "            orpha_code = correction.get('orpha_code', '')\n",
    "            is_rare_disease = correction.get('is_rare_disease', False)\n",
    "            category = correction.get('category', '')\n",
    "            \n",
    "            if self.debug:\n",
    "                logger.info(f\"\\nProcessing Correction:\")\n",
    "                logger.info(f\"  Document ID: {document_id}\")\n",
    "                logger.info(f\"  Entity: {entity}\")\n",
    "                logger.info(f\"  ORPHA Code: {orpha_code}\")\n",
    "                logger.info(f\"  Category: {category}\")\n",
    "                logger.info(f\"  Is Rare Disease: {is_rare_disease}\")\n",
    "            \n",
    "            # Skip if critical information is missing\n",
    "            if not document_id or not entity or not orpha_code:\n",
    "                if self.debug:\n",
    "                    logger.info(\"  Skipping: Missing critical information\")\n",
    "                self.stats['skipped_corrections'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Skip if not a confirmed rare disease\n",
    "            if not is_rare_disease:\n",
    "                if self.debug:\n",
    "                    logger.info(\"  Skipping: Not a confirmed rare disease\")\n",
    "                self.stats['skipped_corrections'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Check if the document exists in existing annotations\n",
    "            if document_id not in self.existing_annotations:\n",
    "                if self.debug:\n",
    "                    logger.info(f\"  Document {document_id} not found in annotations\")\n",
    "                self.stats['skipped_corrections'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Ensure annotations list exists\n",
    "            if 'annotations' not in self.existing_annotations[document_id]:\n",
    "                self.existing_annotations[document_id]['annotations'] = []\n",
    "            \n",
    "            # Try to find and update existing annotation\n",
    "            annotations = self.existing_annotations[document_id]['annotations']\n",
    "            found_match = False\n",
    "            \n",
    "            for annotation in annotations:\n",
    "                # Match by mention, case-insensitive\n",
    "                if annotation.get('mention', '').lower() == entity_lower:\n",
    "                    # Update the annotation with ORPHA code\n",
    "                    annotation['ordo_with_desc'] = f\"{orpha_code} {entity}\"\n",
    "                    found_match = True\n",
    "                    self.stats['updated_annotations'] += 1\n",
    "                    \n",
    "                    if self.debug:\n",
    "                        logger.info(f\"  Updated existing annotation for {entity}\")\n",
    "                    \n",
    "                    # Track document modification\n",
    "                    self.stats['documents_modified'].add(document_id)\n",
    "                    break\n",
    "            \n",
    "            # If no match found, add new annotation\n",
    "            if not found_match:\n",
    "                new_annotation = {\n",
    "                    'mention': entity,\n",
    "                    'umls_with_desc': '',\n",
    "                    'ordo_with_desc': f\"{orpha_code} {entity}\",\n",
    "                    'gold_text_to_umls_label': 0,\n",
    "                    'gold_text_to_ordo_label': 1,\n",
    "                    'document_structure': '',\n",
    "                    'semehr_label': 0,\n",
    "                    'correction_source': 'iterative_verification'\n",
    "                }\n",
    "                \n",
    "                annotations.append(new_annotation)\n",
    "                self.stats['added_annotations'] += 1\n",
    "                \n",
    "                if self.debug:\n",
    "                    logger.info(f\"  Added new annotation for {entity}\")\n",
    "                \n",
    "                # Track document modification\n",
    "                self.stats['documents_modified'].add(document_id)\n",
    "\n",
    "    def _remove_false_annotations(self) -> None:\n",
    "        \"\"\"\n",
    "        Remove annotations that were identified as not being true rare diseases.\n",
    "        \"\"\"\n",
    "        for document_id, entities in self.entities_to_remove.items():\n",
    "            if document_id not in self.existing_annotations:\n",
    "                continue\n",
    "                \n",
    "            if 'annotations' not in self.existing_annotations[document_id]:\n",
    "                continue\n",
    "                \n",
    "            # Get the annotations list\n",
    "            annotations = self.existing_annotations[document_id]['annotations']\n",
    "            \n",
    "            # Create a new list excluding the entities to remove\n",
    "            new_annotations = []\n",
    "            for annotation in annotations:\n",
    "                mention = annotation.get('mention', '').lower()\n",
    "                \n",
    "                if mention in entities:\n",
    "                    # This annotation should be removed\n",
    "                    self.stats['removed_annotations'] += 1\n",
    "                    \n",
    "                    if self.debug:\n",
    "                        logger.info(f\"  Removed annotation for '{mention}' in document {document_id}\")\n",
    "                else:\n",
    "                    # Keep this annotation\n",
    "                    new_annotations.append(annotation)\n",
    "            \n",
    "            # Update the annotations list\n",
    "            if len(annotations) != len(new_annotations):\n",
    "                self.existing_annotations[document_id]['annotations'] = new_annotations\n",
    "                self.stats['documents_modified'].add(document_id)\n",
    "\n",
    "    def process_corrections(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process the corrections to update the annotations.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Updated annotations with statistics\n",
    "        \"\"\"\n",
    "        # Step 1: Identify annotations to be removed\n",
    "        self._identify_removals()\n",
    "        \n",
    "        # Step 2: Process updates and additions\n",
    "        self._process_updates_and_additions()\n",
    "        \n",
    "        # Step 3: Remove false annotations\n",
    "        self._remove_false_annotations()\n",
    "        \n",
    "        # Convert document set to list for JSON serialization\n",
    "        self.stats['documents_modified'] = list(self.stats['documents_modified'])\n",
    "        \n",
    "        # Prepare output metadata\n",
    "        output_metadata = {\n",
    "            'update_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'stats': self.stats\n",
    "        }\n",
    "        \n",
    "        # Create final output dictionary\n",
    "        final_output = {\n",
    "            'metadata': output_metadata,\n",
    "            'documents': self.existing_annotations\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        if self.debug:\n",
    "            logger.info(\"\\n=== Correction Processing Summary ===\")\n",
    "            logger.info(f\"Total corrections processed: {self.stats['total_corrections']}\")\n",
    "            logger.info(f\"Annotations added: {self.stats['added_annotations']}\")\n",
    "            logger.info(f\"Annotations updated: {self.stats['updated_annotations']}\")\n",
    "            logger.info(f\"Annotations removed: {self.stats['removed_annotations']}\")\n",
    "            logger.info(f\"Corrections skipped: {self.stats['skipped_corrections']}\")\n",
    "            logger.info(f\"Documents modified: {len(self.stats['documents_modified'])}\")\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "\n",
    "def detailed_correction_diagnostic(\n",
    "    existing_annotations: Dict[str, Any], \n",
    "    corrections: Dict[str, Any]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Provide detailed diagnostic information about the correction process.\n",
    "    \n",
    "    Args:\n",
    "        existing_annotations: Original annotations dictionary\n",
    "        corrections: Corrections dictionary\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n===== CORRECTION DIAGNOSTIC =====\")\n",
    "    \n",
    "    # Print basic information about corrections\n",
    "    correction_list = corrections.get('corrected_annotations', [])\n",
    "    logger.info(f\"Total corrections found: {len(correction_list)}\")\n",
    "    \n",
    "    # Categorize corrections\n",
    "    categories = {\n",
    "        'false_negatives': [],\n",
    "        'false_positives': [],\n",
    "        'true_positives': []\n",
    "    }\n",
    "    \n",
    "    for correction in correction_list:\n",
    "        category = correction.get('category', 'unknown')\n",
    "        if category in categories:\n",
    "            categories[category].append(correction)\n",
    "        \n",
    "    logger.info(f\"False negatives to review: {len(categories['false_negatives'])}\")\n",
    "    logger.info(f\"False positives to review: {len(categories['false_positives'])}\")\n",
    "    logger.info(f\"True positives to review: {len(categories['true_positives'])}\")\n",
    "    \n",
    "    # Count entities to be removed\n",
    "    removals = []\n",
    "    for correction in correction_list:\n",
    "        category = correction.get('category', '')\n",
    "        is_rare_disease = correction.get('is_rare_disease', False)\n",
    "        \n",
    "        if category == 'false_negatives' and not is_rare_disease:\n",
    "            removals.append(correction)\n",
    "    \n",
    "    logger.info(f\"Entities identified for removal: {len(removals)}\")\n",
    "    \n",
    "    # Print a sample of entities to be removed if any\n",
    "    if removals:\n",
    "        logger.info(\"\\nSample entities to be removed:\")\n",
    "        for i, removal in enumerate(removals[:5]):\n",
    "            logger.info(f\"  {i+1}. Entity: '{removal.get('entity', '')}' - Document: {removal.get('document_id', '')}\")\n",
    "            logger.info(f\"     Explanation: {removal.get('explanation', 'No explanation')}\")\n",
    "    \n",
    "    # Print detailed document statistics\n",
    "    doc_counts = {}\n",
    "    for correction in correction_list:\n",
    "        doc_id = correction.get('document_id', '')\n",
    "        if doc_id not in doc_counts:\n",
    "            doc_counts[doc_id] = 0\n",
    "        doc_counts[doc_id] += 1\n",
    "    \n",
    "    logger.info(f\"\\nDocuments with corrections: {len(doc_counts)}\")\n",
    "    most_corrections = sorted(doc_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    if most_corrections:\n",
    "        logger.info(\"Documents with most corrections:\")\n",
    "        for doc_id, count in most_corrections:\n",
    "            logger.info(f\"  Document {doc_id}: {count} corrections\")\n",
    "\n",
    "\n",
    "def update_annotations_with_corrections(\n",
    "    existing_annotations: Dict[str, Any], \n",
    "    corrections: Dict[str, Any], \n",
    "    output_file: str = None,\n",
    "    debug: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Update existing annotations with corrections from the review process,\n",
    "    including removing false annotations that aren't actually rare diseases.\n",
    "    \n",
    "    Args:\n",
    "        existing_annotations: Original annotations dictionary\n",
    "        corrections: Corrections from the review process\n",
    "        output_file: Path to save the updated annotations\n",
    "        debug: Enable debug output\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Updated annotations dictionary with enriched rare disease annotations\n",
    "    \"\"\"\n",
    "    # Print diagnostic information if debug is on\n",
    "    if debug:\n",
    "        detailed_correction_diagnostic(existing_annotations, corrections)\n",
    "    \n",
    "    # Validate input\n",
    "    if not isinstance(existing_annotations, dict):\n",
    "        raise ValueError(\"Existing annotations must be a dictionary\")\n",
    "    \n",
    "    if not isinstance(corrections, dict) or 'corrected_annotations' not in corrections:\n",
    "        raise ValueError(\"Corrections must be a dictionary with 'corrected_annotations' key\")\n",
    "    \n",
    "    # Process corrections using the AnnotationCorrector\n",
    "    corrector = AnnotationCorrector(existing_annotations, corrections, debug)\n",
    "    result = corrector.process_corrections()\n",
    "    \n",
    "    # Save to output file if specified\n",
    "    if output_file:\n",
    "        output_dir = os.path.dirname(os.path.abspath(output_file))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        \n",
    "        if debug:\n",
    "            logger.info(f\"Updated annotations saved to: {output_file}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def process_annotations_correction(\n",
    "    existing_annotations_path: str, \n",
    "    corrections_path: str, \n",
    "    output_path: str = None,\n",
    "    debug: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convenience function to load and process annotations corrections.\n",
    "    \n",
    "    Args:\n",
    "        existing_annotations_path: Path to existing annotations JSON file\n",
    "        corrections_path: Path to corrections JSON file\n",
    "        output_path: Path to save updated annotations\n",
    "        debug: Enable debug output\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Updated annotations dictionary\n",
    "    \"\"\"\n",
    "    # Load existing annotations\n",
    "    try:\n",
    "        with open(existing_annotations_path, 'r') as f:\n",
    "            existing_annotations = json.load(f)\n",
    "            \n",
    "        if debug:\n",
    "            logger.info(f\"Loaded existing annotations from {existing_annotations_path}\")\n",
    "            \n",
    "            # Check if we need to extract the documents key\n",
    "            if 'documents' in existing_annotations and isinstance(existing_annotations['documents'], dict):\n",
    "                existing_annotations = existing_annotations['documents']\n",
    "                logger.info(\"Extracted 'documents' key from annotations\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading existing annotations: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Load corrections\n",
    "    try:\n",
    "        with open(corrections_path, 'r') as f:\n",
    "            corrections = json.load(f)\n",
    "            \n",
    "        if debug:\n",
    "            logger.info(f\"Loaded corrections from {corrections_path}\")\n",
    "            \n",
    "            # Check if we need to extract the results key\n",
    "            if 'results' in corrections:\n",
    "                # Use the flattened corrected_annotations list if available\n",
    "                if 'corrected_annotations' not in corrections:\n",
    "                    # Extract from all categories and build a unified list\n",
    "                    all_corrections = []\n",
    "                    for category in ['false_negatives', 'false_positives', 'true_positives']:\n",
    "                        if category in corrections['results']:\n",
    "                            all_corrections.extend(corrections['results'][category])\n",
    "                    \n",
    "                    corrections = {'corrected_annotations': all_corrections}\n",
    "                    logger.info(f\"Extracted and merged corrections from categories: {len(all_corrections)} total\")\n",
    "            elif 'summary' in corrections and 'flagged_entities' in corrections['summary']:\n",
    "                # Handle supervisor output format\n",
    "                corrections = {'corrected_annotations': corrections['summary']['flagged_entities']}\n",
    "                logger.info(f\"Extracted flagged entities as corrections: {len(corrections['corrected_annotations'])} total\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading corrections: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Update and save annotations\n",
    "    return update_annotations_with_corrections(\n",
    "        existing_annotations, \n",
    "        corrections, \n",
    "        output_path, \n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "\n",
    "def print_annotations_differences(\n",
    "    original_path: str, \n",
    "    corrected_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Print differences between original and corrected annotations.\n",
    "    \n",
    "    Args:\n",
    "        original_path: Path to original annotations file\n",
    "        corrected_path: Path to corrected annotations file\n",
    "    \"\"\"\n",
    "    # Load original and corrected annotations\n",
    "    with open(original_path, 'r') as f:\n",
    "        original_data = json.load(f)\n",
    "        # Extract documents if needed\n",
    "        original_annos = original_data.get('documents', original_data)\n",
    "    \n",
    "    with open(corrected_path, 'r') as f:\n",
    "        corrected_data = json.load(f)\n",
    "        # Extract documents if needed\n",
    "        corrected_annos = corrected_data.get('documents', corrected_data)\n",
    "    \n",
    "    logger.info(\"\\n===== ANNOTATION DIFFERENCES =====\")\n",
    "    \n",
    "    # Track overall changes\n",
    "    modifications = {\n",
    "        'added': 0,\n",
    "        'updated': 0,\n",
    "        'removed': 0,\n",
    "        'unchanged': 0\n",
    "    }\n",
    "    \n",
    "    # Compare document IDs with annotations\n",
    "    for doc_id, doc_data in corrected_annos.items():\n",
    "        # Check if document exists in original\n",
    "        orig_doc = original_annos.get(doc_id, {})\n",
    "        \n",
    "        # Get annotations from both\n",
    "        corr_anns = doc_data.get('annotations', [])\n",
    "        orig_anns = orig_doc.get('annotations', [])\n",
    "        \n",
    "        if not corr_anns and not orig_anns:\n",
    "            continue\n",
    "            \n",
    "        # Track mentions for easy comparison\n",
    "        orig_mentions = {ann.get('mention', '').lower(): ann for ann in orig_anns}\n",
    "        corr_mentions = {ann.get('mention', '').lower(): ann for ann in corr_anns}\n",
    "        \n",
    "        # Find new, changed, and removed entries\n",
    "        all_mentions = set(orig_mentions.keys()) | set(corr_mentions.keys())\n",
    "        document_changes = {\n",
    "            'added': [],\n",
    "            'updated': [],\n",
    "            'removed': [],\n",
    "            'unchanged': 0\n",
    "        }\n",
    "        \n",
    "        for mention in all_mentions:\n",
    "            orig_ann = orig_mentions.get(mention)\n",
    "            corr_ann = corr_mentions.get(mention)\n",
    "            \n",
    "            if not orig_ann and corr_ann:\n",
    "                # Added annotation\n",
    "                document_changes['added'].append(corr_ann)\n",
    "                modifications['added'] += 1\n",
    "            elif orig_ann and not corr_ann:\n",
    "                # Removed annotation\n",
    "                document_changes['removed'].append(orig_ann)\n",
    "                modifications['removed'] += 1\n",
    "            elif orig_ann and corr_ann:\n",
    "                # Check for changes\n",
    "                if orig_ann.get('ordo_with_desc') != corr_ann.get('ordo_with_desc'):\n",
    "                    # Updated annotation\n",
    "                    document_changes['updated'].append((orig_ann, corr_ann))\n",
    "                    modifications['updated'] += 1\n",
    "                else:\n",
    "                    # Unchanged annotation\n",
    "                    document_changes['unchanged'] += 1\n",
    "                    modifications['unchanged'] += 1\n",
    "        \n",
    "        # Print document summary if changes exist\n",
    "        changes_count = len(document_changes['added']) + len(document_changes['updated']) + len(document_changes['removed'])\n",
    "        if changes_count > 0:\n",
    "            logger.info(f\"\\nDocument {doc_id} changes:\")\n",
    "            logger.info(f\"  Added: {len(document_changes['added'])}\")\n",
    "            logger.info(f\"  Updated: {len(document_changes['updated'])}\")\n",
    "            logger.info(f\"  Removed: {len(document_changes['removed'])}\")\n",
    "            logger.info(f\"  Unchanged: {document_changes['unchanged']}\")\n",
    "            \n",
    "            # Print details of changed annotations\n",
    "            if document_changes['added']:\n",
    "                logger.info(\"\\n  Added Annotations:\")\n",
    "                for i, ann in enumerate(document_changes['added'][:3], 1):  # Limit to first 3\n",
    "                    logger.info(f\"    {i}. '{ann.get('mention', '')}' - {ann.get('ordo_with_desc', '')}\")\n",
    "                if len(document_changes['added']) > 3:\n",
    "                    logger.info(f\"    ... and {len(document_changes['added']) - 3} more\")\n",
    "                    \n",
    "            if document_changes['removed']:\n",
    "                logger.info(\"\\n  Removed Annotations:\")\n",
    "                for i, ann in enumerate(document_changes['removed'][:3], 1):  # Limit to first 3\n",
    "                    logger.info(f\"    {i}. '{ann.get('mention', '')}' - {ann.get('ordo_with_desc', '')}\")\n",
    "                if len(document_changes['removed']) > 3:\n",
    "                    logger.info(f\"    ... and {len(document_changes['removed']) - 3} more\")\n",
    "                    \n",
    "            if document_changes['updated']:\n",
    "                logger.info(\"\\n  Updated Annotations:\")\n",
    "                for i, (orig, corr) in enumerate(document_changes['updated'][:3], 1):  # Limit to first 3\n",
    "                    logger.info(f\"    {i}. '{orig.get('mention', '')}'\")\n",
    "                    logger.info(f\"       Old: {orig.get('ordo_with_desc', '')}\")\n",
    "                    logger.info(f\"       New: {corr.get('ordo_with_desc', '')}\")\n",
    "                if len(document_changes['updated']) > 3:\n",
    "                    logger.info(f\"    ... and {len(document_changes['updated']) - 3} more\")\n",
    "    \n",
    "    # Print overall summary\n",
    "    logger.info(\"\\n=== Overall Changes Summary ===\")\n",
    "    logger.info(f\"Total annotations added: {modifications['added']}\")\n",
    "    logger.info(f\"Total annotations updated: {modifications['updated']}\")\n",
    "    logger.info(f\"Total annotations removed: {modifications['removed']}\")\n",
    "    logger.info(f\"Total annotations unchanged: {modifications['unchanged']}\")\n",
    "    logger.info(f\"Total annotations affected: {modifications['added'] + modifications['updated'] + modifications['removed']}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example paths - update with your actual paths\n",
    "    EXISTING_ANNOTATIONS_PATH = \"data/dataset/filtered_rd_annos_updated_adam.json\"\n",
    "    CORRECTIONS_PATH = \"data/dataset/adam_corrections_v2.json\"\n",
    "    OUTPUT_PATH = \"data/dataset/rd_annos_adam_corrected_v1.json\" # technically its v1, but let's just leave it at that.\n",
    "    \n",
    "    # Process the corrections\n",
    "    updated_annotations = process_annotations_correction(\n",
    "        EXISTING_ANNOTATIONS_PATH,\n",
    "        CORRECTIONS_PATH,\n",
    "        OUTPUT_PATH,\n",
    "        debug=True\n",
    "    )\n",
    "    \n",
    "    # Print differences between original and updated annotations\n",
    "    print_annotations_differences(EXISTING_ANNOTATIONS_PATH, OUTPUT_PATH)\n",
    "# Example usage (commented out)\n",
    "# updated_annotations = process_annotations_correction(\n",
    "#     \"data/dataset/filtered_rd_annos_updated_adam.json\", \n",
    "#     \"data/dataset/adam_corrections_v2.json\", \n",
    "#     \"data/dataset/rd_annos_adam_corrected_v1.json\",\n",
    "#     debug=True\n",
    "# )\n",
    "\n",
    "# print_annotations_differences(EXISTING_ANNOTATIONS_PATH, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hporag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
