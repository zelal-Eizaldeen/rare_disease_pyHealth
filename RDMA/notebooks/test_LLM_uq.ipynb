{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "# Set the parent directory as the current directory\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnwu3/miniconda3/envs/hporag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-07 14:01:18.511928: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741377678.525749 1330782 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741377678.529974 1330782 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-07 14:01:18.545560: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized ModelLoader with cache directory: /shared/rsaas/jw3/rare_disease/model_cache\n",
      "Loading LLM!\n",
      "Device configuration: cuda:4\n",
      "Using device map: {'': 'cuda:4'}\n",
      "Loading 70B model with quantization: mistral_24b\n",
      "Generated cache path: /shared/rsaas/jw3/rare_disease/model_cache/Mistral-Small-24B-Instruct-2501_4bit_nf4\n",
      "Valid cache found at /shared/rsaas/jw3/rare_disease/model_cache/Mistral-Small-24B-Instruct-2501_4bit_nf4\n",
      "Loading cached quantized model from /shared/rsaas/jw3/rare_disease/model_cache/Mistral-Small-24B-Instruct-2501_4bit_nf4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnwu3/miniconda3/envs/hporag/lib/python3.10/site-packages/transformers/quantizers/auto.py:195: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid cache found at /shared/rsaas/jw3/rare_disease/model_cache/Mistral-Small-24B-Instruct-2501_4bit_nf4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm here to help. How can I assist you today? If you have any medical questions or need information on a specific topic, feel free to ask. Please note that while I strive to provide accurate and helpful information, I am an AI and my knowledge cutoff is 2023, I don't have real-time web browsing capabilities, and I can't provide medical advice. For medical advice, please consult a healthcare professional. Here are some examples of how I can help:\n",
      "\n",
      "* Explain medical terms or concepts\n",
      "* Provide information on diseases, conditions, or treatments\n",
      "* Offer insights into medical procedures or tests\n",
      "* Discuss healthcare trends and research (up to my knowledge cutoff in 2023)\n",
      "* Help with medical coding or terminology questions\n",
      "\n",
      "What do you need help with?\n"
     ]
    }
   ],
   "source": [
    "def load_mistral_llm_client():\n",
    "    \"\"\"\n",
    "    Load a Mistral 24B LLM client configured with default cache directories\n",
    "    and assigned to cuda:0 device.\n",
    "    \n",
    "    Returns:\n",
    "        LocalLLMClient: Initialized LLM client for Mistral 24B\n",
    "    \"\"\"\n",
    "    from utils.llm_client import LocalLLMClient\n",
    "    \n",
    "    # Default cache directory from mine_hpo.py\n",
    "    default_cache_dir = \"/u/zelalae2/scratch/rdma_cache\"\n",
    "    \n",
    "    # Initialize and return the client with specific configuration\n",
    "    llm_client = LocalLLMClient(\n",
    "        model_type=\"mistral_24b\",  # Explicitly request mistral_24b model\n",
    "        device=\"cuda:4\",           # Assign to first GPU (cuda:0)\n",
    "        cache_dir=default_cache_dir,\n",
    "        temperature=0.5           # Default temperature from mine_hpo.py\n",
    "    )\n",
    "    \n",
    "    return llm_client\n",
    "\n",
    "llm_client = load_mistral_llm_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: torch.Size([131072])\n",
      "DEBUG: torch.Size([131072])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'YES',\n",
       " 'token_probs': [0.9994131326675415, 0.990872323513031],\n",
       " 'mean_confidence': 0.9951427280902863,\n",
       " 'min_confidence': 0.990872323513031,\n",
       " 'token_distribution_entropies': [0.007497226303451916, 0.07494210298559717],\n",
       " 'mean_distribution_entropy': 0.04121966464452454,\n",
       " 'max_distribution_entropy': 0.07494210298559717,\n",
       " 'top_alternatives': [[{'token': 'YES',\n",
       "    'probability': 0.0,\n",
       "    'is_selected': True},\n",
       "   {'token': 'NO', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'Yes', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'Y', 'probability': 1.099445881558303e-10, 'is_selected': False},\n",
       "   {'token': \"'\", 'probability': 1.021105866527705e-08, 'is_selected': False}],\n",
       "  [{'token': '</s>', 'probability': 0.990872323513031, 'is_selected': True},\n",
       "   {'token': '.', 'probability': 0.009125618264079094, 'is_selected': False},\n",
       "   {'token': '\\n\\n',\n",
       "    'probability': 1.0579583431535866e-06,\n",
       "    'is_selected': False},\n",
       "   {'token': ',', 'probability': 9.632819910621038e-07, 'is_selected': False},\n",
       "   {'token': ' (',\n",
       "    'probability': 1.9377182525204262e-08,\n",
       "    'is_selected': False}]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sys_implication_prompt = (\n",
    "            \"You are a clinical expert specializing in phenotype identification for Human Phenotype Ontology (HPO) mapping. \"\n",
    "            \"Determine if the given term IMPLIES a phenotype, even though it's not a direct phenotype itself. \"\n",
    "            \"\\nEXAMPLES:\\n\"\n",
    "            \"1. Laboratory test names (e.g., 'white blood cell count', 'hemoglobin level') imply phenotypes if the value is abnormal.\\n\"\n",
    "            \"2. Diagnostic procedures (e.g., 'kidney biopsy', 'chest X-ray') typically do NOT imply phenotypes unless findings are mentioned.\\n\"\n",
    "            \"3. Medications (e.g., 'insulin', 'lisinopril') can imply phenotypes related to the condition being treated.\\n\"\n",
    "            \"4. Microorganisms or pathogens (e.g., 'E. coli', 'Staphylococcus aureus') imply infection phenotypes.\\n\"\n",
    "            \"\\nRespond with ONLY 'YES' if the term implies a phenotype, or 'NO' if it doesn't imply any phenotype. \"\n",
    "            \"Consider both the term itself AND its context in the clinical note.\"\n",
    "        )\n",
    "entity = \"retina\"\n",
    "context_part = \"and peripheral retina revealed multiple yellowish white lesion-like flecks in the mid-periphery, and few blot haemorrhages indicative of hypertensive changes.\"\n",
    "context_text = \"\"\n",
    "# context_part = \"\"\n",
    "prompt = (\n",
    "            f\"I need to determine if '{entity}' implies a phenotype, even though it's not a direct phenotype itself. \"\n",
    "            f\"Here is some context surrounding the entity to aid in your decision:{context_part}\"\n",
    "            f\"Here are some phenotype terms from the Human Phenotype Ontology for context:\\n\\n\"\n",
    "            f\"{context_text}\\n\\n\"\n",
    "            f\"Based on this information and clinical knowledge, does '{entity}' imply a phenotype? \"\n",
    "            f\"For example, '\"\n",
    "            f\"'E. coli in urine' implies 'urinary tract infection'.\\n\\n\"\n",
    "            f\"Respond with ONLY 'YES' if it implies a phenotype or 'NO' if it doesn't.\"\n",
    "        )\n",
    "llm_client.query_with_full_entropy(prompt, sys_implication_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: torch.Size([131072])\n",
      "Note: Generated token 'ret' (p=0.2202) differs from highest probability token 'Ret' (p=0.5282)\n",
      "DEBUG: torch.Size([131072])\n",
      "Note: Generated token 'in' (p=0.4666) differs from highest probability token 'inal' (p=0.5288)\n",
      "DEBUG: torch.Size([131072])\n",
      "DEBUG: torch.Size([131072])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'retinopathy',\n",
       " 'token_probs': [0.22017405927181244,\n",
       "  0.46664196252822876,\n",
       "  0.999996542930603,\n",
       "  0.9973772764205933],\n",
       " 'mean_confidence': 0.6710474602878094,\n",
       " 'min_confidence': 0.22017405927181244,\n",
       " 'token_distribution_entropies': [1.6835854792669336,\n",
       "  1.0347701456240839,\n",
       "  4.987488448070481e-06,\n",
       "  0.02697477074947463],\n",
       " 'mean_distribution_entropy': 0.686333845782235,\n",
       " 'max_distribution_entropy': 1.6835854792669336,\n",
       " 'top_alternatives': [[{'token': 'Ret',\n",
       "    'probability': 0.0,\n",
       "    'is_selected': False},\n",
       "   {'token': 'ret', 'probability': 0.0, 'is_selected': True},\n",
       "   {'token': 'Hy', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'hy', 'probability': 4.807127473860362e-10, 'is_selected': False},\n",
       "   {'token': 'H', 'probability': 0.0, 'is_selected': False}],\n",
       "  [{'token': 'inal', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'in', 'probability': 0.0, 'is_selected': True},\n",
       "   {'token': 'initis', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'ina', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'init', 'probability': 0.0, 'is_selected': False}],\n",
       "  [{'token': 'opathy', 'probability': 0.0, 'is_selected': True},\n",
       "   {'token': 'opath', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'osis', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'opathic', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'op', 'probability': 0.0, 'is_selected': False}],\n",
       "  [{'token': '</s>', 'probability': 0.9973772764205933, 'is_selected': True},\n",
       "   {'token': '.', 'probability': 0.0024722511880099773, 'is_selected': False},\n",
       "   {'token': ',', 'probability': 0.00013947486877441406, 'is_selected': False},\n",
       "   {'token': '\\n\\n',\n",
       "    'probability': 5.080371465737699e-06,\n",
       "    'is_selected': False},\n",
       "   {'token': ' hypertensive',\n",
       "    'probability': 4.483411885303212e-06,\n",
       "    'is_selected': False}]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_implication_prompt = (\n",
    "            \"You are a clinical expert specializing in phenotype identification for Human Phenotype Ontology (HPO) mapping. \"\n",
    "            \"A previous analysis determined that a given term implies a phenotype but is not a direct phenotype itself. \"\n",
    "            \"Your task is to precisely identify what specific phenotype is implied by this term. \"\n",
    "            \"\\nEXAMPLES:\\n\"\n",
    "            \"1. 'Elevated white blood cell count' implies 'leukocytosis'\\n\"\n",
    "            \"2. 'Low hemoglobin' implies 'anemia'\\n\"\n",
    "            \"3. 'E. coli in urine' implies 'urinary tract infection' or 'bacteriuria'\\n\"\n",
    "            \"4. 'Taking insulin' implies 'diabetes mellitus'\\n\"\n",
    "            \"\\nProvide ONLY the name of the implied phenotype as it would appear in medical terminology. \"\n",
    "            \"Be specific and concise. Do not include explanations or multiple options separated by commas or slashes. \"\n",
    "            \"Consider the term's context in the clinical note to determine the most accurate phenotype.\"\n",
    "        )\n",
    "\n",
    "entity = \"retina\"\n",
    "context_part = \"and peripheral retina revealed multiple yellowish white lesion-like flecks in the mid-periphery, and few blot haemorrhages indicative of hypertensive changes.\"\n",
    "# context_text = \"- retina issue (HP:0000479) - absent retina (HP:0010728) - abnormal retina (HP:0000479) - retina tumor (HP:0009919) - retinal disease (HP:0000479) - retinopathy (HP:0000488) - absent/small retina (HP:0008061) - detached retina (HP:0000541) - retinal dots (HP:0032027)- a small break in the retina. (HP:0011530)\"\n",
    "# context_text = \"Pseudo-chilblain, White lesion of the oral mucosa, White retinal lesion, Psoriasiform lesion\"\n",
    "# entity = \"Escherichia coli\"\n",
    "# context_part = \"Urine samples for cultures were sent which reported pure growth of Escherichia coli.\"\n",
    "# context_text = f\" e coli infections (HP:0002740) - increased susceptibility to infections with escherichia coli, as manifested by recurrent episodes of infection with this agent. (HP:0002740) - bacteriuria (HP:0012461) - bacteremia (HP:0031864) - repeated e coli bacterial invasions (HP:0002740) - e coli infections, recurrent (HP:0002740) - recurrent e. coli infections (HP:0002740) - absence of specific immunoglobulins directed against a specific antigen or microorganism. (HP:0005424) - diarrhoea (HP:0002014)- increased predisposition to e coli infections (HP:0002740)\"\n",
    "context_text = \"\"\n",
    "prompt = (\n",
    "        f\"The entity term '{entity}' implies a phenotype but is not a direct phenotype itself. \"\n",
    "        f\"The context sentence below will help you in identifying an implied phenotype: {context_part}\"\n",
    "        f\"Here are some phenotype terms that are related to the entity in its context that may help you make your decision:\\n\\n\"\n",
    "        f\"{context_text}\\n\\n\"\n",
    "        f\"What specific phenotype is implied by '{entity}'? \"\n",
    "        f\"For example, \"\n",
    "        f\"'hemoglobin of 8 g/dL' implies 'anemia'.\\n\\n\"\n",
    "        f\"Provide ONLY the name of the implied phenotype, without any explanation. \"\n",
    "        f\"Use standard medical terminology.\"\n",
    "    )\n",
    "\n",
    "llm_client.query_with_full_entropy(prompt, sys_implication_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: torch.Size([131072])\n",
      "Note: Generated token 'ur' (p=0.2376) differs from highest probability token 'b' (p=0.4170)\n",
      "DEBUG: torch.Size([131072])\n",
      "DEBUG: torch.Size([131072])\n",
      "DEBUG: torch.Size([131072])\n",
      "DEBUG: torch.Size([131072])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'urinary tract infection',\n",
       " 'token_probs': [0.23757345974445343,\n",
       "  0.9999997615814209,\n",
       "  0.9999995231628418,\n",
       "  0.9999990463256836,\n",
       "  0.9967190623283386],\n",
       " 'mean_confidence': 0.8468581706285476,\n",
       " 'min_confidence': 0.23757345974445343,\n",
       " 'token_distribution_entropies': [1.8887087169573893,\n",
       "  3.439652719694516e-07,\n",
       "  6.87930436766833e-07,\n",
       "  1.375860558531952e-06,\n",
       "  0.0317450581122454],\n",
       " 'mean_distribution_entropy': 0.3840912365651804,\n",
       " 'max_distribution_entropy': 1.8887087169573893,\n",
       " 'top_alternatives': [[{'token': 'b',\n",
       "    'probability': 0.0,\n",
       "    'is_selected': False},\n",
       "   {'token': 'B', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'ur', 'probability': 0.0, 'is_selected': True},\n",
       "   {'token': 'Ur', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'U', 'probability': 0.0, 'is_selected': False}],\n",
       "  [{'token': 'inary', 'probability': 0.0, 'is_selected': True},\n",
       "   {'token': 'ine', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'inal', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'ti', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'in', 'probability': 0.0, 'is_selected': False}],\n",
       "  [{'token': ' tract', 'probability': 0.0, 'is_selected': True},\n",
       "   {'token': ' infection',\n",
       "    'probability': 6.427621102034209e-09,\n",
       "    'is_selected': False},\n",
       "   {'token': '-t', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': ' ', 'probability': 1.6577052974753315e-07, 'is_selected': False},\n",
       "   {'token': ' E', 'probability': 0.0, 'is_selected': False}],\n",
       "  [{'token': ' infection',\n",
       "    'probability': 6.427621102034209e-09,\n",
       "    'is_selected': True},\n",
       "   {'token': ' infections', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': ' Infection', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': ' inf', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': '</s>', 'probability': 0.9967190623283386, 'is_selected': False}],\n",
       "  [{'token': '</s>', 'probability': 0.9967190623283386, 'is_selected': True},\n",
       "   {'token': '.', 'probability': 0.0032730388920754194, 'is_selected': False},\n",
       "   {'token': \"'\", 'probability': 2.1835987809026847e-06, 'is_selected': False},\n",
       "   {'token': ' or',\n",
       "    'probability': 2.0513011804723646e-06,\n",
       "    'is_selected': False},\n",
       "   {'token': ',',\n",
       "    'probability': 1.3452760185828083e-06,\n",
       "    'is_selected': False}]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_implication_prompt = (\n",
    "            \"You are a clinical expert specializing in phenotype identification for Human Phenotype Ontology (HPO) mapping. \"\n",
    "            \"A previous analysis determined that a given term implies a phenotype but is not a direct phenotype itself. \"\n",
    "            \"Your task is to precisely identify what specific phenotype is implied by this term. \"\n",
    "            \"\\nEXAMPLES:\\n\"\n",
    "            \"1. 'Elevated white blood cell count' implies 'leukocytosis'\\n\"\n",
    "            \"2. 'Low hemoglobin' implies 'anemia'\\n\"\n",
    "            \"3. 'E. coli in urine' implies 'urinary tract infection' or 'bacteriuria'\\n\"\n",
    "            \"4. 'Taking insulin' implies 'diabetes mellitus'\\n\"\n",
    "            \"\\nProvide ONLY the name of the implied phenotype as it would appear in medical terminology. \"\n",
    "            \"Be specific and concise. Do not include explanations or multiple options separated by commas or slashes. \"\n",
    "            \"Consider the term's context in the clinical note to determine the most accurate phenotype.\"\n",
    "        )\n",
    "\n",
    "entity = \"retina\"\n",
    "context_part = \"and peripheral retina revealed multiple yellowish white lesion-like flecks in the mid-periphery, and few blot haemorrhages indicative of hypertensive changes.\"\n",
    "context_text = \"\"\n",
    "\n",
    "entity = \"Escherichia coli\"\n",
    "context_part = \"Urine samples for cultures were sent which reported pure growth of Escherichia coli.\"\n",
    "# context_text = f\" e coli infections (HP:0002740) - increased susceptibility to infections with escherichia coli, as manifested by recurrent episodes of infection with this agent. (HP:0002740) - bacteriuria (HP:0012461) - bacteremia (HP:0031864) - repeated e coli bacterial invasions (HP:0002740) - e coli infections, recurrent (HP:0002740) - recurrent e. coli infections (HP:0002740) - absence of specific immunoglobulins directed against a specific antigen or microorganism. (HP:0005424) - diarrhoea (HP:0002014)- increased predisposition to e coli infections (HP:0002740)\"\n",
    "context_text = \"\"\n",
    "prompt = (\n",
    "        f\"The term '{entity}' implies a phenotype but is not a direct phenotype itself. \"\n",
    "        f\"{context_part}\"\n",
    "        f\"Here are some phenotype terms that may help you make your decision:\\n\\n\"\n",
    "        f\"{context_text}\\n\\n\"\n",
    "        f\"What specific phenotype is implied by '{entity}'? \"\n",
    "        f\"For example, \"\n",
    "        f\"'hemoglobin of 8 g/dL' implies 'anemia'.\\n\\n\"\n",
    "        f\"Provide ONLY the name of the implied phenotype, without any explanation. \"\n",
    "        f\"Use standard medical terminology.\"\n",
    "    )\n",
    "llm_client.query_with_full_entropy(prompt, sys_implication_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'Sure, here are five random words:\\n\\n1. Elephant\\n2. Whisper\\n3. Galaxy\\n4. Melody\\n5. Sunset',\n",
       " 'token_probs': [0.9840844869613647,\n",
       "  0.9591541886329651,\n",
       "  0.9999997615814209,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999958276748657,\n",
       "  1.0,\n",
       "  0.9979816675186157,\n",
       "  0.9991676807403564,\n",
       "  1.0,\n",
       "  0.14874207973480225,\n",
       "  0.9999998807907104,\n",
       "  0.9999997615814209,\n",
       "  0.9999998807907104,\n",
       "  1.0,\n",
       "  0.9973176121711731,\n",
       "  0.9999340772628784,\n",
       "  0.9997296929359436,\n",
       "  0.9999998807907104,\n",
       "  1.0,\n",
       "  0.9951379895210266,\n",
       "  0.9999998807907104,\n",
       "  0.9999994039535522,\n",
       "  0.9999998807907104,\n",
       "  0.08173133432865143,\n",
       "  0.9999994039535522,\n",
       "  0.9999997615814209,\n",
       "  0.9999996423721313,\n",
       "  0.00570232467725873,\n",
       "  0.8395789861679077],\n",
       " 'mean_confidence': 0.9002751695768286,\n",
       " 'min_confidence': 0.00570232467725873,\n",
       " 'token_distribution_entropies': [0.11780561852053495,\n",
       "  0.24615654061023995,\n",
       "  3.439652719694516e-07,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  6.019380076266586e-06,\n",
       "  0.0,\n",
       "  0.021164348523316077,\n",
       "  0.010110189441618227,\n",
       "  0.0,\n",
       "  1.2083050684472325,\n",
       "  1.719826422758055e-07,\n",
       "  3.439652719694516e-07,\n",
       "  1.719826422758055e-07,\n",
       "  0.0,\n",
       "  0.02993222687981053,\n",
       "  0.0008781670533270988,\n",
       "  0.003585275334223684,\n",
       "  1.719826422758055e-07,\n",
       "  0.0,\n",
       "  0.054858975920644024,\n",
       "  1.719826422758055e-07,\n",
       "  8.599129718705616e-07,\n",
       "  1.719826422758055e-07,\n",
       "  3.1017767242114362,\n",
       "  8.599129718705616e-07,\n",
       "  3.439652719694516e-07,\n",
       "  5.159478606592288e-07,\n",
       "  2.666203156591094,\n",
       "  0.6374461490073184],\n",
       " 'mean_distribution_entropy': 0.26994108625012353,\n",
       " 'max_distribution_entropy': 3.1017767242114362,\n",
       " 'top_alternatives': [[{'token': 'Sure',\n",
       "    'probability': 0.0,\n",
       "    'is_selected': True},\n",
       "   {'token': 'Here', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': '1', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'Abs', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': 'Of', 'probability': 0.0, 'is_selected': False}],\n",
       "  [{'token': ',', 'probability': 3.4415126304310206e-09, 'is_selected': True},\n",
       "   {'token': '!', 'probability': 4.703990086341037e-09, 'is_selected': False},\n",
       "   {'token': ' here', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': '!\\n\\n',\n",
       "    'probability': 4.1750283785724207e-10,\n",
       "    'is_selected': False},\n",
       "   {'token': ' thing', 'probability': 0.0, 'is_selected': False}],\n",
       "  [{'token': ' here', 'probability': 0.0, 'is_selected': True},\n",
       "   {'token': ' how', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': ' Here', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': ' let', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': ' I', 'probability': 0.0, 'is_selected': False}],\n",
       "  [{'token': ' are', 'probability': 0.0, 'is_selected': True},\n",
       "   {'token': ' they', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': ' you', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': ' is', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': ' five', 'probability': 0.0, 'is_selected': False}],\n",
       "  [{'token': ' five', 'probability': 0.0, 'is_selected': True},\n",
       "   {'token': ' your', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': ' some', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': ' four', 'probability': 0.0, 'is_selected': False},\n",
       "   {'token': ' the', 'probability': 0.0, 'is_selected': False}]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_implication_prompt = \"Generate some random words please.\"\n",
    "prompt = \"Generate me five random words. Do whatever you want.\"\n",
    "\n",
    "llm_client.query_with_full_entropy(prompt, sys_implication_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hporag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
