{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "# Set the parent directory as the current directory\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnwu3/miniconda3/envs/hporag/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/johnwu3/miniconda3/envs/hporag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized ModelLoader with cache directory: /shared/rsaas/jw3/rare_disease/model_cache\n",
      "Loading LLM!\n",
      "Device configuration: cuda:0\n",
      "Using device map: {'': 'cuda:0'}\n",
      "Loading 70B model with quantization: mistral_24b\n",
      "Generated cache path: /shared/rsaas/jw3/rare_disease/model_cache/Mistral-Small-24B-Instruct-2501_4bit_nf4\n",
      "Valid cache found at /shared/rsaas/jw3/rare_disease/model_cache/Mistral-Small-24B-Instruct-2501_4bit_nf4\n",
      "Loading cached quantized model from /shared/rsaas/jw3/rare_disease/model_cache/Mistral-Small-24B-Instruct-2501_4bit_nf4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnwu3/miniconda3/envs/hporag/lib/python3.10/site-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [02:18<00:00, 46.19s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm here to help. How can I assist you today? If you have any medical questions or need information on a specific topic, feel free to ask. Please note that while I strive to provide accurate and helpful information, I am an AI and my knowledge cutoff is 2023, and I don't have real-time web browsing capabilities or personal experiences. For urgent medical concerns, always consult a healthcare professional.\n",
      "\n",
      "Here are a few examples of how I can assist you:\n",
      "\n",
      "* Explain medical terms or concepts\n",
      "* Provide information on diseases, conditions, or treatments\n",
      "* Offer insights into medical procedures or tests\n",
      "* Discuss healthcare guidelines or recommendations\n",
      "* Answer questions related to biomedical research or studies\n",
      "\n",
      "What would you like to know or discuss?\n"
     ]
    }
   ],
   "source": [
    "def load_mistral_llm_client():\n",
    "    \"\"\"\n",
    "    Load a Mistral 24B LLM client configured with default cache directories\n",
    "    and assigned to cuda:0 device.\n",
    "    \n",
    "    Returns:\n",
    "        LocalLLMClient: Initialized LLM client for Mistral 24B\n",
    "    \"\"\"\n",
    "    from utils.llm_client import LocalLLMClient\n",
    "    \n",
    "    # Default cache directory from mine_hpo.py\n",
    "    default_cache_dir = \"/u/zelalae2/scratch/rdma_cache\"\n",
    "    \n",
    "    # Initialize and return the client with specific configuration\n",
    "    llm_client = LocalLLMClient(\n",
    "        model_type=\"mistral_24b\",  # Explicitly request mistral_24b model\n",
    "        device=\"cuda:0\",           # Assign to first GPU (cuda:0)\n",
    "        cache_dir=default_cache_dir,\n",
    "        temperature=0.0001           # Default temperature from mine_hpo.py\n",
    "    )\n",
    "    \n",
    "    return llm_client\n",
    "\n",
    "llm_client = load_mistral_llm_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"ACE, CSF\": {\\n    \"Age\": [\"0–5 years\", \"6–17 years\", \"18–50 years\"],\\n    \"Male\": [\"Not Estab.\", \"0.0–2.1\", \"0.0–2.5\"],\\n    \"Female\": [\"Not Estab.\", \"0.0–2.1\", \"0.0–2.5\"]\\n  },\\n  \"Adiponectin\": {\\n    \"Age\": [\"0–7 years\", \"8–9 years\", \"10–11 years\", \"12–13 years\", \"14–15 years\", \"16–19 years\"],\\n    \"Male\": [\"2.3–26.5\", \"4.0–14.9\", \"3.4–13.8\", \"4.5–13.2\", \"3.7–13.7\", \"2.7–13.3\"],\\n    \"Female\": [\"2.3–26.5\", \"4.0–14.9\", \"3.4–13.8\", \"4.5–13.2\", \"3.7–13.7\", \"2.7–13.3\"]\\n  },\\n  \"Albumin\": {\\n    \"Age\": [\"0–7 days\", \"8–30 days\", \"1–6 months\", \"7 months–2 years\", \"3–5 years\", \"6–12 years\", \"13–30 years\"],\\n    \"Male\": [\"3.6–4.9\", \"3.4–4.7\", \"3.7–4.8\", \"3.9–5.0\", \"4.0–5.0\", \"4.1–5.0\", \"4.1–5.2\"],\\n    \"Female\": [\"3.6–4.9\", \"3.4–4.7\", \"3.7–4.8\", \"3.9–5.0\", \"4.0–5.0\", \"4.1–5.0\", \"3.9–5.0\"]\\n  },\\n  \"A/G Ratio\": {\\n    \"Age\": [\"0–7 days\", \"8–30 days\", \"1–6 months\", \"7 months–5 years\", \">5 years\"],\\n    \"Male\": [\"1.1–2.3\", \"1.2–2.8\", \"1.3–3.6\", \"1.5–2.6\", \"1.2–2.2\"],\\n    \"Female\": [\"1.1–2.3\", \"1.2–2.8\", \"1.3–3.6\", \"1.5–2.6\", \"1.2–2.2\"]\\n  }\\n}\\n```'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.data import read_json_file, print_json_structure\n",
    "\n",
    "file_path = \"data/tools/extracted_pages.json\"\n",
    "data = read_json_file(file_path)\n",
    "data[0]\n",
    "\n",
    "system_message = \"Extract all the corresponding tables for each \\\"result name\\\" into a json format with each sub-key based on the column headers. Return on only the json header.\"\n",
    "# for page in data:\n",
    "#     content = page[\"content\"]\n",
    "content = data[0][\"content\"]\n",
    "llm_client.query(content, system_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hporag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
