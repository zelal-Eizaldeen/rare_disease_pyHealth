{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "# Set the parent directory as the current directory\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from data/results/agents/rd/rd_mistral24b_medembed_supervised.json\n",
      "Analyzing false positives...\n",
      "Verifying entity existence in original text...\n",
      "Verified 18/18 reclassified entities exist in at least one document (100.0% exist)\n",
      "18/18 entities exist in all their associated documents (100.0%)\n",
      "Analyzing rare disease matches...\n",
      "\n",
      "===== False Positive Analysis =====\n",
      "Total unique false positives: 92\n",
      "Reclassified as true positives: 18 (19.6%)\n",
      "\n",
      "False positive document frequency:\n",
      "  1 document: 73 entities\n",
      "  2 documents: 10 entities\n",
      "  3 documents: 4 entities\n",
      "  4 documents: 2 entities\n",
      "  5 documents: 2 entities\n",
      "  6 documents: 1 entities\n",
      "\n",
      "Top 20 most frequent false positives:\n",
      "  1. afib (6 docs) ✗ \n",
      "  2. cholangitis (5 docs) ✗ \n",
      "  3. hepatic encephalopathy (5 docs) ✗ \n",
      "  4. respiratory distress syndrome (4 docs) ✗ \n",
      "  5. hcc (4 docs) ✗ \n",
      "  6. tracheomalacia (3 docs) ✓ 📄 (exists in 100.0% of docs)\n",
      "  7. atrial septal defect (3 docs) ✗ \n",
      "  8. pulmonary artery systolic hypertension (3 docs) ✗ \n",
      "  9. apnea of prematurity (3 docs) ✗ \n",
      "  10. complete heart block (2 docs) ✗ \n",
      "  11. pud (2 docs) ✗ \n",
      "  12. cns lymphoma (2 docs) ✓ 📄 (exists in 100.0% of docs)\n",
      "  13. aml (2 docs) ✓ 📄 (exists in 100.0% of docs)\n",
      "  14. neuromuscular disease (2 docs) ✗ \n",
      "  15. aortic dissection (2 docs) ✗ \n",
      "  16. ck (2 docs) ✗ \n",
      "  17. paf (2 docs) ✗ \n",
      "  18. sss (2 docs) ✗ \n",
      "  19. adrenal insufficiency (2 docs) ✗ \n",
      "  20. nocardia (1 docs) ✗ \n",
      "\n",
      "Top 10 rare disease matches for reclassified false positives:\n",
      "  1. congenital tracheomalacia (ORPHA:95430): 1 entities\n",
      "  2. pibids syndrome (ORPHA:670): 1 entities\n",
      "  3. lyme disease (ORPHA:91546): 1 entities\n",
      "  4. fetal cytomegalovirus syndrome (ORPHA:294): 1 entities\n",
      "  5. sclerosing cholangitis (ORPHA:447771): 1 entities\n",
      "  6. kyphoscoliotic ehlers-danlos syndrome (ORPHA:536545): 1 entities\n",
      "  7. rare pulmonary hypertension (ORPHA:71198): 1 entities\n",
      "  8. homocystinuria due to cystathionine beta-synthase deficiency (ORPHA:394): 1 entities\n",
      "  9. epstein-barr virus-positive diffuse large b-cell lymphoma (ORPHA:289661): 1 entities\n",
      "  10. primary central nervous system lymphoma (ORPHA:46135): 1 entities\n",
      "\n",
      "False positive sets:\n",
      "1. All false positives:\n",
      "['acalculous cholecystitis', 'adrenal insufficiency', 'afib', 'aml', 'ampullary mass', 'aortic dissection', 'aplastic anemia', 'apnea of prematurity', 'apnea prematurity', 'arrhythmogenic right ventricular dysplasia', 'asbestosis exposure', 'atrial septal defect', 'autonomic dysfunction', 'calvarial hyperostosis', 'cervical myelopathy', 'cholangitis', 'chronic adrenal insuffiency', 'chronic osteomyelitis', 'ck', 'cmv', 'cns lymphoma', 'complete heart block', 'cryptococcal ag', 'descending thoracic aortic aneursym', 'diffuse pulmonary hemorrhage', 'ebv positive b-cell lymphoma', 'empyema', 'encephalomalacia', 'fasciitis', 'fibrous dysplasia', 'fmf', 'gastroparesis', 'genetic iron storage disease', 'gtc epilepsy', 'hcc', 'hepatic encephalopathy', 'hepatic venous thrombosis', 'hepatocellular ca', 'hiv-associated cardiomyopathy', 'hiv-nephropathy', 'hyperhomocysteinemia', 'in utero cocaine exposure', 'kyphoscoliosis', 'left breast hamartoma', 'linezolid resistant enterococcus abscess in liver', 'lyme', 'lymphangitic disease', 'lymphedema', 'metastatic cns lymphoma', 'microfollicular thyroid adenoma', 'microvascular ischemic disease', 'myeloid hypoplasia', 'necrotizing soft tissue infection', 'necrotizing soft tissue infection of groin', 'necrotizing soft tissue infection of perineum', 'neuromuscular disease', 'nocardia', 'non-epileptic seizure disorder', 'oculogyrate crisis', 'osteonecrosis', 'pacemaker for sick sinus', 'paf', 'pah', 'pancreatic cystic dz.', 'panniculitis', 'pid', 'pneumonia empyema', 'polycythemia', 'polyradiculopathies', 'portal fibrosis', 'portal venous thrombosis', 'primary pulmonary hypertension', 'pud', 'pulmonary arterial systolic hypertension', 'pulmonary artery systolic hypertension', 'respiratory distress syndrome', 'right common iliac artery aneurysm', 'sarcoma', 'sclerosing cholangitis', 'secondary pulmonary hypertension', 'sessile serrated adenoma', 'spontaneous pneumothorax', 'spontaneous ptx', 'sss', 'subarachnoid hemmorhage', 'synovial osteochondromatosis', 'tbi', 'thalassemia', 'toxic megacolon', 'tracheomalacia', 'vasculitis', 'vre pyelonephritis']\n",
      "\n",
      "2. Reclassified false positives:\n",
      "['aml', 'aplastic anemia', 'arrhythmogenic right ventricular dysplasia', 'cmv', 'cns lymphoma', 'ebv positive b-cell lymphoma', 'fibrous dysplasia', 'fmf', 'hyperhomocysteinemia', 'kyphoscoliosis', 'lyme', 'lymphedema', 'pid', 'primary pulmonary hypertension', 'sclerosing cholangitis', 'spontaneous ptx', 'thalassemia', 'tracheomalacia']\n",
      "\n",
      "3. Confirmed false positives (not reclassified):\n",
      "['acalculous cholecystitis', 'adrenal insufficiency', 'afib', 'ampullary mass', 'aortic dissection', 'apnea of prematurity', 'apnea prematurity', 'asbestosis exposure', 'atrial septal defect', 'autonomic dysfunction', 'calvarial hyperostosis', 'cervical myelopathy', 'cholangitis', 'chronic adrenal insuffiency', 'chronic osteomyelitis', 'ck', 'complete heart block', 'cryptococcal ag', 'descending thoracic aortic aneursym', 'diffuse pulmonary hemorrhage', 'empyema', 'encephalomalacia', 'fasciitis', 'gastroparesis', 'genetic iron storage disease', 'gtc epilepsy', 'hcc', 'hepatic encephalopathy', 'hepatic venous thrombosis', 'hepatocellular ca', 'hiv-associated cardiomyopathy', 'hiv-nephropathy', 'in utero cocaine exposure', 'left breast hamartoma', 'linezolid resistant enterococcus abscess in liver', 'lymphangitic disease', 'metastatic cns lymphoma', 'microfollicular thyroid adenoma', 'microvascular ischemic disease', 'myeloid hypoplasia', 'necrotizing soft tissue infection', 'necrotizing soft tissue infection of groin', 'necrotizing soft tissue infection of perineum', 'neuromuscular disease', 'nocardia', 'non-epileptic seizure disorder', 'oculogyrate crisis', 'osteonecrosis', 'pacemaker for sick sinus', 'paf', 'pah', 'pancreatic cystic dz.', 'panniculitis', 'pneumonia empyema', 'polycythemia', 'polyradiculopathies', 'portal fibrosis', 'portal venous thrombosis', 'pud', 'pulmonary arterial systolic hypertension', 'pulmonary artery systolic hypertension', 'respiratory distress syndrome', 'right common iliac artery aneurysm', 'sarcoma', 'secondary pulmonary hypertension', 'sessile serrated adenoma', 'spontaneous pneumothorax', 'sss', 'subarachnoid hemmorhage', 'synovial osteochondromatosis', 'tbi', 'toxic megacolon', 'vasculitis', 'vre pyelonephritis']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Set, Tuple, Optional, Any\n",
    "\n",
    "def analyze_false_positives(\n",
    "    results_file: str,\n",
    "    output_file: Optional[str] = None,\n",
    "    match_summary: bool = True,\n",
    "    detailed: bool = False,\n",
    "    verbose: bool = True,\n",
    "    verify_existence: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze supervised evaluation results to extract information about false positives.\n",
    "    \n",
    "    Args:\n",
    "        results_file: Path to supervised evaluation JSON results file\n",
    "        output_file: Optional path to save analysis results as CSV\n",
    "        match_summary: Whether to generate summary of rare disease matches\n",
    "        detailed: Whether to include detailed information in output\n",
    "        verbose: Whether to print progress and results to console\n",
    "        verify_existence: Whether to verify reclassified entities exist in original text\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    # Load results\n",
    "    if verbose:\n",
    "        print(f\"Loading results from {results_file}\")\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'r') as f:\n",
    "            results = json.load(f)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error loading results file: {str(e)}\"\n",
    "        if verbose:\n",
    "            print(error_msg)\n",
    "        return {\"error\": error_msg}\n",
    "    \n",
    "    # Extract false positives\n",
    "    if verbose:\n",
    "        print(\"Analyzing false positives...\")\n",
    "        \n",
    "    all_fps, reclassified_fps, fp_document_counts, entity_details = _extract_false_positives(results)\n",
    "    \n",
    "    # Verify existence in original text if requested\n",
    "    existence_verification = None\n",
    "    if verify_existence:\n",
    "        if verbose:\n",
    "            print(\"Verifying entity existence in original text...\")\n",
    "        existence_verification = _verify_entities_in_text(results, reclassified_fps)\n",
    "        \n",
    "        if verbose:\n",
    "            verified_count = sum(1 for e in existence_verification['verification_results'].values() \n",
    "                               if e['document_existence_rate'] > 0)\n",
    "            total_count = len(existence_verification['verification_results'])\n",
    "            print(f\"Verified {verified_count}/{total_count} reclassified entities exist in at least one document \"\n",
    "                  f\"({verified_count/total_count*100:.1f}% exist)\")\n",
    "            \n",
    "            # Count entities that exist in all their documents\n",
    "            perfect_count = sum(1 for e in existence_verification['verification_results'].values() \n",
    "                              if e['document_existence_rate'] == 1.0)\n",
    "            print(f\"{perfect_count}/{total_count} entities exist in all their associated documents \"\n",
    "                  f\"({perfect_count/total_count*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_fps = len(all_fps)\n",
    "    total_reclassified = len(reclassified_fps)\n",
    "    reclassification_rate = (total_reclassified / total_fps) * 100 if total_fps > 0 else 0\n",
    "    \n",
    "    # Calculate document frequency statistics\n",
    "    doc_freq = Counter(fp_document_counts.values())\n",
    "    \n",
    "    # Sort false positives by frequency\n",
    "    fps_by_freq = sorted(\n",
    "        [(entity, count) for entity, count in fp_document_counts.items()],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Analyze matches if requested\n",
    "    match_analysis = None\n",
    "    if match_summary:\n",
    "        if verbose:\n",
    "            print(\"Analyzing rare disease matches...\")\n",
    "        match_analysis = _analyze_matches(entity_details)\n",
    "    \n",
    "    # Print summary statistics if verbose\n",
    "    if verbose:\n",
    "        print(\"\\n===== False Positive Analysis =====\")\n",
    "        print(f\"Total unique false positives: {total_fps}\")\n",
    "        print(f\"Reclassified as true positives: {total_reclassified} ({reclassification_rate:.1f}%)\")\n",
    "        \n",
    "        print(\"\\nFalse positive document frequency:\")\n",
    "        for count, freq in sorted(doc_freq.items()):\n",
    "            print(f\"  {count} document{'s' if count > 1 else ''}: {freq} entities\")\n",
    "        \n",
    "        print(\"\\nTop 20 most frequent false positives:\")\n",
    "        for i, (entity, count) in enumerate(fps_by_freq[:20], 1):\n",
    "            reclassified = \"✓\" if entity in reclassified_fps else \"✗\"\n",
    "            # Add existence verification if available\n",
    "            existence_mark = \"\"\n",
    "            existence_info = \"\"\n",
    "            if verify_existence and entity in reclassified_fps:\n",
    "                if entity in existence_verification['verification_results']:\n",
    "                    result = existence_verification['verification_results'][entity]\n",
    "                    exists_rate = result['document_existence_rate'] * 100\n",
    "                    existence_mark = \"📄\" if exists_rate > 0 else \"❌\"\n",
    "                    existence_info = f\" (exists in {exists_rate:.1f}% of docs)\"\n",
    "            print(f\"  {i}. {entity} ({count} docs) {reclassified} {existence_mark}{existence_info}\")\n",
    "        \n",
    "        if match_analysis:\n",
    "            print(\"\\nTop 10 rare disease matches for reclassified false positives:\")\n",
    "            for i, (match, count) in enumerate(match_analysis['match_counts'].most_common(10), 1):\n",
    "                print(f\"  {i}. {match}: {count} entities\")\n",
    "    \n",
    "    # Prepare data for output\n",
    "    if output_file:\n",
    "        if verbose:\n",
    "            print(f\"\\nSaving analysis to {output_file}\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        data = []\n",
    "        for entity in all_fps:\n",
    "            details = entity_details[entity]\n",
    "            row = {\n",
    "                'entity': entity,\n",
    "                'document_count': fp_document_counts[entity],\n",
    "                'reclassified': entity in reclassified_fps,\n",
    "                'document_ids': ','.join(details['documents']),\n",
    "            }\n",
    "            \n",
    "            # Add existence verification if available\n",
    "            if verify_existence and entity in reclassified_fps and entity in existence_verification['verification_results']:\n",
    "                result = existence_verification['verification_results'][entity]\n",
    "                row['existence_rate'] = result['document_existence_rate']\n",
    "                row['documents_exists_in'] = ','.join(result['documents_exists_in'])\n",
    "                row['documents_missing_from'] = ','.join(result['documents_missing_from'])\n",
    "            \n",
    "            # Add match information if available and entity was reclassified\n",
    "            if match_summary and entity in reclassified_fps and entity in match_analysis.get('entity_to_match', {}):\n",
    "                row['matched_to'] = match_analysis['entity_to_match'][entity]\n",
    "            \n",
    "            data.append(row)\n",
    "        \n",
    "        # Convert to DataFrame and sort\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df.sort_values('document_count', ascending=False)\n",
    "        \n",
    "        # Save to CSV\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Analysis saved to {output_file}\")\n",
    "    \n",
    "    # Prepare return value with FP sets and statistics\n",
    "    confirmed_fps = all_fps - reclassified_fps\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nFalse positive sets:\")\n",
    "        print(\"1. All false positives:\")\n",
    "        print(sorted(all_fps))\n",
    "        \n",
    "        print(\"\\n2. Reclassified false positives:\")\n",
    "        print(sorted(reclassified_fps))\n",
    "        \n",
    "        print(\"\\n3. Confirmed false positives (not reclassified):\")\n",
    "        print(sorted(confirmed_fps))\n",
    "        \n",
    "        if verify_existence:\n",
    "            # Entities that don't exist in any documents\n",
    "            nonexistent = [e for e, v in existence_verification['verification_results'].items() \n",
    "                          if v['document_existence_rate'] == 0]\n",
    "            if nonexistent:\n",
    "                print(\"\\n4. Reclassified entities that don't exist in ANY of their documents:\")\n",
    "                print(sorted(nonexistent))\n",
    "            \n",
    "            # Entities that exist in some but not all documents\n",
    "            partially_existent = [e for e, v in existence_verification['verification_results'].items() \n",
    "                                if 0 < v['document_existence_rate'] < 1]\n",
    "            if partially_existent:\n",
    "                print(\"\\n5. Reclassified entities that exist in SOME but not ALL of their documents:\")\n",
    "                for entity in sorted(partially_existent):\n",
    "                    result = existence_verification['verification_results'][entity]\n",
    "                    print(f\"  - {entity}\")\n",
    "                    print(f\"    ✓ Exists in: {', '.join(result['documents_exists_in'])}\")\n",
    "                    print(f\"    ✗ Missing from: {', '.join(result['documents_missing_from'])}\")\n",
    "    \n",
    "    # Return analysis results\n",
    "    analysis_results = {\n",
    "        'statistics': {\n",
    "            'total_fps': total_fps,\n",
    "            'reclassified_fps': total_reclassified,\n",
    "            'reclassification_rate': reclassification_rate,\n",
    "            'document_frequency': dict(doc_freq)\n",
    "        },\n",
    "        'sets': {\n",
    "            'all_fps': sorted(list(all_fps)),\n",
    "            'reclassified_fps': sorted(list(reclassified_fps)),\n",
    "            'confirmed_fps': sorted(list(confirmed_fps))\n",
    "        },\n",
    "        'entity_details': entity_details if detailed else None,\n",
    "        'top_fps': fps_by_freq[:50],  # Top 50 most frequent FPs\n",
    "        'match_analysis': match_analysis,\n",
    "        'existence_verification': existence_verification\n",
    "    }\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "def _extract_false_positives(results: Dict) -> Tuple[Set[str], Set[str], Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Extract all false positives and reclassified false positives.\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from supervised evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - Set of all original false positive entities\n",
    "        - Set of reclassified false positive entities\n",
    "        - Dictionary mapping entity to document count\n",
    "        - Dictionary with entity details\n",
    "    \"\"\"\n",
    "    all_fps = set()\n",
    "    reclassified_fps = set()\n",
    "    fp_document_counts = {}  # Track how many documents each FP appears in\n",
    "    entity_details = {}      # Track detailed info for each entity\n",
    "    \n",
    "    # Process each document\n",
    "    for doc_id, doc_results in results['document_results'].items():\n",
    "        # Process original false positives\n",
    "        for fp in doc_results.get('old_false_positives', []):\n",
    "            entity = fp['entity'].lower()\n",
    "            all_fps.add(entity)\n",
    "            \n",
    "            # Update document count\n",
    "            fp_document_counts[entity] = fp_document_counts.get(entity, 0) + 1\n",
    "            \n",
    "            # Store entity details\n",
    "            if entity not in entity_details:\n",
    "                entity_details[entity] = {\n",
    "                    'entity': entity,\n",
    "                    'documents': [],\n",
    "                    'reclassified': False,\n",
    "                    'matches': []\n",
    "                }\n",
    "            \n",
    "            entity_details[entity]['documents'].append(doc_id)\n",
    "        \n",
    "        # Process reclassified false positives\n",
    "        for fp in doc_results.get('new_true_positives', []):\n",
    "            entity = fp['entity'].lower()\n",
    "            reclassified_fps.add(entity)\n",
    "            \n",
    "            # Update entity details\n",
    "            if entity in entity_details:\n",
    "                entity_details[entity]['reclassified'] = True\n",
    "                \n",
    "                # Store match details if available\n",
    "                if 'rd_term' in fp and 'orpha_id' in fp:\n",
    "                    match = {\n",
    "                        'rd_term': fp['rd_term'],\n",
    "                        'orpha_id': fp['orpha_id']\n",
    "                    }\n",
    "                    entity_details[entity]['matches'].append(match)\n",
    "    \n",
    "    return all_fps, reclassified_fps, fp_document_counts, entity_details\n",
    "\n",
    "def _verify_entities_in_text(results: Dict, reclassified_fps: Set[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Verify if reclassified entities actually exist in the original text of each document.\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from supervised evaluation\n",
    "        reclassified_fps: Set of reclassified false positive entities\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with verification results\n",
    "    \"\"\"\n",
    "    verification_results = {}\n",
    "    entity_doc_map = defaultdict(list)\n",
    "    \n",
    "    # Create a mapping of entity to documents\n",
    "    for doc_id, doc_results in results['document_results'].items():\n",
    "        # Get all documents where an entity was classified as a false positive\n",
    "        for fp in doc_results.get('old_false_positives', []):\n",
    "            entity = fp['entity'].lower()\n",
    "            if entity in reclassified_fps:\n",
    "                entity_doc_map[entity].append(doc_id)\n",
    "    \n",
    "    # Check each reclassified entity in each of its documents\n",
    "    for entity in reclassified_fps:\n",
    "        # Get all documents this entity appears in\n",
    "        docs = entity_doc_map.get(entity, [])\n",
    "        \n",
    "        # Track documents where entity exists vs. doesn't exist\n",
    "        exists_in_docs = []\n",
    "        missing_from_docs = []\n",
    "        \n",
    "        if not docs:\n",
    "            verification_results[entity] = {\n",
    "                'document_existence_rate': 0,\n",
    "                'documents_exists_in': [],\n",
    "                'documents_missing_from': [],\n",
    "                'doc_count': 0\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        # Check each document for this entity\n",
    "        for doc_id in docs:\n",
    "            doc_text = results['document_results'][doc_id].get('original_text', '')\n",
    "            if not doc_text:\n",
    "                missing_from_docs.append(doc_id)  # No text available\n",
    "                continue\n",
    "                \n",
    "            if _entity_exists_in_text(entity, doc_text):\n",
    "                exists_in_docs.append(doc_id)\n",
    "            else:\n",
    "                missing_from_docs.append(doc_id)\n",
    "        \n",
    "        # Calculate existence rate for this entity\n",
    "        total_docs = len(docs)\n",
    "        existence_rate = len(exists_in_docs) / total_docs if total_docs > 0 else 0\n",
    "        \n",
    "        verification_results[entity] = {\n",
    "            'document_existence_rate': existence_rate,\n",
    "            'documents_exists_in': exists_in_docs,\n",
    "            'documents_missing_from': missing_from_docs,\n",
    "            'doc_count': total_docs\n",
    "        }\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    total_verified = len(verification_results)\n",
    "    exists_in_any_count = sum(1 for result in verification_results.values() \n",
    "                           if result['document_existence_rate'] > 0)\n",
    "    exists_in_all_count = sum(1 for result in verification_results.values() \n",
    "                           if result['document_existence_rate'] == 1.0)\n",
    "    missing_count = sum(1 for result in verification_results.values() \n",
    "                      if result['document_existence_rate'] == 0)\n",
    "    \n",
    "    return {\n",
    "        'verification_results': verification_results,\n",
    "        'summary': {\n",
    "            'total_verified': total_verified,\n",
    "            'exists_in_any_count': exists_in_any_count,\n",
    "            'exists_in_all_count': exists_in_all_count,\n",
    "            'missing_count': missing_count,\n",
    "            'exists_in_any_rate': (exists_in_any_count / total_verified * 100) if total_verified else 0,\n",
    "            'exists_in_all_rate': (exists_in_all_count / total_verified * 100) if total_verified else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "def _entity_exists_in_text(entity: str, text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if an entity exists in the original text.\n",
    "    \n",
    "    Args:\n",
    "        entity: Entity text to search for\n",
    "        text: Original text to search in\n",
    "        \n",
    "    Returns:\n",
    "        True if entity is found in text, False otherwise\n",
    "    \"\"\"\n",
    "    # Normalize both entity and text for more accurate matching\n",
    "    entity_normalized = entity.lower()\n",
    "    text_normalized = text.lower()\n",
    "    \n",
    "    # Check for exact match\n",
    "    if entity_normalized in text_normalized:\n",
    "        return True\n",
    "        \n",
    "    # Check for case where entity has punctuation that might be different in text\n",
    "    # Strip punctuation and check again\n",
    "    entity_no_punct = re.sub(r'[^\\w\\s]', '', entity_normalized)\n",
    "    if entity_no_punct and entity_no_punct in text_normalized:\n",
    "        return True\n",
    "        \n",
    "    # Check for other common formatting issues\n",
    "    # Try splitting entity on common separators and check if parts exist together\n",
    "    parts = re.split(r'[-/,\\s]+', entity_normalized)\n",
    "    if len(parts) > 1:\n",
    "        # Check if all parts appear within a reasonable window of each other\n",
    "        all_parts_present = True\n",
    "        for part in parts:\n",
    "            if part and len(part) > 2 and part not in text_normalized:  # Only check meaningful parts\n",
    "                all_parts_present = False\n",
    "                break\n",
    "                \n",
    "        if all_parts_present:\n",
    "            return True\n",
    "            \n",
    "    return False\n",
    "\n",
    "def _analyze_matches(entity_details: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the matches for reclassified false positives.\n",
    "    \n",
    "    Args:\n",
    "        entity_details: Dictionary with entity details\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with match statistics\n",
    "    \"\"\"\n",
    "    match_counts = Counter()\n",
    "    entity_to_match = {}\n",
    "    \n",
    "    for entity, details in entity_details.items():\n",
    "        if details['reclassified'] and details['matches']:\n",
    "            # Use the most common match for this entity\n",
    "            matches = [f\"{m['rd_term']} ({m['orpha_id']})\" for m in details['matches']]\n",
    "            most_common_match = Counter(matches).most_common(1)[0][0]\n",
    "            \n",
    "            match_counts[most_common_match] += 1\n",
    "            entity_to_match[entity] = most_common_match\n",
    "    \n",
    "    return {\n",
    "        'match_counts': match_counts,\n",
    "        'entity_to_match': entity_to_match\n",
    "    }\n",
    "\n",
    "# Example usage as a script if needed\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    # def parse_arguments() -> argparse.Namespace:\n",
    "    #     \"\"\"Parse command line arguments.\"\"\"\n",
    "    #     parser = argparse.ArgumentParser(\n",
    "    #         description='Analyze supervised evaluation results to extract false positive information'\n",
    "    #     )\n",
    "    #     parser.add_argument('--results_file', type=str, required=True,\n",
    "    #                       help='Path to supervised evaluation JSON results file')\n",
    "    #     parser.add_argument('--output_file', type=str,\n",
    "    #                       help='Path to save analysis results (CSV)')\n",
    "    #     parser.add_argument('--detailed', action='store_true',\n",
    "    #                       help='Include detailed information in output')\n",
    "    #     parser.add_argument('--match_summary', action='store_true',\n",
    "    #                       help='Generate summary of rare disease matches')\n",
    "    #     parser.add_argument('--quiet', action='store_true',\n",
    "    #                       help='Do not print progress and results')\n",
    "        \n",
    "    #     return parser.parse_args()\n",
    "    \n",
    "    # args = parse_arguments()\n",
    "    results_file = \"data/results/agents/rd/rd_mistral24b_medembed_supervised.json\"\n",
    "    verbose = True \n",
    "    detailed = True \n",
    "    match_summary = True\n",
    "    # Call the main function\n",
    "    results = analyze_false_positives(\n",
    "        results_file=results_file,\n",
    "        verify_existence=True\n",
    "    )\n",
    "    results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 'pyruvate kinase deficiency' missing from documents: 52501\n",
      "Entity 'thrombotic thrombocytopenic purpura' missing from documents: 52501\n",
      "Entity 'hemolytic uremic syndrome' missing from documents: 52501\n",
      "Entity 'paroxysmal nocturnal hemoglobinuria' missing from documents: 52501\n",
      "Entity 'restrictive cardiomyopathy' missing from documents: 52501\n",
      "Entity 'beta thalassemia' missing from documents: 52501\n",
      "Entity 'guillain-barre syndrome' missing from documents: 38775\n",
      "Entity 'myelomeningocele' missing from documents: 53406\n"
     ]
    }
   ],
   "source": [
    "for entity, details in results['existence_verification']['verification_results'].items():\n",
    "    if details['document_existence_rate'] < 1.0:\n",
    "        print(f\"Entity '{entity}' missing from documents: {', '.join(details['documents_missing_from'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking again if there are any false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Total documents: 117\n",
      "Documents with annotations: 117\n",
      "Total annotations: 333\n",
      "Document categories: {'Discharge summary': 117}\n",
      "\n",
      "===== ERROR ANALYSIS =====\n",
      "True positives: 54\n",
      "False positives: 137\n",
      "False negatives: 59\n",
      "Precision: 0.283\n",
      "Recall: 0.478\n",
      "F1 Score: 0.355\n",
      "{'myelomeningocele', 'spontaneous pneumothorax', 'pancreatic cystic dz.', 'myeloproliferative neoplasm', 'polycythemia', 'moyamoya disease', 'mitral valve stenosis', 'malignant salivary gland neoplasms', 'tbi', 'creat', 'necrotizing soft tissue infection', 'adrenal insufficiency', 'intestinal necrosis', 'vre pyelonephritis', 'nocardia', 'vascular malformation', 'fmf', 'thrombotic thrombocytopenic purpura', 'thalassemia', 'ebv positive b-cell lymphoma', 'eosinophilic granulomatosis with polyangiitis', 'cryptococcal ag', 'apnea prematurity', 'sessile serrated adenoma', 'giant cell arteritis', 'ampullary intestinal adenoma', 'esophageal candidiasis', 'sss', 'vogt-koyanagi syndrome', 'juvenile idiopathic arthritis', 'churg-strauss syndrome', 'asbestosis exposure', 'hyperhomocysteinemia', 'cervical myelopathy', 'in utero cocaine exposure', 'sarcoma', 'primary pulmonary hypertension', 'methylenetetrahydrofolate reductase mutation', 'scleroderma', 'strep viridans endocarditis', 'oncology/systemic lymphoma', 'pulmonary arterial systolic hypertension', \"kaposi's sarcoma\", 'arteriovenous malformation', 'disseminated intravascular coagulation', 'mitral stenosis', 'enteropathic arthritis', 'apnea of prematurity', 'arrhythmogenic right ventricular dysplasia', 'proctitis', \"raynaud's disease\", 'fibrous dysplasia', 'synovial osteochondromatosis', 'left ventricular aneurysm', 'protein s deficiency', 'aortic valve stenosis', 'atrial septal defect', 'antiphospholipid syndrome', 'ttp-hus', 'hiv-nephropathy', 'metastatic cns lymphoma', 'empyema', 'thromboangiitis obliterans', 'hcc', 'necrotizing soft tissue infection of perineum', 'factor v leiden', 'spindle cell carcinomas', 'necrotizing soft tissue infection of groin', 'hypoplastic kidney', 'relapsing polychondritis', 'hypophysitis', 'protein c deficiency', 'right common iliac artery aneurysm', 'vascular ectasia', 'cholangitis', 'aml', 'tracheomalacia', 'guillain-barre syndrome', 'microscopic polyangiitis', 'calvarial hyperostosis', 'descending thoracic aortic aneursym', 'unconjugated hyperbilirubinemia', 'sick sinus', \"buerger's disease\", 'mixed connective tissue disease', 'fasciitis', 'lyme', 'ck', 'hiv-associated cardiomyopathy', 'temporal arteritis', 'gastroparesis', 'diffuse pulmonary hemorrhage', 'agranulocytosis', 'hemolytic uremic syndrome', 'portal venous thrombosis', 'polymyositis', 'pah', 'subarachnoid hemmorhage', 'takayasu arteritis', 'erythromelalgia', 'fungal panniculitis', 'paf', 'hepatocellular ca', 'melanotic skin lesion', 'secondary pulmonary hypertension', 'acrocyanosis', 'spontaneous ptx', 'sclerosing cholangitis', 'polyradiculopathies', 'telangiectasia', 'spontaneous bacterial peritonitis', 'cmv', 'hepatic venous thrombosis', 'kawasaki disease', 'neuromuscular disease', 'antithrombin deficiency', 'henoch-schonlein purpura', 'hepatosplenic candidiasis', 'gtc epilepsy', 'aplastic anemia', 'osteonecrosis', 'cryoglobulinemic vasculitis', 'dermatomyositis', 'lymphangioma', 'myelodysplastic syndrome', 'phlegmasia alba dolens', 'hemophagocytic syndrome', 'biliomas', 'systemic lupus erythematosus', 'microvascular ischemic disease', 'chronic adrenal insuffiency', 'respiratory distress syndrome', 'linezolid resistant enterococcus abscess in liver', 'ams', 'genetic iron storage disease', 'phlegmasia cerulea dolens', 'cns lymphoma'}\n",
      "{'ema', 'hypothyroidism secondary', 'papillary carcinoma', 'alopecia', 'polymyalgia rheumatica', 'epilepsy', 'sarcoid', 'complex tracheal stenosis', 'glioblastoma multiforme', 'her disease', 'budd-chiari syndrome', 'hit', \"lyme's disease\", 'sclerosis cholangitis', 'sick sinus syndrome', 'microcytic anemia', 'heparin induced thrombocytopenia', 'congenital bleeding disorder', 'central nervous system and systemic lymphoma', 'legionella', 'tetanus', 'endometriosis', 'bronchiectasis', 'protein s', 'scds', 'primary adrenal insufficiency', 'nph', 'epileptic seizures', 'thymoma', 'fulminant liver failure', 'hepatocellular carcinoma', 'pulmonary arterial hypertension', 'hypogammaglobulinemia', 'protein c', 'central retinal vein occlusion', 'neovascular glaucoma', 'iron storage disease', 'lyme disease', 'ttp', 'epileptic seizure', \"alport's syndrome\", 'mediastinitis', 'hyperthyroidism', 'recurrent fever', 'epileptic', 'meningocele', 'seizure disorder', 'transitional cell carcinoma', 'rheumatic fever', 'pmr', 'primary cns lymphoma', 'portal vein thrombosis', 'acute hepatic failure', 'hemochromatosis', 'beta-thalassemia', 'antiphospholipid antibody syndrome', \"bechet's disease\", 'beta thalassemia', 'essential thrombocytosis'}\n",
      "\n",
      "----- TRUE POSITIVES -----\n",
      "  acute myelogenous leukemia\n",
      "  alkaptonuria\n",
      "  alport syndrome\n",
      "  als\n",
      "  amyloidosis\n",
      "  amyotrophic lateral sclerosis\n",
      "  anaplastic thyroid carcinoma\n",
      "  arachnoid cyst\n",
      "  asbestosis\n",
      "  autoimmune hepatitis\n",
      "  autoimmune pancreatitis\n",
      "  babesiosis\n",
      "  budd chiari syndrome\n",
      "  budd-chiari\n",
      "  bullous pemphigoid\n",
      "  calciphylaxis\n",
      "  cervical stenosis\n",
      "  crest syndrome\n",
      "  dilated cardiomyopathy\n",
      "  essential thrombocythemia\n",
      "  familial mediterranean fever\n",
      "  fulminant hepatic failure\n",
      "  hemochromatosis\n",
      "  heparin induced thrombocytopenia\n",
      "  heparin-induced thrombocytopenia\n",
      "  hepatocellular carcinoma\n",
      "  hit\n",
      "  hyperparathyroidism\n",
      "  intraductal papillary mucinous tumor\n",
      "  legionella\n",
      "  mediastinitis\n",
      "  medullary sponge kidney\n",
      "  mesenteric vein thrombosis\n",
      "  methemoglobinemia\n",
      "  multifocal atrial tachycardia\n",
      "  multiple myeloma\n",
      "  necrotizing enterocolitis\n",
      "  neovascular glaucoma angle closure\n",
      "  nocardiosis\n",
      "  pml\n",
      "  portal vein thrombosis\n",
      "  post polio syndrome\n",
      "  primary sclerosing cholangitis\n",
      "  protein c deficiency\n",
      "  pulmonary arterial hypertension\n",
      "  pyoderma gangrenosum\n",
      "  retinitis pigmentosa\n",
      "  retinopathy of prematurity\n",
      "  sarcoid\n",
      "  sarcoidosis\n",
      "  sick sinus syndrome\n",
      "  thrombotic thrombocytopenic purpura\n",
      "  tracheal stenosis\n",
      "  tracheobronchomalacia\n",
      "\n",
      "----- FALSE POSITIVES -----\n",
      "  acrocyanosis\n",
      "  adrenal insufficiency\n",
      "  agranulocytosis\n",
      "  aml\n",
      "  ampullary intestinal adenoma\n",
      "  ams\n",
      "  antiphospholipid syndrome\n",
      "  antithrombin deficiency\n",
      "  aortic valve stenosis\n",
      "  aplastic anemia\n",
      "  apnea of prematurity\n",
      "  apnea prematurity\n",
      "  arrhythmogenic right ventricular dysplasia\n",
      "  arteriovenous malformation\n",
      "  asbestosis exposure\n",
      "  atrial septal defect\n",
      "  biliomas\n",
      "  buerger's disease\n",
      "  calvarial hyperostosis\n",
      "  cervical myelopathy\n",
      "  cholangitis\n",
      "  chronic adrenal insuffiency\n",
      "  churg-strauss syndrome\n",
      "  ck\n",
      "  cmv\n",
      "  cns lymphoma\n",
      "  creat\n",
      "  cryoglobulinemic vasculitis\n",
      "  cryptococcal ag\n",
      "  dermatomyositis\n",
      "  descending thoracic aortic aneursym\n",
      "  diffuse pulmonary hemorrhage\n",
      "  disseminated intravascular coagulation\n",
      "  ebv positive b-cell lymphoma\n",
      "  empyema\n",
      "  enteropathic arthritis\n",
      "  eosinophilic granulomatosis with polyangiitis\n",
      "  erythromelalgia\n",
      "  esophageal candidiasis\n",
      "  factor v leiden\n",
      "  fasciitis\n",
      "  fibrous dysplasia\n",
      "  fmf\n",
      "  fungal panniculitis\n",
      "  gastroparesis\n",
      "  genetic iron storage disease\n",
      "  giant cell arteritis\n",
      "  gtc epilepsy\n",
      "  guillain-barre syndrome\n",
      "  hcc\n",
      "  hemolytic uremic syndrome\n",
      "  hemophagocytic syndrome\n",
      "  henoch-schonlein purpura\n",
      "  hepatic venous thrombosis\n",
      "  hepatocellular ca\n",
      "  hepatosplenic candidiasis\n",
      "  hiv-associated cardiomyopathy\n",
      "  hiv-nephropathy\n",
      "  hyperhomocysteinemia\n",
      "  hypophysitis\n",
      "  hypoplastic kidney\n",
      "  in utero cocaine exposure\n",
      "  intestinal necrosis\n",
      "  juvenile idiopathic arthritis\n",
      "  kaposi's sarcoma\n",
      "  kawasaki disease\n",
      "  left ventricular aneurysm\n",
      "  linezolid resistant enterococcus abscess in liver\n",
      "  lyme\n",
      "  lymphangioma\n",
      "  malignant salivary gland neoplasms\n",
      "  melanotic skin lesion\n",
      "  metastatic cns lymphoma\n",
      "  methylenetetrahydrofolate reductase mutation\n",
      "  microscopic polyangiitis\n",
      "  microvascular ischemic disease\n",
      "  mitral stenosis\n",
      "  mitral valve stenosis\n",
      "  mixed connective tissue disease\n",
      "  moyamoya disease\n",
      "  myelodysplastic syndrome\n",
      "  myelomeningocele\n",
      "  myeloproliferative neoplasm\n",
      "  necrotizing soft tissue infection\n",
      "  necrotizing soft tissue infection of groin\n",
      "  necrotizing soft tissue infection of perineum\n",
      "  neuromuscular disease\n",
      "  nocardia\n",
      "  oncology/systemic lymphoma\n",
      "  osteonecrosis\n",
      "  paf\n",
      "  pah\n",
      "  pancreatic cystic dz.\n",
      "  phlegmasia alba dolens\n",
      "  phlegmasia cerulea dolens\n",
      "  polycythemia\n",
      "  polymyositis\n",
      "  polyradiculopathies\n",
      "  portal venous thrombosis\n",
      "  primary pulmonary hypertension\n",
      "  proctitis\n",
      "  protein c deficiency\n",
      "  protein s deficiency\n",
      "  pulmonary arterial systolic hypertension\n",
      "  raynaud's disease\n",
      "  relapsing polychondritis\n",
      "  respiratory distress syndrome\n",
      "  right common iliac artery aneurysm\n",
      "  sarcoma\n",
      "  scleroderma\n",
      "  sclerosing cholangitis\n",
      "  secondary pulmonary hypertension\n",
      "  sessile serrated adenoma\n",
      "  sick sinus\n",
      "  spindle cell carcinomas\n",
      "  spontaneous bacterial peritonitis\n",
      "  spontaneous pneumothorax\n",
      "  spontaneous ptx\n",
      "  sss\n",
      "  strep viridans endocarditis\n",
      "  subarachnoid hemmorhage\n",
      "  synovial osteochondromatosis\n",
      "  systemic lupus erythematosus\n",
      "  takayasu arteritis\n",
      "  tbi\n",
      "  telangiectasia\n",
      "  temporal arteritis\n",
      "  thalassemia\n",
      "  thromboangiitis obliterans\n",
      "  thrombotic thrombocytopenic purpura\n",
      "  tracheomalacia\n",
      "  ttp-hus\n",
      "  unconjugated hyperbilirubinemia\n",
      "  vascular ectasia\n",
      "  vascular malformation\n",
      "  vogt-koyanagi syndrome\n",
      "  vre pyelonephritis\n",
      "\n",
      "----- FALSE NEGATIVES -----\n",
      "  acute hepatic failure\n",
      "  alopecia\n",
      "  alport's syndrome\n",
      "  antiphospholipid antibody syndrome\n",
      "  bechet's disease\n",
      "  beta thalassemia\n",
      "  beta-thalassemia\n",
      "  bronchiectasis\n",
      "  budd-chiari syndrome\n",
      "  central nervous system and systemic lymphoma\n",
      "  central retinal vein occlusion\n",
      "  complex tracheal stenosis\n",
      "  congenital bleeding disorder\n",
      "  ema\n",
      "  endometriosis\n",
      "  epilepsy\n",
      "  epileptic\n",
      "  epileptic seizure\n",
      "  epileptic seizures\n",
      "  essential thrombocytosis\n",
      "  fulminant liver failure\n",
      "  glioblastoma multiforme\n",
      "  hemochromatosis\n",
      "  heparin induced thrombocytopenia\n",
      "  hepatocellular carcinoma\n",
      "  her disease\n",
      "  hit\n",
      "  hyperthyroidism\n",
      "  hypogammaglobulinemia\n",
      "  hypothyroidism secondary\n",
      "  iron storage disease\n",
      "  legionella\n",
      "  lyme disease\n",
      "  lyme's disease\n",
      "  mediastinitis\n",
      "  meningocele\n",
      "  microcytic anemia\n",
      "  neovascular glaucoma\n",
      "  nph\n",
      "  papillary carcinoma\n",
      "  pmr\n",
      "  polymyalgia rheumatica\n",
      "  portal vein thrombosis\n",
      "  primary adrenal insufficiency\n",
      "  primary cns lymphoma\n",
      "  protein c\n",
      "  protein s\n",
      "  pulmonary arterial hypertension\n",
      "  recurrent fever\n",
      "  rheumatic fever\n",
      "  sarcoid\n",
      "  scds\n",
      "  sclerosis cholangitis\n",
      "  seizure disorder\n",
      "  sick sinus syndrome\n",
      "  tetanus\n",
      "  thymoma\n",
      "  transitional cell carcinoma\n",
      "  ttp\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "def process_mimic_json(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Process MIMIC-style JSON with annotations.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to JSON file containing clinical notes with annotations\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with processed notes and annotations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load JSON data\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Process each document\n",
    "        records = []\n",
    "        for doc_id, doc_data in data.items():\n",
    "            if 'note_details' not in doc_data:\n",
    "                continue\n",
    "                \n",
    "            note_details = doc_data['note_details']\n",
    "            annotations = doc_data.get('annotations', [])\n",
    "            \n",
    "            # Extract relevant fields\n",
    "            record = {\n",
    "                'document_id': doc_id,\n",
    "                'patient_id': note_details.get('subject_id'),\n",
    "                'admission_id': note_details.get('hadm_id'),\n",
    "                'category': note_details.get('category'),\n",
    "                'chart_date': note_details.get('chartdate'),\n",
    "                'clinical_note': note_details.get('text', ''),\n",
    "                'gold_annotations': []\n",
    "            }\n",
    "            \n",
    "            # Process all annotations that have a mention\n",
    "            for ann in annotations:\n",
    "                if ann.get('mention'):  # Include any annotation with a mention\n",
    "                    gold_annotation = {\n",
    "                        'mention': ann['mention'],\n",
    "                        'orpha_id': ann.get('ordo_with_desc', '').split()[0] if ann.get('ordo_with_desc') else '',\n",
    "                        'orpha_desc': ' '.join(ann.get('ordo_with_desc', '').split()[1:]) if ann.get('ordo_with_desc') else '',\n",
    "                        'document_section': ann.get('document_structure'),\n",
    "                        'confidence': 1.0\n",
    "                    }\n",
    "                    record['gold_annotations'].append(gold_annotation)\n",
    "            \n",
    "            records.append(record)\n",
    "            \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(records)\n",
    "        \n",
    "        # Basic validation and cleaning\n",
    "        df['clinical_note'] = df['clinical_note'].astype(str)\n",
    "        df = df.dropna(subset=['clinical_note'])\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        print(f\"\\nDataset Statistics:\")\n",
    "        print(f\"Total documents: {len(df)}\")\n",
    "        print(f\"Documents with annotations: {len(df[df['gold_annotations'].str.len() > 0])}\")\n",
    "        print(f\"Total annotations: {sum(df['gold_annotations'].str.len())}\")\n",
    "        print(f\"Document categories: {df['category'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSON file: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_error_sets(predictions_df: pd.DataFrame, gold_df: pd.DataFrame) -> Dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    Get sets of all false positives, false negatives, and true positives.\n",
    "    \n",
    "    Args:\n",
    "        predictions_df: DataFrame with predicted entities\n",
    "        gold_df: DataFrame with gold standard annotations from process_mimic_json\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing sets of all false positives, false negatives, and true positives\n",
    "    \"\"\"\n",
    "    # Convert document_id to string in both DataFrames for consistent comparison\n",
    "    predictions_df = predictions_df.copy()\n",
    "    gold_df = gold_df.copy()\n",
    "    predictions_df['document_id'] = predictions_df['document_id'].astype(str)\n",
    "    gold_df['document_id'] = gold_df['document_id'].astype(str)\n",
    "    \n",
    "    # Initialize sets to collect all errors and correct predictions\n",
    "    all_true_positives = set()\n",
    "    all_false_positives = set()\n",
    "    all_false_negatives = set()\n",
    "    \n",
    "    # Document-level tracking for detailed analysis\n",
    "    doc_error_counts = {}\n",
    "    \n",
    "    # Process all documents\n",
    "    for _, gold_row in gold_df.iterrows():\n",
    "        doc_id = gold_row['document_id']\n",
    "        gold_anns = gold_row['gold_annotations']\n",
    "        \n",
    "        # Create gold standard set\n",
    "        gold_entities = {ann['mention'].lower() for ann in gold_anns}\n",
    "        \n",
    "        # Get all predictions for this document\n",
    "        doc_preds = predictions_df[predictions_df['document_id'] == doc_id]\n",
    "        \n",
    "        # Collect all predictions for this document\n",
    "        pred_entities = set()\n",
    "        \n",
    "        if not doc_preds.empty:\n",
    "            for _, pred_row in doc_preds.iterrows():\n",
    "                entity = pred_row.get('entity', '').lower() if pd.notna(pred_row.get('entity')) else ''\n",
    "                if entity:\n",
    "                    pred_entities.add(entity)\n",
    "        \n",
    "        # Calculate document-specific sets\n",
    "        true_pos = gold_entities.intersection(pred_entities)\n",
    "        false_pos = pred_entities - gold_entities\n",
    "        false_neg = gold_entities - pred_entities\n",
    "        \n",
    "        # Track document-level error counts\n",
    "        doc_error_counts[doc_id] = {\n",
    "            'tp': len(true_pos),\n",
    "            'fp': len(false_pos),\n",
    "            'fn': len(false_neg)\n",
    "        }\n",
    "        \n",
    "        # Update global sets\n",
    "        all_true_positives.update(true_pos)\n",
    "        all_false_positives.update(false_pos)\n",
    "        all_false_negatives.update(false_neg)\n",
    "    \n",
    "    # Return all error sets\n",
    "    result = {\n",
    "        'true_positives': all_true_positives,\n",
    "        'false_positives': all_false_positives,\n",
    "        'false_negatives': all_false_negatives,\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def print_error_analysis(error_sets: Dict[str, Set[str]]):\n",
    "    \"\"\"\n",
    "    Print detailed analysis of error sets.\n",
    "    \n",
    "    Args:\n",
    "        error_sets: Dict containing sets of true positives, false positives, and false negatives\n",
    "    \"\"\"\n",
    "    # Print counts\n",
    "    print(\"\\n===== ERROR ANALYSIS =====\")\n",
    "    print(f\"True positives: {len(error_sets['true_positives'])}\")\n",
    "    print(f\"False positives: {len(error_sets['false_positives'])}\")\n",
    "    print(f\"False negatives: {len(error_sets['false_negatives'])}\")\n",
    "    \n",
    "    # Calculate precision, recall, F1\n",
    "    tp_count = len(error_sets['true_positives'])\n",
    "    fp_count = len(error_sets['false_positives'])\n",
    "    fn_count = len(error_sets['false_negatives'])\n",
    "    \n",
    "    precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0\n",
    "    recall = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    print(error_sets[\"false_positives\"])\n",
    "    print(error_sets[\"false_negatives\"])\n",
    "    # Print entity lists\n",
    "    print(\"\\n----- TRUE POSITIVES -----\")\n",
    "    for entity in sorted(error_sets['true_positives']):\n",
    "        print(f\"  {entity}\")\n",
    "    \n",
    "    print(\"\\n----- FALSE POSITIVES -----\")\n",
    "    for entity in sorted(error_sets['false_positives']):\n",
    "        print(f\"  {entity}\")\n",
    "    \n",
    "    print(\"\\n----- FALSE NEGATIVES -----\")\n",
    "    for entity in sorted(error_sets['false_negatives']):\n",
    "        print(f\"  {entity}\")\n",
    "\n",
    "# Example of usage in a Jupyter notebook\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Process gold standard data\n",
    "gold_df = process_mimic_json(\"data/dataset/filtered_rd_annos_updated_adam.json\")\n",
    "\n",
    "# Load predictions\n",
    "predictions_df = pd.read_csv(\"data/results/rd_llama70b_fastembed.csv\")\n",
    "\n",
    "# Get error sets\n",
    "error_sets = get_error_sets(predictions_df, gold_df)\n",
    "\n",
    "# Print analysis\n",
    "print_error_analysis(error_sets)\n",
    "\n",
    "# To save to files for further analysis\n",
    "# import csv\n",
    "\n",
    "# # Save true positives\n",
    "# with open('true_positives.txt', 'w') as f:\n",
    "#     for entity in sorted(error_sets['true_positives']):\n",
    "#         f.write(f\"{entity}\\n\")\n",
    "\n",
    "# # Save false positives  \n",
    "# with open('false_positives.txt', 'w') as f:\n",
    "#     for entity in sorted(error_sets['false_positives']):\n",
    "#         f.write(f\"{entity}\\n\")\n",
    "        \n",
    "# # Save false negatives\n",
    "# with open('false_negatives.txt', 'w') as f:\n",
    "#     for entity in sorted(error_sets['false_negatives']):\n",
    "#         f.write(f\"{entity}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Total documents: 117\n",
      "Documents with annotations: 117\n",
      "Total annotations: 333\n",
      "Document categories: {'Discharge summary': 117}\n",
      "\n",
      "===== DOCUMENT-LEVEL METRICS COMPARISON =====\n",
      "                   ORIGINAL    CORRECTED\n",
      "True Positives:       87          98\n",
      "False Positives:     145         134\n",
      "False Negatives:     108         108\n",
      "Precision:         0.375      0.422\n",
      "Recall:            0.446      0.476\n",
      "F1 Score:          0.407      0.447\n",
      "\n",
      "Entities moved from false positives to true positives: 11\n",
      "  aplastic anemia\n",
      "  arrhythmogenic right ventricular dysplasia\n",
      "  fibrous dysplasia\n",
      "  fmf\n",
      "  hemophagocytic syndrome\n",
      "  myelomeningocele\n",
      "  pah\n",
      "  primary pulmonary hypertension\n",
      "  synovial osteochondromatosis\n",
      "  thrombotic thrombocytopenic purpura\n",
      "  vogt-koyanagi syndrome\n",
      "\n",
      "Entities in correction list but not found in false positives: 19\n",
      "  antithrombin deficiency\n",
      "  buerger's disease\n",
      "  churg-strauss syndrome\n",
      "  cryoglobulinemic vasculitis\n",
      "  dermatomyositis\n",
      "  eosinophilic granulomatosis with polyangiitis\n",
      "  giant cell arteritis\n",
      "  henoch-schonlein purpura\n",
      "  juvenile idiopathic arthritis\n",
      "  kawasaki disease\n",
      "  microscopic polyangiitis\n",
      "  mixed connective tissue disease\n",
      "  myelodysplastic syndrome\n",
      "  relapsing polychondritis\n",
      "  scleroderma\n",
      "  systemic lupus erythematosus\n",
      "  takayasu arteritis\n",
      "  temporal arteritis\n",
      "  thromboangiitis obliterans\n"
     ]
    }
   ],
   "source": [
    "def calculate_document_level_metrics(predictions_df, gold_df, corrected_rare_diseases=None):\n",
    "    \"\"\"\n",
    "    Calculate metrics at the document level similar to evaluate_predictions function.\n",
    "    Optionally corrects false positives based on a list of diseases that should be considered true positives.\n",
    "    \n",
    "    Args:\n",
    "        predictions_df: DataFrame with predicted entities\n",
    "        gold_df: DataFrame with gold standard annotations\n",
    "        corrected_rare_diseases: Optional list of diseases that should not be considered false positives\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing original and corrected metrics\n",
    "    \"\"\"\n",
    "    # Convert document_id to string in both DataFrames for consistent comparison\n",
    "    predictions_df = predictions_df.copy()\n",
    "    gold_df = gold_df.copy()\n",
    "    predictions_df['document_id'] = predictions_df['document_id'].astype(str)\n",
    "    gold_df['document_id'] = gold_df['document_id'].astype(str)\n",
    "    \n",
    "    # Convert corrected list to lowercase set for faster lookups\n",
    "    corrected_set = set(disease.lower() for disease in (corrected_rare_diseases or []))\n",
    "    \n",
    "    # Initialize counters for original metrics\n",
    "    entity_true_positives = 0\n",
    "    entity_false_positives = 0\n",
    "    entity_false_negatives = 0\n",
    "    \n",
    "    # Initialize counters for corrected metrics\n",
    "    corrected_true_positives = 0\n",
    "    corrected_false_positives = 0\n",
    "    corrected_false_negatives = 0\n",
    "    \n",
    "    # Tracking for which entities were actually corrected\n",
    "    moved_to_true_positives = set()\n",
    "    \n",
    "    # Process all documents\n",
    "    for _, gold_row in gold_df.iterrows():\n",
    "        doc_id = gold_row['document_id']\n",
    "        gold_anns = gold_row['gold_annotations']\n",
    "        \n",
    "        # Create gold standard sets for this document\n",
    "        gold_entities = {ann['mention'].lower() for ann in gold_anns}\n",
    "        \n",
    "        # Get all predictions for this document\n",
    "        doc_preds = predictions_df[predictions_df['document_id'] == doc_id]\n",
    "        \n",
    "        # Collect all predictions for this document\n",
    "        pred_entities = set()\n",
    "        if not doc_preds.empty:\n",
    "            for _, pred_row in doc_preds.iterrows():\n",
    "                entity = pred_row.get('entity', '').lower() if pd.notna(pred_row.get('entity')) else ''\n",
    "                if entity:\n",
    "                    pred_entities.add(entity)\n",
    "        \n",
    "        # Calculate document-specific metrics\n",
    "        doc_true_positives = gold_entities.intersection(pred_entities)\n",
    "        doc_false_positives = pred_entities - gold_entities\n",
    "        doc_false_negatives = gold_entities - pred_entities\n",
    "        \n",
    "        # Update original counters\n",
    "        entity_true_positives += len(doc_true_positives)\n",
    "        entity_false_positives += len(doc_false_positives)\n",
    "        entity_false_negatives += len(doc_false_negatives)\n",
    "        \n",
    "        # Correct false positives if they're in the corrected list\n",
    "        corrected_doc_false_positives = set()\n",
    "        corrected_doc_true_positives = set(doc_true_positives)  # Start with original TPs\n",
    "        \n",
    "        for entity in doc_false_positives:\n",
    "            if entity.lower() in corrected_set:\n",
    "                # This is actually a true positive\n",
    "                corrected_doc_true_positives.add(entity)\n",
    "                moved_to_true_positives.add(entity)\n",
    "            else:\n",
    "                # This remains a false positive\n",
    "                corrected_doc_false_positives.add(entity)\n",
    "        \n",
    "        # Update corrected counters\n",
    "        corrected_true_positives += len(corrected_doc_true_positives)\n",
    "        corrected_false_positives += len(corrected_doc_false_positives)\n",
    "        corrected_false_negatives += len(doc_false_negatives)  # False negatives remain the same\n",
    "    \n",
    "    # Calculate original metrics\n",
    "    precision_original = entity_true_positives / (entity_true_positives + entity_false_positives) if (entity_true_positives + entity_false_positives) > 0 else 0\n",
    "    recall_original = entity_true_positives / (entity_true_positives + entity_false_negatives) if (entity_true_positives + entity_false_negatives) > 0 else 0\n",
    "    f1_original = 2 * (precision_original * recall_original) / (precision_original + recall_original) if (precision_original + recall_original) > 0 else 0\n",
    "    \n",
    "    # Calculate corrected metrics\n",
    "    precision_corrected = corrected_true_positives / (corrected_true_positives + corrected_false_positives) if (corrected_true_positives + corrected_false_positives) > 0 else 0\n",
    "    recall_corrected = corrected_true_positives / (corrected_true_positives + corrected_false_negatives) if (corrected_true_positives + corrected_false_negatives) > 0 else 0\n",
    "    f1_corrected = 2 * (precision_corrected * recall_corrected) / (precision_corrected + recall_corrected) if (precision_corrected + recall_corrected) > 0 else 0\n",
    "    \n",
    "    # Collect not found entities (corrected diseases that weren't in any document's false positives)\n",
    "    corrected_disease_found = {disease.lower(): False for disease in (corrected_rare_diseases or [])}\n",
    "    for entity in moved_to_true_positives:\n",
    "        if entity.lower() in corrected_disease_found:\n",
    "            corrected_disease_found[entity.lower()] = True\n",
    "    \n",
    "    not_found_in_false_positives = {disease for disease, found in corrected_disease_found.items() if not found}\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'original': {\n",
    "            'true_positives': entity_true_positives,\n",
    "            'false_positives': entity_false_positives,\n",
    "            'false_negatives': entity_false_negatives,\n",
    "            'precision': precision_original,\n",
    "            'recall': recall_original,\n",
    "            'f1': f1_original\n",
    "        },\n",
    "        'corrected': {\n",
    "            'true_positives': corrected_true_positives,\n",
    "            'false_positives': corrected_false_positives,\n",
    "            'false_negatives': corrected_false_negatives,\n",
    "            'precision': precision_corrected,\n",
    "            'recall': recall_corrected,\n",
    "            'f1': f1_corrected\n",
    "        },\n",
    "        'moved_to_true_positives': moved_to_true_positives,\n",
    "        'not_found_in_false_positives': not_found_in_false_positives\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_document_level_metrics(results):\n",
    "    \"\"\"\n",
    "    Print a detailed comparison of original and corrected document-level metrics.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary returned by calculate_document_level_metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n===== DOCUMENT-LEVEL METRICS COMPARISON =====\")\n",
    "    print(\"                   ORIGINAL    CORRECTED\")\n",
    "    print(f\"True Positives:    {results['original']['true_positives']:5d}       {results['corrected']['true_positives']:5d}\")\n",
    "    print(f\"False Positives:   {results['original']['false_positives']:5d}       {results['corrected']['false_positives']:5d}\")\n",
    "    print(f\"False Negatives:   {results['original']['false_negatives']:5d}       {results['corrected']['false_negatives']:5d}\")\n",
    "    print(f\"Precision:         {results['original']['precision']:.3f}      {results['corrected']['precision']:.3f}\")\n",
    "    print(f\"Recall:            {results['original']['recall']:.3f}      {results['corrected']['recall']:.3f}\")\n",
    "    print(f\"F1 Score:          {results['original']['f1']:.3f}      {results['corrected']['f1']:.3f}\")\n",
    "    \n",
    "    # Print details about corrections\n",
    "    if results['moved_to_true_positives']:\n",
    "        print(f\"\\nEntities moved from false positives to true positives: {len(results['moved_to_true_positives'])}\")\n",
    "        for entity in sorted(results['moved_to_true_positives']):\n",
    "            print(f\"  {entity}\")\n",
    "    \n",
    "    if results['not_found_in_false_positives']:\n",
    "        print(f\"\\nEntities in correction list but not found in false positives: {len(results['not_found_in_false_positives'])}\")\n",
    "        for entity in sorted(results['not_found_in_false_positives']):\n",
    "            print(f\"  {entity}\")\n",
    "\n",
    "# Example usage\n",
    "\"\"\"\n",
    "# Define the corrected list of rare diseases\n",
    "rare_diseases = [\n",
    "    \"myelomeningocele\",\n",
    "    \"fmf\",\n",
    "    \"thrombotic thrombocytopenic purpura\",\n",
    "    # ... other diseases ...\n",
    "]\n",
    "\n",
    "# Process gold standard data\n",
    "gold_df = process_mimic_json(\"data/dataset/filtered_rd_annos_updated_adam.json\")\n",
    "\n",
    "# Load predictions\n",
    "predictions_df = pd.read_csv(\"rd_llama70b_fastembed.csv\")\n",
    "\n",
    "# Calculate document-level metrics with corrections\n",
    "doc_metrics = calculate_document_level_metrics(predictions_df, gold_df, rare_diseases)\n",
    "\n",
    "# Print metrics comparison\n",
    "print_document_level_metrics(doc_metrics)\n",
    "\"\"\"\n",
    "# Example usage\n",
    "\n",
    "# Define the corrected list of rare diseases (these should not be considered false positives)\n",
    "rare_diseases = [\n",
    "    \"myelomeningocele\",\n",
    "    \"fmf\",\n",
    "    \"thrombotic thrombocytopenic purpura\",\n",
    "    \"eosinophilic granulomatosis with polyangiitis\",\n",
    "    \"giant cell arteritis\",\n",
    "    \"vogt-koyanagi syndrome\",\n",
    "    \"juvenile idiopathic arthritis\",\n",
    "    \"churg-strauss syndrome\",\n",
    "    \"primary pulmonary hypertension\",\n",
    "    \"scleroderma\",\n",
    "    \"arrhythmogenic right ventricular dysplasia\",\n",
    "    \"fibrous dysplasia\",\n",
    "    \"synovial osteochondromatosis\",\n",
    "    \"relapsing polychondritis\",\n",
    "    \"takayasu arteritis\",\n",
    "    \"cryoglobulinemic vasculitis\",\n",
    "    \"dermatomyositis\",\n",
    "    \"hemophagocytic syndrome\",\n",
    "    \"systemic lupus erythematosus\",\n",
    "    \"mixed connective tissue disease\",\n",
    "    \"thromboangiitis obliterans\",\n",
    "    \"buerger's disease\",\n",
    "    \"microscopic polyangiitis\",\n",
    "    \"aplastic anemia\",\n",
    "    \"kawasaki disease\",\n",
    "    \"antithrombin deficiency\",\n",
    "    \"henoch-schonlein purpura\",\n",
    "    \"myelodysplastic syndrome\",\n",
    "    \"temporal arteritis\",\n",
    "    \"pah\"\n",
    "]\n",
    "\n",
    "# Recalculate metrics\n",
    "# Process gold standard data\n",
    "gold_df = process_mimic_json(\"data/dataset/filtered_rd_annos_updated_adam.json\")\n",
    "\n",
    "# Load predictions\n",
    "predictions_df = pd.read_csv(\"data/results/rd_llama70b_medembed.csv\")\n",
    "\n",
    "# Calculate document-level metrics with corrections\n",
    "doc_metrics = calculate_document_level_metrics(predictions_df, gold_df, rare_diseases)\n",
    "\n",
    "# Print metrics comparison\n",
    "print_document_level_metrics(doc_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset cleaning further to re-label false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnwu3/miniconda3/envs/hporag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting embeddings from: data/vector_stores/rd_orpha_fastembed.npy\n",
      "--------------------------------------------------\n",
      "Total number of documents: 26750\n",
      "\n",
      "First document structure:\n",
      "-------------------------\n",
      "name: 48,xxyy syndrome\n",
      "id: ORPHA:10\n",
      "definition: a rare sex chromosome number anomaly disorder characterized, genetically, by the presence of an extra x and y chromosome in males and, clinically, by tall stature, dysfunctional testes associated with infertility and insufficient testosterone production, cognitive, affective and social functioning impairments, global developmental delay, and an increased risk of congenital malformations.\n",
      "\n",
      "Embedding details:\n",
      "Shape: (384,)\n",
      "Type: float32\n",
      "First 5 values: [-0.03409642 -0.01160857  0.05178694  0.04140279 -0.00915304]\n",
      "Min value: -0.32415685057640076\n",
      "Max value: 0.329723596572876\n",
      "Mean value: 0.0003787704335991293\n",
      "\n",
      "Testing retrieval functionality\n",
      "--------------------------------------------------\n",
      "Loading model...\n",
      "Model type: fastembed\n",
      "Model name: BAAI/bge-small-en-v1.5\n",
      "Device: None\n",
      "Data type: <class 'numpy.ndarray'>\n",
      "Shape or length: (26750,)\n",
      "First element type: <class 'dict'>\n",
      "Loaded 26750 documents\n",
      "\n",
      "Query: rare genetic disorder affecting bone development\n",
      "-------------------------\n",
      "Top 5 matches:\n",
      "1. rare genetic bone development disorder (ORPHA:ORPHA:404584) - Similarity: 0.937\n",
      "\n",
      "2. rare bone development disorder (ORPHA:ORPHA:139012) - Similarity: 0.904\n",
      "\n",
      "3. rare genetic bone disease (ORPHA:ORPHA:183524) - Similarity: 0.883\n",
      "\n",
      "4. rare bone disease (ORPHA:ORPHA:93419) - Similarity: 0.833\n",
      "\n",
      "5. rare bone disease related to a common gene or pathway defect (ORPHA:ORPHA:364803) - Similarity: 0.830\n",
      "\n",
      "\n",
      "Query: inherited metabolic disorder\n",
      "-------------------------\n",
      "Top 5 matches:\n",
      "1. inherited retinal disorder (ORPHA:ORPHA:71862) - Similarity: 0.747\n",
      "\n",
      "2. disorder of energy metabolism (ORPHA:ORPHA:79200) - Similarity: 0.740\n",
      "\n",
      "3. rare hereditary metabolic disease with peripheral neuropathy (ORPHA:ORPHA:207018) - Similarity: 0.737\n",
      "\n",
      "4. disorder of lipid metabolism (ORPHA:ORPHA:309005) - Similarity: 0.737\n",
      "\n",
      "5. metabolic disease involving other neurotransmitter deficiency (ORPHA:ORPHA:79219) - Similarity: 0.730\n",
      "\n",
      "\n",
      "Query: rare autoimmune condition\n",
      "-------------------------\n",
      "Top 5 matches:\n",
      "1. rare immune disease (ORPHA:ORPHA:98004) - Similarity: 0.828\n",
      "\n",
      "2. rare genetic immune disease (ORPHA:ORPHA:183770) - Similarity: 0.809\n",
      "\n",
      "3. rare allergic disease (ORPHA:ORPHA:98050) - Similarity: 0.808\n",
      "\n",
      "4. rare hereditary autoinflammatory disease (ORPHA:ORPHA:619238) - Similarity: 0.779\n",
      "\n",
      "5. rare rheumatologic disease (ORPHA:ORPHA:182231) - Similarity: 0.776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils.embedding import EmbeddingsManager\n",
    "import json\n",
    "\n",
    "def inspect_embeddings(embeddings_file: str):\n",
    "    \"\"\"Inspect the first document in the embeddings file.\"\"\"\n",
    "    print(f\"\\nInspecting embeddings from: {embeddings_file}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Load the embeddings file\n",
    "    try:\n",
    "        data = np.load(embeddings_file, allow_pickle=True)\n",
    "        print(f\"Total number of documents: {len(data)}\")\n",
    "        \n",
    "        # Print first document details\n",
    "        first_doc = data[0]\n",
    "        print(\"\\nFirst document structure:\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        # Print all fields except embedding\n",
    "        for key, value in first_doc.items():\n",
    "            if key != 'embedding':\n",
    "                print(f\"{key}: {value}\")\n",
    "        \n",
    "        # Print embedding details\n",
    "        embedding = first_doc['embedding']\n",
    "        print(f\"\\nEmbedding details:\")\n",
    "        print(f\"Shape: {embedding.shape}\")\n",
    "        print(f\"Type: {embedding.dtype}\")\n",
    "        print(f\"First 5 values: {embedding[:5]}\")\n",
    "        print(f\"Min value: {np.min(embedding)}\")\n",
    "        print(f\"Max value: {np.max(embedding)}\")\n",
    "        print(f\"Mean value: {np.mean(embedding)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embeddings: {str(e)}\")\n",
    "\n",
    "def test_retrieval(embeddings_file: str, model_type: str = 'fastembed', model_name: str = \"BAAI/bge-small-en-v1.5\"):\n",
    "    \"\"\"Test the retrieval component with a sample query.\"\"\"\n",
    "    print(f\"\\nTesting retrieval functionality\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Initialize EmbeddingsManager\n",
    "        embeddings_manager = EmbeddingsManager(\n",
    "            model_type=model_type,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        \n",
    "        # Load embedded documents\n",
    "        embedded_documents = embeddings_manager.load_documents(embeddings_file)\n",
    "        print(f\"Loaded {len(embedded_documents)} documents\")\n",
    "        \n",
    "        # Prepare embeddings array and create index\n",
    "        embeddings_array = embeddings_manager.prepare_embeddings(embedded_documents)\n",
    "        index = embeddings_manager.create_index(embeddings_array)\n",
    "        \n",
    "        # Test queries\n",
    "        test_queries = [\n",
    "            \"rare genetic disorder affecting bone development\",\n",
    "            \"inherited metabolic disorder\",\n",
    "            \"rare autoimmune condition\"\n",
    "        ]\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            print(\"-\" * 25)\n",
    "            \n",
    "            # Get query embedding and search\n",
    "            query_vector = embeddings_manager.query_text(query).reshape(1, -1)\n",
    "            distances, indices = embeddings_manager.search(query_vector, index, k=5)\n",
    "            \n",
    "            # Print top 5 results\n",
    "            print(\"Top 5 matches:\")\n",
    "            for idx, (distance, doc_idx) in enumerate(zip(distances[0], indices[0]), 1):\n",
    "                doc = embedded_documents[doc_idx]\n",
    "                similarity = 1 / (1 + distance)\n",
    "                print(f\"{idx}. {doc['name']} (ORPHA:{doc['id']}) - Similarity: {similarity:.3f}\")\n",
    "                if doc.get('definition'):\n",
    "                    print(f\"   Definition: {doc['definition'][:200]}...\")\n",
    "                print()\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing retrieval: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update this path to your embeddings file\n",
    "    EMBEDDINGS_FILE = \"data/vector_stores/rd_orpha_fastembed.npy\"\n",
    "    \n",
    "    # Inspect embeddings\n",
    "    inspect_embeddings(EMBEDDINGS_FILE)\n",
    "    \n",
    "    # Test retrieval\n",
    "    test_retrieval(EMBEDDINGS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrections to Updated_Adam.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Update Statistics:\n",
      "Documents updated: 31\n",
      "New annotations added: 74\n",
      "Total mentions found: 74\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "def update_annotations_with_validated_mentions(\n",
    "    annotations_file: str,\n",
    "    validated_mentions: List[str],\n",
    "    output_file: str\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Update annotations by adding validated false positives as new annotations.\n",
    "    \n",
    "    Args:\n",
    "        annotations_file: Path to original annotations file\n",
    "        validated_mentions: List of validated mentions to add\n",
    "        output_file: Path to save updated annotations\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing statistics about updates made\n",
    "    \"\"\"\n",
    "    # Load original annotations\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    stats = {\n",
    "        'documents_updated': 0,\n",
    "        'new_annotations_added': 0,\n",
    "        'mentions_found': 0\n",
    "    }\n",
    "    \n",
    "    # Compile case-insensitive patterns for each mention\n",
    "    mention_patterns = {\n",
    "        mention: re.compile(r'\\b' + re.escape(mention) + r'\\b', re.IGNORECASE)\n",
    "        for mention in validated_mentions\n",
    "    }\n",
    "    \n",
    "    # Process each document\n",
    "    for doc_id, doc in annotations.items():\n",
    "        text = doc['note_details']['text']\n",
    "        existing_mentions = {anno['mention'].lower() for anno in doc['annotations']}\n",
    "        new_annotations = []\n",
    "        \n",
    "        # Check for each validated mention in the text\n",
    "        for mention, pattern in mention_patterns.items():\n",
    "            # Skip if mention is already annotated\n",
    "            if mention.lower() in existing_mentions:\n",
    "                continue\n",
    "                \n",
    "            # Find all occurrences\n",
    "            matches = list(pattern.finditer(text))\n",
    "            if matches:\n",
    "                stats['mentions_found'] += len(matches)\n",
    "                # Add each occurrence as a new annotation\n",
    "                for match in matches:\n",
    "                    new_anno = {\n",
    "                        'mention': text[match.start():match.end()],  # Use exact text from document\n",
    "                        'start': match.start(),\n",
    "                        'end': match.end(),\n",
    "                        'type': 'RARE_DISEASE',  # Assuming this is the standard type\n",
    "                        'validated_false_positive': True  # Mark as validated false positive\n",
    "                    }\n",
    "                    new_annotations.append(new_anno)\n",
    "        \n",
    "        # Update document if new annotations were found\n",
    "        if new_annotations:\n",
    "            doc['annotations'].extend(new_annotations)\n",
    "            stats['documents_updated'] += 1\n",
    "            stats['new_annotations_added'] += len(new_annotations)\n",
    "    \n",
    "    # Save updated annotations\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(annotations, f, indent=2)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Example usage:\n",
    "validated_mentions = [\n",
    "    'papillary carcinoma',\n",
    "    'glioblastoma multiforme',\n",
    "    'transitional cell carcinoma',\n",
    "    'multifocal atrial tachycardia (mat)',\n",
    "    'sarcoidosis',\n",
    "    'methemoglobinemia',\n",
    "    'central nervous system and systemic lymphoma',\n",
    "    'sclerosis cholangitis',  # corrected from 'sclerosing cholangitis'\n",
    "    'mediastinitis',\n",
    "    'mesenteric vein thrombosis',\n",
    "    'multiple myeloma',\n",
    "    'hepatocellular carcinoma',  # corrected from 'hepatocellular ca'\n",
    "    'primary cns lymphoma',\n",
    "    \"bechet's disease\",\n",
    "    'neovascular glaucoma',\n",
    "    'meningocele',\n",
    "    'alopecia',\n",
    "    'neovascular glaucoma angle closure',\n",
    "    'pyoderma gangrenosum',\n",
    "    'budd-chiari',\n",
    "    'intraductal papillary mucinous tumor',\n",
    "    'complex tracheal stenosis',\n",
    "    'cervical stenosis',\n",
    "    'bronchiectasis',\n",
    "    'medullary sponge kidney',\n",
    "    'protein s',\n",
    "    'antiphospholipid antibody syndrome',\n",
    "    'protein c',\n",
    "    'acute myelogenous leukemia',\n",
    "    'anaplastic thyroid carcinoma',\n",
    "    'thymoma',\n",
    "    'congenital bleeding disorder',\n",
    "    'tracheal stenosis'\n",
    "]\n",
    "# Update annotations\n",
    "stats = update_annotations_with_validated_mentions(\n",
    "    annotations_file='data/dataset/rd_annos.json',\n",
    "    validated_mentions=validated_mentions,\n",
    "    output_file='data/dataset/filtered_rd_annos_updated_adam.json'\n",
    ")\n",
    "\n",
    "print(\"\\nUpdate Statistics:\")\n",
    "print(f\"Documents updated: {stats['documents_updated']}\")\n",
    "print(f\"New annotations added: {stats['new_annotations_added']}\")\n",
    "print(f\"Total mentions found: {stats['mentions_found']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Metrics:\n",
      "Precision: 0.347\n",
      "Recall: 0.316\n",
      "F1: 0.331\n",
      "True Positives: 50\n",
      "False Positives: 94\n",
      "False Negatives: 108\n",
      "\n",
      "Metrics with Validated Mentions:\n",
      "Precision: 0.590\n",
      "Recall: 0.440\n",
      "F1: 0.504\n",
      "True Positives: 85\n",
      "False Positives: 59\n",
      "False Negatives: 108\n",
      "\n",
      "Metrics with Validated Mentions + No Abbreviations:\n",
      "Precision: 0.590\n",
      "Recall: 0.475\n",
      "F1: 0.526\n",
      "True Positives: 85\n",
      "False Positives: 59\n",
      "False Negatives: 94\n",
      "\n",
      "Improvements:\n",
      "\n",
      "1. Impact of Validating Mentions:\n",
      "Precision: 0.243\n",
      "Recall: 0.124\n",
      "F1: 0.173\n",
      "\n",
      "2. Additional Impact of Excluding Abbreviations:\n",
      "Precision: 0.000\n",
      "Recall: 0.034\n",
      "F1: 0.022\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Set, Dict, Tuple\n",
    "\n",
    "def is_abbreviation(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a string is likely an abbreviation.\n",
    "    \"\"\"\n",
    "    # Remove spaces and convert to uppercase for checking\n",
    "    cleaned_text = text.replace(\" \", \"\")\n",
    "    \n",
    "    # Skip strings that are too long or too short to be abbreviations\n",
    "    if len(cleaned_text) < 2 or len(cleaned_text) > 5:\n",
    "        return False\n",
    "    \n",
    "    # Special cases - don't count these as abbreviations\n",
    "    special_cases = {\"H1N1\", \"B12\"}\n",
    "    if cleaned_text.upper() in special_cases:\n",
    "        return False\n",
    "    \n",
    "    # Check if it's all uppercase letters (allowing spaces)\n",
    "    if text.upper() == text and any(c.isalpha() for c in text):\n",
    "        return True\n",
    "    \n",
    "    # Check for pattern of multiple capital letters (allowing spaces/numbers)\n",
    "    capital_count = sum(1 for c in text if c.isupper())\n",
    "    return capital_count >= 2\n",
    "\n",
    "def should_remove_mention(mention: str) -> bool:\n",
    "    \"\"\"Determine if a mention should be removed.\"\"\"\n",
    "    cleaned_mention = mention.strip().lower()\n",
    "    \n",
    "    # Special lowercase terms to remove\n",
    "    lowercase_remove = {\"hits\", \"als\", \"nph\"}\n",
    "    if cleaned_mention in lowercase_remove:\n",
    "        return True\n",
    "    \n",
    "    # Remove if it's an abbreviation\n",
    "    return is_abbreviation(mention)\n",
    "\n",
    "def calculate_metrics(tp: int, fp: int, fn: int) -> Dict:\n",
    "    \"\"\"Calculate precision, recall, and F1 score.\"\"\"\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn\n",
    "    }\n",
    "\n",
    "def recalculate_metrics_with_improvements(\n",
    "    results_file: str,\n",
    "    validated_mentions: List[str],\n",
    "    exclude_abbreviations: bool = False\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Recalculate metrics showing incremental improvements.\n",
    "    \n",
    "    Args:\n",
    "        results_file: Path to the original results JSON file\n",
    "        validated_mentions: List of mentions that were validated as true positives\n",
    "        exclude_abbreviations: Whether to exclude abbreviations\n",
    "    \"\"\"\n",
    "    # Load results file\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Convert validated mentions to set\n",
    "    validated_set = {mention.lower() for mention in validated_mentions}\n",
    "    \n",
    "    # Initialize counters for different scenarios\n",
    "    metrics = {\n",
    "        'original': {'tp': 0, 'fp': 0, 'fn': 0},\n",
    "        'with_validated': {'tp': 0, 'fp': 0, 'fn': 0},\n",
    "        'with_validated_no_abbrev': {'tp': 0, 'fp': 0, 'fn': 0}\n",
    "    }\n",
    "    \n",
    "    # Process each document's metrics\n",
    "    for doc_id, doc_metrics in results['evaluation_results']['document_metrics'].items():\n",
    "        analysis = doc_metrics['analysis']\n",
    "        \n",
    "        # Get the sets of mentions\n",
    "        correct = set(analysis['correct_mentions'])\n",
    "        spurious = set(analysis['spurious_mentions'])\n",
    "        missed = set(analysis['missed_mentions'])\n",
    "        \n",
    "        # 1. Original metrics\n",
    "        metrics['original']['tp'] += len(correct)\n",
    "        metrics['original']['fp'] += len(spurious)\n",
    "        metrics['original']['fn'] += len(missed)\n",
    "        \n",
    "        # 2. Metrics with validated mentions\n",
    "        validated_spurious = {m for m in spurious if m.lower() in validated_set}\n",
    "        remaining_spurious = spurious - validated_spurious\n",
    "        \n",
    "        metrics['with_validated']['tp'] += len(correct) + len(validated_spurious)\n",
    "        metrics['with_validated']['fp'] += len(remaining_spurious)\n",
    "        metrics['with_validated']['fn'] += len(missed)\n",
    "        \n",
    "        # 3. Metrics with validated mentions and no abbreviations\n",
    "        if exclude_abbreviations:\n",
    "            # Filter mentions\n",
    "            filtered_correct = {m for m in correct if not should_remove_mention(m)}\n",
    "            filtered_spurious = {m for m in remaining_spurious if not should_remove_mention(m)}\n",
    "            filtered_validated = {m for m in validated_spurious if not should_remove_mention(m)}\n",
    "            filtered_missed = {m for m in missed if not should_remove_mention(m)}\n",
    "            \n",
    "            metrics['with_validated_no_abbrev']['tp'] += len(filtered_correct) + len(filtered_validated)\n",
    "            metrics['with_validated_no_abbrev']['fp'] += len(filtered_spurious)\n",
    "            metrics['with_validated_no_abbrev']['fn'] += len(filtered_missed)\n",
    "    \n",
    "    # Calculate final metrics for each scenario\n",
    "    return {\n",
    "        'original': calculate_metrics(\n",
    "            metrics['original']['tp'],\n",
    "            metrics['original']['fp'],\n",
    "            metrics['original']['fn']\n",
    "        ),\n",
    "        'with_validated': calculate_metrics(\n",
    "            metrics['with_validated']['tp'],\n",
    "            metrics['with_validated']['fp'],\n",
    "            metrics['with_validated']['fn']\n",
    "        ),\n",
    "        'with_validated_no_abbrev': calculate_metrics(\n",
    "            metrics['with_validated_no_abbrev']['tp'],\n",
    "            metrics['with_validated_no_abbrev']['fp'],\n",
    "            metrics['with_validated_no_abbrev']['fn']\n",
    "        ) if exclude_abbreviations else None\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "validated_mentions = [\n",
    "    'papillary carcinoma',\n",
    "    'glioblastoma multiforme',\n",
    "    'transitional cell carcinoma',\n",
    "    'multifocal atrial tachycardia (mat)',\n",
    "    'sarcoidosis',\n",
    "    'methemoglobinemia',\n",
    "    'central nervous system and systemic lymphoma',\n",
    "    'sclerosis cholangitis',  # corrected from 'sclerosing cholangitis'\n",
    "    'mediastinitis',\n",
    "    'mesenteric vein thrombosis',\n",
    "    'multiple myeloma',\n",
    "    'hepatocellular carcinoma',  # corrected from 'hepatocellular ca'\n",
    "    'primary cns lymphoma',\n",
    "    \"bechet's disease\",\n",
    "    'neovascular glaucoma',\n",
    "    'meningocele',\n",
    "    'alopecia',\n",
    "    'neovascular glaucoma angle closure',\n",
    "    'pyoderma gangrenosum',\n",
    "    'budd-chiari',\n",
    "    'intraductal papillary mucinous tumor',\n",
    "    'complex tracheal stenosis',\n",
    "    'cervical stenosis',\n",
    "    'bronchiectasis',\n",
    "    'medullary sponge kidney',\n",
    "    'protein s',\n",
    "    'antiphospholipid antibody syndrome',\n",
    "    'protein c',\n",
    "    'acute myelogenous leukemia',\n",
    "    'anaplastic thyroid carcinoma',\n",
    "    'thymoma',\n",
    "    'congenital bleeding disorder',\n",
    "    'tracheal stenosis'\n",
    "]\n",
    "\n",
    "results_file = \"data/results/rd_results_llama3_70b.json\"\n",
    "\n",
    "# Get metrics for both scenarios\n",
    "metrics = recalculate_metrics_with_improvements(\n",
    "    results_file, \n",
    "    validated_mentions,\n",
    "    exclude_abbreviations=True\n",
    ")\n",
    "\n",
    "def print_metrics(metrics_dict: Dict, title: str):\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(f\"Precision: {metrics_dict['precision']:.3f}\")\n",
    "    print(f\"Recall: {metrics_dict['recall']:.3f}\")\n",
    "    print(f\"F1: {metrics_dict['f1']:.3f}\")\n",
    "    print(f\"True Positives: {metrics_dict['true_positives']}\")\n",
    "    print(f\"False Positives: {metrics_dict['false_positives']}\")\n",
    "    print(f\"False Negatives: {metrics_dict['false_negatives']}\")\n",
    "\n",
    "print_metrics(metrics['original'], \"Original Metrics\")\n",
    "print_metrics(metrics['with_validated'], \"Metrics with Validated Mentions\")\n",
    "print_metrics(metrics['with_validated_no_abbrev'], \"Metrics with Validated Mentions + No Abbreviations\")\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\nImprovements:\")\n",
    "print(\"\\n1. Impact of Validating Mentions:\")\n",
    "print(f\"Precision: {metrics['with_validated']['precision'] - metrics['original']['precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['with_validated']['recall'] - metrics['original']['recall']:.3f}\")\n",
    "print(f\"F1: {metrics['with_validated']['f1'] - metrics['original']['f1']:.3f}\")\n",
    "\n",
    "print(\"\\n2. Additional Impact of Excluding Abbreviations:\")\n",
    "print(f\"Precision: {metrics['with_validated_no_abbrev']['precision'] - metrics['with_validated']['precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['with_validated_no_abbrev']['recall'] - metrics['with_validated']['recall']:.3f}\")\n",
    "print(f\"F1: {metrics['with_validated_no_abbrev']['f1'] - metrics['with_validated']['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# printing of commonly missed spurious correlations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['evaluation_results', 'args', 'device', 'processing_stats'])\n",
      "------Correct-----\n",
      "{'arachnoid cyst', 'pulmonary arterial hypertension', 'dilated cardiomyopathy', 'essential thrombocythemia', 'legionella', 'heparin-induced thrombocytopenia', 'babesiosis', 'retinopathy of prematurity', 'sarcoidosis', 'budd chiari syndrome', 'autoimmune pancreatitis', 'primary adrenal insufficiency', 'essential thrombocytosis', 'multifocal atrial tachycardia', 'nocardiosis', 'heparin induced thrombocytopenia', 'necrotizing enterocolitis', 'post polio syndrome', 'hypogammaglobulinemia', 'alport syndrome', 'autoimmune hepatitis', 'beta-thalassemia', 'budd-chiari syndrome', 'protein c deficiency', 'sick sinus syndrome', 'crest syndrome', 'bullous pemphigoid', 'hemochromatosis', 'primary sclerosing cholangitis', 'familial mediterranean fever', 'alkaptonuria', 'retinitis pigmentosa', 'central retinal vein occlusion', 'calciphylaxis', 'amyotrophic lateral sclerosis'}\n",
      "------Spurious-----\n",
      "{'fibrous dysplasia', 'papillary carcinoma', 'avascular necrosis', 'end-stage renal disease', 'tracheomalacia', 'apnea prematurity', 'glioblastoma multiforme', 'transitional cell carcinoma', 'gtc epilepsy', 'cardiogenic', 'multifocal atrial tachycardia (mat)', 'sarcoidosis', 'methemoglobinemia', 'central nervous system and systemic lymphoma', 'sclerosis cholangitis', 'angioedema', 'neutropenia', 'history of multiple myeloma', 'mediastinitis', 'hypercholesterolemia', 'mesenteric vein thrombosis', 'multiple myeloma', 'necrotizing soft tissue infection', 'pulmonary arterial systolic hypertension', 'thrombocythemia', 'hepatocellular carcinoma', 'primary cns lymphoma', 'chronic adrenal insuffiency', 'necrotizing soft tissue infection of perineum', 'extensive calvarial hyperostosis', 'diffuse pulmonary hemorrhage', 'lymphedema', 'connective tissue disorder', 'apnea of prematurity', 'sclerosing cholangitis', 'neuromuscular failure', 'vasculitis', \"bechet's disease\", 'neovascular glaucoma', 'pulmonary alveolar hemorrhage', 'polycythemia', 'neuromuscular disease', 'osteonecrosis', 'meningocele', 'alopecia', 'neovascular glaucoma angle closure', 'pyoderma gangrenosum', 'budd-chiari', 'intraductal papillary mucinous tumor', 'complex tracheal stenosis', 'cervical stenosis', 'toxic/metabolic syndrome', 'bronchiectasis', 'medullary sponge kidney', 'pneumonia empyema', 'protein s', 'uveitis', 'antiphospholipid antibody syndrome', 'protein c', 'plus disease', 'fistula', 'hepatocellular ca', 'hydrocephalus', 'status post apnea of prematurity', 'encephalalitis', 'pulmonary artery systolic hypertension', 'acute myelogenous leukemia', 'anaplastic thyroid carcinoma', 'lipase 46', 'mild disease', 'adrenal insufficiency', 'thymoma', 'alveolar hemorrhage', 'congenital bleeding disorder', 'gastroparesis', 'tracheal stenosis'}\n",
      "------Missed-----\n",
      "{'ema', 'pulmonary arterial hypertension', 'iron storage disease', 'thrombotic thrombocytopenic purpura', 'seizure disorder', 'endometriosis', 'legionella', 'lyme disease', 'epileptic', 'epileptic seizure', 'recurrent fever', 'sarcoid', 'beta thalassemia', 'fulminant liver failure', 'pml', 'als', 'portal vein thrombosis', 'epileptic seizures', 'hyperparathyroidism', 'amyloidosis', 'polymyalgia rheumatica', 'hyperthyroidism', 'heparin induced thrombocytopenia', 'scds', 'asbestosis', 'hypothyroidism secondary', 'tracheobronchomalacia', 'ttp', 'fulminant hepatic failure', 'acute hepatic failure', 'epilepsy', 'sick sinus syndrome', 'tetanus', 'pmr', 'hit', \"lyme's disease\", 'hemochromatosis', \"alport's syndrome\", 'nph', 'microcytic anemia', 'rheumatic fever', 'her disease'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Dict, Set, Tuple\n",
    "\n",
    "def analyze_mentions(results: Dict) -> Tuple[Set[str], Set[str], Set[str]]:\n",
    "    \"\"\"\n",
    "    Analyze mentions from evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Dictionary containing evaluation results\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Sets of correct, spurious, and missed mentions\n",
    "    \"\"\"\n",
    "    correct_mentions = set()\n",
    "    spurious_mentions = set()\n",
    "    missed_mentions = set()\n",
    "    print(results.keys())\n",
    "    # Iterate through all documents\n",
    "    for doc_id, doc_results in results[\"evaluation_results\"][\"document_metrics\"].items():\n",
    "        if 'analysis' in doc_results:\n",
    "            analysis = doc_results['analysis']\n",
    "            correct_mentions.update(analysis.get('correct_mentions', []))\n",
    "            spurious_mentions.update(analysis.get('spurious_mentions', []))\n",
    "            missed_mentions.update(analysis.get('missed_mentions', []))\n",
    "    \n",
    "    return correct_mentions, spurious_mentions, missed_mentions\n",
    "\n",
    "def analyze_results_file(file_path: str) -> Tuple[Set[str], Set[str], Set[str]]:\n",
    "    \"\"\"\n",
    "    Load and analyze mentions from a results JSON file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the results JSON file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Sets of correct, spurious, and missed mentions\n",
    "    \"\"\"\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Analyze the results\n",
    "    return analyze_mentions(results)\n",
    "\n",
    "# Example usage in notebook:\n",
    "file_path = \"results/results_llama3_70b.json\"\n",
    "correct, spurious, missed = analyze_results_file(file_path)\n",
    "\n",
    "print(\"------Correct-----\")\n",
    "print(correct)\n",
    "print(\"------Spurious-----\")\n",
    "print(spurious)\n",
    "print(\"------Missed-----\")\n",
    "print(missed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontology Structure Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RareDisease_Phenotype_Triples.json Analysis ===\n",
      "\n",
      "Structure:\n",
      "Type: list\n",
      "Length: 114994\n",
      "Element sample:\n",
      "  Type: dict\n",
      "  Length: 3\n",
      "  Keys:\n",
      "  source:\n",
      "    Type: dict\n",
      "    Length: 3\n",
      "    Keys:\n",
      "    id:\n",
      "      Type: str\n",
      "      Length: 9\n",
      "      Sample: Orpha:324\n",
      "    name:\n",
      "      Type: list\n",
      "      Length: 6\n",
      "      Element sample:\n",
      "        Type: str\n",
      "        Length: 13\n",
      "        Sample: Fabry disease\n",
      "    definition:\n",
      "      Type: str\n",
      "      Length: 400\n",
      "      Sample: A rare genetic, multisystemic lysosomal disease characterized by specific cutaneous (angiokeratoma),\n",
      "  link:\n",
      "    Type: dict\n",
      "    Length: 1\n",
      "    Keys:\n",
      "    frequency:\n",
      "      Type: str\n",
      "      Length: 22\n",
      "      Sample: Very frequent (99-80%)\n",
      "  target:\n",
      "    Type: dict\n",
      "    Length: 3\n",
      "    Keys:\n",
      "    id:\n",
      "      Type: str\n",
      "      Length: 10\n",
      "      Sample: HP:0001635\n",
      "    name:\n",
      "      Type: list\n",
      "      Length: 7\n",
      "      Element sample:\n",
      "        Type: str\n",
      "        Length: 3\n",
      "        Sample: CHF\n",
      "    definition:\n",
      "      Type: str\n",
      "      Length: 344\n",
      "      Sample: The presence of an abnormality of cardiac function that is responsible for the failure of the heart \n",
      "\n",
      "Total entries: 114994\n",
      "\n",
      "Field coverage:\n",
      "  link: 100.0%\n",
      "  source: 100.0%\n",
      "  target: 100.0%\n",
      "\n",
      "=== rare_disease_ontology.jsonl Analysis ===\n",
      "\n",
      "Total entries: 14588\n",
      "\n",
      "Structure of first entry:\n",
      "Type: dict\n",
      "Length: 3\n",
      "Keys:\n",
      "id:\n",
      "  Type: str\n",
      "  Length: 8\n",
      "  Sample: ORPHA:10\n",
      "name:\n",
      "  Type: str\n",
      "  Length: 16\n",
      "  Sample: 48,XXYY syndrome\n",
      "definition:\n",
      "  Type: str\n",
      "  Length: 390\n",
      "  Sample: A rare sex chromosome number anomaly disorder characterized, genetically, by the presence of an extr\n",
      "\n",
      "Field coverage:\n",
      "  definition: 100.0%\n",
      "  id: 100.0%\n",
      "  name: 100.0%\n",
      "\n",
      "ORPHA ID analysis:\n",
      "  Total IDs: 14588\n",
      "  Malformed IDs: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Any, Set, List\n",
    "from pathlib import Path\n",
    "\n",
    "class RareDiseaseDataInspector:\n",
    "    \"\"\"Inspector class for analyzing rare disease data files.\"\"\"\n",
    "    \n",
    "    def __init__(self, triples_path: str, ontology_path: str):\n",
    "        \"\"\"\n",
    "        Initialize with paths to both data files.\n",
    "        \n",
    "        Args:\n",
    "            triples_path: Path to RareDisease_Phenotype_Triples.json\n",
    "            ontology_path: Path to rare_disease_ontology.jsonl\n",
    "        \"\"\"\n",
    "        self.triples_path = Path(triples_path)\n",
    "        self.ontology_path = Path(ontology_path)\n",
    "        \n",
    "    def _load_jsonl(self, path: Path) -> List[Dict]:\n",
    "        \"\"\"Load JSONL file line by line.\"\"\"\n",
    "        data = []\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding line in {path}: {e}\")\n",
    "        return data\n",
    "    \n",
    "    def _analyze_structure(self, data: Any, prefix: str = \"\") -> Dict:\n",
    "        \"\"\"\n",
    "        Recursively analyze the structure of nested data.\n",
    "        \n",
    "        Args:\n",
    "            data: Data structure to analyze\n",
    "            prefix: Current key prefix for nested structures\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing structure analysis\n",
    "        \"\"\"\n",
    "        structure = {\n",
    "            \"type\": type(data).__name__,\n",
    "            \"sample\": str(data)[:100] if isinstance(data, (str, bytes)) else None,\n",
    "            \"length\": len(data) if hasattr(data, \"__len__\") else None\n",
    "        }\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            structure[\"keys\"] = {\n",
    "                k: self._analyze_structure(v, f\"{prefix}.{k}\" if prefix else k)\n",
    "                for k, v in (data.items() if len(data) < 5 else list(data.items())[:5])\n",
    "            }\n",
    "        elif isinstance(data, list) and data:\n",
    "            structure[\"element_sample\"] = self._analyze_structure(data[0])\n",
    "            \n",
    "        return structure\n",
    "    \n",
    "    def _count_field_occurrences(self, data: List[Dict]) -> Dict[str, int]:\n",
    "        \"\"\"Count how often each field appears across all entries.\"\"\"\n",
    "        field_counts = defaultdict(int)\n",
    "        total_entries = len(data)\n",
    "        \n",
    "        for entry in data:\n",
    "            if isinstance(entry, dict):\n",
    "                for key in entry.keys():\n",
    "                    field_counts[key] += 1\n",
    "        \n",
    "        # Convert to percentages\n",
    "        return {\n",
    "            key: (count / total_entries * 100)\n",
    "            for key, count in field_counts.items()\n",
    "        }\n",
    "    \n",
    "    def inspect_files(self) -> None:\n",
    "        \"\"\"Inspect and print analysis of both data files.\"\"\"\n",
    "        # Analyze triples file\n",
    "        print(\"\\n=== RareDisease_Phenotype_Triples.json Analysis ===\")\n",
    "        try:\n",
    "            with open(self.triples_path) as f:\n",
    "                triples_data = json.load(f)\n",
    "            \n",
    "            print(\"\\nStructure:\")\n",
    "            structure = self._analyze_structure(triples_data)\n",
    "            self._print_structure(structure)\n",
    "            \n",
    "            if isinstance(triples_data, list):\n",
    "                print(f\"\\nTotal entries: {len(triples_data)}\")\n",
    "                if triples_data:\n",
    "                    field_stats = self._count_field_occurrences(triples_data)\n",
    "                    print(\"\\nField coverage:\")\n",
    "                    for field, percentage in sorted(field_stats.items()):\n",
    "                        print(f\"  {field}: {percentage:.1f}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing triples file: {e}\")\n",
    "        \n",
    "        # Analyze ontology file\n",
    "        print(\"\\n=== rare_disease_ontology.jsonl Analysis ===\")\n",
    "        try:\n",
    "            ontology_data = self._load_jsonl(self.ontology_path)\n",
    "            \n",
    "            print(f\"\\nTotal entries: {len(ontology_data)}\")\n",
    "            if ontology_data:\n",
    "                print(\"\\nStructure of first entry:\")\n",
    "                structure = self._analyze_structure(ontology_data[0])\n",
    "                self._print_structure(structure)\n",
    "                \n",
    "                field_stats = self._count_field_occurrences(ontology_data)\n",
    "                print(\"\\nField coverage:\")\n",
    "                for field, percentage in sorted(field_stats.items()):\n",
    "                    print(f\"  {field}: {percentage:.1f}%\")\n",
    "                \n",
    "                # Analyze ORPHA IDs\n",
    "                orpha_ids = [entry.get('id') for entry in ontology_data if 'id' in entry]\n",
    "                malformed_ids = [id for id in orpha_ids if not str(id).startswith('ORPHA:')]\n",
    "                print(f\"\\nORPHA ID analysis:\")\n",
    "                print(f\"  Total IDs: {len(orpha_ids)}\")\n",
    "                print(f\"  Malformed IDs: {len(malformed_ids)}\")\n",
    "                if malformed_ids:\n",
    "                    print(f\"  Sample malformed ID: {malformed_ids[0]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing ontology file: {e}\")\n",
    "    \n",
    "    def _print_structure(self, structure: Dict, indent: int = 2) -> None:\n",
    "        \"\"\"Pretty print the structure analysis.\"\"\"\n",
    "        def _print_level(data: Dict, level: int = 0):\n",
    "            prefix = \" \" * (level * indent)\n",
    "            \n",
    "            if \"type\" in data:\n",
    "                print(f\"{prefix}Type: {data['type']}\")\n",
    "                if data.get(\"length\") is not None:\n",
    "                    print(f\"{prefix}Length: {data['length']}\")\n",
    "                if data.get(\"sample\"):\n",
    "                    print(f\"{prefix}Sample: {data['sample']}\")\n",
    "            \n",
    "            if \"keys\" in data:\n",
    "                print(f\"{prefix}Keys:\")\n",
    "                for key, value in data[\"keys\"].items():\n",
    "                    print(f\"{prefix}{key}:\")\n",
    "                    _print_level(value, level + 1)\n",
    "            \n",
    "            if \"element_sample\" in data:\n",
    "                print(f\"{prefix}Element sample:\")\n",
    "                _print_level(data[\"element_sample\"], level + 1)\n",
    "        \n",
    "        _print_level(structure)\n",
    "\n",
    "def inspect_rare_disease_files(triples_path: str, ontology_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Convenience function to inspect rare disease data files.\n",
    "    \n",
    "    Args:\n",
    "        triples_path: Path to RareDisease_Phenotype_Triples.json\n",
    "        ontology_path: Path to rare_disease_ontology.jsonl\n",
    "    \"\"\"\n",
    "    inspector = RareDiseaseDataInspector(triples_path, ontology_path)\n",
    "    inspector.inspect_files()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    inspect_rare_disease_files(\n",
    "        \"/home/johnwu3/projects/rare_disease/workspace/ontology/RareDisease_Phenotype_Triples.json\",\n",
    "        \"/home/johnwu3/projects/rare_disease/workspace/repos/AutoRD/data/rare_disease_ontology.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing the SimpleAutoRD pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from difflib import get_close_matches\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class Entity:\n",
    "    name: str  # Original extracted term\n",
    "    start: int\n",
    "    end: int\n",
    "    entity_type: str\n",
    "    negated: bool = False\n",
    "    orpha_id: Optional[str] = None\n",
    "    confidence: float = 1.0\n",
    "    extracted_phrase: str = \"\"  # The exact phrase from text\n",
    "    matched_orpha_name: Optional[str] = None  # The name from Orphanet if matched\n",
    "\n",
    "class SimpleAutoRD:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline: Any,\n",
    "        rare_disease_ontology_path: str,\n",
    "    ):\n",
    "        self.pipeline = pipeline\n",
    "        self.rare_disease_ontology = self._load_jsonl(rare_disease_ontology_path)\n",
    "        # Create a mapping of disease names to their ontology entries\n",
    "        self.disease_mapping = {}\n",
    "        for entry in self.rare_disease_ontology:\n",
    "            # Keep the original ID format but also add a normalized version\n",
    "            normalized_entry = {\n",
    "                'name': entry['name'],\n",
    "                'id': entry['id'],  # Keep original format\n",
    "                'orpha_id': entry['id'],  # Add normalized version\n",
    "                'definition': entry.get('definition', ''),\n",
    "            }\n",
    "            self.disease_mapping[entry['name'].lower()] = normalized_entry\n",
    "\n",
    "    def inspect_ontology(self) -> None:\n",
    "        \"\"\"Print the structure and statistics of the loaded ontology file.\"\"\"\n",
    "        print(\"\\nOntology Structure Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(f\"Total entries: {len(self.rare_disease_ontology)}\")\n",
    "        \n",
    "        # Analyze structure of first few entries\n",
    "        if self.rare_disease_ontology:\n",
    "            print(\"\\nSample entry structure:\")\n",
    "            sample_entry = self.rare_disease_ontology[0]\n",
    "            for key, value in sample_entry.items():\n",
    "                print(f\"  {key}: {type(value).__name__} = {value[:100] if isinstance(value, str) else value}\")\n",
    "        \n",
    "        # Analyze key consistency\n",
    "        all_keys = set()\n",
    "        key_counts = {}\n",
    "        \n",
    "        for entry in self.rare_disease_ontology:\n",
    "            entry_keys = set(entry.keys())\n",
    "            all_keys.update(entry_keys)\n",
    "            \n",
    "            # Count how many entries have each key\n",
    "            for key in entry_keys:\n",
    "                key_counts[key] = key_counts.get(key, 0) + 1\n",
    "        \n",
    "        print(\"\\nKey statistics:\")\n",
    "        total_entries = len(self.rare_disease_ontology)\n",
    "        for key in sorted(all_keys):\n",
    "            count = key_counts[key]\n",
    "            percentage = (count / total_entries) * 100\n",
    "            print(f\"  {key}: present in {count}/{total_entries} entries ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Check for potential issues\n",
    "        print(\"\\nPotential issues:\")\n",
    "        required_keys = {'name', 'id'}  # Updated required keys\n",
    "        missing_required = [entry for entry in self.rare_disease_ontology \n",
    "                        if not all(key in entry for key in required_keys)]\n",
    "        \n",
    "        if missing_required:\n",
    "            print(f\"- {len(missing_required)} entries missing required keys\")\n",
    "            print(\"  Sample problematic entry:\")\n",
    "            print(f\"  {missing_required[0]}\")\n",
    "        \n",
    "        # Value analysis for critical fields\n",
    "        print(\"\\nValue analysis:\")\n",
    "        print(\"Name statistics:\")\n",
    "        empty_names = sum(1 for entry in self.rare_disease_ontology if not entry.get('name'))\n",
    "        print(f\"- Empty names: {empty_names}\")\n",
    "        \n",
    "        print(\"\\nORPHA ID statistics:\")\n",
    "        orpha_ids = [entry['id'] for entry in self.rare_disease_ontology if 'id' in entry]\n",
    "        malformed_ids = [id for id in orpha_ids if not id.startswith('ORPHA:')]\n",
    "        print(f\"- Total ORPHA IDs: {len(orpha_ids)}\")\n",
    "        print(f\"- Malformed ORPHA IDs: {len(malformed_ids)}\")\n",
    "        if malformed_ids:\n",
    "            print(f\"  Sample malformed ID: {malformed_ids[0]}\")\n",
    "            \n",
    "        print(\"\\nDefinition statistics:\")\n",
    "        has_definition = sum(1 for entry in self.rare_disease_ontology if entry.get('definition'))\n",
    "        print(f\"- Entries with definitions: {has_definition}/{total_entries} ({(has_definition/total_entries)*100:.1f}%)\")\n",
    "        \n",
    "    def _load_jsonl(self, path: str) -> List[Dict]:\n",
    "        with open(path) as f:\n",
    "            return [json.loads(line) for line in f]\n",
    "\n",
    "    def _generate_text(self, prompt: str) -> str:\n",
    "        \"\"\"Generate text using the pipeline with chat template.\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are an expert in healthcare and biomedical domain. Extract medical entities accurately.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        full_prompt = self.pipeline.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        outputs = self.pipeline(\n",
    "            full_prompt,\n",
    "            max_new_tokens=20000, # 20k new tokens to see full response in case it's that long.\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        \n",
    "        return outputs[0][\"generated_text\"][len(full_prompt):]\n",
    "\n",
    "    def _extract_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract medical terms from text, returning a simple list of terms.\"\"\"\n",
    "        prompt = f\"\"\"Extract all diseases and conditions that are NOT negated (i.e., don't include terms that are preceded by 'no', 'not', 'without', etc.) from the text below.\n",
    "\n",
    "            Text: {text}\n",
    "\n",
    "            Return only a Python list of strings, with each term exactly as it appears in the text.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self._generate_text(prompt).strip()\n",
    "            # print(\"LLM DEBUG:\", response)\n",
    "            \n",
    "            # Extract content between square brackets if present\n",
    "            if '[' in response and ']' in response:\n",
    "                response = response[response.find('[') + 1:response.rfind(']')]\n",
    "            \n",
    "            # Split on commas and clean up each term\n",
    "            terms = []\n",
    "            for term in response.split(','):\n",
    "                # Clean up the term\n",
    "                cleaned_term = term.strip()\n",
    "                # Remove surrounding quotes (single or double)\n",
    "                cleaned_term = cleaned_term.strip('\"').strip(\"'\")\n",
    "                # Only add non-empty terms\n",
    "                if cleaned_term:\n",
    "                    terms.append(cleaned_term)\n",
    "            \n",
    "            # print(\"TERMS DEBUG:\", terms)\n",
    "            return terms\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in term extraction: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _string_similarity(self, s1: str, s2: str) -> float:\n",
    "        \"\"\"Calculate string similarity score using a combination of metrics.\"\"\"\n",
    "        from difflib import SequenceMatcher\n",
    "        \n",
    "        # Convert to lowercase for comparison\n",
    "        s1, s2 = s1.lower(), s2.lower()\n",
    "        \n",
    "        # Get word sets\n",
    "        words1 = set(s1.split())\n",
    "        words2 = set(s2.split())\n",
    "        \n",
    "        # Calculate Jaccard similarity for words\n",
    "        word_similarity = len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "        \n",
    "        # Calculate sequence similarity\n",
    "        seq_similarity = SequenceMatcher(None, s1, s2).ratio()\n",
    "        \n",
    "        # Combine both metrics (weighing sequence similarity more heavily)\n",
    "        return (0.3 * word_similarity + 0.7 * seq_similarity)\n",
    "\n",
    "    def _verify_match_with_llm(self, term: str, candidates: List[Tuple[str, float]]) -> Optional[str]:\n",
    "        \"\"\"Use LLM to verify the best match among candidates.\"\"\"\n",
    "        if not candidates:\n",
    "            return None\n",
    "            \n",
    "        candidates_str = \"\\n\".join(f\"- {name} (similarity: {score:.2f})\" \n",
    "                                 for name, score in candidates)\n",
    "        \n",
    "        prompt = f\"\"\"Given a medical term and potential matches from our ontology, determine if any are valid matches.\n",
    "\n",
    "Term: {term}\n",
    "\n",
    "Potential matches:\n",
    "{candidates_str}\n",
    "\n",
    "Respond with only the name of the best matching term, or \"none\" if none are valid matches.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self._generate_text(prompt).strip().lower()\n",
    "            if response == \"none\":\n",
    "                return None\n",
    "            # Find the candidate that most closely matches the LLM's response\n",
    "            matches = get_close_matches(response, [c[0] for c in candidates], n=1, cutoff=0.9)\n",
    "            return matches[0] if matches else None\n",
    "        except Exception:\n",
    "            return candidates[0][0] if candidates else None\n",
    "        \n",
    "    def _split_text(self, text: str, max_chunk_size: int = 4000) -> List[str]:\n",
    "        \"\"\"Split text into chunks, ensuring splits occur at delimiters.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to split\n",
    "            max_chunk_size: Maximum size of each chunk before forcing a split\n",
    "            \n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        # Define preferred split points in order of preference\n",
    "        delimiters = [\"\\n\\n\", \"\\n\", \". \", \"; \", \", \"]\n",
    "        \n",
    "        # Helper function to find the best delimiter position\n",
    "        def find_split_point(text: str, max_size: int) -> int:\n",
    "            if len(text) <= max_size:\n",
    "                return len(text)\n",
    "                \n",
    "            # Try each delimiter in order of preference\n",
    "            for delimiter in delimiters:\n",
    "                # Look for the last delimiter before max_size\n",
    "                pos = text[:max_size].rfind(delimiter)\n",
    "                if pos > 0:  # Found a good split point\n",
    "                    return pos + len(delimiter)\n",
    "                    \n",
    "            # If no good delimiter found, force split at max_size\n",
    "            return max_size\n",
    "        \n",
    "        while text:\n",
    "            split_point = find_split_point(text, max_chunk_size)\n",
    "            \n",
    "            if split_point == len(text):\n",
    "                chunks.append(text)\n",
    "                break\n",
    "                \n",
    "            # Add chunk and continue with remaining text\n",
    "            chunks.append(text[:split_point].strip())\n",
    "            text = text[split_point:].strip()\n",
    "        \n",
    "        return [c for c in chunks if c]  # Remove any empty chunks\n",
    "        \n",
    "    def process_text(self, text: str) -> List[Entity]:\n",
    "        \"\"\"Process text through the pipeline.\"\"\"\n",
    "        # Split text into natural chunks\n",
    "        chunks = self._split_text(text)\n",
    "        \n",
    "        # Process each chunk and combine results\n",
    "        all_entities = []\n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                chunk_entities = self._process_chunk(chunk)\n",
    "                all_entities.extend(chunk_entities)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return all_entities\n",
    "    \n",
    "    def _fuzzy_match_disease(self, term: str, threshold: float = 0.6) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find closest matching diseases in ontology using fuzzy matching.\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples (disease_name, similarity_score) above threshold\n",
    "        \"\"\"\n",
    "        term = term.lower()\n",
    "        matches = []\n",
    "        \n",
    "        # First try exact word set matching\n",
    "        term_words = set(term.split())\n",
    "        \n",
    "        for disease_name in self.disease_mapping.keys():\n",
    "            # Calculate similarity score\n",
    "            similarity = self._string_similarity(term, disease_name)\n",
    "            if similarity >= threshold:\n",
    "                matches.append((disease_name, similarity))\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        matches.sort(key=lambda x: x[1], reverse=True)\n",
    "        return matches[:5]  # Return top 5 matches\n",
    "\n",
    "\n",
    "    def _verify_rare_disease_with_llm(self, term: str, ontology_entry: Optional[Dict] = None) -> Tuple[bool, float]:\n",
    "        \"\"\"Verify if the term is actually a rare disease using LLM.\n",
    "        \n",
    "        Args:\n",
    "            term: The extracted term to verify\n",
    "            ontology_entry: Optional ontology entry if available\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[bool, float]: (is_rare_disease, confidence)\n",
    "        \"\"\"\n",
    "        context = \"\"\n",
    "        if ontology_entry:\n",
    "            context = f\"\\nOrphanet information:\\nDisease: {ontology_entry['name']}\\nOrpha ID: {ontology_entry['orpha_id']}\"\n",
    "        \n",
    "        prompt = f\"\"\"Determine if the following medical term represents a rare disease.\n",
    "\n",
    "        Term: {term}{context}\n",
    "\n",
    "        Consider:\n",
    "        1. Is this a disease (not just a symptom or condition)?\n",
    "        2. Is it rare (affecting less than 1 in 2000 people)?\n",
    "        3. If an Orphanet entry is provided, does the term actually match it?\n",
    "        4. If the orphanet diseases have rare in its name, make sure that the extracted term contains the word rare for it to be considered a rare disease.\n",
    "        5. Specific forms/variants of common diseases are not rare diseases unless explicitly stated as rare in the term and context.\n",
    "        \n",
    "        Respond with only one line in this format:\n",
    "        DECISION: true/false\n",
    "\n",
    "        Example responses:\n",
    "        DECISION: true\n",
    "        DECISION: false\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self._generate_text(prompt).strip().lower()\n",
    "            # Extract decision\n",
    "            match = re.search(r'decision:\\s*(true|false)', response)\n",
    "            if match:\n",
    "                return match.group(1) == 'true'\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error in LLM verification: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _process_chunk(self, text: str) -> List[Entity]:\n",
    "        \"\"\"Process a single chunk of text.\"\"\"\n",
    "        try:\n",
    "            extracted_terms = self._extract_terms(text)\n",
    "            \n",
    "            # Deduplicate terms while preserving order\n",
    "            seen = set()\n",
    "            unique_terms = []\n",
    "            for term in extracted_terms:\n",
    "                if term.lower() not in seen:\n",
    "                    seen.add(term.lower())\n",
    "                    unique_terms.append(term)\n",
    "            \n",
    "            entities = []\n",
    "            for term in unique_terms:\n",
    "                try:\n",
    "                    # Find the exact phrase in text\n",
    "                    term_pattern = re.escape(term)\n",
    "                    matches = list(re.finditer(term_pattern, text, re.IGNORECASE))\n",
    "                    if not matches:\n",
    "                        continue\n",
    "                        \n",
    "                    # First try exact matching\n",
    "                    ontology_entry = self.disease_mapping.get(term.lower())\n",
    "                    matched_name = None\n",
    "                    \n",
    "                    if not ontology_entry:\n",
    "                        # If no exact match, try fuzzy matching\n",
    "                        candidates = self._fuzzy_match_disease(term)\n",
    "                        if candidates:\n",
    "                            # Only verify with LLM if we have close matches\n",
    "                            if candidates[0][1] > 0.9:  # High confidence match\n",
    "                                best_match = candidates[0][0]\n",
    "                                ontology_entry = self.disease_mapping[best_match]\n",
    "                                matched_name = best_match\n",
    "                            else:\n",
    "                                # Verify with LLM for less confident matches\n",
    "                                best_match = self._verify_match_with_llm(term, candidates)\n",
    "                                if best_match:\n",
    "                                    ontology_entry = self.disease_mapping[best_match]\n",
    "                                    matched_name = best_match\n",
    "                    \n",
    "                    # Verify if it's actually a rare disease using LLM\n",
    "                    is_rare = self._verify_rare_disease_with_llm(term, ontology_entry)\n",
    "                    \n",
    "                    # Only create entity if LLM confirms it's a rare disease\n",
    "                    if is_rare:\n",
    "                        for match in matches:\n",
    "                            entity = Entity(\n",
    "                                name=term,\n",
    "                                start=match.start(),\n",
    "                                end=match.end(),\n",
    "                                entity_type='rare_disease',\n",
    "                                orpha_id=ontology_entry.get('orpha_id') if ontology_entry else None,\n",
    "                                extracted_phrase=match.group(),\n",
    "                                matched_orpha_name=matched_name or (ontology_entry['name'] if ontology_entry else None)\n",
    "                            )\n",
    "                            entities.append(entity)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing term '{term}': {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "            return entities\n",
    "        except Exception as e:\n",
    "            print(f\"Error in _process_chunk: {str(e)}\")\n",
    "            return []\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ModelLoader with cache directory: /shared/rsaas/jw3/rare_disease/model_cache\n",
      "Initialized ModelLoader with cache directory: /shared/rsaas/jw3/rare_disease/model_cache\n",
      "Loading llama3_70b model...\n",
      "Loading LLM!\n",
      "Loading 70B model: llama3_70b\n",
      "Generated cache path: /shared/rsaas/jw3/rare_disease/model_cache/Llama-3.3-70B-Instruct_4bit_nf4\n",
      "Valid cache found at /shared/rsaas/jw3/rare_disease/model_cache/Llama-3.3-70B-Instruct_4bit_nf4\n",
      "Loading cached quantized model from /shared/rsaas/jw3/rare_disease/model_cache/Llama-3.3-70B-Instruct_4bit_nf4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnwu3/miniconda3/envs/hporag/lib/python3.10/site-packages/transformers/quantizers/auto.py:195: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:12,  1.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/johnwu3/projects/rare_disease/workspace/repos/AutoRD/test.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/johnwu3/projects/rare_disease/workspace/repos/AutoRD/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m model...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/johnwu3/projects/rare_disease/workspace/repos/AutoRD/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda:1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/johnwu3/projects/rare_disease/workspace/repos/AutoRD/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m pipeline \u001b[39m=\u001b[39m model_loader\u001b[39m.\u001b[39;49mget_llm_pipeline(device, model)\n",
      "File \u001b[0;32m~/projects/rare_disease/workspace/repos/AutoRD/extraction/llm.py:57\u001b[0m, in \u001b[0;36mModelLoader.get_llm_pipeline\u001b[0;34m(self, devices, model)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m70b\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlower():\n\u001b[1;32m     56\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoading 70B model:\u001b[39m\u001b[39m\"\u001b[39m, model)\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_70b_model(device_map, model)\n\u001b[1;32m     58\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m8b\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlower():\n\u001b[1;32m     59\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoading 8B model:\u001b[39m\u001b[39m\"\u001b[39m, model)\n",
      "File \u001b[0;32m~/projects/rare_disease/workspace/repos/AutoRD/extraction/llm.py:110\u001b[0m, in \u001b[0;36mModelLoader.load_70b_model\u001b[0;34m(self, device_map, model)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_cached(cache_path):\n\u001b[1;32m    109\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading cached quantized model from \u001b[39m\u001b[39m{\u001b[39;00mcache_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m     model_nf4 \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mAutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    111\u001b[0m         cache_path,\n\u001b[1;32m    112\u001b[0m         quantization_config\u001b[39m=\u001b[39;49mnf4_config,\n\u001b[1;32m    113\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel not cached. Loading and quantizing to \u001b[39m\u001b[39m{\u001b[39;00mcache_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hporag/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hporag/lib/python3.10/site-packages/transformers/modeling_utils.py:4245\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4235\u001b[0m         load_contexts\u001b[39m.\u001b[39mappend(tp_device)\n\u001b[1;32m   4237\u001b[0m     \u001b[39mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[1;32m   4238\u001b[0m         (\n\u001b[1;32m   4239\u001b[0m             model,\n\u001b[1;32m   4240\u001b[0m             missing_keys,\n\u001b[1;32m   4241\u001b[0m             unexpected_keys,\n\u001b[1;32m   4242\u001b[0m             mismatched_keys,\n\u001b[1;32m   4243\u001b[0m             offload_index,\n\u001b[1;32m   4244\u001b[0m             error_msgs,\n\u001b[0;32m-> 4245\u001b[0m         ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   4246\u001b[0m             model,\n\u001b[1;32m   4247\u001b[0m             state_dict,\n\u001b[1;32m   4248\u001b[0m             loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4249\u001b[0m             resolved_archive_file,\n\u001b[1;32m   4250\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   4251\u001b[0m             ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   4252\u001b[0m             sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   4253\u001b[0m             _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   4254\u001b[0m             low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   4255\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   4256\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   4257\u001b[0m             offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   4258\u001b[0m             dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   4259\u001b[0m             hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   4260\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   4261\u001b[0m             gguf_path\u001b[39m=\u001b[39;49mgguf_path,\n\u001b[1;32m   4262\u001b[0m             weights_only\u001b[39m=\u001b[39;49mweights_only,\n\u001b[1;32m   4263\u001b[0m         )\n\u001b[1;32m   4265\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4266\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/hporag/lib/python3.10/site-packages/transformers/modeling_utils.py:4815\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4813\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4814\u001b[0m         fixed_state_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_fix_state_dict_keys_on_load(state_dict)\n\u001b[0;32m-> 4815\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4816\u001b[0m             model_to_load,\n\u001b[1;32m   4817\u001b[0m             fixed_state_dict,\n\u001b[1;32m   4818\u001b[0m             start_prefix,\n\u001b[1;32m   4819\u001b[0m             expected_keys,\n\u001b[1;32m   4820\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   4821\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   4822\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   4823\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   4824\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   4825\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   4826\u001b[0m             hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   4827\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   4828\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   4829\u001b[0m             unexpected_keys\u001b[39m=\u001b[39;49munexpected_keys,\n\u001b[1;32m   4830\u001b[0m         )\n\u001b[1;32m   4831\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   4832\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4833\u001b[0m     \u001b[39m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hporag/lib/python3.10/site-packages/transformers/modeling_utils.py:875\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    873\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 875\u001b[0m     hf_quantizer\u001b[39m.\u001b[39;49mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n\u001b[1;32m    876\u001b[0m     \u001b[39m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[1;32m    877\u001b[0m     \u001b[39m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[1;32m    878\u001b[0m     \u001b[39m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mif\u001b[39;00m is_fsdp_enabled() \u001b[39mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "File \u001b[0;32m~/miniconda3/envs/hporag/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:226\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.create_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_bnb_supports_quant_storage_module:\n\u001b[1;32m    224\u001b[0m         param_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodule\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m module\n\u001b[0;32m--> 226\u001b[0m     new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mParams4bit\u001b[39m.\u001b[39;49mfrom_prequantized(\n\u001b[1;32m    227\u001b[0m         data\u001b[39m=\u001b[39;49mparam_value,\n\u001b[1;32m    228\u001b[0m         quantized_stats\u001b[39m=\u001b[39;49mquantized_stats,\n\u001b[1;32m    229\u001b[0m         requires_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    230\u001b[0m         device\u001b[39m=\u001b[39;49mtarget_device,\n\u001b[1;32m    231\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparam_kwargs,\n\u001b[1;32m    232\u001b[0m     )\n\u001b[1;32m    233\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     new_value \u001b[39m=\u001b[39m param_value\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hporag/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:278\u001b[0m, in \u001b[0;36mParams4bit.from_prequantized\u001b[0;34m(cls, data, quantized_stats, requires_grad, device, module, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mfrom_prequantized\u001b[39m(\n\u001b[1;32m    270\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    277\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mParams4bit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 278\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor\u001b[39m.\u001b[39m_make_subclass(\u001b[39mcls\u001b[39m, data\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m requires_grad\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_state \u001b[39m=\u001b[39m QuantState\u001b[39m.\u001b[39mfrom_dict(qs_dict\u001b[39m=\u001b[39mquantized_stats, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from extraction.llm import ModelLoader\n",
    "cache_dir = \"/u/zelalae2/scratch/rdma_cache\"\n",
    "print(f\"Initializing ModelLoader with cache directory: {cache_dir}\")\n",
    "model_loader = ModelLoader(cache_dir=cache_dir)\n",
    "model = \"llama3_70b\"\n",
    "print(f\"Loading {model} model...\")\n",
    "device = \"cuda:1\"\n",
    "pipeline = model_loader.get_llm_pipeline(device, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['287', '869', '1423', '1208', '950', '977', '1552', '1284', '1790', '2452', '2541', '2795', '3060', '3149', '3390', '4226', '4043', '4735', '4806', '4938', '5873', '10406', '5738', '6465', '5811', '7363', '13231', '7688', '6188', '8979', '7699', '10859', '6206', '10004', '10715', '7949', '8877', '9512', '6819', '8001', '8960', '7519', '13600', '11238', '16328', '16330', '16334', '16347', '13666', '14936', '11052', '11938', '11838', '11996', '11604', '18299', '22297', '17850', '15908', '17640', '15693', '20208', '31392', '24716', '30442', '31210', '24774', '31762', '29858', '20162', '32721', '25095', '20589', '20617', '23916', '22133', '20387', '21573', '21179', '21197', '39676', '38775', '37158', '37801', '39348', '40555', '37217', '37240', '40196', '25829', '28965', '26976', '26123', '27335', '29045', '29048', '26825', '30504', '32261', '28690', '41498', '47026', '47035', '45138', '42584', '42597', '44132', '44150', '44472', '46706', '45078', '46361', '46820', '46636', '32946', '35397', '34416', '35579', '35597', '33110', '35658', '33192', '35228', '34689', '33893', '40395', '33909', '34305', '34316', '36547', '35323', '36588', '34391', '52501', '50797', '51891', '52841', '52528', '51061', '52206', '49795', '49021', '41073', '41300', '42827', '41451', '44412', '41455', '41466', '44246', '42186', '41991', '43823', '41739', '53850', '54806', '54929', '54937', '49532', '49955', '55265', '48587', '47713', '49482', '48386', '48652', '49514', '49519', '47233', '53382', '53406', '53691', '59366'])\n"
     ]
    }
   ],
   "source": [
    "def load_mimic_annotations(file_path: str) -> Dict:\n",
    "    \"\"\"Load and parse the MIMIC annotations file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "annotations_file = \"/home/johnwu3/projects/rare_disease/workspace/repos/RareDiseaseMention/filtered_rd_annos.json\"\n",
    "annotations = load_mimic_annotations(annotations_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'mention': 'sick sinus syndrome', 'umls_with_desc': 'C0037052 Sick Sinus Syndrome', 'ordo_with_desc': 'Orphanet_166282  Familial sick sinus syndrome', 'gold_text_to_umls_label': 1, 'gold_text_to_ordo_label': 0, 'document_structure': 'History_of_Past_Illness', 'semehr_label': 1}]\n"
     ]
    }
   ],
   "source": [
    "# print(annotations['287'])\n",
    "auto_rd = SimpleAutoRD(pipeline, \"data_preprocessing/data/rare_disease_ontology.jsonl\")\n",
    "test_text = annotations['287']['note_details']['text']\n",
    "test_annotations = annotations['287']['annotations']\n",
    "# rare_diseases = test_annotations.keys()\n",
    "print(test_annotations)\n",
    "mentions = []\n",
    "for annotation in test_annotations:\n",
    "    mention = annotation['mention']\n",
    "    mentions.append(mention)\n",
    "\n",
    "entities = auto_rd.process_text(test_text)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ontology Structure Analysis:\n",
      "--------------------------------------------------\n",
      "Total entries: 14588\n",
      "\n",
      "Sample entry structure:\n",
      "  id: str = ORPHA:10\n",
      "  name: str = 48,XXYY syndrome\n",
      "  definition: str = A rare sex chromosome number anomaly disorder characterized, genetically, by the presence of an extr\n",
      "\n",
      "Key statistics:\n",
      "  definition: present in 14588/14588 entries (100.0%)\n",
      "  id: present in 14588/14588 entries (100.0%)\n",
      "  name: present in 14588/14588 entries (100.0%)\n",
      "\n",
      "Potential issues:\n",
      "- 14588 entries missing required keys\n",
      "  Sample problematic entry:\n",
      "  {'id': 'ORPHA:10', 'name': '48,XXYY syndrome', 'definition': 'A rare sex chromosome number anomaly disorder characterized, genetically, by the presence of an extra X and Y chromosome in males and, clinically, by tall stature, dysfunctional testes associated with infertility and insufficient testosterone production, cognitive, affective and social functioning impairments, global developmental delay, and an increased risk of congenital malformations.'}\n",
      "\n",
      "Value analysis:\n",
      "- Empty names: 0\n",
      "- Empty ORPHA IDs: 14588\n"
     ]
    }
   ],
   "source": [
    "auto_rd.inspect_ontology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Entity(name='sick sinus syndrome', start=3609, end=3628, entity_type='rare_disease', negated=False, orpha_id='ORPHA:166282', confidence=1.0, extracted_phrase='sick sinus syndrome', matched_orpha_name='familial sick sinus syndrome')]\n",
      "['sick sinus syndrome']\n"
     ]
    }
   ],
   "source": [
    "def get_rare_diseases(entities) -> List[Entity]:\n",
    "    \"\"\"\n",
    "    Process text and return only the rare disease entities.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to process\n",
    "        \n",
    "    Returns:\n",
    "        List[Entity]: List of entities where entity_type is 'rare_disease'\n",
    "    \"\"\"\n",
    "    # Process the text to get all entities\n",
    "    \n",
    "    # Filter for only rare disease entities\n",
    "    rare_diseases = [\n",
    "        entity for entity in entities \n",
    "        if entity.entity_type == 'rare_disease' and entity.orpha_id is not None\n",
    "    ]\n",
    "    \n",
    "    return rare_diseases\n",
    "rds = get_rare_diseases(entities)\n",
    "for rd in rds:\n",
    "    rd.extracted_phrase in mentions\n",
    "print(rds)\n",
    "print(mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Cases for _extract_terms ---\n",
      "\n",
      "Running test: Simple diseases\n",
      "Extracting terms from text...\n",
      "Generating response from LLM...\n",
      "\n",
      "Parsing LLM response...\n",
      "\n",
      "Extracted 2 valid terms\n",
      "Sample terms:\n",
      "  - type 2 diabetes\n",
      "  - hypertension\n",
      "  Text: 'The patient has type 2 diabetes and hypertension.'\n",
      "  Expected Terms: ['type 2 diabetes', 'hypertension']\n",
      "  Extracted Terms: ['type 2 diabetes', 'hypertension']\n",
      "  Test 'Simple diseases' passed!\n",
      "\n",
      "\n",
      "Running test: Negated term\n",
      "Extracting terms from text...\n",
      "Generating response from LLM...\n",
      "\n",
      "Parsing LLM response...\n",
      "\n",
      "Extracted 1 valid terms\n",
      "Sample terms:\n",
      "  - COPD\n",
      "  Text: 'The patient has no asthma, but does have COPD.'\n",
      "  Expected Terms: ['COPD']\n",
      "  Extracted Terms: ['COPD']\n",
      "  Test 'Negated term' passed!\n",
      "\n",
      "\n",
      "Running test: Symptoms and Conditions\n",
      "Extracting terms from text...\n",
      "Generating response from LLM...\n",
      "\n",
      "Parsing LLM response...\n",
      "\n",
      "Extracted 4 valid terms\n",
      "Sample terms:\n",
      "  - fatigue\n",
      "  - joint pain\n",
      "  - muscle weakness\n",
      "  - rheumatoid arthritis\n",
      "  Text: 'Symptoms include fatigue, joint pain, and muscle weakness.  The condition is rheumatoid arthritis.'\n",
      "  Expected Terms: ['fatigue', 'joint pain', 'muscle weakness', 'rheumatoid arthritis']\n",
      "  Extracted Terms: ['fatigue', 'joint pain', 'muscle weakness', 'rheumatoid arthritis']\n",
      "  Test 'Symptoms and Conditions' passed!\n",
      "\n",
      "\n",
      "Running test: Comma-separated list\n",
      "Extracting terms from text...\n",
      "Generating response from LLM...\n",
      "\n",
      "Parsing LLM response...\n",
      "\n",
      "Extracted 3 valid terms\n",
      "Sample terms:\n",
      "  - fever\n",
      "  - cough\n",
      "  - malaise\n",
      "  Text: 'The patient presented with fever, cough, and malaise.'\n",
      "  Expected Terms: ['fever', 'cough', 'malaise']\n",
      "  Extracted Terms: ['fever', 'cough', 'malaise']\n",
      "  Test 'Comma-separated list' passed!\n",
      "\n",
      "\n",
      "Running test: No medical terms\n",
      "Extracting terms from text...\n",
      "Generating response from LLM...\n",
      "\n",
      "Parsing LLM response...\n",
      "\n",
      "Extracted 0 valid terms\n",
      "  Text: 'This is a normal sentence with no medical terms.'\n",
      "  Expected Terms: []\n",
      "  Extracted Terms: []\n",
      "  Test 'No medical terms' passed!\n",
      "\n",
      "\n",
      "Running test: Only negated terms\n",
      "Extracting terms from text...\n",
      "Generating response from LLM...\n",
      "\n",
      "Parsing LLM response...\n",
      "\n",
      "Extracted 0 valid terms\n",
      "  Text: 'The patient has no fever and no cough.'\n",
      "  Expected Terms: []\n",
      "  Extracted Terms: []\n",
      "  Test 'Only negated terms' passed!\n",
      "\n",
      "\n",
      "Running test: Terms with numbers and hyphens\n",
      "Extracting terms from text...\n",
      "Generating response from LLM...\n",
      "\n",
      "Parsing LLM response...\n",
      "\n",
      "Extracted 2 valid terms\n",
      "Sample terms:\n",
      "  - type 1 diabetes\n",
      "  - pre-diabetes\n",
      "  Text: 'Patient has type 1 diabetes and pre-diabetes.'\n",
      "  Expected Terms: ['type 1 diabetes', 'pre-diabetes']\n",
      "  Extracted Terms: ['type 1 diabetes', 'pre-diabetes']\n",
      "  Test 'Terms with numbers and hyphens' passed!\n",
      "\n",
      "--- _extract_terms tests completed ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "auto_rd = SimpleAutoRD(pipeline, \"data_preprocessing/data/rare_disease_ontology.jsonl\")\n",
    "print(\"\\n--- Test Cases for _extract_terms ---\")\n",
    "\n",
    "def test_extract_terms(text, expected_terms, test_name):\n",
    "    print(f\"\\nRunning test: {test_name}\")\n",
    "    extracted_terms = auto_rd._extract_terms(text)\n",
    "    print(f\"  Text: '{text}'\")\n",
    "    print(f\"  Expected Terms: {expected_terms}\")\n",
    "    print(f\"  Extracted Terms: {extracted_terms}\")\n",
    "    assert extracted_terms == expected_terms, f\"Test '{test_name}' failed. Expected {expected_terms}, but got {extracted_terms}\"\n",
    "    print(f\"  Test '{test_name}' passed!\\n\")\n",
    "\n",
    "\n",
    "# Test case 1: Simple case with a few diseases\n",
    "text1 = \"The patient has type 2 diabetes and hypertension.\"\n",
    "expected_terms1 = [\"type 2 diabetes\", \"hypertension\"]\n",
    "test_extract_terms(text1, expected_terms1, \"Simple diseases\")\n",
    "\n",
    "# Test case 2: Negated term\n",
    "text2 = \"The patient has no asthma, but does have COPD.\"\n",
    "expected_terms2 = [\"COPD\"]\n",
    "test_extract_terms(text2, expected_terms2, \"Negated term\")\n",
    "\n",
    "# Test case 3: Symptoms and conditions\n",
    "text3 = \"Symptoms include fatigue, joint pain, and muscle weakness.  The condition is rheumatoid arthritis.\"\n",
    "expected_terms3 = [\"fatigue\", \"joint pain\", \"muscle weakness\", \"rheumatoid arthritis\"]\n",
    "test_extract_terms(text3, expected_terms3, \"Symptoms and Conditions\")\n",
    "\n",
    "# Test case 4: Comma-separated list in a sentence\n",
    "text4 = \"The patient presented with fever, cough, and malaise.\"\n",
    "expected_terms4 = [\"fever\", \"cough\", \"malaise\"] # LLM might return slightly different order, adjust expected if needed.\n",
    "test_extract_terms(text4, expected_terms4, \"Comma-separated list\")\n",
    "\n",
    "# Test case 5: No medical terms\n",
    "text5 = \"This is a normal sentence with no medical terms.\"\n",
    "expected_terms5 = []\n",
    "test_extract_terms(text5, expected_terms5, \"No medical terms\")\n",
    "\n",
    "# Test case 6: Only negated term\n",
    "text6 = \"The patient has no fever and no cough.\"\n",
    "expected_terms6 = []\n",
    "test_extract_terms(text6, expected_terms6, \"Only negated terms\")\n",
    "\n",
    "# # Test case 7: Empty text\n",
    "# text7 = \"\"\n",
    "# expected_terms7 = []\n",
    "# test_extract_terms(text7, expected_terms7, \"Empty text\")\n",
    "\n",
    "# Test case 8:  Terms with numbers and hyphens\n",
    "text8 = \"Patient has type 1 diabetes and pre-diabetes.\"\n",
    "expected_terms8 = [\"type 1 diabetes\", \"pre-diabetes\"]\n",
    "test_extract_terms(text8, expected_terms8, \"Terms with numbers and hyphens\")\n",
    "\n",
    "print(\"--- _extract_terms tests completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Cases for _fuzzy_match_disease ---\n",
      "\n",
      "Running test: Exact match (threshold=0.6)\n",
      "['spinocerebellar ataxia with axonal neuropathy type 2', 'spinocerebellar ataxia type 2']\n",
      "\n",
      "Running test: Slightly fuzzy - Roman numeral (threshold=0.6)\n",
      "['spinocerebellar ataxia type 8', 'spinocerebellar ataxia type 7', 'spinocerebellar ataxia type 6', 'spinocerebellar ataxia type 5', 'spinocerebellar ataxia type 4']\n",
      "\n",
      "Running test: More fuzzy - typo (threshold=0.6)\n",
      "['spinocerebellar ataxia type 2', 'spinocerebellar ataxia type 42', 'spinocerebellar ataxia type 32', 'spinocerebellar ataxia type 29', 'spinocerebellar ataxia type 28']\n",
      "\n",
      "Running test: No match - different type (threshold=0.9)\n",
      "['spinocerebellar ataxia type 49', 'spinocerebellar ataxia type 29', 'spinocerebellar ataxia type 8', 'spinocerebellar ataxia type 7', 'spinocerebellar ataxia type 6']\n",
      "\n",
      "Running test: Possible matches at lower threshold (threshold=0.5)\n",
      "['spinocerebellar ataxia type 49', 'spinocerebellar ataxia type 29', 'spinocerebellar ataxia type 8', 'spinocerebellar ataxia type 7', 'spinocerebellar ataxia type 6']\n",
      "\n",
      "Running test: No match - unrelated term (threshold=0.6)\n",
      "['complete septate uterus']\n",
      "--- _fuzzy_match_disease tests completed ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Test Cases for _fuzzy_match_disease ---\")\n",
    "\n",
    "def test_fuzzy_match_disease(term, expected_matches, test_name, threshold=0.6):\n",
    "    print(f\"\\nRunning test: {test_name} (threshold={threshold})\")\n",
    "    matches = auto_rd._fuzzy_match_disease(term, threshold=threshold)\n",
    "    print(matches)\n",
    "    # print(f\"  Term: '{term}'\")\n",
    "    # print(f\"  Expected Matches: {expected_matches}\")\n",
    "    # print(f\"  Fuzzy Matches: {matches}\")\n",
    "    # assert set(matches) == set(expected_matches), f\"Test '{test_name}' failed. Expected matches to contain {expected_matches}, but got {matches}\"\n",
    "    # print(f\"  Test '{test_name}' passed!\\n\")\n",
    "\n",
    "# Assuming \"Spinocerebellar Ataxia Type 2\" and \"Spinocerebellar Ataxia Type 3\" are in your ontology\n",
    "\n",
    "# Test case 1: Exact match\n",
    "test_fuzzy_match_disease(\"Spinocerebellar Ataxia Type 2\", [\"Spinocerebellar Ataxia Type 2\"], \"Exact match\")\n",
    "\n",
    "# Test case 2: Slightly fuzzy match\n",
    "test_fuzzy_match_disease(\"Spinocerebellar Ataxia Type II\", [\"Spinocerebellar Ataxia Type 2\"], \"Slightly fuzzy - Roman numeral\")\n",
    "\n",
    "# Test case 3: More fuzzy, typo\n",
    "test_fuzzy_match_disease(\"Spinocerebellar Ataxia typ 2\", [\"Spinocerebellar Ataxia Type 2\"], \"More fuzzy - typo\")\n",
    "\n",
    "# Test case 4:  Similar but different disease (should not match highly at higher threshold)\n",
    "test_fuzzy_match_disease(\"Spinocerebellar Ataxia Type 99\", [], \"No match - different type\", threshold=0.9)\n",
    "test_fuzzy_match_disease(\"Spinocerebellar Ataxia Type 99\", ['Spinocerebellar Ataxia Type 2', 'Spinocerebellar Ataxia Type 3'], \"Possible matches at lower threshold\", threshold=0.5) # May match at lower threshold depending on ontology\n",
    "\n",
    "# Test case 5: No match at all\n",
    "test_fuzzy_match_disease(\"Completely unrelated term\", [], \"No match - unrelated term\")\n",
    "\n",
    "print(\"--- _fuzzy_match_disease tests completed ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hporag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
