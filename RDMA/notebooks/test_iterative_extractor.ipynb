{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking batched querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "def test_batch_processing_performance(client, batch_sizes=[2, 4, 8, 16], max_tokens=1000, timeout=None):\n",
    "    \"\"\"\n",
    "    Test and compare the performance of different batch processing methods.\n",
    "    \n",
    "    Args:\n",
    "        client: Initialized LocalLLMClient instance\n",
    "        batch_sizes (list): List of batch sizes to test\n",
    "        max_tokens (int): Maximum number of tokens to generate per response\n",
    "        timeout (int, optional): Maximum time in seconds for each test\n",
    "                              \n",
    "    Returns:\n",
    "        dict: Performance metrics\n",
    "    \"\"\"\n",
    "    # Create base test data\n",
    "    base_prompts = [\n",
    "        \"What is machine learning?\",\n",
    "        \"Explain the concept of neural networks.\",\n",
    "        \"How does natural language processing work?\",\n",
    "        \"What are the applications of computer vision?\",\n",
    "        \"Describe the key challenges in reinforcement learning.\",\n",
    "        \"What is deep learning?\",\n",
    "        \"Explain the concept of transfer learning.\",\n",
    "        \"How do transformers work in NLP?\",\n",
    "        \"What is the difference between supervised and unsupervised learning?\",\n",
    "        \"How does backpropagation work?\",\n",
    "        \"What is gradient descent?\",\n",
    "        \"Explain the concept of embeddings.\",\n",
    "        \"What are attention mechanisms?\",\n",
    "        \"How do GANs work?\",\n",
    "        \"What is reinforcement learning?\",\n",
    "        \"Explain the concept of Q-learning.\"\n",
    "    ]\n",
    "    print(len)\n",
    "    base_prompts = base_prompts + base_prompts\n",
    "    system_message = \"You are a helpful AI assistant specializing in AI and ML concepts.\"\n",
    "    \n",
    "    results = {\n",
    "        'batch_sizes': batch_sizes,\n",
    "        'sequential_times': [],\n",
    "        'batch_times': []\n",
    "    }\n",
    "    \n",
    "    # Get GPU info if available\n",
    "    gpu_info = \"N/A\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_info = torch.cuda.get_device_name(0)\n",
    "        print(f\"Testing on GPU: {gpu_info}\")\n",
    "    \n",
    "    # Get CPU count for reference\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    print(f\"Testing on a machine with {num_cores} CPU cores\")\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        if batch_size > len(base_prompts):\n",
    "            # Generate more prompts if needed by cycling through the base prompts\n",
    "            user_inputs = []\n",
    "            for i in range(batch_size):\n",
    "                idx = i % len(base_prompts)\n",
    "                user_inputs.append(f\"{base_prompts[idx]} (Example {i+1})\")\n",
    "        else:\n",
    "            user_inputs = base_prompts[:batch_size]\n",
    "            \n",
    "        system_messages = [system_message] * batch_size\n",
    "        \n",
    "        print(f\"\\n===== Testing with batch size {batch_size} =====\")\n",
    "        \n",
    "        # Test sequential processing\n",
    "        print(\"Running sequential processing...\")\n",
    "        start_time = time.time()\n",
    "        sequential_results = []\n",
    "        \n",
    "        try:\n",
    "            for i in range(batch_size):\n",
    "                if timeout and time.time() - start_time > timeout:\n",
    "                    print(f\"Sequential test timed out after {timeout} seconds\")\n",
    "                    break\n",
    "                result = client.query(user_inputs[i], system_messages[i])\n",
    "                sequential_results.append(result)\n",
    "            sequential_time = time.time() - start_time\n",
    "            print(f\"Sequential processing completed in {sequential_time:.2f} seconds\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in sequential processing: {e}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "        \n",
    "        # Test GPU batch processing\n",
    "        print(\"Running GPU batch processing...\")\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            batch_results = client.batched_query(user_inputs, system_messages)\n",
    "            batch_time = time.time() - start_time\n",
    "            print(f\"GPU batch processing completed in {batch_time:.2f} seconds\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in GPU batch processing: {e}\")\n",
    "            traceback.print_exc()\n",
    "            batch_time = float('inf')  # Set to infinity to indicate failure\n",
    "            batch_results = []\n",
    "            \n",
    "        # Calculate metrics\n",
    "        batch_speedup = sequential_time / batch_time if batch_time > 0 else 0\n",
    "        \n",
    "        # Store results\n",
    "        results['sequential_times'].append(sequential_time)\n",
    "        results['batch_times'].append(batch_time)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nResults for batch size {batch_size}:\")\n",
    "        print(f\"Sequential time: {sequential_time:.2f} seconds\")\n",
    "        print(f\"GPU batch time: {batch_time:.2f} seconds\")\n",
    "        print(f\"GPU batch speedup: {batch_speedup:.2f}x\")\n",
    "        \n",
    "        # Optional: Verify results match content length (exact matches unlikely due to sampling)\n",
    "        if len(sequential_results) > 0 and len(batch_results) > 0:\n",
    "            seq_lengths = [len(r) for r in sequential_results]\n",
    "            batch_lengths = [len(r) for r in batch_results]\n",
    "            print(f\"Avg. sequential response length: {sum(seq_lengths)/len(seq_lengths):.1f} chars\")\n",
    "            print(f\"Avg. GPU batch response length: {sum(batch_lengths)/len(batch_lengths):.1f} chars\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_performance_results(results):\n",
    "    \"\"\"\n",
    "    Plot the performance results.\n",
    "    \"\"\"\n",
    "    batch_sizes = results['batch_sizes']\n",
    "    valid_indices = [i for i, t in enumerate(results['batch_times']) if t != float('inf')]\n",
    "    \n",
    "    if not valid_indices:\n",
    "        print(\"No valid GPU batch results to plot\")\n",
    "        return\n",
    "    \n",
    "    # Filter out invalid results\n",
    "    valid_batch_sizes = [batch_sizes[i] for i in valid_indices]\n",
    "    valid_sequential_times = [results['sequential_times'][i] for i in valid_indices]\n",
    "    valid_batch_times = [results['batch_times'][i] for i in valid_indices]\n",
    "    \n",
    "    # Calculate speedups\n",
    "    speedups = [seq/batch for seq, batch in zip(valid_sequential_times, valid_batch_times)]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot 1: Processing times\n",
    "    axs[0].plot(valid_batch_sizes, valid_sequential_times, 'o-', label='Sequential')\n",
    "    axs[0].plot(valid_batch_sizes, valid_batch_times, '^-', label='GPU batch')\n",
    "    axs[0].set_xlabel('Batch Size')\n",
    "    axs[0].set_ylabel('Processing Time (seconds)')\n",
    "    axs[0].set_title('Processing Time vs Batch Size')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot 2: Speedup ratio\n",
    "    axs[1].plot(valid_batch_sizes, speedups, '^-')\n",
    "    axs[1].axhline(y=1.0, color='r', linestyle='--', label='Break-even (1.0)')\n",
    "    axs[1].set_xlabel('Batch Size')\n",
    "    axs[1].set_ylabel('Speedup Ratio')\n",
    "    axs[1].set_title('GPU Batch Speedup vs Sequential')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('batch_processing_performance.png')\n",
    "    plt.show()\n",
    "\n",
    "def run_memory_usage_test(client, batch_sizes=[1, 2, 4, 8, 16]):\n",
    "    \"\"\"\n",
    "    Test GPU memory usage for different batch sizes.\n",
    "    Only works on CUDA devices.\n",
    "    \n",
    "    Args:\n",
    "        client: Initialized LocalLLMClient instance\n",
    "        \n",
    "    Returns:\n",
    "        dict: Memory usage data\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available. Cannot run memory usage test.\")\n",
    "        return None\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'batch_sizes': [],\n",
    "        'memory_before': [],\n",
    "        'memory_peak': [],\n",
    "        'memory_after': []\n",
    "    }\n",
    "    \n",
    "    base_prompt = \"What is machine learning?\"\n",
    "    system_message = \"You are a helpful AI assistant.\"\n",
    "    \n",
    "    # Reset GPU memory stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\nTesting memory usage for batch size {batch_size}\")\n",
    "        \n",
    "        # Create inputs\n",
    "        user_inputs = [f\"{base_prompt} (Example {i+1})\" for i in range(batch_size)]\n",
    "        system_messages = [system_message] * batch_size\n",
    "        \n",
    "        # Record memory before\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "        memory_before = torch.cuda.memory_allocated() / (1024 ** 2)  # Convert to MB\n",
    "        \n",
    "        # Run inference\n",
    "        try:\n",
    "            _ = client.batched_query(user_inputs, system_messages)\n",
    "            \n",
    "            # Record peak memory\n",
    "            torch.cuda.synchronize()\n",
    "            memory_peak = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "            \n",
    "            # Record memory after\n",
    "            torch.cuda.synchronize()\n",
    "            memory_after = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "            \n",
    "            print(f\"Memory before: {memory_before:.2f} MB\")\n",
    "            print(f\"Peak memory: {memory_peak:.2f} MB\")\n",
    "            print(f\"Memory after: {memory_after:.2f} MB\")\n",
    "            print(f\"Memory increase: {memory_peak - memory_before:.2f} MB\")\n",
    "            \n",
    "            # Store results\n",
    "            results['batch_sizes'].append(batch_size)\n",
    "            results['memory_before'].append(memory_before)\n",
    "            results['memory_peak'].append(memory_peak)\n",
    "            results['memory_after'].append(memory_after)\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(f\"CUDA out of memory for batch size {batch_size}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Error during memory test: {e}\")\n",
    "                traceback.print_exc()\n",
    "                break\n",
    "        \n",
    "        # Clean up\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Plot memory usage\n",
    "    if len(results['batch_sizes']) > 1:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(results['batch_sizes'], results['memory_peak'], 'o-', label='Peak Memory')\n",
    "        plt.xlabel('Batch Size')\n",
    "        plt.ylabel('GPU Memory Usage (MB)')\n",
    "        plt.title('GPU Memory Usage vs. Batch Size')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig('gpu_memory_usage.png')\n",
    "        plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Import at runtime to allow this script to be imported elsewhere\n",
    "    from utils.llm_client import LocalLLMClient\n",
    "    \n",
    "    # Initialize client\n",
    "    client = LocalLLMClient(model_type=\"mistral_24b\", device=\"cuda:0\", temperature=0.0001)\n",
    "    \n",
    "    # Run performance test - start with smaller batches and test\n",
    "    results = test_batch_processing_performance(\n",
    "        client, \n",
    "        batch_sizes=[16,32],  # Start with smaller batches\n",
    "        timeout=300  # 5-minute timeout per batch\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    if results:\n",
    "        plot_performance_results(results)\n",
    "    \n",
    "    # Optionally run memory test\n",
    "    # memory_results = run_memory_usage_test(client, batch_sizes=[1, 2, 4, 8, 16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from utils.data import read_json_file\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text for more robust matching:\n",
    "    - Convert to lowercase\n",
    "    - Remove extra whitespaces\n",
    "    - Remove punctuation\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove punctuation (but keep some meaningful characters)\n",
    "    text = re.sub(r'[^\\w\\s°-]', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def find_phenotype_in_text(phenotype, text):\n",
    "    \"\"\"\n",
    "    Robust phenotype matching with multiple strategies:\n",
    "    1. Exact match (case-insensitive, normalized)\n",
    "    2. Partial match\n",
    "    3. Fuzzy matching\n",
    "    \"\"\"\n",
    "    # Normalize phenotype and text\n",
    "    norm_phenotype = normalize_text(phenotype)\n",
    "    norm_text = normalize_text(text)\n",
    "    \n",
    "    # 1. Exact match\n",
    "    if norm_phenotype in norm_text:\n",
    "        return True\n",
    "    \n",
    "    # 2. Partial match\n",
    "    if norm_phenotype in norm_text.split():\n",
    "        return True\n",
    "    \n",
    "    # 3. Fuzzy matching\n",
    "    def fuzzy_match(a, b, threshold=0.8):\n",
    "        return difflib.SequenceMatcher(None, a, b).ratio() >= threshold\n",
    "    \n",
    "    # Check if any part of the phenotype matches with high similarity\n",
    "    for text_word in norm_text.split():\n",
    "        if fuzzy_match(norm_phenotype, text_word):\n",
    "            return True\n",
    "    \n",
    "    # 4. Special handling for medical terms with variations\n",
    "    # Handle plural/singular, hyphenated terms, etc.\n",
    "    medical_variations = [\n",
    "        # Plurals\n",
    "        phenotype + 's',\n",
    "        phenotype[:-1] + 'es',\n",
    "        # Hyphenated variations\n",
    "        phenotype.replace(' ', '-'),\n",
    "        phenotype.replace('-', ' '),\n",
    "        # Common medical term variations\n",
    "        *_generate_medical_variations(phenotype)\n",
    "    ]\n",
    "    \n",
    "    for variation in medical_variations:\n",
    "        norm_variation = normalize_text(variation)\n",
    "        if norm_variation in norm_text:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def _generate_medical_variations(term):\n",
    "    \"\"\"\n",
    "    Generate variations of medical terms\n",
    "    \"\"\"\n",
    "    variations = []\n",
    "    \n",
    "    # Common prefixes/suffixes to strip\n",
    "    prefixes = ['', 'non-', 'mild ', 'severe ']\n",
    "    suffixes = ['', ' syndrome', ' disorder']\n",
    "    \n",
    "    for prefix in prefixes:\n",
    "        for suffix in suffixes:\n",
    "            variations.append(f\"{prefix}{term}{suffix}\")\n",
    "    \n",
    "    return variations\n",
    "\n",
    "def analyze_phenotype_text_presence(hpo_data):\n",
    "    total_phenotypes = 0\n",
    "    in_text_phenotypes = 0\n",
    "    not_in_text_phenotypes = 0\n",
    "    \n",
    "    # Track phenotypes across all documents\n",
    "    phenotype_details = []\n",
    "    \n",
    "    # Track documents with most phenotypes not in text\n",
    "    doc_not_in_text_count = defaultdict(list)\n",
    "    \n",
    "    for doc_id, doc in hpo_data.items():\n",
    "        # Skip if clinical text is not present\n",
    "        if \"clinical_text\" not in doc or \"phenotypes\" not in doc:\n",
    "            continue\n",
    "        \n",
    "        text = doc[\"clinical_text\"]\n",
    "        phenotypes = doc[\"phenotypes\"]\n",
    "        \n",
    "        doc_not_in_text = []\n",
    "        \n",
    "        for phenotype in phenotypes:\n",
    "            total_phenotypes += 1\n",
    "            phenotype_name = phenotype[\"phenotype_name\"]\n",
    "            \n",
    "            # Check if phenotype exists in text\n",
    "            if find_phenotype_in_text(phenotype_name, text):\n",
    "                in_text_phenotypes += 1\n",
    "                phenotype_details.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"phenotype\": phenotype_name,\n",
    "                    \"in_text\": True\n",
    "                })\n",
    "            else:\n",
    "                not_in_text_phenotypes += 1\n",
    "                doc_not_in_text.append(phenotype_name)\n",
    "                phenotype_details.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"phenotype\": phenotype_name,\n",
    "                    \"in_text\": False\n",
    "                })\n",
    "        \n",
    "        # Track documents with phenotypes not in text\n",
    "        if doc_not_in_text:\n",
    "            doc_not_in_text_count[doc_id] = doc_not_in_text\n",
    "    \n",
    "    # Calculate percentages\n",
    "    percentage_in_text = (in_text_phenotypes / total_phenotypes) * 100 if total_phenotypes > 0 else 0\n",
    "    percentage_not_in_text = (not_in_text_phenotypes / total_phenotypes) * 100 if total_phenotypes > 0 else 0\n",
    "    \n",
    "    # Sort documents by number of phenotypes not in text (descending)\n",
    "    top_docs_not_in_text = sorted(\n",
    "        doc_not_in_text_count.items(), \n",
    "        key=lambda x: len(x[1]), \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"Total Phenotypes\": total_phenotypes,\n",
    "        \"Phenotypes in Text\": in_text_phenotypes,\n",
    "        \"Phenotypes Not in Text\": not_in_text_phenotypes,\n",
    "        \"Percentage in Text\": percentage_in_text,\n",
    "        \"Percentage Not in Text\": percentage_not_in_text,\n",
    "        \"Top Documents with Phenotypes Not in Text\": top_docs_not_in_text[:10],  # Top 10\n",
    "        \"Phenotype Details\": phenotype_details\n",
    "    }\n",
    "\n",
    "# Load the data\n",
    "hpo_data = read_json_file('data/dataset/mine_hpo.json')\n",
    "\n",
    "# Perform the analysis\n",
    "results = analyze_phenotype_text_presence(hpo_data)\n",
    "\n",
    "# Print results\n",
    "print(\"Phenotype Text Presence Analysis:\")\n",
    "for key, value in results.items():\n",
    "    if key not in [\"Phenotype Details\", \"Top Documents with Phenotypes Not in Text\"]:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Print top documents with phenotypes not in text\n",
    "print(\"\\nTop 10 Documents with Most Phenotypes Not in Text:\")\n",
    "for doc_id, phenotypes in results[\"Top Documents with Phenotypes Not in Text\"]:\n",
    "    print(f\"Document ID: {doc_id}\")\n",
    "    print(f\"Number of Phenotypes Not in Text: {len(phenotypes)}\")\n",
    "    print(\"Phenotypes:\")\n",
    "    for phenotype in phenotypes:\n",
    "        print(f\"  - {phenotype}\")\n",
    "    print()  # Empty line for readability\n",
    "\n",
    "# Optional: Detailed analysis of a specific document\n",
    "def analyze_specific_document(hpo_data, doc_id):\n",
    "    text = hpo_data[doc_id][\"clinical_text\"]\n",
    "    phenotypes = hpo_data[doc_id][\"phenotypes\"]\n",
    "    \n",
    "    print(f\"\\nDetailed Analysis for Document {doc_id}:\")\n",
    "    for phenotype in phenotypes:\n",
    "        phenotype_name = phenotype[\"phenotype_name\"]\n",
    "        in_text = find_phenotype_in_text(phenotype_name, text)\n",
    "        print(f\"{phenotype_name}: {'✓ Found' if in_text else '✗ Not Found'}\")\n",
    "        if not in_text:\n",
    "            print(f\"  Normalized Phenotype: {normalize_text(phenotype_name)}\")\n",
    "            print(f\"  Normalized Text: {normalize_text(text)[:500]}...\")  # First 500 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import read_json_file, print_json_structure\n",
    "hpo_data = read_json_file('data/dataset/mine_hpo.json')\n",
    "print(len(hpo_data))\n",
    "# print_json_structure(hpo_data)\n",
    "truth = hpo_data[\"53\"][\"phenotypes\"]\n",
    "# sample 5 texts and their ground truth phenotypes for testing.\n",
    "ids = [\"53\", \"54\", \"55\", \"56\", \"57\"] \n",
    "texts = []\n",
    "ground_truths = []\n",
    "for id in ids:\n",
    "    text = hpo_data[id][\"clinical_text\"]\n",
    "    texts.append(text)\n",
    "    truth = hpo_data[id][\"phenotypes\"]\n",
    "    ground_truth = []\n",
    "    for item in truth:\n",
    "        ground_truth.append(item[\"phenotype_name\"])\n",
    "    ground_truths.append(ground_truth)\n",
    "\n",
    "    sanity_check_list = []\n",
    "    for item in ground_truth:\n",
    "        # print(item)\n",
    "        if item in text:\n",
    "            sanity_check_list.append(item)\n",
    "    print(\"Pairwise checks:\")\n",
    "    print(len(sanity_check_list))\n",
    "    print(len(truth))\n",
    "# ground_truth = []\n",
    "# for item in truth:\n",
    "#     ground_truth.append(item[\"phenotype_name\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = hpo_data[\"53\"][\"clinical_text\"]\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def calculate_inverse_frequency(text, use_sklearn=True):\n",
    "    \"\"\"\n",
    "    Calculate the inverse frequency of each word in the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to analyze\n",
    "        use_sklearn (bool): Whether to use scikit-learn for faster processing\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping each word to its inverse frequency\n",
    "    \"\"\"\n",
    "    if use_sklearn:\n",
    "        # Using scikit-learn's vectorizer for better performance\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize and fit the vectorizer\n",
    "        vectorizer = CountVectorizer(lowercase=True, token_pattern=r'\\b\\w+\\b')\n",
    "        X = vectorizer.fit_transform([text])\n",
    "        \n",
    "        # Get word counts and feature names (words)\n",
    "        counts = X.toarray()[0]\n",
    "        words = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Calculate total words and frequencies\n",
    "        total_words = np.sum(counts)\n",
    "        frequencies = counts / total_words\n",
    "        \n",
    "        # Calculate inverse frequencies\n",
    "        inverse_frequencies = {}\n",
    "        for word, freq in zip(words, frequencies):\n",
    "            if freq > 0:  # Avoid division by zero\n",
    "                inverse_frequencies[word] = 1 / freq\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"Processing completed in {processing_time:.4f} seconds using scikit-learn\")\n",
    "        \n",
    "        return inverse_frequencies\n",
    "    else:\n",
    "        # Original implementation using regex and Counter\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert to lowercase and tokenize the text\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        \n",
    "        # Count occurrences of each word\n",
    "        word_counts = Counter(words)\n",
    "        total_words = len(words)\n",
    "        \n",
    "        # Calculate inverse frequency (1/frequency)\n",
    "        inverse_freq = {}\n",
    "        for word, count in word_counts.items():\n",
    "            frequency = count / total_words\n",
    "            inverse_freq[word] = 1 / frequency\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"Processing completed in {processing_time:.4f} seconds using basic implementation\")\n",
    "        \n",
    "        return inverse_freq\n",
    "\n",
    "def print_results(inverse_freq, limit=None, sort_by='inverse_freq'):\n",
    "    \"\"\"\n",
    "    Print the inverse frequency results in a formatted table.\n",
    "    \n",
    "    Args:\n",
    "        inverse_freq (dict): Dictionary of words and their inverse frequencies\n",
    "        limit (int, optional): Limit the number of results to display\n",
    "        sort_by (str): Sort by 'inverse_freq', 'frequency', 'count', or 'alphabetical'\n",
    "    \"\"\"\n",
    "    # Create a list of (word, count, frequency, inverse_freq) tuples\n",
    "    results = []\n",
    "    total_words = sum(1/value for value in inverse_freq.values())\n",
    "    \n",
    "    for word, inv_freq in inverse_freq.items():\n",
    "        frequency = 1 / inv_freq\n",
    "        count = int(frequency * total_words)\n",
    "        results.append((word, count, frequency, inv_freq))\n",
    "    \n",
    "    # Sort results\n",
    "    if sort_by == 'inverse_freq':\n",
    "        results.sort(key=lambda x: x[3], reverse=True)  # Sort by inverse freq (descending)\n",
    "    elif sort_by == 'frequency':\n",
    "        results.sort(key=lambda x: x[2], reverse=True)  # Sort by frequency (descending)\n",
    "    elif sort_by == 'count':\n",
    "        results.sort(key=lambda x: x[1], reverse=True)  # Sort by count (descending)\n",
    "    elif sort_by == 'alphabetical':\n",
    "        results.sort(key=lambda x: x[0])  # Sort alphabetically\n",
    "    \n",
    "    # Limit results if specified\n",
    "    if limit:\n",
    "        results = results[:limit]\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"{'Word':<20} {'Count':<10} {'Frequency':<15} {'Inverse Freq':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Print results\n",
    "    for word, count, freq, inv_freq in results:\n",
    "        print(f\"{word:<20} {count:<10} {freq:<15.6f} {inv_freq:<15.6f}\")\n",
    "\n",
    "inverse_freq = calculate_inverse_frequency(text, use_sklearn=True)\n",
    "print_results(inverse_freq, limit=10, sort_by='inverse_freq')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mistral_llm_client():\n",
    "    \"\"\"\n",
    "    Load a Mistral 24B LLM client configured with default cache directories\n",
    "    and assigned to cuda:0 device.\n",
    "    \n",
    "    Returns:\n",
    "        LocalLLMClient: Initialized LLM client for Mistral 24B\n",
    "    \"\"\"\n",
    "    from utils.llm_client import LocalLLMClient\n",
    "    \n",
    "    # Default cache directory from mine_hpo.py\n",
    "    default_cache_dir = \"/u/zelalae2/scratch/rdma_cache\"\n",
    "    \n",
    "    # Initialize and return the client with specific configuration\n",
    "    llm_client = LocalLLMClient(\n",
    "        model_type=\"mistral_24b\",  # Explicitly request mistral_24b model\n",
    "        device=\"cuda:0\",           # Assign to first GPU (cuda:0)\n",
    "        cache_dir=default_cache_dir,\n",
    "        temperature=0.1           # Default temperature from mine_hpo.py\n",
    "    )\n",
    "    \n",
    "    return llm_client\n",
    "\n",
    "llm_client = load_mistral_llm_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import numpy as np\n",
    "from hporag.entity import BaseEntityExtractor\n",
    "from hporag.context import ContextExtractor\n",
    "\n",
    "class RetrievalEnhancedEntityExtractor(BaseEntityExtractor):\n",
    "    \"\"\"\n",
    "    Entity extractor that uses embedding retrieval to enhance LLM-based extraction.\n",
    "    \n",
    "    For each sentence:\n",
    "    1. Retrieves relevant HPO terms to provide context\n",
    "    2. Uses this context to prompt the LLM for more accurate entity extraction\n",
    "    \n",
    "    This approach helps the LLM with domain knowledge before extraction begins.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, embedding_manager, embedded_documents, system_message: str, top_k: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize the retrieval-enhanced entity extractor.\n",
    "        \n",
    "        Args:\n",
    "            llm_client: Client for querying the language model\n",
    "            embedding_manager: Manager for vector embeddings and search\n",
    "            embedded_documents: HPO terms with embeddings\n",
    "            system_message: System message for LLM extraction\n",
    "            top_k: Number of top candidates to retrieve per sentence\n",
    "        \"\"\"\n",
    "        self.llm_client = llm_client\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.embedded_documents = embedded_documents\n",
    "        self.index = None\n",
    "        self.system_message = system_message\n",
    "        self.top_k = top_k\n",
    "        self.context_extractor = ContextExtractor()\n",
    "        \n",
    "    def prepare_index(self):\n",
    "        \"\"\"Prepare FAISS index from embedded documents if not already prepared.\"\"\"\n",
    "        if self.index is None:\n",
    "            embeddings_array = self.embedding_manager.prepare_embeddings(self.embedded_documents)\n",
    "            self.index = self.embedding_manager.create_index(embeddings_array)\n",
    "    \n",
    "    def _retrieve_candidates(self, sentence: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant HPO candidates for a sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Clinical sentence to find relevant HPO terms for\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with HPO term information and similarity scores\n",
    "        \"\"\"\n",
    "        self.prepare_index()\n",
    "        \n",
    "        # Embed the query\n",
    "        query_vector = self.embedding_manager.query_text(sentence).reshape(1, -1)\n",
    "        \n",
    "        # Search for similar items\n",
    "        distances, indices = self.embedding_manager.search(query_vector, self.index, k=min(800, len(self.embedded_documents)))\n",
    "        \n",
    "        # Extract unique metadata\n",
    "        candidates = []\n",
    "        seen_metadata = set()\n",
    "        \n",
    "        for idx, distance in zip(indices[0], distances[0]):\n",
    "            metadata = self.embedded_documents[idx]['unique_metadata']\n",
    "            metadata_str = json.dumps(metadata)\n",
    "            \n",
    "            if metadata_str not in seen_metadata:\n",
    "                seen_metadata.add(metadata_str)\n",
    "                candidates.append({\n",
    "                    'term': metadata.get('info', ''),\n",
    "                    'hp_id': metadata.get('hp_id', ''),\n",
    "                    'similarity_score': 1.0 / (1.0 + distance)  # Convert distance to similarity\n",
    "                })\n",
    "                \n",
    "                if len(candidates) >= self.top_k:\n",
    "                    break\n",
    "                    \n",
    "        return candidates\n",
    "    \n",
    "    def _create_enhanced_prompt(self, sentence: str, candidates: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Create a prompt enhanced with retrieved candidates.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Clinical sentence to extract entities from\n",
    "            candidates: Retrieved HPO candidates to use as context\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt for LLM\n",
    "        \"\"\"\n",
    "        # Format candidates as context\n",
    "        context_items = []\n",
    "        for candidate in candidates:\n",
    "            context_items.append(f\"- {candidate['term']} ({candidate['hp_id']})\")\n",
    "        \n",
    "        context_text = \"\\n\".join(context_items)\n",
    "        \n",
    "        # Create the enhanced prompt\n",
    "        prompt = (\n",
    "            f\"I have a clinical sentence: \\\"{sentence}\\\"\\n\\n\"\n",
    "            f\"Here are some relevant HPO terms for context that are potentially within the sentence:\\n\\n\"\n",
    "            f\"{context_text}\\n\\n\"\n",
    "            f\"Based on this sentence and the provided HPO terms for context, extract all phenotype terms \"\n",
    "            f\"(genetic inheritance patterns, anatomical anomalies, clinical symptoms, diagnostic findings, \"\n",
    "            f\"test results, conditions or syndromes) from the sentence. \"\n",
    "            f\"IGNORE NEGATIVE FINDINGS, NORMAL FINDINGS, AND ANY TERMS MENTIONED IN FAMILY HISTORY.\\n\\n\"\n",
    "            f\"Please include the full term and any additional context that is part of the term. \"\n",
    "            f\"MAKE SURE IT MATCHES EXACTLY AS IT APPEARS IN THE SENTENCE.\\n\"\n",
    "            f\"Return the extracted terms as a JSON object with a single key 'findings', which contains \"\n",
    "            f\"the list of extracted terms spelled correctly.\"\n",
    "        )\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract entities from text using retrieval-enhanced prompting.\n",
    "        \n",
    "        Args:\n",
    "            text: Clinical text to extract entities from\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted phenotype entities\n",
    "        \"\"\"\n",
    "        # Split text into sentences\n",
    "        sentences = self.context_extractor.extract_sentences(text)\n",
    "        \n",
    "        all_entities = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Skip empty or very short sentences\n",
    "            if not sentence or len(sentence) < 5:\n",
    "                continue\n",
    "                \n",
    "            # Retrieve candidates for this sentence\n",
    "            candidates = self._retrieve_candidates(sentence)\n",
    "            \n",
    "            # Create enhanced prompt\n",
    "            prompt = self._create_enhanced_prompt(sentence, candidates)\n",
    "            \n",
    "            # Query LLM\n",
    "            findings_text = self.llm_client.query(prompt, self.system_message)\n",
    "            print(\"Prompt\")\n",
    "            print(prompt)\n",
    "            print(\"Findings Text\")\n",
    "            print(findings_text)\n",
    "            print()\n",
    "\n",
    "            # Extract entities from response\n",
    "            entities = self._extract_findings_from_response(findings_text)\n",
    "            \n",
    "            # Add to results\n",
    "            all_entities.extend(entities)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_entities = []\n",
    "        seen = set()\n",
    "        for entity in all_entities:\n",
    "            entity_lower = entity.lower()\n",
    "            if entity_lower not in seen and entity:\n",
    "                seen.add(entity_lower)\n",
    "                unique_entities.append(entity)\n",
    "        \n",
    "        return unique_entities\n",
    "    \n",
    "    def _extract_findings_from_response(self, response_content: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract findings from LLM response.\n",
    "        \n",
    "        Args:\n",
    "            response_content: Raw LLM response text\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted entities\n",
    "        \"\"\"\n",
    "        sanitized = response_content.replace(\"```\", \"\").strip()\n",
    "        start = sanitized.find(\"{\")\n",
    "        end = sanitized.rfind(\"}\")\n",
    "        if start == -1 or end == -1 or start > end:\n",
    "            return []\n",
    "\n",
    "        json_str = sanitized[start:end+1]\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            findings = data.get(\"findings\", [])\n",
    "            return findings if isinstance(findings, list) else []\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "    \n",
    "    def process_batch(self, texts: List[str]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Process a batch of texts for entity extraction.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of clinical texts to process\n",
    "            \n",
    "        Returns:\n",
    "            List of lists containing extracted entities for each text\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            entities = self.extract_entities(text)\n",
    "            results.append(entities)\n",
    "        return results\n",
    "    \n",
    "from hporag.entity import BaseEntityExtractor\n",
    "from utils.embedding import EmbeddingsManager\n",
    "import numpy as np\n",
    "# Load system prompts\n",
    "with open('data/prompts/system_prompts.json', 'r') as f:\n",
    "    prompts = json.load(f)\n",
    "    system_message_extraction = prompts.get(\"system_message_I\", \"\")\n",
    "    system_message_verification = prompts.get(\"system_message_II\", \"\")\n",
    "\n",
    "# Initialize embedding manager with MedEmbed using sentence transformers\n",
    "embedding_manager = EmbeddingsManager(\n",
    "    model_type=\"sentence_transformer\",\n",
    "    model_name=\"abhinand/MedEmbed-small-v0.1\",  # Medical-domain sentence transformer model\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "# Load embeddings\n",
    "embedded_documents = np.load('data/vector_stores/G2GHPO_metadata_medembed.npy', allow_pickle=True)\n",
    "llm_client.temperature = 0.001  # Lower temperature for more precise extraction\n",
    "# Initialize the retrieval-enhanced extractor\n",
    "retrieval_extractor = RetrievalEnhancedEntityExtractor(\n",
    "    llm_client=llm_client,\n",
    "    embedding_manager=embedding_manager,\n",
    "    embedded_documents=embedded_documents,\n",
    "    system_message=system_message_extraction,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "retrieval_results = []\n",
    "for text in texts:\n",
    "    retrieval_results.append(retrieval_extractor.extract_entities(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in retrieval_results:\n",
    "    print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from hporag.entity import BaseEntityExtractor\n",
    "from hporag.context import ContextExtractor\n",
    "\n",
    "class DirectSentenceEntityExtractor(BaseEntityExtractor):\n",
    "    \"\"\"\n",
    "    Entity extractor that processes text sentence by sentence to extract all \n",
    "    medically-relevant entities, whether they are phenotypes or imply phenotypes.\n",
    "    \n",
    "    This extractor:\n",
    "    1. Splits text into sentences\n",
    "    2. Processes each sentence independently with an LLM\n",
    "    3. Combines all extracted entities into a single list\n",
    "    \n",
    "    Simple but effective for capturing all possible medical entities without\n",
    "    filtering or verification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, system_message: str, min_sentence_length: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize the direct sentence entity extractor.\n",
    "        \n",
    "        Args:\n",
    "            llm_client: Client for querying the language model\n",
    "            system_message: System message for LLM extraction\n",
    "            min_sentence_length: Minimum length for a sentence to be processed\n",
    "        \"\"\"\n",
    "        self.llm_client = llm_client\n",
    "        self.system_message = system_message\n",
    "        self.min_sentence_length = min_sentence_length\n",
    "        self.context_extractor = ContextExtractor()\n",
    "    \n",
    "    def _create_extraction_prompt(self, sentence: str) -> str:\n",
    "        \"\"\"\n",
    "        Create a prompt for medical entity extraction from a sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Clinical sentence to extract entities from\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt for LLM\n",
    "        \"\"\"\n",
    "        prompt = (\n",
    "            f\"Extract ALL medically-relevant entities from the following clinical sentence:\\n\\n\"\n",
    "            f\"\\\"{sentence}\\\"\\n\\n\"\n",
    "            f\"Include any terms that are or could imply:\\n\"\n",
    "            f\"- Symptoms and clinical findings\\n\"\n",
    "            f\"- Medical conditions and diseases\\n\"\n",
    "            f\"- Anatomical anomalies\\n\"\n",
    "            f\"- Genetic or inherited conditions\\n\"\n",
    "            f\"- Abnormal laboratory or test results\\n\"\n",
    "            f\"- Specific syndromes or disorders\\n\\n\"\n",
    "            f\"Please include the full term. Ignore negative findings, normal findings, and family history.\\n\\n\"\n",
    "            f\"Return the extracted terms as a JSON object with a single key 'findings', \"\n",
    "            f\"which contains the list of extracted terms spelled correctly.\"\n",
    "        )\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def _extract_entities_from_sentence(self, sentence: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract medical entities from a single sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Clinical sentence to extract entities from\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted medical entities\n",
    "        \"\"\"\n",
    "        # Skip if sentence is too short\n",
    "        if len(sentence) < self.min_sentence_length:\n",
    "            return []\n",
    "        \n",
    "        # Create prompt for this sentence\n",
    "        prompt = self._create_extraction_prompt(sentence)\n",
    "        \n",
    "        # Query LLM\n",
    "        findings_text = self.llm_client.query(prompt, self.system_message)\n",
    "        \n",
    "        # Extract and return entities\n",
    "        return self._extract_findings_from_response(findings_text)\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract all medically-relevant entities from text by processing each sentence.\n",
    "        \n",
    "        Args:\n",
    "            text: Clinical text to extract entities from\n",
    "            \n",
    "        Returns:\n",
    "            List of all extracted medical entities\n",
    "        \"\"\"\n",
    "        # Split text into sentences\n",
    "        sentences = self.context_extractor.extract_sentences(text)\n",
    "        \n",
    "        # Extract entities from each sentence\n",
    "        all_entities = []\n",
    "        for sentence in sentences:\n",
    "            sentence_entities = self._extract_entities_from_sentence(sentence)\n",
    "            all_entities.extend(sentence_entities)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_entities = []\n",
    "        seen = set()\n",
    "        for entity in all_entities:\n",
    "            entity_lower = entity.lower()\n",
    "            if entity_lower not in seen and entity:\n",
    "                seen.add(entity_lower)\n",
    "                unique_entities.append(entity)\n",
    "        \n",
    "        return unique_entities\n",
    "    \n",
    "    def _extract_findings_from_response(self, response_content: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract findings from LLM response.\n",
    "        \n",
    "        Args:\n",
    "            response_content: Raw LLM response text\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted entities\n",
    "        \"\"\"\n",
    "        sanitized = response_content.replace(\"```\", \"\").strip()\n",
    "        start = sanitized.find(\"{\")\n",
    "        end = sanitized.rfind(\"}\")\n",
    "        if start == -1 or end == -1 or start > end:\n",
    "            return []\n",
    "\n",
    "        json_str = sanitized[start:end+1]\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            findings = data.get(\"findings\", [])\n",
    "            return findings if isinstance(findings, list) else []\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "    \n",
    "    def process_batch(self, texts: List[str]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Process a batch of texts for entity extraction.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of clinical texts to process\n",
    "            \n",
    "        Returns:\n",
    "            List of lists containing extracted entities for each text\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            entities = self.extract_entities(text)\n",
    "            results.append(entities)\n",
    "        return results\n",
    "    \n",
    "direct_extractor = DirectSentenceEntityExtractor(\n",
    "    llm_client=llm_client,\n",
    "    system_message=system_message_extraction,\n",
    "    min_sentence_length=5\n",
    ")\n",
    "direct_results = []\n",
    "for text in texts:\n",
    "    res = direct_extractor.extract_entities(text)\n",
    "    direct_results.append(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in direct_results:\n",
    "    print(len(res))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hporag.entity import BaseEntityExtractor\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from fuzzywuzzy import fuzz\n",
    "class IterativeLLMEntityExtractor(BaseEntityExtractor):\n",
    "    \"\"\"Entity extraction pipeline using iterative LLM passes.\n",
    "    \n",
    "    Performs multiple passes with the LLM, accumulating unique entities across passes.\n",
    "    Stops early if no new entities are found or after reaching the maximum number of iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, system_message: str, max_iterations: int = 3):\n",
    "        \"\"\"Initialize the iterative LLM entity extractor.\n",
    "        \n",
    "        Args:\n",
    "            llm_client: LLM client for querying the language model\n",
    "            system_message: System message to use for all LLM queries\n",
    "            max_iterations: Maximum number of iterations to perform\n",
    "        \"\"\"\n",
    "        self.llm_client = llm_client\n",
    "        self.system_message = system_message\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    def extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract entities from text using iterative LLM passes.\n",
    "        \n",
    "        Args:\n",
    "            text: Input clinical text\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted entities after iterative refinement\n",
    "        \"\"\"\n",
    "        # Initialize with empty set of entities\n",
    "        current_entities = set()\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Create prompt with already extracted entities\n",
    "            if iteration == 0:\n",
    "                # First pass - no previous entities\n",
    "                prompt = text\n",
    "            else:\n",
    "                # Subsequent passes - include previously extracted entities\n",
    "                already_extracted = \", \".join(sorted(current_entities))\n",
    "                prompt = self._create_iterative_prompt(text, already_extracted, iteration)\n",
    "            \n",
    "            # Query LLM\n",
    "            findings_text = self.llm_client.query(prompt, self.system_message)\n",
    "            # print(\"Findings text:\")\n",
    "            # print(findings_text)\n",
    "            # print(\"----------------\")\n",
    "            if not findings_text:\n",
    "                break\n",
    "                \n",
    "            # Extract entities from this pass\n",
    "            iteration_entities = set(self._extract_findings_from_response(findings_text))\n",
    "            \n",
    "            # Combine with previous entities (union)\n",
    "            combined_entities = current_entities.union(iteration_entities)\n",
    "            print(\"Combined entities:\", combined_entities)\n",
    "            # Check if we've converged (no new entities)\n",
    "            if combined_entities == current_entities:\n",
    "                print(f\"Early stopping at iteration {iteration+1}: No new entities found.\")\n",
    "                break\n",
    "                \n",
    "            # Calculate new entities found in this iteration\n",
    "            new_entities = combined_entities - current_entities\n",
    "            if new_entities:\n",
    "                print(f\"Iteration {iteration+1}: Found {len(new_entities)} new entities\")\n",
    "            else:\n",
    "                print(f\"Iteration {iteration+1}: No new entities found.\")\n",
    "                \n",
    "            # Update current entities\n",
    "            current_entities = combined_entities\n",
    "        \n",
    "        # Convert set back to list for return\n",
    "        return list(current_entities)\n",
    "\n",
    "    def _create_iterative_prompt(self, original_text: str, already_extracted: str, iteration: int) -> str:\n",
    "        \"\"\"Create a prompt for iterative entity extraction.\n",
    "        \n",
    "        Args:\n",
    "            original_text: The original clinical text\n",
    "            already_extracted: Comma-separated list of already extracted entities\n",
    "            iteration: Current iteration number (0-based)\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt for the current iteration\n",
    "        \"\"\"\n",
    "        return (\n",
    "            f\"{original_text}\\n\\n\"\n",
    "            f\"I have already extracted the following terms: {already_extracted}.\\n\"\n",
    "            f\"Please examine the clinical text again carefully (iteration {iteration+1} of up to {self.max_iterations}) \"\n",
    "            f\"and identify any additional genetic inheritance patterns, anatomical anomalies, clinical symptoms, diagnostic findings, lab test results, and specific conditions or syndromes that were missed in the previous extractions. \"\n",
    "            f\"Find terms that aren't in the already extracted list. Include appropriate context based only on the passage.\"\n",
    "            f\"Return the extracted terms in a JSON object with a single key 'findings', which contains the list of extracted terms spelled correctly. If you don't find any new terms, return an empty list.\"\n",
    "        )\n",
    "\n",
    "    def process_batch(self, texts: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Process a batch of texts for entity extraction.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of clinical texts to process\n",
    "            \n",
    "        Returns:\n",
    "            List of lists containing extracted entities for each text\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            entities = self.extract_entities(text)\n",
    "            results.append(entities)\n",
    "        return results\n",
    "\n",
    "    def _extract_findings_from_response(self, response_content: str) -> List[str]:\n",
    "        \"\"\"Extract findings from LLM response.\n",
    "        \n",
    "        Args:\n",
    "            response_content: Raw LLM response text\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted entities\n",
    "        \"\"\"\n",
    "        sanitized = response_content.replace(\"```\", \"\").strip()\n",
    "        start = sanitized.find(\"{\")\n",
    "        end = sanitized.rfind(\"}\")\n",
    "        if start == -1 or end == -1 or start > end:\n",
    "            return []\n",
    "\n",
    "        json_str = sanitized[start:end+1]\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            findings = data.get(\"findings\", [])\n",
    "            return findings if isinstance(findings, list) else []\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_extractor = IterativeLLMEntityExtractor(\n",
    "            llm_client=llm_client,\n",
    "            # llm_client=llama_client,\n",
    "            system_message=system_message_extraction,\n",
    "            max_iterations=3 # iterative extractor\n",
    "        )\n",
    "single_iter_results = []\n",
    "for text in texts:\n",
    "    res = llm_extractor.extract_entities(text)\n",
    "    single_iter_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_disjoint_set(list1, list2):\n",
    "    \"\"\"\n",
    "    Find elements that are in either list1 or list2, but not in both.\n",
    "    \n",
    "    Args:\n",
    "    list1 (list): First list of elements\n",
    "    list2 (list): Second list of elements\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing two lists\n",
    "        - First list: Elements in list1 but not in list2\n",
    "        - Second list: Elements in list2 but not in list1\n",
    "    \"\"\"\n",
    "    # Convert lists to sets for efficient comparison\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    \n",
    "    # Find elements unique to each list\n",
    "    disjoint_list1 = list(set1 - set2)\n",
    "    disjoint_list2 = list(set2 - set1)\n",
    "    \n",
    "    return disjoint_list1, disjoint_list2\n",
    "\n",
    "for i in range(len(single_iter_results)):\n",
    "    print(f\"Text {i+1}:\")\n",
    "    print(len(retrieval_results[i]))\n",
    "    print(len(direct_results[i]))\n",
    "    print(len(single_iter_results[i]))\n",
    "    print(\"Single Iterations:\")\n",
    "    print(single_iter_results[i])\n",
    "    print(\"Retrieval:\")\n",
    "    print(retrieval_results[i])\n",
    "    iter_unique, retrieved_unique = find_disjoint_set(single_iter_results[i], retrieval_results[i])\n",
    "    # retrieved_unique, direct_unique = find_disjoint_set(retrieval_results[i], direct_results[i])\n",
    "    # print(\"Unique to Iteration:\")\n",
    "    # print(iter_unique)\n",
    "    # print(len(iter_unique))\n",
    "    # print(\"Unique to Retrieval:\")\n",
    "    # print(retrieved_unique)\n",
    "    # print(len(retrieved_unique))\n",
    "    # print(\"Unique to Direct:\")\n",
    "    # print(direct_unique)\n",
    "    # print()\n",
    "    # print(find_disjoint_set(retrieval_results[i], direct_results[i]))\n",
    "    # print(find_disjoint_set(retrieval_results[i], single_iter_results[i]))\n",
    "    # print(find_disjoint_set(direct_results[i], single_iter_results[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Optional, Union, Any\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class ContextExtractor:\n",
    "    \"\"\"\n",
    "    Extracts relevant context for entities from clinical notes.\n",
    "    \n",
    "    This class is responsible for finding the most relevant sentences or sections\n",
    "    in a clinical note that provide context for extracted entities. It can be used\n",
    "    by different components in the pipeline to ensure consistent context extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, debug: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the context extractor.\n",
    "        \n",
    "        Args:\n",
    "            debug: Whether to print debug information\n",
    "        \"\"\"\n",
    "        self.debug = debug\n",
    "    \n",
    "    def _debug_print(self, message: str, level: int = 0):\n",
    "        \"\"\"Print debug messages if debug mode is enabled.\"\"\"\n",
    "        if self.debug:\n",
    "            indent = \"  \" * level\n",
    "            print(f\"{datetime.now().strftime('%H:%M:%S')} | {indent}{message}\")\n",
    "    \n",
    "    def extract_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split clinical text into sentences.\n",
    "        \n",
    "        Args:\n",
    "            text: Clinical note text\n",
    "            \n",
    "        Returns:\n",
    "            List of sentences extracted from the text\n",
    "        \"\"\"\n",
    "        # First split by common sentence terminators while preserving them\n",
    "        sentence_parts = []\n",
    "        for part in re.split(r'([.!?])', text):\n",
    "            if part.strip():\n",
    "                if part in '.!?':\n",
    "                    if sentence_parts:\n",
    "                        sentence_parts[-1] += part\n",
    "                else:\n",
    "                    sentence_parts.append(part.strip())\n",
    "        \n",
    "        # Then handle other clinical note delimiters like line breaks and semicolons\n",
    "        sentences = []\n",
    "        for part in sentence_parts:\n",
    "            # Split by semicolons and newlines\n",
    "            for subpart in re.split(r'[;\\n]', part):\n",
    "                if subpart.strip():\n",
    "                    sentences.append(subpart.strip())\n",
    "        \n",
    "        self._debug_print(f\"Extracted {len(sentences)} sentences from text\")\n",
    "        return sentences\n",
    "    \n",
    "    def find_entity_context(self, entity: str, sentences: List[str], \n",
    "                          window_size: int = 0) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Find the most relevant context for a given entity.\n",
    "        \n",
    "        Args:\n",
    "            entity: Entity to find context for\n",
    "            sentences: List of sentences from the clinical note\n",
    "            window_size: Number of additional sentences to include (default: 0 - just the matching sentence)\n",
    "            \n",
    "        Returns:\n",
    "            The most relevant context or None if no match found\n",
    "        \"\"\"\n",
    "        entity_lower = entity.lower()\n",
    "        \n",
    "        # Try exact matching first\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if entity_lower in sentence.lower():\n",
    "                # Found exact match - just return the sentence for token efficiency\n",
    "                return sentence.strip()\n",
    "        \n",
    "        # If no exact match, try fuzzy matching based on word overlap\n",
    "        entity_words = set(re.findall(r'\\b\\w+\\b', entity_lower))\n",
    "        if not entity_words:\n",
    "            return None\n",
    "            \n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_words = set(re.findall(r'\\b\\w+\\b', sentence.lower()))\n",
    "            if not sentence_words:\n",
    "                continue\n",
    "                \n",
    "            common_words = entity_words & sentence_words\n",
    "            # Calculate Jaccard similarity\n",
    "            similarity = len(common_words) / (len(entity_words) + len(sentence_words) - len(common_words))\n",
    "            \n",
    "            # Prioritize sentences with higher word overlap\n",
    "            overlap_ratio = len(common_words) / len(entity_words) if entity_words else 0\n",
    "            \n",
    "            # Combined score giving more weight to overlap ratio\n",
    "            score = (0.7 * overlap_ratio) + (0.3 * similarity)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = sentence\n",
    "        \n",
    "        # If we found a reasonably good match\n",
    "        if best_score > 0.3 and best_match:\n",
    "            return best_match.strip()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_contexts(self, entities: List[str], text: str, \n",
    "                        window_size: int = 0) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Extract contexts for a list of entities from a clinical note.\n",
    "        \n",
    "        Args:\n",
    "            entities: List of entities to find context for\n",
    "            text: Clinical note text\n",
    "            window_size: Number of additional sentences to include (default: 0 - just the matching sentence)\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with entity and context pairs\n",
    "        \"\"\"\n",
    "        self._debug_print(f\"Extracting contexts for {len(entities)} entities\")\n",
    "        \n",
    "        # Extract sentences once\n",
    "        sentences = self.extract_sentences(text)\n",
    "        \n",
    "        # Find context for each entity\n",
    "        results = []\n",
    "        for entity in entities:\n",
    "            context = self.find_entity_context(entity, sentences, window_size)\n",
    "            results.append({\n",
    "                \"entity\": entity,\n",
    "                \"context\": context or \"\"  # Empty string if no context found\n",
    "            })\n",
    "            \n",
    "            self._debug_print(f\"Entity: '{entity}'\", level=1)\n",
    "            self._debug_print(f\"Context: '{context}'\", level=2)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_extract_contexts(self, batch_entities: List[List[str]], \n",
    "                             texts: List[str], \n",
    "                             window_size: int = 0) -> List[List[Dict[str, str]]]:\n",
    "        \"\"\"\n",
    "        Extract contexts for multiple batches of entities from multiple clinical notes.\n",
    "        \n",
    "        Args:\n",
    "            batch_entities: List of lists of entities\n",
    "            texts: List of clinical note texts\n",
    "            window_size: Number of additional sentences to include (default: 0 - just the matching sentence)\n",
    "            \n",
    "        Returns:\n",
    "            List of lists of dictionaries with entity and context pairs\n",
    "        \"\"\"\n",
    "        if len(batch_entities) != len(texts):\n",
    "            raise ValueError(f\"Mismatch between number of entity batches ({len(batch_entities)}) and texts ({len(texts)})\")\n",
    "            \n",
    "        results = []\n",
    "        for entities, text in zip(batch_entities, texts):\n",
    "            batch_results = self.extract_contexts(entities, text, window_size)\n",
    "            results.append(batch_results)\n",
    "            \n",
    "        return results\n",
    "\n",
    "\n",
    "# Standalone function for simpler usage\n",
    "def extract_entity_contexts(entities: List[str], text: str, window_size: int = 0) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Utility function to extract contexts for entities from a clinical note.\n",
    "    \n",
    "    Args:\n",
    "        entities: List of entities to find context for\n",
    "        text: Clinical note text\n",
    "        window_size: Number of additional sentences to include (default: 0 - just the matching sentence)\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with entity and context pairs\n",
    "    \"\"\"\n",
    "    extractor = ContextExtractor()\n",
    "    return extractor.extract_contexts(entities, text, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text by converting to lowercase and removing extra whitespace.\"\"\"\n",
    "    return ' '.join(text.lower().split())\n",
    "\n",
    "\n",
    "def calculate_similarity_matrix(predictions: List[str], ground_truth: List[str], \n",
    "                               similarity_threshold: float = 80.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the similarity matrix between predictions and ground truth using fuzzy matching.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of predicted phenotypes\n",
    "        ground_truth: List of ground truth phenotypes\n",
    "        similarity_threshold: Minimum similarity score (0-100) to consider a potential match\n",
    "        \n",
    "    Returns:\n",
    "        2D numpy array of similarity scores\n",
    "    \"\"\"\n",
    "    # Normalize all texts\n",
    "    norm_predictions = [normalize_text(p) for p in predictions]\n",
    "    norm_ground_truth = [normalize_text(g) for g in ground_truth]\n",
    "    \n",
    "    # Create similarity matrix\n",
    "    similarity_matrix = np.zeros((len(norm_predictions), len(norm_ground_truth)))\n",
    "    \n",
    "    for i, pred in enumerate(norm_predictions):\n",
    "        for j, truth in enumerate(norm_ground_truth):\n",
    "            similarity = fuzz.ratio(pred, truth)\n",
    "            # Only consider matches above threshold\n",
    "            if similarity >= similarity_threshold:\n",
    "                similarity_matrix[i, j] = similarity\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def find_best_matches(similarity_matrix: np.ndarray) -> List[Tuple[int, int, float]]:\n",
    "    \"\"\"\n",
    "    Find the best matches between predictions and ground truth using a greedy approach.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: 2D array of similarity scores\n",
    "        \n",
    "    Returns:\n",
    "        List of (pred_idx, truth_idx, similarity) tuples representing the matches\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    # Create a copy to modify\n",
    "    sim_matrix = similarity_matrix.copy()\n",
    "    \n",
    "    # While there are non-zero elements in the matrix\n",
    "    while np.max(sim_matrix) > 0:\n",
    "        # Find the highest similarity\n",
    "        max_val = np.max(sim_matrix)\n",
    "        max_pos = np.unravel_index(np.argmax(sim_matrix), sim_matrix.shape)\n",
    "        \n",
    "        # Add the match\n",
    "        pred_idx, truth_idx = max_pos\n",
    "        matches.append((pred_idx, truth_idx, max_val))\n",
    "        \n",
    "        # Mark the row and column as used\n",
    "        sim_matrix[pred_idx, :] = 0\n",
    "        sim_matrix[:, truth_idx] = 0\n",
    "    \n",
    "    return matches\n",
    "\n",
    "\n",
    "def calculate_soft_f1(predictions: List[str], ground_truth: List[str], \n",
    "                     similarity_threshold: float = 80.0) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate soft precision, recall, and F1 score using fuzzy matching.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of predicted phenotypes\n",
    "        ground_truth: List of ground truth phenotypes\n",
    "        similarity_threshold: Minimum similarity score to consider a potential match\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, F1 score, matched pairs, and counts of false positives/negatives\n",
    "    \"\"\"\n",
    "    # Handle empty lists\n",
    "    if not predictions or not ground_truth:\n",
    "        return {\n",
    "            \"precision\": 0.0 if predictions else 1.0,\n",
    "            \"recall\": 0.0 if ground_truth else 1.0,\n",
    "            \"f1_score\": 0.0 if (not predictions or not ground_truth) else 1.0,\n",
    "            \"matches\": [],\n",
    "            \"false_positives\": len(predictions),\n",
    "            \"false_negatives\": len(ground_truth)\n",
    "        }\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = calculate_similarity_matrix(predictions, ground_truth, similarity_threshold)\n",
    "    \n",
    "    # Find best matches\n",
    "    matches = find_best_matches(similarity_matrix)\n",
    "    \n",
    "    # Calculate weighted TP (true positives)\n",
    "    weighted_tp = sum(similarity / 100.0 for _, _, similarity in matches)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = weighted_tp / len(predictions) if predictions else 0.0\n",
    "    recall = weighted_tp / len(ground_truth) if ground_truth else 0.0\n",
    "    \n",
    "    f1 = 0.0\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Calculate false positives and false negatives\n",
    "    # False positives: predictions that didn't match any ground truth\n",
    "    matched_pred_indices = set(pred_idx for pred_idx, _, _ in matches)\n",
    "    false_positives = len(predictions) - len(matched_pred_indices)\n",
    "    \n",
    "    # False negatives: ground truth items that weren't matched by any prediction\n",
    "    matched_truth_indices = set(truth_idx for _, truth_idx, _ in matches)\n",
    "    false_negatives = len(ground_truth) - len(matched_truth_indices)\n",
    "    \n",
    "    # Create detailed match information\n",
    "    match_details = []\n",
    "    for pred_idx, truth_idx, similarity in matches:\n",
    "        match_details.append({\n",
    "            \"prediction\": predictions[pred_idx],\n",
    "            \"ground_truth\": ground_truth[truth_idx],\n",
    "            \"similarity\": similarity\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"matches\": match_details,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives\n",
    "    }\n",
    "\n",
    "\n",
    "def print_results(results: Dict[str, float], show_matches: bool = False) -> None:\n",
    "    \"\"\"Print the evaluation results.\"\"\"\n",
    "    print(f\"Precision: {results['precision']:.4f}\")\n",
    "    print(f\"Recall: {results['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {results['f1_score']:.4f}\")\n",
    "    print(f\"False Positives: {results['false_positives']}\")\n",
    "    print(f\"False Negatives: {results['false_negatives']}\")\n",
    "    \n",
    "    if show_matches and results['matches']:\n",
    "        print(\"\\nMatched Pairs:\")\n",
    "        for match in results['matches']:\n",
    "            print(f\"- Prediction: '{match['prediction']}' → Ground Truth: '{match['ground_truth']}' (Similarity: {match['similarity']:.1f}%)\")\n",
    "\n",
    "\n",
    "def evaluate_results_file(ground_truth_file: str, predictions_file: str, \n",
    "                         similarity_threshold: float = 80.0,\n",
    "                         output_file: Optional[str] = None,\n",
    "                         verbose: bool = False) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate results by reading from files.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_file: Path to file with ground truth phenotypes (one per line)\n",
    "        predictions_file: Path to file with predicted phenotypes (one per line)\n",
    "        similarity_threshold: Minimum similarity score to consider a potential match\n",
    "        output_file: Optional path to save detailed results\n",
    "        verbose: Whether to print detailed match information\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, F1 score\n",
    "    \"\"\"\n",
    "    # Read files\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        ground_truth = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = calculate_soft_f1(predictions, ground_truth, similarity_threshold)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Evaluated {len(predictions)} predictions against {len(ground_truth)} ground truth items\")\n",
    "    print(f\"Using similarity threshold: {similarity_threshold}%\")\n",
    "    print_results(results, verbose)\n",
    "    \n",
    "    # Save detailed results if requested\n",
    "    if output_file:\n",
    "        output_df = pd.DataFrame(results['matches'])\n",
    "        output_df.to_csv(output_file, index=False)\n",
    "        print(f\"Detailed results saved to {output_file}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_result_sets(ground_truth: List[str], result_set_a: List[str], result_set_b: List[str],\n",
    "                       similarity_threshold: float = 80.0) -> None:\n",
    "    \"\"\"\n",
    "    Compare two sets of results against the same ground truth.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth: List of ground truth phenotypes\n",
    "        result_set_a: First set of result phenotypes\n",
    "        result_set_b: Second set of result phenotypes\n",
    "        similarity_threshold: Minimum similarity score to consider a potential match\n",
    "    \"\"\"\n",
    "    # Calculate metrics for both result sets\n",
    "    results_a = calculate_soft_f1(result_set_a, ground_truth, similarity_threshold)\n",
    "    results_b = calculate_soft_f1(result_set_b, ground_truth, similarity_threshold)\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"=== Results Set A ===\")\n",
    "    print(f\"Number of predictions: {len(result_set_a)}\")\n",
    "    print_results(results_a, False)\n",
    "    \n",
    "    print(\"\\n=== Results Set B ===\")\n",
    "    print(f\"Number of predictions: {len(result_set_b)}\")\n",
    "    print_results(results_b, False)\n",
    "    \n",
    "    # Determine which is better\n",
    "    if results_a['f1_score'] > results_b['f1_score']:\n",
    "        print(\"\\nResults Set A has a higher F1 score\")\n",
    "    elif results_b['f1_score'] > results_a['f1_score']:\n",
    "        print(\"\\nResults Set B has a higher F1 score\")\n",
    "    else:\n",
    "        print(\"\\nBoth result sets have the same F1 score\")\n",
    "        \n",
    "    # Calculate the difference\n",
    "    f1_diff = abs(results_a['f1_score'] - results_b['f1_score'])\n",
    "    print(f\"F1 Score difference: {f1_diff:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rerun whole pipeline\n",
    "text =  \"A 35-year-old woman presented to the medical emergency department with low-grade fever for 3 weeks, vomiting for 1 week and anuria for 3 days. She also reported dysuria and breathlessness for 1 week. There was no history of decreased urine output, dialysis, effort intolerance, chest pain or palpitation, dyspnoea and weight loss. Menstrual history was within normal limit but she reported gradually progressive loss of appetite. Family history included smoky urine in her younger brother in his childhood, who died in an accident. On general survey, the patient was conscious and alert. She was dyspnoeic and febrile. Severe pallor was present with mild pedal oedema. Blood pressure was 180/100 mm Hg and pulse rate of 116/min regular. No evidence of jaundice, clubbing cyanosis or lymphadenopathy was found. Physical examination revealed bibasilar end-inspiratory crepitations in lungs and suprapubic tenderness. There was no hepatosplenomegaly or ascites. Cardiac examination was normal. She was found to have severe bilateral hearing loss, which was gradually progressive for 5 years. The fundi were bilaterally pale. The patient was referred to the department of ophthalmology for a comprehensive eye examination. Her visual acuity was documented as 6/18 in both eyes with no obvious lenticular opacity. Slit-lamp examination showed bilateral anterior lentiglobus with posterior lenticonus. Distant direct ophthalmoscopy revealed oil droplet sign (a suggestive confirmation of the presence of lenticonus); and peripheral retina revealed multiple yellowish white lesion-like flecks in the mid-periphery, and few blot haemorrhages indicative of hypertensive changes. Haemogram showed haemoglobin (Hb) 5.7 g/dL, erythrocyte sedimentation rate 15 mm in first hour, white cell count 17 200/\\u00b5L with 82% polymorphs, adequate platelets and mean corpuscular volume 83.3 fL. Peripheral smear showed normocytic normochromic anaemia with mild anisocytosis. Fasting sugar 78 mg/dL, blood urea 325 mg/dL, serum creatinine 11.2 mg/dL and uric acid 8.3 mg/dL. Liver function tests were within normal limit as were serum electrolytes, except serum calcium (conc.) 5.8 mg/dL (adjusted with serum albumin). Lipid profile and iron profile were also normal. HIV and viral markers for HbsAg and hepatitis C virus were non-reactive. ECG showed sinus tachycardia with features of left ventricular hypertrophy and chest X-ray posteroanterior view revealed cardiomegaly. Urinalysis showed full field of pus cells with 35\\u201340 RBCs/hpf and 3(+) proteinuria. Urine samples for cultures were sent which reported pure growth of Escherichia coli. Spot urine for protein:creatinine ratio was 2.07 g/g Cr. She underwent pure tone audiometry which revealed features suggestive of severe bilateral sensorineural hearing loss (SHNL). Ultrasound of the abdomen showed bilateral contracted kidneys: right measured 6.7\\u00d72.3 cm and left 7.8\\u00d73 cm, with increased cortical echogenecity and loss of corticomedullary differentiation, suggestive of medical renal disease. Two-dimensional Echo reported dilated left ventricular cavity with mild mitral regurgitation and ejection fraction of 55%. Renal and skin biopsies were conducted and specimens were sent for light and electron microscopy (EM). Renal tissue on H&E stain was reported as focal segmental glomerulonephritis (FSGS). Ultrathin sections of EM study of renal tissue revealed disruption of glomerular basement membrane (GBM) with diffuse thickening of glomerular capillary wall. Dermal tissue depicted discontinuity of lamina densa with basket weaving pattern under EM.\"\n",
    "\n",
    "system_message_I = \"You are a rare disease expert with extensive medical knowledge. Carefully review every sentence of the clinical passage to identify terms related to genetic inheritance patterns, anatomical anomalies, clinical symptoms, diagnostic findings, lab test results, and specific conditions or syndromes. Completely ignore negative findings, normal findings (i.e. 'normal', 'no', or 'negative'), procedures and family history.  Return the extracted terms in a JSON object with a single key 'findings', which contains the list of extracted terms spelled correctly. Ensure the output is concise without any additional notes, commentary, or meta explanations.\"\n",
    "\n",
    "# so we can get an idea of the variance across entity extraction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "# average of 3 runs with different temperatures, we should just compare average performance\n",
    "for i in range(5):\n",
    "    runs = []\n",
    "    # temperatures = [0.1]\n",
    "    # temperatures = [0.1, 0.1, 0.1]\n",
    "    # temperatures = [0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "    temperatures = [0.01, 0.1, 0.3, 0.7, 0.9]\n",
    "    # temperatures = [0.7,0.7,0.7,0.7,0.7]\n",
    "    # temperatures = [0.1,0.1,0.3, 0.7, 0.7]\n",
    "    #bimodal (?)\n",
    "    # temperatures = [0.1,0.1, 0.3,0.3, 0.7, 0.7, 0.9, 0.9]\n",
    "    # temperatures = \n",
    "    for i in range(len(temperatures)):\n",
    "        llm_client.temperature = temperatures[i] # iterate temperature from hot to cold\n",
    "        llm_extractor = IterativeLLMEntityExtractor(\n",
    "            llm_client=llm_client,\n",
    "            # llm_client=llama_client,\n",
    "            system_message=system_message_I,\n",
    "            max_iterations=2 # iterative extractor\n",
    "        )\n",
    "        extracted = llm_extractor.process_batch([text])\n",
    "        context_extractor = ContextExtractor(debug=False)\n",
    "        # Extract contexts for all entities\n",
    "        extracted = context_extractor.batch_extract_contexts(extracted, [text], window_size=1)\n",
    "        runs.append(extracted)\n",
    "\n",
    "\n",
    "    unioned_results = []\n",
    "    intersection_results = []\n",
    "    hybrid_count_results = []\n",
    "\n",
    "    # Union of all results\n",
    "    for run in runs:\n",
    "        for entity in run[0]:\n",
    "            if entity[\"entity\"] not in unioned_results:\n",
    "                unioned_results.append(entity[\"entity\"])\n",
    "\n",
    "    # Convert intersection to a list\n",
    "    intersection_results = list(set.intersection(*[set(run[0][j][\"entity\"] for j in range(len(run[0]))) for run in runs]))\n",
    "\n",
    "    # Hybrid count: entities that appear in at least 2 runs\n",
    "    entity_run_counts = {}\n",
    "    for run in runs:\n",
    "        for entity in run[0]:\n",
    "            if entity[\"entity\"] not in entity_run_counts:\n",
    "                entity_run_counts[entity[\"entity\"]] = 1\n",
    "            else:\n",
    "                entity_run_counts[entity[\"entity\"]] += 1\n",
    "\n",
    "    # Select entities that appear in at least 2 runs\n",
    "    hybrid_count_results = [entity for entity, count in entity_run_counts.items() if count >= 2]\n",
    "\n",
    "    print(\"Union results:\", calculate_soft_f1(unioned_results, ground_truth, similarity_threshold=50.0))\n",
    "    print(\"Intersection results:\", calculate_soft_f1(intersection_results, ground_truth, similarity_threshold=50.0))\n",
    "    print(\"Hybrid count results:\", calculate_soft_f1(hybrid_count_results, ground_truth, similarity_threshold=50.0))\n",
    "\n",
    "    metrics.append({\n",
    "        \"union\": calculate_soft_f1(unioned_results, ground_truth, similarity_threshold=50.0),\n",
    "        \"intersection\": calculate_soft_f1(intersection_results, ground_truth, similarity_threshold=50.0),\n",
    "        \"hybrid\": calculate_soft_f1(hybrid_count_results, ground_truth, similarity_threshold=50.0)\n",
    "    })\n",
    "    print(\"Union vs. Intersection\")\n",
    "    compare_result_sets(ground_truth, unioned_results, intersection_results, similarity_threshold=50.0)\n",
    "    print()\n",
    "    print(\"Intersection vs. Hybrid\")\n",
    "    compare_result_sets(ground_truth, intersection_results,hybrid_count_results ,similarity_threshold=50.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_average_metric_with_std(all_metrics, aggr_type=\"hybrid\"):\n",
    "    \"\"\"\n",
    "    Computes the average and standard deviation for precision, recall, and F1 score\n",
    "    for a specified aggregation technique across multiple runs.\n",
    "    \n",
    "    Args:\n",
    "        all_metrics (list): List of dictionaries containing metrics for each run\n",
    "        aggr_type (str): Aggregation type (\"union\", \"intersection\", or \"hybrid\")\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing average and standard deviation for each metric\n",
    "    \"\"\"\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    precisions = []\n",
    "    \n",
    "    for metrics in all_metrics:\n",
    "        recalls.append(metrics[aggr_type][\"recall\"])\n",
    "        f1_scores.append(metrics[aggr_type][\"f1_score\"])\n",
    "        precisions.append(metrics[aggr_type][\"precision\"])\n",
    "\n",
    "    return {\n",
    "        \"Recall\": {\n",
    "            \"mean\": np.mean(recalls),\n",
    "            \"std\": np.std(recalls, ddof=1)  # Using sample standard deviation\n",
    "        },\n",
    "        \"F1\": {\n",
    "            \"mean\": np.mean(f1_scores),\n",
    "            \"std\": np.std(f1_scores, ddof=1)\n",
    "        },\n",
    "        \"Precision\": {\n",
    "            \"mean\": np.mean(precisions),\n",
    "            \"std\": np.std(precisions, ddof=1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def compare_all_aggregation_methods(all_metrics):\n",
    "    \"\"\"\n",
    "    Computes means and standard deviations for all three aggregation techniques\n",
    "    and provides a comparative analysis.\n",
    "    \n",
    "    Args:\n",
    "        all_metrics (list): List of dictionaries containing metrics for each run\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing statistics for all aggregation methods\n",
    "    \"\"\"\n",
    "    aggregation_types = [\"union\", \"intersection\", \"hybrid\"]\n",
    "    results = {}\n",
    "    \n",
    "    for aggr_type in aggregation_types:\n",
    "        results[aggr_type] = get_average_metric_with_std(all_metrics, aggr_type)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_performance_summary(all_metrics):\n",
    "    \"\"\"\n",
    "    Prints a formatted summary of performance metrics with means and standard deviations\n",
    "    for all aggregation techniques.\n",
    "    \n",
    "    Args:\n",
    "        all_metrics (list): List of dictionaries containing metrics for each run\n",
    "    \"\"\"\n",
    "    results = compare_all_aggregation_methods(all_metrics)\n",
    "    \n",
    "    print(\"\\n===== PERFORMANCE SUMMARY WITH VARIANCE =====\")\n",
    "    print(f\"{'Metric':<12}{'Union':<30}{'Intersection':<30}{'Hybrid':<30}\")\n",
    "    print(\"-\" * 102)\n",
    "    \n",
    "    for metric in [\"Precision\", \"Recall\", \"F1\"]:\n",
    "        union_stats = f\"{results['union'][metric]['mean']:.4f} ± {results['union'][metric]['std']:.4f}\"\n",
    "        intersection_stats = f\"{results['intersection'][metric]['mean']:.4f} ± {results['intersection'][metric]['std']:.4f}\"\n",
    "        hybrid_stats = f\"{results['hybrid'][metric]['mean']:.4f} ± {results['hybrid'][metric]['std']:.4f}\"\n",
    "        \n",
    "        print(f\"{metric:<12}{union_stats:<30}{intersection_stats:<30}{hybrid_stats:<30}\")\n",
    "    \n",
    "    print(\"\\nNote: Values shown as mean ± standard deviation\")\n",
    "\n",
    "# Example usage at the end of your existing code:\n",
    "# print_performance_summary(metrics)\n",
    "\n",
    "# For detailed analysis of a specific aggregation method:\n",
    "def analyze_metric_stability(all_metrics, aggr_type=\"hybrid\", metric_name=\"F1\"):\n",
    "    \"\"\"\n",
    "    Analyzes the stability of a specific metric for a given aggregation technique.\n",
    "    \n",
    "    Args:\n",
    "        all_metrics (list): List of dictionaries containing metrics for each run\n",
    "        aggr_type (str): Aggregation type (\"union\", \"intersection\", or \"hybrid\")\n",
    "        metric_name (str): Metric to analyze (\"Precision\", \"Recall\", or \"F1\")\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing statistical analysis\n",
    "    \"\"\"\n",
    "    if metric_name == \"F1\":\n",
    "        values = [metrics[aggr_type][\"f1_score\"] for metrics in all_metrics]\n",
    "    elif metric_name == \"Recall\":\n",
    "        values = [metrics[aggr_type][\"recall\"] for metrics in all_metrics]\n",
    "    elif metric_name == \"Precision\":\n",
    "        values = [metrics[aggr_type][\"precision\"] for metrics in all_metrics]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric name. Choose 'Precision', 'Recall', or 'F1'\")\n",
    "    \n",
    "    return {\n",
    "        \"mean\": np.mean(values),\n",
    "        \"std\": np.std(values, ddof=1),\n",
    "        \"min\": np.min(values),\n",
    "        \"max\": np.max(values),\n",
    "        \"range\": np.max(values) - np.min(values),\n",
    "        \"coefficient_of_variation\": np.std(values, ddof=1) / np.mean(values) if np.mean(values) > 0 else float('inf')\n",
    "    }\n",
    "\n",
    "# Add this to your existing code to compute stats for all metrics\n",
    "def compute_all_stats(metrics):\n",
    "    \"\"\"\n",
    "    Computes comprehensive statistics for all metrics and aggregation methods.\n",
    "    \n",
    "    Args:\n",
    "        metrics (list): List of metric dictionaries from multiple runs\n",
    "        \n",
    "    Returns:\n",
    "        dict: Nested dictionary with complete statistical analysis\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for aggr_type in [\"union\", \"intersection\", \"hybrid\"]:\n",
    "        results[aggr_type] = {}\n",
    "        for metric_name in [\"Precision\", \"Recall\", \"F1\"]:\n",
    "            results[aggr_type][metric_name] = analyze_metric_stability(\n",
    "                metrics, aggr_type, metric_name\n",
    "            )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example of how to use this at the end of your existing code:\n",
    "all_stats = compute_all_stats(metrics)\n",
    "print_performance_summary(metrics)\n",
    "# \n",
    "# # Print detailed stats for a specific aggregation method\n",
    "# detailed_stats = analyze_metric_stability(metrics, \"hybrid\", \"F1\")\n",
    "# print(f\"Detailed analysis of F1 score stability for hybrid aggregation:\")\n",
    "# for key, value in detailed_stats.items():\n",
    "#     print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizer for multiLLM and iterative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizer for more complex configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Define your ground truth manually (these would be the correct phenotypic terms for the case)\n",
    "# This is just an example - replace with your actual ground truth terms\n",
    "import itertools\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Any, Optional, Callable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class EntityExtractionOptimizer:\n",
    "    \"\"\"\n",
    "    Grid search optimizer for entity extraction configurations.\n",
    "    \n",
    "    This class evaluates different combinations of extraction strategies:\n",
    "    - Different prompt templates\n",
    "    - With/without self-reflection\n",
    "    - With/without negation checking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_client,\n",
    "        context_extractor,\n",
    "        ground_truth: List[str],\n",
    "        clinical_text: str,\n",
    "        evaluation_func: Callable,\n",
    "        debug: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the optimizer.\n",
    "        \n",
    "        Args:\n",
    "            llm_client: Client for querying the language model\n",
    "            context_extractor: Context extraction utility\n",
    "            ground_truth: List of ground truth entities\n",
    "            clinical_text: Clinical text to extract entities from\n",
    "            evaluation_func: Function to evaluate extraction results\n",
    "            debug: Whether to print debug information\n",
    "        \"\"\"\n",
    "        self.llm_client = llm_client\n",
    "        self.context_extractor = context_extractor\n",
    "        self.ground_truth = ground_truth\n",
    "        self.clinical_text = clinical_text\n",
    "        self.evaluation_func = evaluation_func\n",
    "        self.debug = debug\n",
    "        self.results = []\n",
    "        \n",
    "        # Define prompt templates\n",
    "        self.prompt_templates = {\n",
    "            \"original\": \"\"\"You are a rare disease expert with extensive medical knowledge. Carefully review every sentence of the clinical passage to identify terms related to genetic inheritance patterns, anatomical anomalies, clinical symptoms, diagnostic findings, lab test results, and specific conditions or syndromes. Completely ignore negative findings, normal findings (i.e. 'normal', 'no', or 'negative'), procedures and family history.  Return the extracted terms in a JSON object with a single key 'findings', which contains the list of extracted terms spelled correctly. Ensure the output is concise without any additional notes, commentary, or meta explanations.\"\"\",\n",
    "            \n",
    "            \"enhanced\": \"\"\"You are a rare disease expert with extensive medical knowledge. Review the clinical passage to identify ALL terms that match AT LEAST ONE of these categories:\n",
    "\n",
    "1. Genetic inheritance patterns (e.g., 'autosomal dominant', 'X-linked')\n",
    "2. Anatomical anomalies (e.g., 'cleft palate', 'lenticonus')\n",
    "3. Clinical symptoms (e.g., 'dyspnea', 'anuria', 'pallor')\n",
    "4. Diagnostic findings (e.g., 'hypertelorism', 'bibasilar crepitations')\n",
    "5. Abnormal lab results (e.g., 'creatinine 11.2 mg/dL', 'hemoglobin 5.7 g/dL')\n",
    "6. Specific conditions or syndromes (e.g., 'focal segmental glomerulonephritis')\n",
    "\n",
    "IMPORTANT GUIDELINES:\n",
    "- Include BOTH explicitly stated abnormalities AND indicators of abnormality\n",
    "- Pay special attention to measurements outside normal ranges\n",
    "- Include findings mentioned within negated contexts that still indicate abnormality (e.g., 'no eye contact' implies social deficit)\n",
    "- Extract phenotypes that are implied by medications or treatments\n",
    "- Look for subtle clues like specialist referrals or monitoring\n",
    "\n",
    "DO NOT include:\n",
    "- Normal findings explicitly stated as normal (e.g., 'liver function tests were within normal limit')\n",
    "- Procedures without findings (e.g., 'underwent pure tone audiometry')\n",
    "- Family history unless directly relevant to patient's diagnosis\n",
    "\n",
    "Return ONLY a JSON object with a single key 'findings' containing the list of extracted terms:\n",
    "{\n",
    "  \"findings\": [\"term1\", \"term2\", ...]\n",
    "}\"\"\"\n",
    "        }\n",
    "        \n",
    "    def _debug_print(self, message: str, level: int = 0):\n",
    "        \"\"\"Print debug messages if debug mode is enabled.\"\"\"\n",
    "        if self.debug:\n",
    "            indent = \"  \" * level\n",
    "            print(f\"{datetime.now().strftime('%H:%M:%S')} | {indent}{message}\")\n",
    "    \n",
    "    def _extract_findings_from_response(self, response_content: str) -> List[str]:\n",
    "        \"\"\"Extract findings from LLM response.\"\"\"\n",
    "        sanitized = response_content.replace(\"```\", \"\").strip()\n",
    "        start = sanitized.find(\"{\")\n",
    "        end = sanitized.rfind(\"}\")\n",
    "        if start == -1 or end == -1 or start > end:\n",
    "            return []\n",
    "\n",
    "        json_str = sanitized[start:end+1]\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            findings = data.get(\"findings\", [])\n",
    "            return findings if isinstance(findings, list) else []\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "    \n",
    "    def _create_iterative_prompt(self, original_text: str, already_extracted: str, \n",
    "                                template: str, iteration: int, max_iterations: int) -> str:\n",
    "        \"\"\"Create a prompt for iterative entity extraction.\"\"\"\n",
    "        if template == \"original\":\n",
    "            return (\n",
    "                f\"{original_text}\\n\\n\"\n",
    "                f\"I have already extracted the following terms: {already_extracted}.\\n\"\n",
    "                f\"Please examine the clinical text again carefully (iteration {iteration+1} of up to {max_iterations}) \"\n",
    "                f\"and identify any additional genetic inheritance patterns, anatomical anomalies, clinical symptoms, \"\n",
    "                f\"diagnostic findings, lab test results, and specific conditions or syndromes that were missed in the previous extractions. \"\n",
    "                f\"Find terms that aren't in the already extracted list. \"\n",
    "                f\"Return the extracted terms in a JSON object with a single key 'findings', which contains the list of extracted terms spelled correctly. \"\n",
    "                f\"If you don't find any new terms, return an empty list.\"\n",
    "            )\n",
    "        else:  # enhanced\n",
    "            return (\n",
    "                f\"{original_text}\\n\\n\"\n",
    "                f\"I have already extracted the following terms: {already_extracted}.\\n\\n\"\n",
    "                f\"For this iteration ({iteration+1}/{max_iterations}), examine the clinical text again to find ANY ADDITIONAL abnormalities or clinical features that were missed, including:\\n\\n\"\n",
    "                f\"1. Explicit abnormalities not in the list above\\n\"\n",
    "                f\"2. Implicit phenotypes indicated by:\\n\"\n",
    "                f\"   - Measurements outside normal ranges\\n\"\n",
    "                f\"   - Medications that suggest conditions\\n\"\n",
    "                f\"   - Functional descriptions implying abnormalities\\n\"\n",
    "                f\"   - Negated normal findings that indicate abnormality\\n\"\n",
    "                f\"   - Referrals or monitoring suggesting underlying issues\\n\\n\"\n",
    "                f\"Focus on finding terms NOT already in the extracted list. \"\n",
    "                f\"Return ONLY a JSON object with a single key 'findings' containing NEW terms:\\n\"\n",
    "            )\n",
    "    \n",
    "    def _perform_self_reflection(self, entities: List[str], text: str) -> List[str]:\n",
    "        \"\"\"Validate extracted entities through self-reflection.\"\"\"\n",
    "        if not entities:\n",
    "            return []\n",
    "            \n",
    "        validation_prompt = (\n",
    "            f\"I extracted the following potential phenotypic terms from a clinical text:\\n\\n\"\n",
    "            f\"{', '.join(entities)}\\n\\n\"\n",
    "            f\"For each term, evaluate if it is TRULY one of the following categories:\\n\"\n",
    "            f\"- Genetic inheritance pattern\\n\"\n",
    "            f\"- Anatomical anomaly\\n\"\n",
    "            f\"- Clinical symptom\\n\"\n",
    "            f\"- Diagnostic finding\\n\"\n",
    "            f\"- Abnormal lab test result\\n\"\n",
    "            f\"- Specific condition or syndrome\\n\\n\"\n",
    "            f\"Discard terms that are normal findings, procedures, anatomical parts without abnormalities, or family history.\\n\\n\"\n",
    "            f\"Return only the validated terms in a JSON object with a single key 'validated_findings'\"\n",
    "        )\n",
    "        \n",
    "        response = self.llm_client.query(validation_prompt, \n",
    "                                       \"You are a medical validation expert. Evaluate each extracted term critically.\")\n",
    "        \n",
    "        # Extract validated findings\n",
    "        sanitized = response.replace(\"```\", \"\").strip()\n",
    "        start = sanitized.find(\"{\")\n",
    "        end = sanitized.rfind(\"}\")\n",
    "        if start == -1 or end == -1 or start > end:\n",
    "            return entities  # Return original if parsing fails\n",
    "            \n",
    "        try:\n",
    "            data = json.loads(sanitized[start:end+1])\n",
    "            validated = data.get(\"validated_findings\", [])\n",
    "            if isinstance(validated, list) and validated:\n",
    "                self._debug_print(f\"Self-reflection reduced entities from {len(entities)} to {len(validated)}\")\n",
    "                return validated\n",
    "            return entities  # Return original if no valid data\n",
    "        except:\n",
    "            return entities  # Return original if parsing fails\n",
    "    \n",
    "    def _check_negations(self, entities: List[Dict], text: str) -> List[Dict]:\n",
    "        \"\"\"Analyze context for negations and extract additional implied phenotypes.\"\"\"\n",
    "        if not entities:\n",
    "            return []\n",
    "            \n",
    "        # Extract entities with context\n",
    "        entities_with_context = []\n",
    "        for entity_dict in entities:\n",
    "            entity = entity_dict.get(\"entity\", \"\")\n",
    "            context = entity_dict.get(\"context\", \"\")\n",
    "            \n",
    "            if entity and context:\n",
    "                entities_with_context.append(f\"Entity: '{entity}' | Context: '{context}'\")\n",
    "        \n",
    "        if not entities_with_context:\n",
    "            return entities  # Return original if no contexts available\n",
    "            \n",
    "        negation_prompt = (\n",
    "            f\"Analyze these clinical entities and their contexts for NEGATED normal findings or IMPLICIT phenotypes:\\n\\n\"\n",
    "            f\"{chr(10).join(entities_with_context)}\\n\\n\"\n",
    "            f\"For each entity/context pair:\\n\"\n",
    "            f\"1. Determine if it represents an abnormality (keep it)\\n\"\n",
    "            f\"2. Check if it's a negated NORMAL finding that implies an abnormality (e.g., 'no eye contact' → extract 'poor eye contact')\\n\"\n",
    "            f\"3. Identify any implicit phenotypes not directly stated (e.g., 'hemoglobin 5.7 g/dL' → extract 'anemia')\\n\\n\"\n",
    "            f\"Return a JSON object with keys:\\n\"\n",
    "            f\"- 'keep': list of original entities to keep\\n\"\n",
    "            f\"- 'implied': list of additional phenotypes implied by the entities/contexts\\n\"\n",
    "        )\n",
    "        \n",
    "        response = self.llm_client.query(negation_prompt, \n",
    "                                      \"You are a clinical expert focused on analyzing negations and implications in medical text.\")\n",
    "        \n",
    "        # Parse response\n",
    "        sanitized = response.replace(\"```\", \"\").strip()\n",
    "        start = sanitized.find(\"{\")\n",
    "        end = sanitized.rfind(\"}\")\n",
    "        \n",
    "        try:\n",
    "            if start != -1 and end != -1 and start < end:\n",
    "                data = json.loads(sanitized[start:end+1])\n",
    "                \n",
    "                # Get entities to keep\n",
    "                keep_entities = data.get(\"keep\", [])\n",
    "                if not isinstance(keep_entities, list):\n",
    "                    keep_entities = []\n",
    "                    \n",
    "                # Get implied entities\n",
    "                implied_entities = data.get(\"implied\", [])\n",
    "                if not isinstance(implied_entities, list):\n",
    "                    implied_entities = []\n",
    "                \n",
    "                # Filter original entities to keep only those in the \"keep\" list\n",
    "                filtered_entities = [\n",
    "                    entity_dict for entity_dict in entities\n",
    "                    if entity_dict.get(\"entity\", \"\") in keep_entities\n",
    "                ]\n",
    "                \n",
    "                # Add implied entities with empty context\n",
    "                for implied in implied_entities:\n",
    "                    filtered_entities.append({\n",
    "                        \"entity\": implied,\n",
    "                        \"context\": \"\",  # No specific context for implied entities\n",
    "                        \"implied\": True  # Mark as implied\n",
    "                    })\n",
    "                \n",
    "                self._debug_print(f\"Negation checking: kept {len(keep_entities)}/{len(entities)}, added {len(implied_entities)} implied\")\n",
    "                return filtered_entities\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # Return original if parsing fails\n",
    "        return entities\n",
    "    \n",
    "    def extract_entities_with_config(self, config: Dict) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract entities using a specific configuration.\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dictionary with extraction parameters\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted entities\n",
    "        \"\"\"\n",
    "        # Unpack configuration\n",
    "        prompt_template = config.get(\"prompt_template\", \"original\")\n",
    "        use_self_reflection = config.get(\"use_self_reflection\", False)\n",
    "        use_negation_checking = config.get(\"use_negation_checking\", False)\n",
    "        max_iterations = config.get(\"max_iterations\", 3)\n",
    "        \n",
    "        # Get the appropriate system message\n",
    "        system_message = self.prompt_templates.get(prompt_template, self.prompt_templates[\"original\"])\n",
    "        \n",
    "        # Initialize with empty set of entities\n",
    "        current_entities = set()\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            # Create prompt\n",
    "            if iteration == 0:\n",
    "                prompt = self.clinical_text\n",
    "            else:\n",
    "                already_extracted = \", \".join(sorted(current_entities))\n",
    "                prompt = self._create_iterative_prompt(\n",
    "                    self.clinical_text, \n",
    "                    already_extracted, \n",
    "                    prompt_template,\n",
    "                    iteration,\n",
    "                    max_iterations\n",
    "                )\n",
    "            \n",
    "            # Query LLM\n",
    "            findings_text = self.llm_client.query(prompt, system_message)\n",
    "            if not findings_text:\n",
    "                break\n",
    "                \n",
    "            # Extract entities from this pass\n",
    "            iteration_entities = set(self._extract_findings_from_response(findings_text))\n",
    "            \n",
    "            # Apply self-reflection if enabled\n",
    "            if use_self_reflection and iteration_entities:\n",
    "                iteration_entities = set(self._perform_self_reflection(list(iteration_entities), self.clinical_text))\n",
    "            \n",
    "            # Combine with previous entities (union)\n",
    "            combined_entities = current_entities.union(iteration_entities)\n",
    "            \n",
    "            # Check if we've converged (no new entities)\n",
    "            if combined_entities == current_entities:\n",
    "                self._debug_print(f\"Early stopping at iteration {iteration+1}: No new entities found.\")\n",
    "                break\n",
    "                \n",
    "            # Calculate new entities found in this iteration\n",
    "            new_entities = combined_entities - current_entities\n",
    "            if new_entities:\n",
    "                self._debug_print(f\"Iteration {iteration+1}: Found {len(new_entities)} new entities\")\n",
    "            else:\n",
    "                self._debug_print(f\"Iteration {iteration+1}: No new entities found.\")\n",
    "                \n",
    "            # Update current entities\n",
    "            current_entities = combined_entities\n",
    "        \n",
    "        # Convert set to list\n",
    "        entities_list = list(current_entities)\n",
    "        \n",
    "        # Apply negation checking if enabled\n",
    "        if use_negation_checking:\n",
    "            # First get context for each entity\n",
    "            entities_with_context = self.context_extractor.extract_contexts(entities_list, self.clinical_text)\n",
    "            \n",
    "            # Then check for negations and implications\n",
    "            processed_entities = self._check_negations(entities_with_context, self.clinical_text)\n",
    "            \n",
    "            # Extract just the entity text for the final list\n",
    "            entities_list = [e.get(\"entity\", \"\") for e in processed_entities]\n",
    "        \n",
    "        return entities_list\n",
    "    \n",
    "    def grid_search(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform grid search over all possible configurations.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with results and best configuration\n",
    "        \"\"\"\n",
    "        # Define configuration options\n",
    "        config_options = {\n",
    "            \"prompt_template\": [\"original\", \"enhanced\"],\n",
    "            \"use_self_reflection\": [False, True],\n",
    "            \"use_negation_checking\": [False, True],\n",
    "            \"max_iterations\": [3]  # Can add more options if needed\n",
    "        }\n",
    "        \n",
    "        # Generate all combinations\n",
    "        keys = config_options.keys()\n",
    "        values = [config_options[key] for key in keys]\n",
    "        configurations = [dict(zip(keys, combo)) for combo in itertools.product(*values)]\n",
    "        \n",
    "        # Initialize results\n",
    "        best_config = None\n",
    "        best_score = -1\n",
    "        all_results = []\n",
    "        \n",
    "        # Evaluate each configuration\n",
    "        self._debug_print(f\"Evaluating {len(configurations)} configurations\")\n",
    "        for i, config in enumerate(tqdm(configurations, desc=\"Grid Search Progress\")):\n",
    "            config_name = f\"Config {i+1}: {config}\"\n",
    "            self._debug_print(f\"Evaluating {config_name}\")\n",
    "            \n",
    "            # Run extraction with this configuration\n",
    "            try:\n",
    "                # Run 3 times for stability\n",
    "                results = []\n",
    "                for run in range(3):\n",
    "                    extracted_entities = self.extract_entities_with_config(config)\n",
    "                    results.append(extracted_entities)\n",
    "                \n",
    "                # Use the median result by length\n",
    "                results.sort(key=len)\n",
    "                extracted_entities = results[len(results) // 2]\n",
    "                \n",
    "                # Evaluate results\n",
    "                eval_results = self.evaluation_func(self.ground_truth, extracted_entities)\n",
    "                \n",
    "                # Store results\n",
    "                config_result = {\n",
    "                    \"config\": config,\n",
    "                    \"config_name\": config_name,\n",
    "                    \"entities\": extracted_entities,\n",
    "                    \"entity_count\": len(extracted_entities),\n",
    "                    \"precision\": eval_results.get(\"precision\", 0),\n",
    "                    \"recall\": eval_results.get(\"recall\", 0),\n",
    "                    \"f1_score\": eval_results.get(\"f1_score\", 0),\n",
    "                    \"false_positives\": eval_results.get(\"false_positives\", 0),\n",
    "                    \"false_negatives\": eval_results.get(\"false_negatives\", 0)\n",
    "                }\n",
    "                \n",
    "                all_results.append(config_result)\n",
    "                \n",
    "                # Update best configuration based on F1 score\n",
    "                f1_score = eval_results.get(\"f1_score\", 0)\n",
    "                if f1_score > best_score:\n",
    "                    best_score = f1_score\n",
    "                    best_config = config\n",
    "                    \n",
    "                self._debug_print(f\"Results for {config_name}:\")\n",
    "                self._debug_print(f\"  Entities: {len(extracted_entities)}\")\n",
    "                self._debug_print(f\"  Precision: {eval_results.get('precision', 0):.4f}\")\n",
    "                self._debug_print(f\"  Recall: {eval_results.get('recall', 0):.4f}\")\n",
    "                self._debug_print(f\"  F1 Score: {eval_results.get('f1_score', 0):.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self._debug_print(f\"Error evaluating {config_name}: {str(e)}\")\n",
    "        \n",
    "        # Sort results by F1 score\n",
    "        all_results.sort(key=lambda x: x.get(\"f1_score\", 0), reverse=True)\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            \"best_config\": best_config,\n",
    "            \"best_score\": best_score,\n",
    "            \"all_results\": all_results\n",
    "        }\n",
    "    \n",
    "    def print_optimization_results(self, results: Dict) -> None:\n",
    "        \"\"\"\n",
    "        Print a summary of optimization results.\n",
    "        \n",
    "        Args:\n",
    "            results: Results from grid search\n",
    "        \"\"\"\n",
    "        print(\"\\n===== ENTITY EXTRACTION OPTIMIZATION RESULTS =====\")\n",
    "        print(f\"Total configurations evaluated: {len(results.get('all_results', []))}\")\n",
    "        \n",
    "        # Print top 3 configurations\n",
    "        print(\"\\nTop 3 Configurations:\")\n",
    "        for i, result in enumerate(results.get(\"all_results\", [])[:3]):\n",
    "            config = result.get(\"config\", {})\n",
    "            print(f\"\\n{i+1}. Configuration:\")\n",
    "            print(f\"   - Prompt: {config.get('prompt_template', 'unknown')}\")\n",
    "            print(f\"   - Self-reflection: {config.get('use_self_reflection', False)}\")\n",
    "            print(f\"   - Negation checking: {config.get('use_negation_checking', False)}\")\n",
    "            print(f\"   - Max iterations: {config.get('max_iterations', 'unknown')}\")\n",
    "            print(f\"   Results:\")\n",
    "            print(f\"   - Entities extracted: {result.get('entity_count', 0)}\")\n",
    "            print(f\"   - Precision: {result.get('precision', 0):.4f}\")\n",
    "            print(f\"   - Recall: {result.get('recall', 0):.4f}\")\n",
    "            print(f\"   - F1 Score: {result.get('f1_score', 0):.4f}\")\n",
    "        \n",
    "        # Print best configuration\n",
    "        best_config = results.get(\"best_config\", {})\n",
    "        print(\"\\nBEST CONFIGURATION:\")\n",
    "        print(f\"Prompt template: {best_config.get('prompt_template', 'unknown')}\")\n",
    "        print(f\"Self-reflection: {best_config.get('use_self_reflection', False)}\")\n",
    "        print(f\"Negation checking: {best_config.get('use_negation_checking', False)}\")\n",
    "        print(f\"Max iterations: {best_config.get('max_iterations', 'unknown')}\")\n",
    "        print(f\"F1 Score: {results.get('best_score', 0):.4f}\")\n",
    "        \n",
    "        # Print feature importance analysis\n",
    "        print(\"\\nFeature Importance Analysis:\")\n",
    "        \n",
    "        # Analyze prompt impact\n",
    "        original_results = [r for r in results.get(\"all_results\", []) if r.get(\"config\", {}).get(\"prompt_template\") == \"original\"]\n",
    "        enhanced_results = [r for r in results.get(\"all_results\", []) if r.get(\"config\", {}).get(\"prompt_template\") == \"enhanced\"]\n",
    "        \n",
    "        if original_results and enhanced_results:\n",
    "            avg_f1_original = sum(r.get(\"f1_score\", 0) for r in original_results) / len(original_results)\n",
    "            avg_f1_enhanced = sum(r.get(\"f1_score\", 0) for r in enhanced_results) / len(enhanced_results)\n",
    "            \n",
    "            print(f\"Prompt Impact:\")\n",
    "            print(f\"  - Original prompt avg F1: {avg_f1_original:.4f}\")\n",
    "            print(f\"  - Enhanced prompt avg F1: {avg_f1_enhanced:.4f}\")\n",
    "            print(f\"  - Difference: {(avg_f1_enhanced - avg_f1_original):.4f}\")\n",
    "        \n",
    "        # Analyze self-reflection impact\n",
    "        with_reflection = [r for r in results.get(\"all_results\", []) if r.get(\"config\", {}).get(\"use_self_reflection\")]\n",
    "        without_reflection = [r for r in results.get(\"all_results\", []) if not r.get(\"config\", {}).get(\"use_self_reflection\")]\n",
    "        \n",
    "        if with_reflection and without_reflection:\n",
    "            avg_f1_with = sum(r.get(\"f1_score\", 0) for r in with_reflection) / len(with_reflection)\n",
    "            avg_f1_without = sum(r.get(\"f1_score\", 0) for r in without_reflection) / len(without_reflection)\n",
    "            \n",
    "            print(f\"Self-Reflection Impact:\")\n",
    "            print(f\"  - With self-reflection avg F1: {avg_f1_with:.4f}\")\n",
    "            print(f\"  - Without self-reflection avg F1: {avg_f1_without:.4f}\")\n",
    "            print(f\"  - Difference: {(avg_f1_with - avg_f1_without):.4f}\")\n",
    "        \n",
    "        # Analyze negation checking impact\n",
    "        with_negation = [r for r in results.get(\"all_results\", []) if r.get(\"config\", {}).get(\"use_negation_checking\")]\n",
    "        without_negation = [r for r in results.get(\"all_results\", []) if not r.get(\"config\", {}).get(\"use_negation_checking\")]\n",
    "        \n",
    "        if with_negation and without_negation:\n",
    "            avg_f1_with = sum(r.get(\"f1_score\", 0) for r in with_negation) / len(with_negation)\n",
    "            avg_f1_without = sum(r.get(\"f1_score\", 0) for r in without_negation) / len(without_negation)\n",
    "            \n",
    "            print(f\"Negation Checking Impact:\")\n",
    "            print(f\"  - With negation checking avg F1: {avg_f1_with:.4f}\")\n",
    "            print(f\"  - Without negation checking avg F1: {avg_f1_without:.4f}\")\n",
    "            print(f\"  - Difference: {(avg_f1_with - avg_f1_without):.4f}\")\n",
    "\n",
    "# Example usage\n",
    "\"\"\"\n",
    "# Define your ground truth\n",
    "ground_truth = [\"fever\", \"vomiting\", \"anuria\", ...]\n",
    "\n",
    "# Run optimization\n",
    "optimizer = EntityExtractionOptimizer(\n",
    "    llm_client=llm_client,\n",
    "    context_extractor=context_extractor,\n",
    "    ground_truth=ground_truth,\n",
    "    clinical_text=text,\n",
    "    evaluation_func=calculate_soft_f1,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "results = optimizer.grid_search()\n",
    "\n",
    "# Print results\n",
    "optimizer.print_optimization_results(results)\n",
    "\"\"\"\n",
    "\n",
    "def run_optimization(text, llm_client, context_extractor, similarity_threshold=50.0):\n",
    "    \"\"\"\n",
    "    Run the entity extraction optimization process.\n",
    "    \n",
    "    Args:\n",
    "        text: Clinical text to process\n",
    "        llm_client: Client for LLM queries\n",
    "        context_extractor: Context extraction utility\n",
    "        similarity_threshold: Threshold for fuzzy matching in evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Optimization results\n",
    "    \"\"\"\n",
    "    print(\"Starting entity extraction optimization...\")\n",
    "    \n",
    "    # Define evaluation function to use for comparing results\n",
    "    def evaluate_extraction(ground_truth, extracted):\n",
    "        from fuzzywuzzy import fuzz\n",
    "        import numpy as np\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = np.zeros((len(extracted), len(ground_truth)))\n",
    "        for i, pred in enumerate(extracted):\n",
    "            for j, truth in enumerate(ground_truth):\n",
    "                similarity = fuzz.ratio(pred.lower(), truth.lower())\n",
    "                if similarity >= similarity_threshold:\n",
    "                    similarity_matrix[i, j] = similarity\n",
    "        \n",
    "        # Find best matches\n",
    "        matches = []\n",
    "        sim_matrix = similarity_matrix.copy()\n",
    "        while np.max(sim_matrix) > 0:\n",
    "            max_val = np.max(sim_matrix)\n",
    "            max_pos = np.unravel_index(np.argmax(sim_matrix), sim_matrix.shape)\n",
    "            pred_idx, truth_idx = max_pos\n",
    "            matches.append((pred_idx, truth_idx, max_val))\n",
    "            sim_matrix[pred_idx, :] = 0\n",
    "            sim_matrix[:, truth_idx] = 0\n",
    "        \n",
    "        # Calculate metrics\n",
    "        weighted_tp = sum(similarity / 100.0 for _, _, similarity in matches)\n",
    "        precision = weighted_tp / len(extracted) if extracted else 0.0\n",
    "        recall = weighted_tp / len(ground_truth) if ground_truth else 0.0\n",
    "        f1 = 0.0\n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        # Count false positives and false negatives\n",
    "        matched_pred_indices = set(pred_idx for pred_idx, _, _ in matches)\n",
    "        false_positives = len(extracted) - len(matched_pred_indices)\n",
    "        matched_truth_indices = set(truth_idx for _, truth_idx, _ in matches)\n",
    "        false_negatives = len(ground_truth) - len(matched_truth_indices)\n",
    "        \n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"matches\": [(extracted[p], ground_truth[t], s) for p, t, s in matches]\n",
    "        }\n",
    "    \n",
    "    # Initialize the optimizer\n",
    "    optimizer = EntityExtractionOptimizer(\n",
    "        llm_client=llm_client,\n",
    "        context_extractor=context_extractor,\n",
    "        ground_truth=ground_truth,\n",
    "        clinical_text=text,\n",
    "        evaluation_func=evaluate_extraction,\n",
    "        debug=True\n",
    "    )\n",
    "    \n",
    "    # Perform grid search\n",
    "    start_time = time.time()\n",
    "    results = optimizer.grid_search()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Print results\n",
    "    optimizer.print_optimization_results(results)\n",
    "    \n",
    "    # Save results to CSV for further analysis\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            \"config_id\": i+1,\n",
    "            \"prompt\": r.get(\"config\", {}).get(\"prompt_template\", \"unknown\"),\n",
    "            \"self_reflection\": r.get(\"config\", {}).get(\"use_self_reflection\", False),\n",
    "            \"negation_checking\": r.get(\"config\", {}).get(\"use_negation_checking\", False),\n",
    "            \"max_iterations\": r.get(\"config\", {}).get(\"max_iterations\", 0),\n",
    "            \"entity_count\": r.get(\"entity_count\", 0),\n",
    "            \"precision\": r.get(\"precision\", 0),\n",
    "            \"recall\": r.get(\"recall\", 0),\n",
    "            \"f1_score\": r.get(\"f1_score\", 0),\n",
    "            \"false_positives\": r.get(\"false_positives\", 0),\n",
    "            \"false_negatives\": r.get(\"false_negatives\", 0)\n",
    "        }\n",
    "        for i, r in enumerate(results.get(\"all_results\", []))\n",
    "    ])\n",
    "    \n",
    "    results_df.to_csv(\"entity_extraction_optimization_results.csv\", index=False)\n",
    "    \n",
    "    # Also save the raw results as JSON for reference\n",
    "    with open(\"entity_extraction_optimization_raw_results.json\", \"w\") as f:\n",
    "        # Convert numpy values to native Python types for JSON serialization\n",
    "        serializable_results = {\n",
    "            \"best_config\": results[\"best_config\"],\n",
    "            \"best_score\": float(results[\"best_score\"]),\n",
    "            \"all_results\": []\n",
    "        }\n",
    "        \n",
    "        for result in results[\"all_results\"]:\n",
    "            serializable_result = {}\n",
    "            for k, v in result.items():\n",
    "                if k == \"entities\":\n",
    "                    serializable_result[k] = v\n",
    "                elif isinstance(v, (int, str, bool, dict, list)):\n",
    "                    serializable_result[k] = v\n",
    "                else:\n",
    "                    serializable_result[k] = float(v)\n",
    "            serializable_results[\"all_results\"].append(serializable_result)\n",
    "            \n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Optimization completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Results saved to entity_extraction_optimization_results.csv and entity_extraction_optimization_raw_results.json\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example of how to run the optimization\n",
    "\\\n",
    "# Run the optimization\"\n",
    "context_extractor = ContextExtractor(debug=False)\n",
    "results = run_optimization(\n",
    "    text=text,\n",
    "    llm_client=llm_client,\n",
    "    context_extractor=context_extractor,\n",
    "    similarity_threshold=50.0\n",
    ")\n",
    "\n",
    "# Get the best configuration\n",
    "best_config = results.get(\"best_config\", {})\n",
    "print(f\"Best configuration: {best_config}\")\n",
    "\n",
    "# Extract entities using the best configuration\n",
    "optimizer = EntityExtractionOptimizer(\n",
    "    llm_client=llm_client,\n",
    "    context_extractor=context_extractor,\n",
    "    ground_truth=ground_truth,\n",
    "    clinical_text=text,\n",
    "    evaluation_func=None,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "optimized_entities = optimizer.extract_entities_with_config(best_config)\n",
    "print(f\"Extracted {len(optimized_entities)} entities using optimized configuration\")\n",
    "print(optimized_entities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hporag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
